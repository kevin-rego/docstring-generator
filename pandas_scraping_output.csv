File,Function,Docstring
asv_bench/benchmarks/gil.py,"def test_parallel(num_threads=2, kwargs_list=None):
    assert num_threads > 0
    has_kwargs_list = kwargs_list is not None
    if has_kwargs_list:
        assert len(kwargs_list) == num_threads

    def wrapper(func):

        @wraps(func)
        def inner(*args, **kwargs):
            if has_kwargs_list:
                update_kwargs = lambda i: dict(kwargs, **kwargs_list[i])
            else:
                update_kwargs = lambda i: kwargs
            threads = []
            for i in range(num_threads):
                updated_kwargs = update_kwargs(i)
                thread = threading.Thread(target=func, args=args, kwargs=updated_kwargs)
                threads.append(thread)
            for thread in threads:
                thread.start()
            for thread in threads:
                thread.join()
        return inner
    return wrapper","""""""Decorator to run the same function multiple times in parallel.

Parameters
----------
num_threads : int, optional
    The number of times the function is run in parallel.
kwargs_list : list of dicts, optional
    The list of kwargs to update original
    function kwargs on different threads.

Notes
-----
This decorator does not pass the return value of the decorated function.

Original from scikit-image:

https://github.com/scikit-image/scikit-image/pull/1519"""""""
doc/source/conf.py,"def linkcode_resolve(domain, info):
    if domain != 'py':
        return None
    modname = info['module']
    fullname = info['fullname']
    submod = sys.modules.get(modname)
    if submod is None:
        return None
    obj = submod
    for part in fullname.split('.'):
        try:
            with warnings.catch_warnings():
                warnings.simplefilter('ignore', FutureWarning)
                obj = getattr(obj, part)
        except AttributeError:
            return None
    try:
        fn = inspect.getsourcefile(inspect.unwrap(obj))
    except TypeError:
        try:
            fn = inspect.getsourcefile(inspect.unwrap(obj.fget))
        except (AttributeError, TypeError):
            fn = None
    if not fn:
        return None
    try:
        (source, lineno) = inspect.getsourcelines(obj)
    except TypeError:
        try:
            (source, lineno) = inspect.getsourcelines(obj.fget)
        except (AttributeError, TypeError):
            lineno = None
    except OSError:
        lineno = None
    if lineno:
        linespec = f'#L{lineno}-L{lineno + len(source) - 1}'
    else:
        linespec = ''
    fn = os.path.relpath(fn, start=os.path.dirname(pandas.__file__))
    if '+' in pandas.__version__:
        return f'https://github.com/pandas-dev/pandas/blob/main/pandas/{fn}{linespec}'
    else:
        return f'https://github.com/pandas-dev/pandas/blob/v{pandas.__version__}/pandas/{fn}{linespec}'","""""""Determine the URL corresponding to Python object"""""""
doc/source/conf.py,"def process_class_docstrings(app, what, name, obj, options, lines):
    if what == 'class':
        joined = '\n'.join(lines)
        templates = ['.. rubric:: Attributes\n\n.. autosummary::\n   :toctree:\n\n   None\n', '.. rubric:: Methods\n\n.. autosummary::\n   :toctree:\n\n   None\n']
        for template in templates:
            if template in joined:
                joined = joined.replace(template, '')
        lines[:] = joined.split('\n')","""""""For those classes for which we use ::

:template: autosummary/class_without_autosummary.rst

the documented attributes/methods have to be listed in the class
docstring. However, if one of those lists is empty, we use 'None',
which then generates warnings in sphinx / ugly html output.
This ""autodoc-process-docstring"" event connector removes that part
from the processed docstring."""""""
doc/source/conf.py,"def process_business_alias_docstrings(app, what, name, obj, options, lines):
    if name in _BUSINED_ALIASES:
        lines[:] = []","""""""Starting with sphinx 3.4, the ""autodoc-process-docstring"" event also
gets called for alias classes. This results in numpydoc adding the
methods/attributes to the docstring, which we don't want (+ this
causes warnings with sphinx)."""""""
doc/source/conf.py,"def rstjinja(app, docname, source):
    if app.builder.format != 'html':
        return
    src = source[0]
    rendered = app.builder.templates.render_string(src, app.config.html_context)
    source[0] = rendered","""""""Render our pages as a jinja template for fancy templating goodness."""""""
pandas/_config/config.py,"def register_option(key: str, defval: object, doc: str='', validator: Callable[[object], Any] | None=None, cb: Callable[[str], Any] | None=None) -> None:
    import keyword
    import tokenize
    key = key.lower()
    if key in _registered_options:
        raise OptionError(f""Option '{key}' has already been registered"")
    if key in _reserved_keys:
        raise OptionError(f""Option '{key}' is a reserved key"")
    if validator:
        validator(defval)
    path = key.split('.')
    for k in path:
        if not re.match('^' + tokenize.Name + '$', k):
            raise ValueError(f'{k} is not a valid identifier')
        if keyword.iskeyword(k):
            raise ValueError(f'{k} is a python keyword')
    cursor = _global_config
    msg = ""Path prefix to option '{option}' is already an option""
    for (i, p) in enumerate(path[:-1]):
        if not isinstance(cursor, dict):
            raise OptionError(msg.format(option='.'.join(path[:i])))
        if p not in cursor:
            cursor[p] = {}
        cursor = cursor[p]
    if not isinstance(cursor, dict):
        raise OptionError(msg.format(option='.'.join(path[:-1])))
    cursor[path[-1]] = defval
    _registered_options[key] = RegisteredOption(key=key, defval=defval, doc=doc, validator=validator, cb=cb)","""""""Register an option in the package-wide pandas config object

Parameters
----------
key : str
    Fully-qualified key, e.g. ""x.y.option - z"".
defval : object
    Default value of the option.
doc : str
    Description of the option.
validator : Callable, optional
    Function of a single argument, should raise `ValueError` if
    called with a value which is not a legal value for the option.
cb
    a function of a single argument ""key"", which is called
    immediately after an option value is set/reset. key is
    the full name of the option.

Raises
------
ValueError if `validator` is specified and `defval` is not a valid value."""""""
pandas/_config/config.py,"def deprecate_option(key: str, msg: str | None=None, rkey: str | None=None, removal_ver: str | None=None) -> None:
    key = key.lower()
    if key in _deprecated_options:
        raise OptionError(f""Option '{key}' has already been defined as deprecated."")
    _deprecated_options[key] = DeprecatedOption(key, msg, rkey, removal_ver)","""""""Mark option `key` as deprecated, if code attempts to access this option,
a warning will be produced, using `msg` if given, or a default message
if not.
if `rkey` is given, any access to the key will be re-routed to `rkey`.

Neither the existence of `key` nor that if `rkey` is checked. If they
do not exist, any subsequence access will fail as usual, after the
deprecation warning is given.

Parameters
----------
key : str
    Name of the option to be deprecated.
    must be a fully-qualified option name (e.g ""x.y.z.rkey"").
msg : str, optional
    Warning message to output when the key is referenced.
    if no message is given a default message will be emitted.
rkey : str, optional
    Name of an option to reroute access to.
    If specified, any referenced `key` will be
    re-routed to `rkey` including set/get/reset.
    rkey must be a fully-qualified option name (e.g ""x.y.z.rkey"").
    used by the default message if no `msg` is specified.
removal_ver : str, optional
    Specifies the version in which this option will
    be removed. used by the default message if no `msg` is specified.

Raises
------
OptionError
    If the specified key has already been deprecated."""""""
pandas/_config/config.py,"def _select_options(pat: str) -> list[str]:
    if pat in _registered_options:
        return [pat]
    keys = sorted(_registered_options.keys())
    if pat == 'all':
        return keys
    return [k for k in keys if re.search(pat, k, re.I)]","""""""returns a list of keys matching `pat`

if pat==""all"", returns all registered options"""""""
pandas/_config/config.py,"def _is_deprecated(key: str) -> bool:
    key = key.lower()
    return key in _deprecated_options","""""""Returns True if the given option has been deprecated"""""""
pandas/_config/config.py,"def _get_deprecated_option(key: str):
    try:
        d = _deprecated_options[key]
    except KeyError:
        return None
    else:
        return d","""""""Retrieves the metadata for a deprecated option, if `key` is deprecated.

Returns
-------
DeprecatedOption (namedtuple) if key is deprecated, None otherwise"""""""
pandas/_config/config.py,"def _get_registered_option(key: str):
    return _registered_options.get(key)","""""""Retrieves the option metadata if `key` is a registered option.

Returns
-------
RegisteredOption (namedtuple) if key is deprecated, None otherwise"""""""
pandas/_config/config.py,"def _translate_key(key: str) -> str:
    d = _get_deprecated_option(key)
    if d:
        return d.rkey or key
    else:
        return key","""""""if key id deprecated and a replacement key defined, will return the
replacement key, otherwise returns `key` as - is"""""""
pandas/_config/config.py,"def _warn_if_deprecated(key: str) -> bool:
    d = _get_deprecated_option(key)
    if d:
        if d.msg:
            warnings.warn(d.msg, FutureWarning, stacklevel=find_stack_level())
        else:
            msg = f""'{key}' is deprecated""
            if d.removal_ver:
                msg += f' and will be removed in {d.removal_ver}'
            if d.rkey:
                msg += f"", please use '{d.rkey}' instead.""
            else:
                msg += ', please refrain from using it.'
            warnings.warn(msg, FutureWarning, stacklevel=find_stack_level())
        return True
    return False","""""""Checks if `key` is a deprecated option and if so, prints a warning.

Returns
-------
bool - True if `key` is deprecated, False otherwise."""""""
pandas/_config/config.py,"def _build_option_description(k: str) -> str:
    o = _get_registered_option(k)
    d = _get_deprecated_option(k)
    s = f'{k} '
    if o.doc:
        s += '\n'.join(o.doc.strip().split('\n'))
    else:
        s += 'No description available.'
    if o:
        s += f'\n    [default: {o.defval}] [currently: {_get_option(k, True)}]'
    if d:
        rkey = d.rkey or ''
        s += '\n    (Deprecated'
        s += f', use `{rkey}` instead.'
        s += ')'
    return s","""""""Builds a formatted description of a registered option and prints it"""""""
pandas/_config/config.py,"def pp_options_list(keys: Iterable[str], width: int=80, _print: bool=False):
    from itertools import groupby
    from textwrap import wrap

    def pp(name: str, ks: Iterable[str]) -> list[str]:
        pfx = '- ' + name + '.[' if name else ''
        ls = wrap(', '.join(ks), width, initial_indent=pfx, subsequent_indent='  ', break_long_words=False)
        if ls and ls[-1] and name:
            ls[-1] = ls[-1] + ']'
        return ls
    ls: list[str] = []
    singles = [x for x in sorted(keys) if x.find('.') < 0]
    if singles:
        ls += pp('', singles)
    keys = [x for x in keys if x.find('.') >= 0]
    for (k, g) in groupby(sorted(keys), lambda x: x[:x.rfind('.')]):
        ks = [x[len(k) + 1:] for x in list(g)]
        ls += pp(k, ks)
    s = '\n'.join(ls)
    if _print:
        print(s)
    else:
        return s","""""""Builds a concise listing of available options, grouped by prefix"""""""
pandas/_config/config.py,"@contextmanager
def config_prefix(prefix: str) -> Generator[None, None, None]:
    global register_option, get_option, set_option

    def wrap(func: F) -> F:

        def inner(key: str, *args, **kwds):
            pkey = f'{prefix}.{key}'
            return func(pkey, *args, **kwds)
        return cast(F, inner)
    _register_option = register_option
    _get_option = get_option
    _set_option = set_option
    set_option = wrap(set_option)
    get_option = wrap(get_option)
    register_option = wrap(register_option)
    try:
        yield
    finally:
        set_option = _set_option
        get_option = _get_option
        register_option = _register_option","""""""contextmanager for multiple invocations of API with a common prefix

supported API functions: (register / get / set )__option

Warning: This is not thread - safe, and won't work properly if you import
the API functions into your module using the ""from x import y"" construct.

Example
-------
import pandas._config.config as cf
with cf.config_prefix(""display.font""):
    cf.register_option(""color"", ""red"")
    cf.register_option(""size"", "" 5 pt"")
    cf.set_option(size, "" 6 pt"")
    cf.get_option(size)
    ...

    etc'

will register options ""display.font.color"", ""display.font.size"", set the
value of ""display.font.size""... and so on."""""""
pandas/_config/config.py,"def is_type_factory(_type: type[Any]) -> Callable[[Any], None]:

    def inner(x) -> None:
        if type(x) != _type:
            raise ValueError(f""Value must have type '{_type}'"")
    return inner","""""""Parameters
----------
`_type` - a type to be compared against (e.g. type(x) == `_type`)

Returns
-------
validator - a function of a single argument x , which raises
            ValueError if type(x) is not equal to `_type`"""""""
pandas/_config/config.py,"def is_instance_factory(_type) -> Callable[[Any], None]:
    if isinstance(_type, (tuple, list)):
        _type = tuple(_type)
        type_repr = '|'.join(map(str, _type))
    else:
        type_repr = f""'{_type}'""

    def inner(x) -> None:
        if not isinstance(x, _type):
            raise ValueError(f'Value must be an instance of {type_repr}')
    return inner","""""""Parameters
----------
`_type` - the type to be checked against

Returns
-------
validator - a function of a single argument x , which raises
            ValueError if x is not an instance of `_type`"""""""
pandas/_config/config.py,"def is_nonnegative_int(value: object) -> None:
    if value is None:
        return
    elif isinstance(value, int):
        if value >= 0:
            return
    msg = 'Value must be a nonnegative integer or None'
    raise ValueError(msg)","""""""Verify that value is None or a positive int.

Parameters
----------
value : None or int
        The `value` to be checked.

Raises
------
ValueError
    When the value is not None or is a negative integer"""""""
pandas/_config/config.py,"def is_callable(obj) -> bool:
    if not callable(obj):
        raise ValueError('Value must be a callable')
    return True","""""""Parameters
----------
`obj` - the object to be checked

Returns
-------
validator - returns True if object is callable
    raises ValueError otherwise."""""""
pandas/_config/display.py,"def detect_console_encoding() -> str:
    global _initial_defencoding
    encoding = None
    try:
        encoding = sys.stdout.encoding or sys.stdin.encoding
    except (AttributeError, OSError):
        pass
    if not encoding or 'ascii' in encoding.lower():
        try:
            encoding = locale.getpreferredencoding()
        except locale.Error:
            pass
    if not encoding or 'ascii' in encoding.lower():
        encoding = sys.getdefaultencoding()
    if not _initial_defencoding:
        _initial_defencoding = sys.getdefaultencoding()
    return encoding","""""""Try to find the most capable encoding supported by the console.
slightly modified from the way IPython handles the same issue."""""""
pandas/_config/localization.py,"@contextmanager
def set_locale(new_locale: str | tuple[str, str], lc_var: int=locale.LC_ALL) -> Generator[str | tuple[str, str], None, None]:
    current_locale = locale.setlocale(lc_var)
    try:
        locale.setlocale(lc_var, new_locale)
        (normalized_code, normalized_encoding) = locale.getlocale()
        if normalized_code is not None and normalized_encoding is not None:
            yield f'{normalized_code}.{normalized_encoding}'
        else:
            yield new_locale
    finally:
        locale.setlocale(lc_var, current_locale)","""""""Context manager for temporarily setting a locale.

Parameters
----------
new_locale : str or tuple
    A string of the form <language_country>.<encoding>. For example to set
    the current locale to US English with a UTF8 encoding, you would pass
    ""en_US.UTF-8"".
lc_var : int, default `locale.LC_ALL`
    The category of the locale being set.

Notes
-----
This is useful when you want to run a particular block of code under a
particular locale, without globally setting the locale. This probably isn't
thread-safe."""""""
pandas/_config/localization.py,"def can_set_locale(lc: str, lc_var: int=locale.LC_ALL) -> bool:
    try:
        with set_locale(lc, lc_var=lc_var):
            pass
    except (ValueError, locale.Error):
        return False
    else:
        return True","""""""Check to see if we can set a locale, and subsequently get the locale,
without raising an Exception.

Parameters
----------
lc : str
    The locale to attempt to set.
lc_var : int, default `locale.LC_ALL`
    The category of the locale being set.

Returns
-------
bool
    Whether the passed locale can be set"""""""
pandas/_config/localization.py,"def _valid_locales(locales: list[str] | str, normalize: bool) -> list[str]:
    return [loc for loc in (locale.normalize(loc.strip()) if normalize else loc.strip() for loc in locales) if can_set_locale(loc)]","""""""Return a list of normalized locales that do not throw an ``Exception``
when set.

Parameters
----------
locales : str
    A string where each locale is separated by a newline.
normalize : bool
    Whether to call ``locale.normalize`` on each locale.

Returns
-------
valid_locales : list
    A list of valid locales."""""""
pandas/_config/localization.py,"def get_locales(prefix: str | None=None, normalize: bool=True) -> list[str]:
    if platform.system() in ('Linux', 'Darwin'):
        raw_locales = subprocess.check_output(['locale', '-a'])
    else:
        return []
    try:
        split_raw_locales = raw_locales.split(b'\n')
        out_locales = []
        for x in split_raw_locales:
            try:
                out_locales.append(str(x, encoding=options.display.encoding))
            except UnicodeError:
                out_locales.append(str(x, encoding='windows-1252'))
    except TypeError:
        pass
    if prefix is None:
        return _valid_locales(out_locales, normalize)
    pattern = re.compile(f'{prefix}.*')
    found = pattern.findall('\n'.join(out_locales))
    return _valid_locales(found, normalize)","""""""Get all the locales that are available on the system.

Parameters
----------
prefix : str
    If not ``None`` then return only those locales with the prefix
    provided. For example to get all English language locales (those that
    start with ``""en""``), pass ``prefix=""en""``.
normalize : bool
    Call ``locale.normalize`` on the resulting list of available locales.
    If ``True``, only locales that can be set without throwing an
    ``Exception`` are returned.

Returns
-------
locales : list of strings
    A list of locale strings that can be set with ``locale.setlocale()``.
    For example::

        locale.setlocale(locale.LC_ALL, locale_string)

On error will return an empty list (no locale available, e.g. Windows)"""""""
pandas/_testing/__init__.py,"def reset_display_options() -> None:
    pd.reset_option('^display.', silent=True)","""""""Reset the display options for printing and representing objects."""""""
pandas/_testing/__init__.py,"def equalContents(arr1, arr2) -> bool:
    return frozenset(arr1) == frozenset(arr2)","""""""Checks if the set of unique elements of arr1 and arr2 are equivalent."""""""
pandas/_testing/__init__.py,"def box_expected(expected, box_cls, transpose: bool=True):
    if box_cls is pd.array:
        if isinstance(expected, RangeIndex):
            expected = NumpyExtensionArray(np.asarray(expected._values))
        else:
            expected = pd.array(expected, copy=False)
    elif box_cls is Index:
        expected = Index(expected)
    elif box_cls is Series:
        expected = Series(expected)
    elif box_cls is DataFrame:
        expected = Series(expected).to_frame()
        if transpose:
            expected = expected.T
            expected = pd.concat([expected] * 2, ignore_index=True)
    elif box_cls is np.ndarray or box_cls is np.array:
        expected = np.array(expected)
    elif box_cls is to_array:
        expected = to_array(expected)
    else:
        raise NotImplementedError(box_cls)
    return expected","""""""Helper function to wrap the expected output of a test in a given box_class.

Parameters
----------
expected : np.ndarray, Index, Series
box_cls : {Index, Series, DataFrame}

Returns
-------
subclass of box_cls"""""""
pandas/_testing/__init__.py,"def to_array(obj):
    dtype = getattr(obj, 'dtype', None)
    if dtype is None:
        return np.asarray(obj)
    return extract_array(obj, extract_numpy=True)","""""""Similar to pd.array, but does not cast numpy dtypes to nullable dtypes."""""""
pandas/_testing/__init__.py,"def rands_array(nchars, size: int, dtype: NpDtype='O', replace: bool=True) -> np.ndarray:
    chars = np.array(list(string.ascii_letters + string.digits), dtype=(np.str_, 1))
    retval = np.random.default_rng(2).choice(chars, size=nchars * np.prod(size), replace=replace).view((np.str_, nchars)).reshape(size)
    return retval.astype(dtype)","""""""Generate an array of byte strings."""""""
pandas/_testing/__init__.py,"def makeCategoricalIndex(k: int=10, n: int=3, name=None, **kwargs) -> CategoricalIndex:
    x = rands_array(nchars=4, size=n, replace=False)
    return CategoricalIndex(Categorical.from_codes(np.arange(k) % n, categories=x), name=name, **kwargs)","""""""make a length k index or n categories"""""""
pandas/_testing/__init__.py,"def makeIntervalIndex(k: int=10, name=None, **kwargs) -> IntervalIndex:
    x = np.linspace(0, 100, num=k + 1)
    return IntervalIndex.from_breaks(x, name=name, **kwargs)","""""""make a length k IntervalIndex"""""""
pandas/_testing/__init__.py,"def all_timeseries_index_generator(k: int=10) -> Iterable[Index]:
    make_index_funcs: list[Callable[..., Index]] = [makeDateIndex, makePeriodIndex, makeTimedeltaIndex]
    for make_index_func in make_index_funcs:
        yield make_index_func(k=k)","""""""Generator which can be iterated over to get instances of all the classes
which represent time-series.

Parameters
----------
k: length of each of the index instances"""""""
pandas/_testing/__init__.py,"def makeCustomIndex(nentries, nlevels, prefix: str='#', names: bool | str | list[str] | None=False, ndupe_l=None, idx_type=None) -> Index:
    if ndupe_l is None:
        ndupe_l = [1] * nlevels
    assert is_sequence(ndupe_l) and len(ndupe_l) <= nlevels
    assert names is None or names is False or names is True or (len(names) is nlevels)
    assert idx_type is None or (idx_type in ('i', 'f', 's', 'u', 'dt', 'p', 'td') and nlevels == 1)
    if names is True:
        names = [prefix + str(i) for i in range(nlevels)]
    if names is False:
        names = None
    if isinstance(names, str) and nlevels == 1:
        names = [names]
    idx_func_dict: dict[str, Callable[..., Index]] = {'i': makeIntIndex, 'f': makeFloatIndex, 's': makeStringIndex, 'dt': makeDateIndex, 'td': makeTimedeltaIndex, 'p': makePeriodIndex}
    idx_func = idx_func_dict.get(idx_type)
    if idx_func:
        idx = idx_func(nentries)
        if names:
            idx.name = names[0]
        return idx
    elif idx_type is not None:
        raise ValueError(f""{repr(idx_type)} is not a legal value for `idx_type`, use  'i'/'f'/'s'/'dt'/'p'/'td'."")
    if len(ndupe_l) < nlevels:
        ndupe_l.extend([1] * (nlevels - len(ndupe_l)))
    assert len(ndupe_l) == nlevels
    assert all((x > 0 for x in ndupe_l))
    list_of_lists = []
    for i in range(nlevels):

        def keyfunc(x):
            numeric_tuple = re.sub('[^\\d_]_?', '', x).split('_')
            return [int(num) for num in numeric_tuple]
        div_factor = nentries // ndupe_l[i] + 1
        cnt: Counter[str] = collections.Counter()
        for j in range(div_factor):
            label = f'{prefix}_l{i}_g{j}'
            cnt[label] = ndupe_l[i]
        result = sorted(cnt.elements(), key=keyfunc)[:nentries]
        list_of_lists.append(result)
    tuples = list(zip(*list_of_lists))
    if nentries == 1:
        name = None if names is None else names[0]
        index = Index(tuples[0], name=name)
    elif nlevels == 1:
        name = None if names is None else names[0]
        index = Index((x[0] for x in tuples), name=name)
    else:
        index = MultiIndex.from_tuples(tuples, names=names)
    return index","""""""Create an index/multindex with given dimensions, levels, names, etc'

nentries - number of entries in index
nlevels - number of levels (> 1 produces multindex)
prefix - a string prefix for labels
names - (Optional), bool or list of strings. if True will use default
   names, if false will use no names, if a list is given, the name of
   each level in the index will be taken from the list.
ndupe_l - (Optional), list of ints, the number of rows for which the
   label will repeated at the corresponding level, you can specify just
   the first few, the rest will use the default ndupe_l of 1.
   len(ndupe_l) <= nlevels.
idx_type - ""i""/""f""/""s""/""dt""/""p""/""td"".
   If idx_type is not None, `idx_nlevels` must be 1.
   ""i""/""f"" creates an integer/float index,
   ""s"" creates a string
   ""dt"" create a datetime index.
   ""td"" create a datetime index.

    if unspecified, string labels will be generated."""""""
pandas/_testing/__init__.py,"def makeCustomDataframe(nrows, ncols, c_idx_names: bool | list[str]=True, r_idx_names: bool | list[str]=True, c_idx_nlevels: int=1, r_idx_nlevels: int=1, data_gen_f=None, c_ndupe_l=None, r_ndupe_l=None, dtype=None, c_idx_type=None, r_idx_type=None) -> DataFrame:
    assert c_idx_nlevels > 0
    assert r_idx_nlevels > 0
    assert r_idx_type is None or (r_idx_type in ('i', 'f', 's', 'dt', 'p', 'td') and r_idx_nlevels == 1)
    assert c_idx_type is None or (c_idx_type in ('i', 'f', 's', 'dt', 'p', 'td') and c_idx_nlevels == 1)
    columns = makeCustomIndex(ncols, nlevels=c_idx_nlevels, prefix='C', names=c_idx_names, ndupe_l=c_ndupe_l, idx_type=c_idx_type)
    index = makeCustomIndex(nrows, nlevels=r_idx_nlevels, prefix='R', names=r_idx_names, ndupe_l=r_ndupe_l, idx_type=r_idx_type)
    if data_gen_f is None:
        data_gen_f = lambda r, c: f'R{r}C{c}'
    data = [[data_gen_f(r, c) for c in range(ncols)] for r in range(nrows)]
    return DataFrame(data, index, columns, dtype=dtype)","""""""Create a DataFrame using supplied parameters.

Parameters
----------
nrows,  ncols - number of data rows/cols
c_idx_names, r_idx_names  - False/True/list of strings,  yields No names ,
        default names or uses the provided names for the levels of the
        corresponding index. You can provide a single string when
        c_idx_nlevels ==1.
c_idx_nlevels - number of levels in columns index. > 1 will yield MultiIndex
r_idx_nlevels - number of levels in rows index. > 1 will yield MultiIndex
data_gen_f - a function f(row,col) which return the data value
        at that position, the default generator used yields values of the form
        ""RxCy"" based on position.
c_ndupe_l, r_ndupe_l - list of integers, determines the number
        of duplicates for each label at a given level of the corresponding
        index. The default `None` value produces a multiplicity of 1 across
        all levels, i.e. a unique index. Will accept a partial list of length
        N < idx_nlevels, for just the first N levels. If ndupe doesn't divide
        nrows/ncol, the last label might have lower multiplicity.
dtype - passed to the DataFrame constructor as is, in case you wish to
        have more control in conjunction with a custom `data_gen_f`
r_idx_type, c_idx_type -  ""i""/""f""/""s""/""dt""/""td"".
    If idx_type is not None, `idx_nlevels` must be 1.
    ""i""/""f"" creates an integer/float index,
    ""s"" creates a string index
    ""dt"" create a datetime index.
    ""td"" create a timedelta index.

        if unspecified, string labels will be generated.

Examples
--------
# 5 row, 3 columns, default names on both, single index on both axis
>> makeCustomDataframe(5,3)

# make the data a random int between 1 and 100
>> mkdf(5,3,data_gen_f=lambda r,c:randint(1,100))

# 2-level multiindex on rows with each label duplicated
# twice on first level, default names on both axis, single
# index on both axis
>> a=makeCustomDataframe(5,3,r_idx_nlevels=2,r_ndupe_l=[2])

# DatetimeIndex on row, index with unicode labels on columns
# no names on either axis
>> a=makeCustomDataframe(5,3,c_idx_names=False,r_idx_names=False,
                         r_idx_type=""dt"",c_idx_type=""u"")

# 4-level multindex on rows with names provided, 2-level multindex
# on columns with default labels and default names.
>> a=makeCustomDataframe(5,3,r_idx_nlevels=4,
                         r_idx_names=[""FEE"",""FIH"",""FOH"",""FUM""],
                         c_idx_nlevels=2)

>> a=mkdf(5,3,r_idx_nlevels=2,c_idx_nlevels=4)"""""""
pandas/_testing/__init__.py,"def _make_skipna_wrapper(alternative, skipna_alternative=None):
    if skipna_alternative:

        def skipna_wrapper(x):
            return skipna_alternative(x.values)
    else:

        def skipna_wrapper(x):
            nona = x.dropna()
            if len(nona) == 0:
                return np.nan
            return alternative(nona)
    return skipna_wrapper","""""""Create a function for calling on an array.

Parameters
----------
alternative : function
    The function to be called on the array with no NaNs.
    Only used when 'skipna_alternative' is None.
skipna_alternative : function
    The function to be called on the original array

Returns
-------
function"""""""
pandas/_testing/__init__.py,"def convert_rows_list_to_csv_str(rows_list: list[str]) -> str:
    sep = os.linesep
    return sep.join(rows_list) + sep","""""""Convert list of CSV rows to single CSV-formatted string for current OS.

This method is used for creating expected value of to_csv() method.

Parameters
----------
rows_list : List[str]
    Each element represents the row of csv.

Returns
-------
str
    Expected output of to_csv() in current OS."""""""
pandas/_testing/__init__.py,"def external_error_raised(expected_exception: type[Exception]) -> ContextManager:
    import pytest
    return pytest.raises(expected_exception, match=None)","""""""Helper function to mark pytest.raises that have an external error message.

Parameters
----------
expected_exception : Exception
    Expected error to raise.

Returns
-------
Callable
    Regular `pytest.raises` function with `match` equal to `None`."""""""
pandas/_testing/__init__.py,"def get_cython_table_params(ndframe, func_names_and_expected):
    results = []
    for (func_name, expected) in func_names_and_expected:
        results.append((ndframe, func_name, expected))
        results += [(ndframe, func, expected) for (func, name) in cython_table if name == func_name]
    return results","""""""Combine frame, functions from com._cython_table
keys and expected result.

Parameters
----------
ndframe : DataFrame or Series
func_names_and_expected : Sequence of two items
    The first item is a name of a NDFrame method ('sum', 'prod') etc.
    The second item is the expected return value.

Returns
-------
list
    List of three items (DataFrame, function, expected result)"""""""
pandas/_testing/__init__.py,"def get_op_from_name(op_name: str) -> Callable:
    short_opname = op_name.strip('_')
    try:
        op = getattr(operator, short_opname)
    except AttributeError:
        rop = getattr(operator, short_opname[1:])
        op = lambda x, y: rop(y, x)
    return op","""""""The operator function for a given op name.

Parameters
----------
op_name : str
    The op name, in form of ""add"" or ""__add__"".

Returns
-------
function
    A function performing the operation."""""""
pandas/_testing/__init__.py,"def shares_memory(left, right) -> bool:
    if isinstance(left, np.ndarray) and isinstance(right, np.ndarray):
        return np.shares_memory(left, right)
    elif isinstance(left, np.ndarray):
        return shares_memory(right, left)
    if isinstance(left, RangeIndex):
        return False
    if isinstance(left, MultiIndex):
        return shares_memory(left._codes, right)
    if isinstance(left, (Index, Series)):
        return shares_memory(left._values, right)
    if isinstance(left, NDArrayBackedExtensionArray):
        return shares_memory(left._ndarray, right)
    if isinstance(left, pd.core.arrays.SparseArray):
        return shares_memory(left.sp_values, right)
    if isinstance(left, pd.core.arrays.IntervalArray):
        return shares_memory(left._left, right) or shares_memory(left._right, right)
    if isinstance(left, ExtensionArray) and left.dtype == 'string[pyarrow]':
        left = cast('ArrowExtensionArray', left)
        if isinstance(right, ExtensionArray) and right.dtype == 'string[pyarrow]':
            right = cast('ArrowExtensionArray', right)
            left_pa_data = left._pa_array
            right_pa_data = right._pa_array
            left_buf1 = left_pa_data.chunk(0).buffers()[1]
            right_buf1 = right_pa_data.chunk(0).buffers()[1]
            return left_buf1 == right_buf1
    if isinstance(left, BaseMaskedArray) and isinstance(right, BaseMaskedArray):
        return np.shares_memory(left._data, right._data) or np.shares_memory(left._mask, right._mask)
    if isinstance(left, DataFrame) and len(left._mgr.arrays) == 1:
        arr = left._mgr.arrays[0]
        return shares_memory(arr, right)
    raise NotImplementedError(type(left), type(right))","""""""Pandas-compat for np.shares_memory."""""""
pandas/_testing/_io.py,"def round_trip_pickle(obj: Any, path: FilePath | ReadPickleBuffer | None=None) -> DataFrame | Series:
    _path = path
    if _path is None:
        _path = f'__{uuid.uuid4()}__.pickle'
    with ensure_clean(_path) as temp_path:
        pd.to_pickle(obj, temp_path)
        return pd.read_pickle(temp_path)","""""""Pickle an object and then read it again.

Parameters
----------
obj : any object
    The object to pickle and then re-read.
path : str, path object or file-like object, default None
    The path where the pickled object is written and then read.

Returns
-------
pandas object
    The original object that was pickled and then re-read."""""""
pandas/_testing/_io.py,"def round_trip_pathlib(writer, reader, path: str | None=None):
    Path = pathlib.Path
    if path is None:
        path = '___pathlib___'
    with ensure_clean(path) as path:
        writer(Path(path))
        obj = reader(Path(path))
    return obj","""""""Write an object to file specified by a pathlib.Path and read it back

Parameters
----------
writer : callable bound to pandas object
    IO writing function (e.g. DataFrame.to_csv )
reader : callable
    IO reading function (e.g. pd.read_csv )
path : str, default None
    The path where the object is written and then read.

Returns
-------
pandas object
    The original object that was serialized and then re-read."""""""
pandas/_testing/_io.py,"def round_trip_localpath(writer, reader, path: str | None=None):
    import pytest
    LocalPath = pytest.importorskip('py.path').local
    if path is None:
        path = '___localpath___'
    with ensure_clean(path) as path:
        writer(LocalPath(path))
        obj = reader(LocalPath(path))
    return obj","""""""Write an object to file specified by a py.path LocalPath and read it back.

Parameters
----------
writer : callable bound to pandas object
    IO writing function (e.g. DataFrame.to_csv )
reader : callable
    IO reading function (e.g. pd.read_csv )
path : str, default None
    The path where the object is written and then read.

Returns
-------
pandas object
    The original object that was serialized and then re-read."""""""
pandas/_testing/_io.py,"def write_to_compressed(compression, path, data, dest: str='test') -> None:
    args: tuple[Any, ...] = (data,)
    mode = 'wb'
    method = 'write'
    compress_method: Callable
    if compression == 'zip':
        compress_method = zipfile.ZipFile
        mode = 'w'
        args = (dest, data)
        method = 'writestr'
    elif compression == 'tar':
        compress_method = tarfile.TarFile
        mode = 'w'
        file = tarfile.TarInfo(name=dest)
        bytes = io.BytesIO(data)
        file.size = len(data)
        args = (file, bytes)
        method = 'addfile'
    elif compression == 'gzip':
        compress_method = gzip.GzipFile
    elif compression == 'bz2':
        compress_method = get_bz2_file()
    elif compression == 'zstd':
        compress_method = import_optional_dependency('zstandard').open
    elif compression == 'xz':
        compress_method = get_lzma_file()
    else:
        raise ValueError(f'Unrecognized compression type: {compression}')
    with compress_method(path, mode=mode) as f:
        getattr(f, method)(*args)","""""""Write data to a compressed file.

Parameters
----------
compression : {'gzip', 'bz2', 'zip', 'xz', 'zstd'}
    The compression type to use.
path : str
    The file path to write the data.
data : str
    The data to write.
dest : str, default ""test""
    The destination file (for ZIP only)

Raises
------
ValueError : An invalid compression value was passed in."""""""
pandas/_testing/_warnings.py,"@contextmanager
def assert_produces_warning(expected_warning: type[Warning] | bool | tuple[type[Warning], ...] | None=Warning, filter_level: Literal['error', 'ignore', 'always', 'default', 'module', 'once']='always', check_stacklevel: bool=True, raise_on_extra_warnings: bool=True, match: str | None=None) -> Generator[list[warnings.WarningMessage], None, None]:
    __tracebackhide__ = True
    with warnings.catch_warnings(record=True) as w:
        warnings.simplefilter(filter_level)
        try:
            yield w
        finally:
            if expected_warning:
                expected_warning = cast(type[Warning], expected_warning)
                _assert_caught_expected_warning(caught_warnings=w, expected_warning=expected_warning, match=match, check_stacklevel=check_stacklevel)
            if raise_on_extra_warnings:
                _assert_caught_no_extra_warnings(caught_warnings=w, expected_warning=expected_warning)","""""""Context manager for running code expected to either raise a specific warning,
multiple specific warnings, or not raise any warnings. Verifies that the code
raises the expected warning(s), and that it does not raise any other unexpected
warnings. It is basically a wrapper around ``warnings.catch_warnings``.

Parameters
----------
expected_warning : {Warning, False, tuple[Warning, ...], None}, default Warning
    The type of Exception raised. ``exception.Warning`` is the base
    class for all warnings. To raise multiple types of exceptions,
    pass them as a tuple. To check that no warning is returned,
    specify ``False`` or ``None``.
filter_level : str or None, default ""always""
    Specifies whether warnings are ignored, displayed, or turned
    into errors.
    Valid values are:

    * ""error"" - turns matching warnings into exceptions
    * ""ignore"" - discard the warning
    * ""always"" - always emit a warning
    * ""default"" - print the warning the first time it is generated
      from each location
    * ""module"" - print the warning the first time it is generated
      from each module
    * ""once"" - print the warning the first time it is generated

check_stacklevel : bool, default True
    If True, displays the line that called the function containing
    the warning to show were the function is called. Otherwise, the
    line that implements the function is displayed.
raise_on_extra_warnings : bool, default True
    Whether extra warnings not of the type `expected_warning` should
    cause the test to fail.
match : str, optional
    Match warning message.

Examples
--------
>>> import warnings
>>> with assert_produces_warning():
...     warnings.warn(UserWarning())
...
>>> with assert_produces_warning(False):
...     warnings.warn(RuntimeWarning())
...
Traceback (most recent call last):
    ...
AssertionError: Caused unexpected warning(s): ['RuntimeWarning'].
>>> with assert_produces_warning(UserWarning):
...     warnings.warn(RuntimeWarning())
Traceback (most recent call last):
    ...
AssertionError: Did not see expected warning of class 'UserWarning'.

..warn:: This is *not* thread-safe."""""""
pandas/_testing/_warnings.py,"def maybe_produces_warning(warning: type[Warning], condition: bool, **kwargs):
    if condition:
        return assert_produces_warning(warning, **kwargs)
    else:
        return nullcontext()","""""""Return a context manager that possibly checks a warning based on the condition"""""""
pandas/_testing/_warnings.py,"def _assert_caught_expected_warning(*, caught_warnings: Sequence[warnings.WarningMessage], expected_warning: type[Warning], match: str | None, check_stacklevel: bool) -> None:
    saw_warning = False
    matched_message = False
    unmatched_messages = []
    for actual_warning in caught_warnings:
        if issubclass(actual_warning.category, expected_warning):
            saw_warning = True
            if check_stacklevel:
                _assert_raised_with_correct_stacklevel(actual_warning)
            if match is not None:
                if re.search(match, str(actual_warning.message)):
                    matched_message = True
                else:
                    unmatched_messages.append(actual_warning.message)
    if not saw_warning:
        raise AssertionError(f'Did not see expected warning of class {repr(expected_warning.__name__)}')
    if match and (not matched_message):
        raise AssertionError(f""Did not see warning {repr(expected_warning.__name__)} matching '{match}'. The emitted warning messages are {unmatched_messages}"")","""""""Assert that there was the expected warning among the caught warnings."""""""
pandas/_testing/_warnings.py,"def _assert_caught_no_extra_warnings(*, caught_warnings: Sequence[warnings.WarningMessage], expected_warning: type[Warning] | bool | tuple[type[Warning], ...] | None) -> None:
    extra_warnings = []
    for actual_warning in caught_warnings:
        if _is_unexpected_warning(actual_warning, expected_warning):
            if actual_warning.category == ResourceWarning:
                if 'unclosed <ssl.SSLSocket' in str(actual_warning.message):
                    continue
                if any(('matplotlib' in mod for mod in sys.modules)):
                    continue
            extra_warnings.append((actual_warning.category.__name__, actual_warning.message, actual_warning.filename, actual_warning.lineno))
    if extra_warnings:
        raise AssertionError(f'Caused unexpected warning(s): {repr(extra_warnings)}')","""""""Assert that no extra warnings apart from the expected ones are caught."""""""
pandas/_testing/_warnings.py,"def _is_unexpected_warning(actual_warning: warnings.WarningMessage, expected_warning: type[Warning] | bool | tuple[type[Warning], ...] | None) -> bool:
    if actual_warning and (not expected_warning):
        return True
    expected_warning = cast(type[Warning], expected_warning)
    return bool(not issubclass(actual_warning.category, expected_warning))","""""""Check if the actual warning issued is unexpected."""""""
pandas/_testing/asserters.py,"def assert_almost_equal(left, right, check_dtype: bool | Literal['equiv']='equiv', rtol: float=1e-05, atol: float=1e-08, **kwargs) -> None:
    if isinstance(left, Index):
        assert_index_equal(left, right, check_exact=False, exact=check_dtype, rtol=rtol, atol=atol, **kwargs)
    elif isinstance(left, Series):
        assert_series_equal(left, right, check_exact=False, check_dtype=check_dtype, rtol=rtol, atol=atol, **kwargs)
    elif isinstance(left, DataFrame):
        assert_frame_equal(left, right, check_exact=False, check_dtype=check_dtype, rtol=rtol, atol=atol, **kwargs)
    else:
        if check_dtype:
            if is_number(left) and is_number(right):
                pass
            elif is_bool(left) and is_bool(right):
                pass
            else:
                if isinstance(left, np.ndarray) or isinstance(right, np.ndarray):
                    obj = 'numpy array'
                else:
                    obj = 'Input'
                assert_class_equal(left, right, obj=obj)
        _testing.assert_almost_equal(left, right, check_dtype=bool(check_dtype), rtol=rtol, atol=atol, **kwargs)","""""""Check that the left and right objects are approximately equal.

By approximately equal, we refer to objects that are numbers or that
contain numbers which may be equivalent to specific levels of precision.

Parameters
----------
left : object
right : object
check_dtype : bool or {'equiv'}, default 'equiv'
    Check dtype if both a and b are the same type. If 'equiv' is passed in,
    then `RangeIndex` and `Index` with int64 dtype are also considered
    equivalent when doing type checking.
rtol : float, default 1e-5
    Relative tolerance.
atol : float, default 1e-8
    Absolute tolerance."""""""
pandas/_testing/asserters.py,"def _check_isinstance(left, right, cls):
    cls_name = cls.__name__
    if not isinstance(left, cls):
        raise AssertionError(f'{cls_name} Expected type {cls}, found {type(left)} instead')
    if not isinstance(right, cls):
        raise AssertionError(f'{cls_name} Expected type {cls}, found {type(right)} instead')","""""""Helper method for our assert_* methods that ensures that
the two objects being compared have the right type before
proceeding with the comparison.

Parameters
----------
left : The first object being compared.
right : The second object being compared.
cls : The class type to check against.

Raises
------
AssertionError : Either `left` or `right` is not an instance of `cls`."""""""
pandas/_testing/asserters.py,"def assert_index_equal(left: Index, right: Index, exact: bool | str='equiv', check_names: bool=True, check_exact: bool=True, check_categorical: bool=True, check_order: bool=True, rtol: float=1e-05, atol: float=1e-08, obj: str='Index') -> None:
    __tracebackhide__ = True

    def _check_types(left, right, obj: str='Index') -> None:
        if not exact:
            return
        assert_class_equal(left, right, exact=exact, obj=obj)
        assert_attr_equal('inferred_type', left, right, obj=obj)
        if isinstance(left.dtype, CategoricalDtype) and isinstance(right.dtype, CategoricalDtype):
            if check_categorical:
                assert_attr_equal('dtype', left, right, obj=obj)
                assert_index_equal(left.categories, right.categories, exact=exact)
            return
        assert_attr_equal('dtype', left, right, obj=obj)

    def _get_ilevel_values(index, level):
        unique = index.levels[level]
        level_codes = index.codes[level]
        filled = take_nd(unique._values, level_codes, fill_value=unique._na_value)
        return unique._shallow_copy(filled, name=index.names[level])
    _check_isinstance(left, right, Index)
    _check_types(left, right, obj=obj)
    if left.nlevels != right.nlevels:
        msg1 = f'{obj} levels are different'
        msg2 = f'{left.nlevels}, {left}'
        msg3 = f'{right.nlevels}, {right}'
        raise_assert_detail(obj, msg1, msg2, msg3)
    if len(left) != len(right):
        msg1 = f'{obj} length are different'
        msg2 = f'{len(left)}, {left}'
        msg3 = f'{len(right)}, {right}'
        raise_assert_detail(obj, msg1, msg2, msg3)
    if not check_order:
        left = safe_sort_index(left)
        right = safe_sort_index(right)
    if isinstance(left, MultiIndex):
        right = cast(MultiIndex, right)
        for level in range(left.nlevels):
            llevel = _get_ilevel_values(left, level)
            rlevel = _get_ilevel_values(right, level)
            lobj = f'MultiIndex level [{level}]'
            assert_index_equal(llevel, rlevel, exact=exact, check_names=check_names, check_exact=check_exact, check_categorical=check_categorical, rtol=rtol, atol=atol, obj=lobj)
            _check_types(left.levels[level], right.levels[level], obj=obj)
    elif check_exact and check_categorical:
        if not left.equals(right):
            mismatch = left._values != right._values
            if not isinstance(mismatch, np.ndarray):
                mismatch = cast('ExtensionArray', mismatch).fillna(True)
            diff = np.sum(mismatch.astype(int)) * 100.0 / len(left)
            msg = f'{obj} values are different ({np.round(diff, 5)} %)'
            raise_assert_detail(obj, msg, left, right)
    else:
        exact_bool = bool(exact)
        _testing.assert_almost_equal(left.values, right.values, rtol=rtol, atol=atol, check_dtype=exact_bool, obj=obj, lobj=left, robj=right)
    if check_names:
        assert_attr_equal('names', left, right, obj=obj)
    if isinstance(left, PeriodIndex) or isinstance(right, PeriodIndex):
        assert_attr_equal('dtype', left, right, obj=obj)
    if isinstance(left, IntervalIndex) or isinstance(right, IntervalIndex):
        assert_interval_array_equal(left._values, right._values)
    if check_categorical:
        if isinstance(left.dtype, CategoricalDtype) or isinstance(right.dtype, CategoricalDtype):
            assert_categorical_equal(left._values, right._values, obj=f'{obj} category')","""""""Check that left and right Index are equal.

Parameters
----------
left : Index
right : Index
exact : bool or {'equiv'}, default 'equiv'
    Whether to check the Index class, dtype and inferred_type
    are identical. If 'equiv', then RangeIndex can be substituted for
    Index with an int64 dtype as well.
check_names : bool, default True
    Whether to check the names attribute.
check_exact : bool, default True
    Whether to compare number exactly.
check_categorical : bool, default True
    Whether to compare internal Categorical exactly.
check_order : bool, default True
    Whether to compare the order of index entries as well as their values.
    If True, both indexes must contain the same elements, in the same order.
    If False, both indexes must contain the same elements, but in any order.

    .. versionadded:: 1.2.0
rtol : float, default 1e-5
    Relative tolerance. Only used when check_exact is False.
atol : float, default 1e-8
    Absolute tolerance. Only used when check_exact is False.
obj : str, default 'Index'
    Specify object name being compared, internally used to show appropriate
    assertion message.

Examples
--------
>>> from pandas import testing as tm
>>> a = pd.Index([1, 2, 3])
>>> b = pd.Index([1, 2, 3])
>>> tm.assert_index_equal(a, b)"""""""
pandas/_testing/asserters.py,"def assert_class_equal(left, right, exact: bool | str=True, obj: str='Input') -> None:
    __tracebackhide__ = True

    def repr_class(x):
        if isinstance(x, Index):
            return x
        return type(x).__name__

    def is_class_equiv(idx: Index) -> bool:
        """"""Classes that are a RangeIndex (sub-)instance or exactly an `Index` .

        This only checks class equivalence. There is a separate check that the
        dtype is int64.
        """"""
        return type(idx) is Index or isinstance(idx, RangeIndex)
    if type(left) == type(right):
        return
    if exact == 'equiv':
        if is_class_equiv(left) and is_class_equiv(right):
            return
    msg = f'{obj} classes are different'
    raise_assert_detail(obj, msg, repr_class(left), repr_class(right))","""""""Checks classes are equal."""""""
pandas/_testing/asserters.py,"def assert_attr_equal(attr: str, left, right, obj: str='Attributes') -> None:
    __tracebackhide__ = True
    left_attr = getattr(left, attr)
    right_attr = getattr(right, attr)
    if left_attr is right_attr or is_matching_na(left_attr, right_attr):
        return None
    try:
        result = left_attr == right_attr
    except TypeError:
        result = False
    if (left_attr is pd.NA) ^ (right_attr is pd.NA):
        result = False
    elif not isinstance(result, bool):
        result = result.all()
    if not result:
        msg = f'Attribute ""{attr}"" are different'
        raise_assert_detail(obj, msg, left_attr, right_attr)
    return None","""""""Check attributes are equal. Both objects must have attribute.

Parameters
----------
attr : str
    Attribute name being compared.
left : object
right : object
obj : str, default 'Attributes'
    Specify object name being compared, internally used to show appropriate
    assertion message"""""""
pandas/_testing/asserters.py,"def assert_is_sorted(seq) -> None:
    if isinstance(seq, (Index, Series)):
        seq = seq.values
    assert_numpy_array_equal(seq, np.sort(np.array(seq)))","""""""Assert that the sequence is sorted."""""""
pandas/_testing/asserters.py,"def assert_categorical_equal(left, right, check_dtype: bool=True, check_category_order: bool=True, obj: str='Categorical') -> None:
    _check_isinstance(left, right, Categorical)
    exact: bool | str
    if isinstance(left.categories, RangeIndex) or isinstance(right.categories, RangeIndex):
        exact = 'equiv'
    else:
        exact = True
    if check_category_order:
        assert_index_equal(left.categories, right.categories, obj=f'{obj}.categories', exact=exact)
        assert_numpy_array_equal(left.codes, right.codes, check_dtype=check_dtype, obj=f'{obj}.codes')
    else:
        try:
            lc = left.categories.sort_values()
            rc = right.categories.sort_values()
        except TypeError:
            (lc, rc) = (left.categories, right.categories)
        assert_index_equal(lc, rc, obj=f'{obj}.categories', exact=exact)
        assert_index_equal(left.categories.take(left.codes), right.categories.take(right.codes), obj=f'{obj}.values', exact=exact)
    assert_attr_equal('ordered', left, right, obj=obj)","""""""Test that Categoricals are equivalent.

Parameters
----------
left : Categorical
right : Categorical
check_dtype : bool, default True
    Check that integer dtype of the codes are the same.
check_category_order : bool, default True
    Whether the order of the categories should be compared, which
    implies identical integer codes.  If False, only the resulting
    values are compared.  The ordered attribute is
    checked regardless.
obj : str, default 'Categorical'
    Specify object name being compared, internally used to show appropriate
    assertion message."""""""
pandas/_testing/asserters.py,"def assert_interval_array_equal(left, right, exact: bool | Literal['equiv']='equiv', obj: str='IntervalArray') -> None:
    _check_isinstance(left, right, IntervalArray)
    kwargs = {}
    if left._left.dtype.kind in 'mM':
        kwargs['check_freq'] = False
    assert_equal(left._left, right._left, obj=f'{obj}.left', **kwargs)
    assert_equal(left._right, right._right, obj=f'{obj}.left', **kwargs)
    assert_attr_equal('closed', left, right, obj=obj)","""""""Test that two IntervalArrays are equivalent.

Parameters
----------
left, right : IntervalArray
    The IntervalArrays to compare.
exact : bool or {'equiv'}, default 'equiv'
    Whether to check the Index class, dtype and inferred_type
    are identical. If 'equiv', then RangeIndex can be substituted for
    Index with an int64 dtype as well.
obj : str, default 'IntervalArray'
    Specify object name being compared, internally used to show appropriate
    assertion message"""""""
pandas/_testing/asserters.py,"def assert_numpy_array_equal(left, right, strict_nan: bool=False, check_dtype: bool | Literal['equiv']=True, err_msg=None, check_same=None, obj: str='numpy array', index_values=None) -> None:
    __tracebackhide__ = True
    assert_class_equal(left, right, obj=obj)
    _check_isinstance(left, right, np.ndarray)

    def _get_base(obj):
        return obj.base if getattr(obj, 'base', None) is not None else obj
    left_base = _get_base(left)
    right_base = _get_base(right)
    if check_same == 'same':
        if left_base is not right_base:
            raise AssertionError(f'{repr(left_base)} is not {repr(right_base)}')
    elif check_same == 'copy':
        if left_base is right_base:
            raise AssertionError(f'{repr(left_base)} is {repr(right_base)}')

    def _raise(left, right, err_msg):
        if err_msg is None:
            if left.shape != right.shape:
                raise_assert_detail(obj, f'{obj} shapes are different', left.shape, right.shape)
            diff = 0
            for (left_arr, right_arr) in zip(left, right):
                if not array_equivalent(left_arr, right_arr, strict_nan=strict_nan):
                    diff += 1
            diff = diff * 100.0 / left.size
            msg = f'{obj} values are different ({np.round(diff, 5)} %)'
            raise_assert_detail(obj, msg, left, right, index_values=index_values)
        raise AssertionError(err_msg)
    if not array_equivalent(left, right, strict_nan=strict_nan):
        _raise(left, right, err_msg)
    if check_dtype:
        if isinstance(left, np.ndarray) and isinstance(right, np.ndarray):
            assert_attr_equal('dtype', left, right, obj=obj)","""""""Check that 'np.ndarray' is equivalent.

Parameters
----------
left, right : numpy.ndarray or iterable
    The two arrays to be compared.
strict_nan : bool, default False
    If True, consider NaN and None to be different.
check_dtype : bool, default True
    Check dtype if both a and b are np.ndarray.
err_msg : str, default None
    If provided, used as assertion message.
check_same : None|'copy'|'same', default None
    Ensure left and right refer/do not refer to the same memory area.
obj : str, default 'numpy array'
    Specify object name being compared, internally used to show appropriate
    assertion message.
index_values : numpy.ndarray, default None
    optional index (shared by both left and right), used in output."""""""
pandas/_testing/asserters.py,"def assert_extension_array_equal(left, right, check_dtype: bool | Literal['equiv']=True, index_values=None, check_exact: bool=False, rtol: float=1e-05, atol: float=1e-08, obj: str='ExtensionArray') -> None:
    assert isinstance(left, ExtensionArray), 'left is not an ExtensionArray'
    assert isinstance(right, ExtensionArray), 'right is not an ExtensionArray'
    if check_dtype:
        assert_attr_equal('dtype', left, right, obj=f'Attributes of {obj}')
    if isinstance(left, DatetimeLikeArrayMixin) and isinstance(right, DatetimeLikeArrayMixin) and (type(right) == type(left)):
        if not check_dtype and left.dtype.kind in 'mM':
            if not isinstance(left.dtype, np.dtype):
                l_unit = cast(DatetimeTZDtype, left.dtype).unit
            else:
                l_unit = np.datetime_data(left.dtype)[0]
            if not isinstance(right.dtype, np.dtype):
                r_unit = cast(DatetimeTZDtype, left.dtype).unit
            else:
                r_unit = np.datetime_data(right.dtype)[0]
            if l_unit != r_unit and compare_mismatched_resolutions(left._ndarray, right._ndarray, operator.eq).all():
                return
        assert_numpy_array_equal(np.asarray(left.asi8), np.asarray(right.asi8), index_values=index_values, obj=obj)
        return
    left_na = np.asarray(left.isna())
    right_na = np.asarray(right.isna())
    assert_numpy_array_equal(left_na, right_na, obj=f'{obj} NA mask', index_values=index_values)
    left_valid = left[~left_na].to_numpy(dtype=object)
    right_valid = right[~right_na].to_numpy(dtype=object)
    if check_exact:
        assert_numpy_array_equal(left_valid, right_valid, obj=obj, index_values=index_values)
    else:
        _testing.assert_almost_equal(left_valid, right_valid, check_dtype=bool(check_dtype), rtol=rtol, atol=atol, obj=obj, index_values=index_values)","""""""Check that left and right ExtensionArrays are equal.

Parameters
----------
left, right : ExtensionArray
    The two arrays to compare.
check_dtype : bool, default True
    Whether to check if the ExtensionArray dtypes are identical.
index_values : numpy.ndarray, default None
    Optional index (shared by both left and right), used in output.
check_exact : bool, default False
    Whether to compare number exactly.
rtol : float, default 1e-5
    Relative tolerance. Only used when check_exact is False.
atol : float, default 1e-8
    Absolute tolerance. Only used when check_exact is False.
obj : str, default 'ExtensionArray'
    Specify object name being compared, internally used to show appropriate
    assertion message.

    .. versionadded:: 2.0.0

Notes
-----
Missing values are checked separately from valid values.
A mask of missing values is computed for each and checked to match.
The remaining all-valid values are cast to object dtype and checked.

Examples
--------
>>> from pandas import testing as tm
>>> a = pd.Series([1, 2, 3, 4])
>>> b, c = a.array, a.array
>>> tm.assert_extension_array_equal(b, c)"""""""
pandas/_testing/asserters.py,"def assert_series_equal(left, right, check_dtype: bool | Literal['equiv']=True, check_index_type: bool | Literal['equiv']='equiv', check_series_type: bool=True, check_names: bool=True, check_exact: bool=False, check_datetimelike_compat: bool=False, check_categorical: bool=True, check_category_order: bool=True, check_freq: bool=True, check_flags: bool=True, rtol: float=1e-05, atol: float=1e-08, obj: str='Series', *, check_index: bool=True, check_like: bool=False) -> None:
    __tracebackhide__ = True
    if not check_index and check_like:
        raise ValueError('check_like must be False if check_index is False')
    _check_isinstance(left, right, Series)
    if check_series_type:
        assert_class_equal(left, right, obj=obj)
    if len(left) != len(right):
        msg1 = f'{len(left)}, {left.index}'
        msg2 = f'{len(right)}, {right.index}'
        raise_assert_detail(obj, 'Series length are different', msg1, msg2)
    if check_flags:
        assert left.flags == right.flags, f'{repr(left.flags)} != {repr(right.flags)}'
    if check_index:
        assert_index_equal(left.index, right.index, exact=check_index_type, check_names=check_names, check_exact=check_exact, check_categorical=check_categorical, check_order=not check_like, rtol=rtol, atol=atol, obj=f'{obj}.index')
    if check_like:
        left = left.reindex_like(right)
    if check_freq and isinstance(left.index, (DatetimeIndex, TimedeltaIndex)):
        lidx = left.index
        ridx = right.index
        assert lidx.freq == ridx.freq, (lidx.freq, ridx.freq)
    if check_dtype:
        if isinstance(left.dtype, CategoricalDtype) and isinstance(right.dtype, CategoricalDtype) and (not check_categorical):
            pass
        else:
            assert_attr_equal('dtype', left, right, obj=f'Attributes of {obj}')
    if check_exact and is_numeric_dtype(left.dtype) and is_numeric_dtype(right.dtype):
        left_values = left._values
        right_values = right._values
        if isinstance(left_values, ExtensionArray) and isinstance(right_values, ExtensionArray):
            assert_extension_array_equal(left_values, right_values, check_dtype=check_dtype, index_values=np.asarray(left.index), obj=str(obj))
        else:
            assert_numpy_array_equal(left_values, right_values, check_dtype=check_dtype, obj=str(obj), index_values=np.asarray(left.index))
    elif check_datetimelike_compat and (needs_i8_conversion(left.dtype) or needs_i8_conversion(right.dtype)):
        if not Index(left._values).equals(Index(right._values)):
            msg = f'[datetimelike_compat=True] {left._values} is not equal to {right._values}.'
            raise AssertionError(msg)
    elif isinstance(left.dtype, IntervalDtype) and isinstance(right.dtype, IntervalDtype):
        assert_interval_array_equal(left.array, right.array)
    elif isinstance(left.dtype, CategoricalDtype) or isinstance(right.dtype, CategoricalDtype):
        _testing.assert_almost_equal(left._values, right._values, rtol=rtol, atol=atol, check_dtype=bool(check_dtype), obj=str(obj), index_values=np.asarray(left.index))
    elif isinstance(left.dtype, ExtensionDtype) and isinstance(right.dtype, ExtensionDtype):
        assert_extension_array_equal(left._values, right._values, rtol=rtol, atol=atol, check_dtype=check_dtype, index_values=np.asarray(left.index), obj=str(obj))
    elif is_extension_array_dtype_and_needs_i8_conversion(left.dtype, right.dtype) or is_extension_array_dtype_and_needs_i8_conversion(right.dtype, left.dtype):
        assert_extension_array_equal(left._values, right._values, check_dtype=check_dtype, index_values=np.asarray(left.index), obj=str(obj))
    elif needs_i8_conversion(left.dtype) and needs_i8_conversion(right.dtype):
        assert_extension_array_equal(left._values, right._values, check_dtype=check_dtype, index_values=np.asarray(left.index), obj=str(obj))
    else:
        _testing.assert_almost_equal(left._values, right._values, rtol=rtol, atol=atol, check_dtype=bool(check_dtype), obj=str(obj), index_values=np.asarray(left.index))
    if check_names:
        assert_attr_equal('name', left, right, obj=obj)
    if check_categorical:
        if isinstance(left.dtype, CategoricalDtype) or isinstance(right.dtype, CategoricalDtype):
            assert_categorical_equal(left._values, right._values, obj=f'{obj} category', check_category_order=check_category_order)","""""""Check that left and right Series are equal.

Parameters
----------
left : Series
right : Series
check_dtype : bool, default True
    Whether to check the Series dtype is identical.
check_index_type : bool or {'equiv'}, default 'equiv'
    Whether to check the Index class, dtype and inferred_type
    are identical.
check_series_type : bool, default True
     Whether to check the Series class is identical.
check_names : bool, default True
    Whether to check the Series and Index names attribute.
check_exact : bool, default False
    Whether to compare number exactly.
check_datetimelike_compat : bool, default False
    Compare datetime-like which is comparable ignoring dtype.
check_categorical : bool, default True
    Whether to compare internal Categorical exactly.
check_category_order : bool, default True
    Whether to compare category order of internal Categoricals.
check_freq : bool, default True
    Whether to check the `freq` attribute on a DatetimeIndex or TimedeltaIndex.
check_flags : bool, default True
    Whether to check the `flags` attribute.

    .. versionadded:: 1.2.0

rtol : float, default 1e-5
    Relative tolerance. Only used when check_exact is False.
atol : float, default 1e-8
    Absolute tolerance. Only used when check_exact is False.
obj : str, default 'Series'
    Specify object name being compared, internally used to show appropriate
    assertion message.
check_index : bool, default True
    Whether to check index equivalence. If False, then compare only values.

    .. versionadded:: 1.3.0
check_like : bool, default False
    If True, ignore the order of the index. Must be False if check_index is False.
    Note: same labels must be with the same data.

    .. versionadded:: 1.5.0

Examples
--------
>>> from pandas import testing as tm
>>> a = pd.Series([1, 2, 3, 4])
>>> b = pd.Series([1, 2, 3, 4])
>>> tm.assert_series_equal(a, b)"""""""
pandas/_testing/asserters.py,"def assert_frame_equal(left, right, check_dtype: bool | Literal['equiv']=True, check_index_type: bool | Literal['equiv']='equiv', check_column_type: bool | Literal['equiv']='equiv', check_frame_type: bool=True, check_names: bool=True, by_blocks: bool=False, check_exact: bool=False, check_datetimelike_compat: bool=False, check_categorical: bool=True, check_like: bool=False, check_freq: bool=True, check_flags: bool=True, rtol: float=1e-05, atol: float=1e-08, obj: str='DataFrame') -> None:
    __tracebackhide__ = True
    _check_isinstance(left, right, DataFrame)
    if check_frame_type:
        assert isinstance(left, type(right))
    if left.shape != right.shape:
        raise_assert_detail(obj, f'{obj} shape mismatch', f'{repr(left.shape)}', f'{repr(right.shape)}')
    if check_flags:
        assert left.flags == right.flags, f'{repr(left.flags)} != {repr(right.flags)}'
    assert_index_equal(left.index, right.index, exact=check_index_type, check_names=check_names, check_exact=check_exact, check_categorical=check_categorical, check_order=not check_like, rtol=rtol, atol=atol, obj=f'{obj}.index')
    assert_index_equal(left.columns, right.columns, exact=check_column_type, check_names=check_names, check_exact=check_exact, check_categorical=check_categorical, check_order=not check_like, rtol=rtol, atol=atol, obj=f'{obj}.columns')
    if check_like:
        left = left.reindex_like(right)
    if by_blocks:
        rblocks = right._to_dict_of_blocks(copy=False)
        lblocks = left._to_dict_of_blocks(copy=False)
        for dtype in list(set(list(lblocks.keys()) + list(rblocks.keys()))):
            assert dtype in lblocks
            assert dtype in rblocks
            assert_frame_equal(lblocks[dtype], rblocks[dtype], check_dtype=check_dtype, obj=obj)
    else:
        for (i, col) in enumerate(left.columns):
            lcol = left._ixs(i, axis=1)
            rcol = right._ixs(i, axis=1)
            assert_series_equal(lcol, rcol, check_dtype=check_dtype, check_index_type=check_index_type, check_exact=check_exact, check_names=check_names, check_datetimelike_compat=check_datetimelike_compat, check_categorical=check_categorical, check_freq=check_freq, obj=f'{obj}.iloc[:, {i}] (column name=""{col}"")', rtol=rtol, atol=atol, check_index=False, check_flags=False)","""""""Check that left and right DataFrame are equal.

This function is intended to compare two DataFrames and output any
differences. It is mostly intended for use in unit tests.
Additional parameters allow varying the strictness of the
equality checks performed.

Parameters
----------
left : DataFrame
    First DataFrame to compare.
right : DataFrame
    Second DataFrame to compare.
check_dtype : bool, default True
    Whether to check the DataFrame dtype is identical.
check_index_type : bool or {'equiv'}, default 'equiv'
    Whether to check the Index class, dtype and inferred_type
    are identical.
check_column_type : bool or {'equiv'}, default 'equiv'
    Whether to check the columns class, dtype and inferred_type
    are identical. Is passed as the ``exact`` argument of
    :func:`assert_index_equal`.
check_frame_type : bool, default True
    Whether to check the DataFrame class is identical.
check_names : bool, default True
    Whether to check that the `names` attribute for both the `index`
    and `column` attributes of the DataFrame is identical.
by_blocks : bool, default False
    Specify how to compare internal data. If False, compare by columns.
    If True, compare by blocks.
check_exact : bool, default False
    Whether to compare number exactly.
check_datetimelike_compat : bool, default False
    Compare datetime-like which is comparable ignoring dtype.
check_categorical : bool, default True
    Whether to compare internal Categorical exactly.
check_like : bool, default False
    If True, ignore the order of index & columns.
    Note: index labels must match their respective rows
    (same as in columns) - same labels must be with the same data.
check_freq : bool, default True
    Whether to check the `freq` attribute on a DatetimeIndex or TimedeltaIndex.
check_flags : bool, default True
    Whether to check the `flags` attribute.
rtol : float, default 1e-5
    Relative tolerance. Only used when check_exact is False.
atol : float, default 1e-8
    Absolute tolerance. Only used when check_exact is False.
obj : str, default 'DataFrame'
    Specify object name being compared, internally used to show appropriate
    assertion message.

See Also
--------
assert_series_equal : Equivalent method for asserting Series equality.
DataFrame.equals : Check DataFrame equality.

Examples
--------
This example shows comparing two DataFrames that are equal
but with columns of differing dtypes.

>>> from pandas.testing import assert_frame_equal
>>> df1 = pd.DataFrame({'a': [1, 2], 'b': [3, 4]})
>>> df2 = pd.DataFrame({'a': [1, 2], 'b': [3.0, 4.0]})

df1 equals itself.

>>> assert_frame_equal(df1, df1)

df1 differs from df2 as column 'b' is of a different type.

>>> assert_frame_equal(df1, df2)
Traceback (most recent call last):
...
AssertionError: Attributes of DataFrame.iloc[:, 1] (column name=""b"") are different

Attribute ""dtype"" are different
[left]:  int64
[right]: float64

Ignore differing dtypes in columns with check_dtype.

>>> assert_frame_equal(df1, df2, check_dtype=False)"""""""
pandas/_testing/asserters.py,"def assert_equal(left, right, **kwargs) -> None:
    __tracebackhide__ = True
    if isinstance(left, Index):
        assert_index_equal(left, right, **kwargs)
        if isinstance(left, (DatetimeIndex, TimedeltaIndex)):
            assert left.freq == right.freq, (left.freq, right.freq)
    elif isinstance(left, Series):
        assert_series_equal(left, right, **kwargs)
    elif isinstance(left, DataFrame):
        assert_frame_equal(left, right, **kwargs)
    elif isinstance(left, IntervalArray):
        assert_interval_array_equal(left, right, **kwargs)
    elif isinstance(left, PeriodArray):
        assert_period_array_equal(left, right, **kwargs)
    elif isinstance(left, DatetimeArray):
        assert_datetime_array_equal(left, right, **kwargs)
    elif isinstance(left, TimedeltaArray):
        assert_timedelta_array_equal(left, right, **kwargs)
    elif isinstance(left, ExtensionArray):
        assert_extension_array_equal(left, right, **kwargs)
    elif isinstance(left, np.ndarray):
        assert_numpy_array_equal(left, right, **kwargs)
    elif isinstance(left, str):
        assert kwargs == {}
        assert left == right
    else:
        assert kwargs == {}
        assert_almost_equal(left, right)","""""""Wrapper for tm.assert_*_equal to dispatch to the appropriate test function.

Parameters
----------
left, right : Index, Series, DataFrame, ExtensionArray, or np.ndarray
    The two items to be compared.
**kwargs
    All keyword arguments are passed through to the underlying assert method."""""""
pandas/_testing/asserters.py,"def assert_sp_array_equal(left, right) -> None:
    _check_isinstance(left, right, pd.arrays.SparseArray)
    assert_numpy_array_equal(left.sp_values, right.sp_values)
    assert isinstance(left.sp_index, SparseIndex)
    assert isinstance(right.sp_index, SparseIndex)
    left_index = left.sp_index
    right_index = right.sp_index
    if not left_index.equals(right_index):
        raise_assert_detail('SparseArray.index', 'index are not equal', left_index, right_index)
    else:
        pass
    assert_attr_equal('fill_value', left, right)
    assert_attr_equal('dtype', left, right)
    assert_numpy_array_equal(left.to_dense(), right.to_dense())","""""""Check that the left and right SparseArray are equal.

Parameters
----------
left : SparseArray
right : SparseArray"""""""
pandas/_testing/asserters.py,"def assert_copy(iter1, iter2, **eql_kwargs) -> None:
    for (elem1, elem2) in zip(iter1, iter2):
        assert_almost_equal(elem1, elem2, **eql_kwargs)
        msg = f'Expected object {repr(type(elem1))} and object {repr(type(elem2))} to be different objects, but they were the same object.'
        assert elem1 is not elem2, msg","""""""iter1, iter2: iterables that produce elements
comparable with assert_almost_equal

Checks that the elements are equal, but not
the same object. (Does not check that items
in sequences are also not the same object)"""""""
pandas/_testing/asserters.py,"def is_extension_array_dtype_and_needs_i8_conversion(left_dtype: DtypeObj, right_dtype: DtypeObj) -> bool:
    return isinstance(left_dtype, ExtensionDtype) and needs_i8_conversion(right_dtype)","""""""Checks that we have the combination of an ExtensionArraydtype and
a dtype that should be converted to int64

Returns
-------
bool

Related to issue #37609"""""""
pandas/_testing/asserters.py,"def assert_indexing_slices_equivalent(ser: Series, l_slc: slice, i_slc: slice) -> None:
    expected = ser.iloc[i_slc]
    assert_series_equal(ser.loc[l_slc], expected)
    if not is_integer_dtype(ser.index):
        assert_series_equal(ser[l_slc], expected)","""""""Check that ser.iloc[i_slc] matches ser.loc[l_slc] and, if applicable,
ser[l_slc]."""""""
pandas/_testing/asserters.py,"def assert_metadata_equivalent(left: DataFrame | Series, right: DataFrame | Series | None=None) -> None:
    for attr in left._metadata:
        val = getattr(left, attr, None)
        if right is None:
            assert val is None
        else:
            assert val == getattr(right, attr, None)","""""""Check that ._metadata attributes are equivalent."""""""
pandas/_testing/compat.py,"def get_obj(df: DataFrame, klass):
    if klass is DataFrame:
        return df
    return df._ixs(0, axis=1)","""""""For sharing tests using frame_or_series, either return the DataFrame
unchanged or return it's first column as a Series."""""""
pandas/_testing/contexts.py,"@contextmanager
def decompress_file(path: FilePath | BaseBuffer, compression: CompressionOptions) -> Generator[IO[bytes], None, None]:
    with get_handle(path, 'rb', compression=compression, is_text=False) as handle:
        yield handle.handle","""""""Open a compressed file and return a file object.

Parameters
----------
path : str
    The path where the file is read from.

compression : {'gzip', 'bz2', 'zip', 'xz', 'zstd', None}
    Name of the decompression to use

Returns
-------
file object"""""""
pandas/_testing/contexts.py,"@contextmanager
def set_timezone(tz: str) -> Generator[None, None, None]:
    import time

    def setTZ(tz) -> None:
        if tz is None:
            try:
                del os.environ['TZ']
            except KeyError:
                pass
        else:
            os.environ['TZ'] = tz
            time.tzset()
    orig_tz = os.environ.get('TZ')
    setTZ(tz)
    try:
        yield
    finally:
        setTZ(orig_tz)","""""""Context manager for temporarily setting a timezone.

Parameters
----------
tz : str
    A string representing a valid timezone.

Examples
--------
>>> from datetime import datetime
>>> from dateutil.tz import tzlocal
>>> tzlocal().tzname(datetime(2021, 1, 1))  # doctest: +SKIP
'IST'

>>> with set_timezone('US/Eastern'):
...     tzlocal().tzname(datetime(2021, 1, 1))
...
'EST'"""""""
pandas/_testing/contexts.py,"@contextmanager
def ensure_clean(filename=None, return_filelike: bool=False, **kwargs: Any) -> Generator[Any, None, None]:
    folder = Path(tempfile.gettempdir())
    if filename is None:
        filename = ''
    filename = str(uuid.uuid4()) + filename
    path = folder / filename
    path.touch()
    handle_or_str: str | IO = str(path)
    encoding = kwargs.pop('encoding', None)
    if return_filelike:
        kwargs.setdefault('mode', 'w+b')
        if encoding is None and 'b' not in kwargs['mode']:
            encoding = 'utf-8'
        handle_or_str = open(path, encoding=encoding, **kwargs)
    try:
        yield handle_or_str
    finally:
        if not isinstance(handle_or_str, str):
            handle_or_str.close()
        if path.is_file():
            path.unlink()","""""""Gets a temporary path and agrees to remove on close.

This implementation does not use tempfile.mkstemp to avoid having a file handle.
If the code using the returned path wants to delete the file itself, windows
requires that no program has a file handle to it.

Parameters
----------
filename : str (optional)
    suffix of the created file.
return_filelike : bool (default False)
    if True, returns a file-like which is *always* cleaned. Necessary for
    savefig and other functions which want to append extensions.
**kwargs
    Additional keywords are passed to open()."""""""
pandas/_testing/contexts.py,"@contextmanager
def with_csv_dialect(name: str, **kwargs) -> Generator[None, None, None]:
    import csv
    _BUILTIN_DIALECTS = {'excel', 'excel-tab', 'unix'}
    if name in _BUILTIN_DIALECTS:
        raise ValueError('Cannot override builtin dialect.')
    csv.register_dialect(name, **kwargs)
    try:
        yield
    finally:
        csv.unregister_dialect(name)","""""""Context manager to temporarily register a CSV dialect for parsing CSV.

Parameters
----------
name : str
    The name of the dialect.
kwargs : mapping
    The parameters for the dialect.

Raises
------
ValueError : the name of the dialect conflicts with a builtin one.

See Also
--------
csv : Python's CSV library."""""""
pandas/_version.py,"def get_keywords():
    git_refnames = '$Format:%d$'
    git_full = '$Format:%H$'
    git_date = '$Format:%ci$'
    keywords = {'refnames': git_refnames, 'full': git_full, 'date': git_date}
    return keywords","""""""Get the keywords needed to look up the version information."""""""
pandas/_version.py,"def get_config():
    cfg = VersioneerConfig()
    cfg.VCS = 'git'
    cfg.style = 'pep440'
    cfg.tag_prefix = 'v'
    cfg.parentdir_prefix = 'pandas-'
    cfg.versionfile_source = 'pandas/_version.py'
    cfg.verbose = False
    return cfg","""""""Create, populate and return the VersioneerConfig() object."""""""
pandas/_version.py,"def register_vcs_handler(vcs, method):

    def decorate(f):
        """"""Store f in HANDLERS[vcs][method].""""""
        if vcs not in HANDLERS:
            HANDLERS[vcs] = {}
        HANDLERS[vcs][method] = f
        return f
    return decorate","""""""Create decorator to mark a method as the handler of a VCS."""""""
pandas/_version.py,"def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):
    assert isinstance(commands, list)
    process = None
    popen_kwargs = {}
    if sys.platform == 'win32':
        startupinfo = subprocess.STARTUPINFO()
        startupinfo.dwFlags |= subprocess.STARTF_USESHOWWINDOW
        popen_kwargs['startupinfo'] = startupinfo
    for command in commands:
        dispcmd = str([command] + args)
        try:
            process = subprocess.Popen([command] + args, cwd=cwd, env=env, stdout=subprocess.PIPE, stderr=subprocess.PIPE if hide_stderr else None, **popen_kwargs)
            break
        except OSError:
            e = sys.exc_info()[1]
            if e.errno == errno.ENOENT:
                continue
            if verbose:
                print(f'unable to run {dispcmd}')
                print(e)
            return (None, None)
    else:
        if verbose:
            print(f'unable to find command, tried {commands}')
        return (None, None)
    stdout = process.communicate()[0].strip().decode()
    if process.returncode != 0:
        if verbose:
            print(f'unable to run {dispcmd} (error)')
            print(f'stdout was {stdout}')
        return (None, process.returncode)
    return (stdout, process.returncode)","""""""Call the given command(s)."""""""
pandas/_version.py,"def versions_from_parentdir(parentdir_prefix, root, verbose):
    rootdirs = []
    for _ in range(3):
        dirname = os.path.basename(root)
        if dirname.startswith(parentdir_prefix):
            return {'version': dirname[len(parentdir_prefix):], 'full-revisionid': None, 'dirty': False, 'error': None, 'date': None}
        rootdirs.append(root)
        root = os.path.dirname(root)
    if verbose:
        print(f'Tried directories {str(rootdirs)}             but none started with prefix {parentdir_prefix}')
    raise NotThisMethod(""rootdir doesn't start with parentdir_prefix"")","""""""Try to determine the version from the parent directory name.

Source tarballs conventionally unpack into a directory that includes both
the project name and a version string. We will also support searching up
two directory levels for an appropriately named parent directory"""""""
pandas/_version.py,"@register_vcs_handler('git', 'get_keywords')
def git_get_keywords(versionfile_abs):
    keywords = {}
    try:
        with open(versionfile_abs, encoding='utf-8') as fobj:
            for line in fobj:
                if line.strip().startswith('git_refnames ='):
                    mo = re.search('=\\s*""(.*)""', line)
                    if mo:
                        keywords['refnames'] = mo.group(1)
                if line.strip().startswith('git_full ='):
                    mo = re.search('=\\s*""(.*)""', line)
                    if mo:
                        keywords['full'] = mo.group(1)
                if line.strip().startswith('git_date ='):
                    mo = re.search('=\\s*""(.*)""', line)
                    if mo:
                        keywords['date'] = mo.group(1)
    except OSError:
        pass
    return keywords","""""""Extract version information from the given file."""""""
pandas/_version.py,"@register_vcs_handler('git', 'keywords')
def git_versions_from_keywords(keywords, tag_prefix, verbose):
    if 'refnames' not in keywords:
        raise NotThisMethod('Short version file found')
    date = keywords.get('date')
    if date is not None:
        date = date.splitlines()[-1]
        date = date.strip().replace(' ', 'T', 1).replace(' ', '', 1)
    refnames = keywords['refnames'].strip()
    if refnames.startswith('$Format'):
        if verbose:
            print('keywords are unexpanded, not using')
        raise NotThisMethod('unexpanded keywords, not a git-archive tarball')
    refs = {r.strip() for r in refnames.strip('()').split(',')}
    TAG = 'tag: '
    tags = {r[len(TAG):] for r in refs if r.startswith(TAG)}
    if not tags:
        tags = {r for r in refs if re.search('\\d', r)}
        if verbose:
            print(f""discarding '{','.join(refs - tags)}', no digits"")
    if verbose:
        print(f""likely tags: {','.join(sorted(tags))}"")
    for ref in sorted(tags):
        if ref.startswith(tag_prefix):
            r = ref[len(tag_prefix):]
            if not re.match('\\d', r):
                continue
            if verbose:
                print(f'picking {r}')
            return {'version': r, 'full-revisionid': keywords['full'].strip(), 'dirty': False, 'error': None, 'date': date}
    if verbose:
        print('no suitable tags, using unknown + full revision id')
    return {'version': '0+unknown', 'full-revisionid': keywords['full'].strip(), 'dirty': False, 'error': 'no suitable tags', 'date': None}","""""""Get version information from git keywords."""""""
pandas/_version.py,"@register_vcs_handler('git', 'pieces_from_vcs')
def git_pieces_from_vcs(tag_prefix, root, verbose, runner=run_command):
    GITS = ['git']
    if sys.platform == 'win32':
        GITS = ['git.cmd', 'git.exe']
    env = os.environ.copy()
    env.pop('GIT_DIR', None)
    runner = functools.partial(runner, env=env)
    (_, rc) = runner(GITS, ['rev-parse', '--git-dir'], cwd=root, hide_stderr=not verbose)
    if rc != 0:
        if verbose:
            print(f'Directory {root} not under git control')
        raise NotThisMethod(""'git rev-parse --git-dir' returned error"")
    (describe_out, rc) = runner(GITS, ['describe', '--tags', '--dirty', '--always', '--long', '--match', f'{tag_prefix}[[:digit:]]*'], cwd=root)
    if describe_out is None:
        raise NotThisMethod(""'git describe' failed"")
    describe_out = describe_out.strip()
    (full_out, rc) = runner(GITS, ['rev-parse', 'HEAD'], cwd=root)
    if full_out is None:
        raise NotThisMethod(""'git rev-parse' failed"")
    full_out = full_out.strip()
    pieces = {}
    pieces['long'] = full_out
    pieces['short'] = full_out[:7]
    pieces['error'] = None
    (branch_name, rc) = runner(GITS, ['rev-parse', '--abbrev-ref', 'HEAD'], cwd=root)
    if rc != 0 or branch_name is None:
        raise NotThisMethod(""'git rev-parse --abbrev-ref' returned error"")
    branch_name = branch_name.strip()
    if branch_name == 'HEAD':
        (branches, rc) = runner(GITS, ['branch', '--contains'], cwd=root)
        if rc != 0 or branches is None:
            raise NotThisMethod(""'git branch --contains' returned error"")
        branches = branches.split('\n')
        if '(' in branches[0]:
            branches.pop(0)
        branches = [branch[2:] for branch in branches]
        if 'master' in branches:
            branch_name = 'master'
        elif not branches:
            branch_name = None
        else:
            branch_name = branches[0]
    pieces['branch'] = branch_name
    git_describe = describe_out
    dirty = git_describe.endswith('-dirty')
    pieces['dirty'] = dirty
    if dirty:
        git_describe = git_describe[:git_describe.rindex('-dirty')]
    if '-' in git_describe:
        mo = re.search('^(.+)-(\\d+)-g([0-9a-f]+)$', git_describe)
        if not mo:
            pieces['error'] = f""unable to parse git-describe output: '{describe_out}'""
            return pieces
        full_tag = mo.group(1)
        if not full_tag.startswith(tag_prefix):
            if verbose:
                fmt = ""tag '%s' doesn't start with prefix '%s'""
                print(fmt % (full_tag, tag_prefix))
            pieces['error'] = f""tag '{full_tag}' doesn't start with prefix '{tag_prefix}'""
            return pieces
        pieces['closest-tag'] = full_tag[len(tag_prefix):]
        pieces['distance'] = int(mo.group(2))
        pieces['short'] = mo.group(3)
    else:
        pieces['closest-tag'] = None
        (out, rc) = runner(GITS, ['rev-list', 'HEAD', '--left-right'], cwd=root)
        pieces['distance'] = len(out.split())
    date = runner(GITS, ['show', '-s', '--format=%ci', 'HEAD'], cwd=root)[0].strip()
    date = date.splitlines()[-1]
    pieces['date'] = date.strip().replace(' ', 'T', 1).replace(' ', '', 1)
    return pieces","""""""Get version from 'git describe' in the root of the source tree.

This only gets called if the git-archive 'subst' keywords were *not*
expanded, and _version.py hasn't already been rewritten with a short
version string, meaning we're inside a checked out source tree."""""""
pandas/_version.py,"def plus_or_dot(pieces):
    if '+' in pieces.get('closest-tag', ''):
        return '.'
    return '+'","""""""Return a + if we don't already have one, else return a ."""""""
pandas/_version.py,"def render_pep440(pieces):
    if pieces['closest-tag']:
        rendered = pieces['closest-tag']
        if pieces['distance'] or pieces['dirty']:
            rendered += plus_or_dot(pieces)
            rendered += f""{pieces['distance']}.g{pieces['short']}""
            if pieces['dirty']:
                rendered += '.dirty'
    else:
        rendered = f""0+untagged.{pieces['distance']}.g{pieces['short']}""
        if pieces['dirty']:
            rendered += '.dirty'
    return rendered","""""""Build up version string, with post-release ""local version identifier"".

Our goal: TAG[+DISTANCE.gHEX[.dirty]] . Note that if you
get a tagged build and then dirty it, you'll get TAG+0.gHEX.dirty

Exceptions:
1: no tags. git_describe was just HEX. 0+untagged.DISTANCE.gHEX[.dirty]"""""""
pandas/_version.py,"def render_pep440_branch(pieces):
    if pieces['closest-tag']:
        rendered = pieces['closest-tag']
        if pieces['distance'] or pieces['dirty']:
            if pieces['branch'] != 'master':
                rendered += '.dev0'
            rendered += plus_or_dot(pieces)
            rendered += f""{pieces['distance']}.g{pieces['short']}""
            if pieces['dirty']:
                rendered += '.dirty'
    else:
        rendered = '0'
        if pieces['branch'] != 'master':
            rendered += '.dev0'
        rendered += f""+untagged.{pieces['distance']}.g{pieces['short']}""
        if pieces['dirty']:
            rendered += '.dirty'
    return rendered","""""""TAG[[.dev0]+DISTANCE.gHEX[.dirty]] .

The "".dev0"" means not master branch. Note that .dev0 sorts backwards
(a feature branch will appear ""older"" than the master branch).

Exceptions:
1: no tags. 0[.dev0]+untagged.DISTANCE.gHEX[.dirty]"""""""
pandas/_version.py,"def pep440_split_post(ver):
    vc = str.split(ver, '.post')
    return (vc[0], int(vc[1] or 0) if len(vc) == 2 else None)","""""""Split pep440 version string at the post-release segment.

Returns the release segments before the post-release and the
post-release version number (or -1 if no post-release segment is present)."""""""
pandas/_version.py,"def render_pep440_pre(pieces):
    if pieces['closest-tag']:
        if pieces['distance']:
            (tag_version, post_version) = pep440_split_post(pieces['closest-tag'])
            rendered = tag_version
            if post_version is not None:
                rendered += f"".post{post_version + 1}.dev{pieces['distance']}""
            else:
                rendered += f"".post0.dev{pieces['distance']}""
        else:
            rendered = pieces['closest-tag']
    else:
        rendered = f""0.post0.dev{pieces['distance']}""
    return rendered","""""""TAG[.postN.devDISTANCE] -- No -dirty.

Exceptions:
1: no tags. 0.post0.devDISTANCE"""""""
pandas/_version.py,"def render_pep440_post(pieces):
    if pieces['closest-tag']:
        rendered = pieces['closest-tag']
        if pieces['distance'] or pieces['dirty']:
            rendered += f"".post{pieces['distance']}""
            if pieces['dirty']:
                rendered += '.dev0'
            rendered += plus_or_dot(pieces)
            rendered += f""g{pieces['short']}""
    else:
        rendered = f""0.post{pieces['distance']}""
        if pieces['dirty']:
            rendered += '.dev0'
        rendered += f""+g{pieces['short']}""
    return rendered","""""""TAG[.postDISTANCE[.dev0]+gHEX] .

The "".dev0"" means dirty. Note that .dev0 sorts backwards
(a dirty tree will appear ""older"" than the corresponding clean one),
but you shouldn't be releasing software with -dirty anyways.

Exceptions:
1: no tags. 0.postDISTANCE[.dev0]"""""""
pandas/_version.py,"def render_pep440_post_branch(pieces):
    if pieces['closest-tag']:
        rendered = pieces['closest-tag']
        if pieces['distance'] or pieces['dirty']:
            rendered += f"".post{pieces['distance']}""
            if pieces['branch'] != 'master':
                rendered += '.dev0'
            rendered += plus_or_dot(pieces)
            rendered += f""g{pieces['short']}""
            if pieces['dirty']:
                rendered += '.dirty'
    else:
        rendered = f""0.post{pieces['distance']}""
        if pieces['branch'] != 'master':
            rendered += '.dev0'
        rendered += f""+g{pieces['short']}""
        if pieces['dirty']:
            rendered += '.dirty'
    return rendered","""""""TAG[.postDISTANCE[.dev0]+gHEX[.dirty]] .

The "".dev0"" means not master branch.

Exceptions:
1: no tags. 0.postDISTANCE[.dev0]+gHEX[.dirty]"""""""
pandas/_version.py,"def render_pep440_old(pieces):
    if pieces['closest-tag']:
        rendered = pieces['closest-tag']
        if pieces['distance'] or pieces['dirty']:
            rendered += f""0.post{pieces['distance']}""
            if pieces['dirty']:
                rendered += '.dev0'
    else:
        rendered = f""0.post{pieces['distance']}""
        if pieces['dirty']:
            rendered += '.dev0'
    return rendered","""""""TAG[.postDISTANCE[.dev0]] .

The "".dev0"" means dirty.

Exceptions:
1: no tags. 0.postDISTANCE[.dev0]"""""""
pandas/_version.py,"def render_git_describe(pieces):
    if pieces['closest-tag']:
        rendered = pieces['closest-tag']
        if pieces['distance']:
            rendered += f""-{pieces['distance']}-g{pieces['short']}""
    else:
        rendered = pieces['short']
    if pieces['dirty']:
        rendered += '-dirty'
    return rendered","""""""TAG[-DISTANCE-gHEX][-dirty].

Like 'git describe --tags --dirty --always'.

Exceptions:
1: no tags. HEX[-dirty]  (note: no 'g' prefix)"""""""
pandas/_version.py,"def render_git_describe_long(pieces):
    if pieces['closest-tag']:
        rendered = pieces['closest-tag']
        rendered += f""-{pieces['distance']}-g{pieces['short']}""
    else:
        rendered = pieces['short']
    if pieces['dirty']:
        rendered += '-dirty'
    return rendered","""""""TAG-DISTANCE-gHEX[-dirty].

Like 'git describe --tags --dirty --always -long'.
The distance/hash is unconditional.

Exceptions:
1: no tags. HEX[-dirty]  (note: no 'g' prefix)"""""""
pandas/_version.py,"def render(pieces, style):
    if pieces['error']:
        return {'version': 'unknown', 'full-revisionid': pieces.get('long'), 'dirty': None, 'error': pieces['error'], 'date': None}
    if not style or style == 'default':
        style = 'pep440'
    if style == 'pep440':
        rendered = render_pep440(pieces)
    elif style == 'pep440-branch':
        rendered = render_pep440_branch(pieces)
    elif style == 'pep440-pre':
        rendered = render_pep440_pre(pieces)
    elif style == 'pep440-post':
        rendered = render_pep440_post(pieces)
    elif style == 'pep440-post-branch':
        rendered = render_pep440_post_branch(pieces)
    elif style == 'pep440-old':
        rendered = render_pep440_old(pieces)
    elif style == 'git-describe':
        rendered = render_git_describe(pieces)
    elif style == 'git-describe-long':
        rendered = render_git_describe_long(pieces)
    else:
        raise ValueError(f""unknown style '{style}'"")
    return {'version': rendered, 'full-revisionid': pieces['long'], 'dirty': pieces['dirty'], 'error': None, 'date': pieces.get('date')}","""""""Render the given version pieces into the requested style."""""""
pandas/_version.py,"def get_versions():
    cfg = get_config()
    verbose = cfg.verbose
    try:
        return git_versions_from_keywords(get_keywords(), cfg.tag_prefix, verbose)
    except NotThisMethod:
        pass
    try:
        root = os.path.realpath(__file__)
        for _ in cfg.versionfile_source.split('/'):
            root = os.path.dirname(root)
    except NameError:
        return {'version': '0+unknown', 'full-revisionid': None, 'dirty': None, 'error': 'unable to find root of source tree', 'date': None}
    try:
        pieces = git_pieces_from_vcs(cfg.tag_prefix, root, verbose)
        return render(pieces, cfg.style)
    except NotThisMethod:
        pass
    try:
        if cfg.parentdir_prefix:
            return versions_from_parentdir(cfg.parentdir_prefix, root, verbose)
    except NotThisMethod:
        pass
    return {'version': '0+unknown', 'full-revisionid': None, 'dirty': None, 'error': 'unable to compute version', 'date': None}","""""""Get version information or return default if unable to do so."""""""
pandas/compat/__init__.py,"def set_function_name(f: F, name: str, cls: type) -> F:
    f.__name__ = name
    f.__qualname__ = f'{cls.__name__}.{name}'
    f.__module__ = cls.__module__
    return f","""""""Bind the name/qualname attributes of the function."""""""
pandas/compat/__init__.py,"def is_platform_little_endian() -> bool:
    return sys.byteorder == 'little'","""""""Checking if the running platform is little endian.

Returns
-------
bool
    True if the running platform is little endian."""""""
pandas/compat/__init__.py,"def is_platform_windows() -> bool:
    return sys.platform in ['win32', 'cygwin']","""""""Checking if the running platform is windows.

Returns
-------
bool
    True if the running platform is windows."""""""
pandas/compat/__init__.py,"def is_platform_linux() -> bool:
    return sys.platform == 'linux'","""""""Checking if the running platform is linux.

Returns
-------
bool
    True if the running platform is linux."""""""
pandas/compat/__init__.py,"def is_platform_mac() -> bool:
    return sys.platform == 'darwin'","""""""Checking if the running platform is mac.

Returns
-------
bool
    True if the running platform is mac."""""""
pandas/compat/__init__.py,"def is_platform_arm() -> bool:
    return platform.machine() in ('arm64', 'aarch64') or platform.machine().startswith('armv')","""""""Checking if the running platform use ARM architecture.

Returns
-------
bool
    True if the running platform uses ARM architecture."""""""
pandas/compat/__init__.py,"def is_platform_power() -> bool:
    return platform.machine() in ('ppc64', 'ppc64le')","""""""Checking if the running platform use Power architecture.

Returns
-------
bool
    True if the running platform uses ARM architecture."""""""
pandas/compat/__init__.py,"def is_ci_environment() -> bool:
    return os.environ.get('PANDAS_CI', '0') == '1'","""""""Checking if running in a continuous integration environment by checking
the PANDAS_CI environment variable.

Returns
-------
bool
    True if the running in a continuous integration environment."""""""
pandas/compat/__init__.py,"def get_lzma_file() -> type[pandas.compat.compressors.LZMAFile]:
    if not pandas.compat.compressors.has_lzma:
        raise RuntimeError('lzma module not available. A Python re-install with the proper dependencies, might be required to solve this issue.')
    return pandas.compat.compressors.LZMAFile","""""""Importing the `LZMAFile` class from the `lzma` module.

Returns
-------
class
    The `LZMAFile` class from the `lzma` module.

Raises
------
RuntimeError
    If the `lzma` module was not imported correctly, or didn't exist."""""""
pandas/compat/__init__.py,"def get_bz2_file() -> type[pandas.compat.compressors.BZ2File]:
    if not pandas.compat.compressors.has_bz2:
        raise RuntimeError('bz2 module not available. A Python re-install with the proper dependencies, might be required to solve this issue.')
    return pandas.compat.compressors.BZ2File","""""""Importing the `BZ2File` class from the `bz2` module.

Returns
-------
class
    The `BZ2File` class from the `bz2` module.

Raises
------
RuntimeError
    If the `bz2` module was not imported correctly, or didn't exist."""""""
pandas/compat/_optional.py,"def import_optional_dependency(name: str, extra: str='', errors: str='raise', min_version: str | None=None):
    assert errors in {'warn', 'raise', 'ignore'}
    package_name = INSTALL_MAPPING.get(name)
    install_name = package_name if package_name is not None else name
    msg = f""Missing optional dependency '{install_name}'. {extra} Use pip or conda to install {install_name}.""
    try:
        module = importlib.import_module(name)
    except ImportError:
        if errors == 'raise':
            raise ImportError(msg)
        return None
    parent = name.split('.')[0]
    if parent != name:
        install_name = parent
        module_to_get = sys.modules[install_name]
    else:
        module_to_get = module
    minimum_version = min_version if min_version is not None else VERSIONS.get(parent)
    if minimum_version:
        version = get_version(module_to_get)
        if version and Version(version) < Version(minimum_version):
            msg = f""Pandas requires version '{minimum_version}' or newer of '{parent}' (version '{version}' currently installed).""
            if errors == 'warn':
                warnings.warn(msg, UserWarning, stacklevel=find_stack_level())
                return None
            elif errors == 'raise':
                raise ImportError(msg)
    return module","""""""Import an optional dependency.

By default, if a dependency is missing an ImportError with a nice
message will be raised. If a dependency is present, but too old,
we raise.

Parameters
----------
name : str
    The module name.
extra : str
    Additional text to include in the ImportError message.
errors : str {'raise', 'warn', 'ignore'}
    What to do when a dependency is not found or its version is too old.

    * raise : Raise an ImportError
    * warn : Only applicable when a module's version is to old.
      Warns that the version is too old and returns None
    * ignore: If the module is not installed, return None, otherwise,
      return the module, even if the version is too old.
      It's expected that users validate the version locally when
      using ``errors=""ignore""`` (see. ``io/html.py``)
min_version : str, default None
    Specify a minimum version that is different from the global pandas
    minimum version required.
Returns
-------
maybe_module : Optional[ModuleType]
    The imported module, when found and the version is correct.
    None is returned when the package is not found and `errors`
    is False, or when the package's version is too old and `errors`
    is ``'warn'``."""""""
pandas/compat/compressors.py,"def flatten_buffer(b: bytes | bytearray | memoryview | PickleBuffer) -> bytes | bytearray | memoryview:
    if isinstance(b, (bytes, bytearray)):
        return b
    if not isinstance(b, PickleBuffer):
        b = PickleBuffer(b)
    try:
        return b.raw()
    except BufferError:
        return memoryview(b).tobytes('A')","""""""Return some 1-D `uint8` typed buffer.

Coerces anything that does not match that description to one that does
without copying if possible (otherwise will copy)."""""""
pandas/compat/numpy/function.py,"def validate_argmin_with_skipna(skipna: bool | ndarray | None, args, kwargs) -> bool:
    (skipna, args) = process_skipna(skipna, args)
    validate_argmin(args, kwargs)
    return skipna","""""""If 'Series.argmin' is called via the 'numpy' library, the third parameter
in its signature is 'out', which takes either an ndarray or 'None', so
check if the 'skipna' parameter is either an instance of ndarray or is
None, since 'skipna' itself should be a boolean"""""""
pandas/compat/numpy/function.py,"def validate_argmax_with_skipna(skipna: bool | ndarray | None, args, kwargs) -> bool:
    (skipna, args) = process_skipna(skipna, args)
    validate_argmax(args, kwargs)
    return skipna","""""""If 'Series.argmax' is called via the 'numpy' library, the third parameter
in its signature is 'out', which takes either an ndarray or 'None', so
check if the 'skipna' parameter is either an instance of ndarray or is
None, since 'skipna' itself should be a boolean"""""""
pandas/compat/numpy/function.py,"def validate_argsort_with_ascending(ascending: bool | int | None, args, kwargs) -> bool:
    if is_integer(ascending) or ascending is None:
        args = (ascending,) + args
        ascending = True
    validate_argsort_kind(args, kwargs, max_fname_arg_count=3)
    ascending = cast(bool, ascending)
    return ascending","""""""If 'Categorical.argsort' is called via the 'numpy' library, the first
parameter in its signature is 'axis', which takes either an integer or
'None', so check if the 'ascending' parameter has either integer type or is
None, since 'ascending' itself should be a boolean"""""""
pandas/compat/numpy/function.py,"def validate_clip_with_axis(axis: ndarray | AxisNoneT, args, kwargs) -> AxisNoneT | None:
    if isinstance(axis, ndarray):
        args = (axis,) + args
        axis = None
    validate_clip(args, kwargs)
    return axis","""""""If 'NDFrame.clip' is called via the numpy library, the third parameter in
its signature is 'out', which can takes an ndarray, so check if the 'axis'
parameter is an instance of ndarray, since 'axis' itself should either be
an integer or None"""""""
pandas/compat/numpy/function.py,"def validate_cum_func_with_skipna(skipna: bool, args, kwargs, name) -> bool:
    if not is_bool(skipna):
        args = (skipna,) + args
        skipna = True
    elif isinstance(skipna, np.bool_):
        skipna = bool(skipna)
    validate_cum_func(args, kwargs, fname=name)
    return skipna","""""""If this function is called via the 'numpy' library, the third parameter in
its signature is 'dtype', which takes either a 'numpy' dtype or 'None', so
check if the 'skipna' parameter is a boolean or not"""""""
pandas/compat/numpy/function.py,"def validate_take_with_convert(convert: ndarray | bool | None, args, kwargs) -> bool:
    if isinstance(convert, ndarray) or convert is None:
        args = (convert,) + args
        convert = True
    validate_take(args, kwargs, max_fname_arg_count=3, method='both')
    return convert","""""""If this function is called via the 'numpy' library, the third parameter in
its signature is 'axis', which takes either an ndarray or 'None', so check
if the 'convert' parameter is either an instance of ndarray or is None"""""""
pandas/compat/numpy/function.py,"def validate_groupby_func(name: str, args, kwargs, allowed=None) -> None:
    if allowed is None:
        allowed = []
    kwargs = set(kwargs) - set(allowed)
    if len(args) + len(kwargs) > 0:
        raise UnsupportedFunctionCall(f'numpy operations are not valid with groupby. Use .groupby(...).{name}() instead')","""""""'args' and 'kwargs' should be empty, except for allowed kwargs because all
of their necessary parameters are explicitly listed in the function
signature"""""""
pandas/compat/numpy/function.py,"def validate_resampler_func(method: str, args, kwargs) -> None:
    if len(args) + len(kwargs) > 0:
        if method in RESAMPLER_NUMPY_OPS:
            raise UnsupportedFunctionCall(f'numpy operations are not valid with resample. Use .resample(...).{method}() instead')
        raise TypeError('too many arguments passed in')","""""""'args' and 'kwargs' should be empty because all of their necessary
parameters are explicitly listed in the function signature"""""""
pandas/compat/numpy/function.py,"def validate_minmax_axis(axis: AxisInt | None, ndim: int=1) -> None:
    if axis is None:
        return
    if axis >= ndim or (axis < 0 and ndim + axis < 0):
        raise ValueError(f'`axis` must be fewer than the number of dimensions ({ndim})')","""""""Ensure that the axis argument passed to min, max, argmin, or argmax is zero
or None, as otherwise it will be incorrectly ignored.

Parameters
----------
axis : int or None
ndim : int, default 1

Raises
------
ValueError"""""""
pandas/compat/pickle_compat.py,"def load(fh, encoding: str | None=None, is_verbose: bool=False):
    try:
        fh.seek(0)
        if encoding is not None:
            up = Unpickler(fh, encoding=encoding)
        else:
            up = Unpickler(fh)
        up.is_verbose = is_verbose
        return up.load()
    except (ValueError, TypeError):
        raise","""""""Load a pickle, with a provided encoding,

Parameters
----------
fh : a filelike object
encoding : an optional encoding
is_verbose : show exception output"""""""
pandas/compat/pickle_compat.py,"def loads(bytes_object: bytes, *, fix_imports: bool=True, encoding: str='ASCII', errors: str='strict'):
    fd = io.BytesIO(bytes_object)
    return Unpickler(fd, fix_imports=fix_imports, encoding=encoding, errors=errors).load()","""""""Analogous to pickle._loads."""""""
pandas/compat/pickle_compat.py,"@contextlib.contextmanager
def patch_pickle() -> Generator[None, None, None]:
    orig_loads = pkl.loads
    try:
        setattr(pkl, 'loads', loads)
        yield
    finally:
        setattr(pkl, 'loads', orig_loads)","""""""Temporarily patch pickle to use our unpickler."""""""
pandas/conftest.py,"def ignore_doctest_warning(item: pytest.Item, path: str, message: str) -> None:
    if item.name.endswith(path):
        item.add_marker(pytest.mark.filterwarnings(f'ignore:{message}'))","""""""Ignore doctest warning.

Parameters
----------
item : pytest.Item
    pytest test item.
path : str
    Module path to Python object, e.g. ""pandas.core.frame.DataFrame.append"". A
    warning will be filtered when item.name ends with in given path. So it is
    sufficient to specify e.g. ""DataFrame.append"".
message : str
    Message to be filtered."""""""
pandas/conftest.py,"@pytest.fixture
def add_doctest_imports(doctest_namespace) -> None:
    doctest_namespace['np'] = np
    doctest_namespace['pd'] = pd","""""""Make `np` and `pd` names available for doctests."""""""
pandas/conftest.py,"@pytest.fixture(autouse=True)
def configure_tests() -> None:
    pd.set_option('chained_assignment', 'raise')","""""""Configure settings for all tests and test modules."""""""
pandas/conftest.py,"@pytest.fixture(params=[0, 1, 'index', 'columns'], ids=lambda x: f'axis={repr(x)}')
def axis(request):
    return request.param","""""""Fixture for returning the axis numbers of a DataFrame."""""""
pandas/conftest.py,"@pytest.fixture(params=[1, 'columns'], ids=lambda x: f'axis={repr(x)}')
def axis_1(request):
    return request.param","""""""Fixture for returning aliases of axis 1 of a DataFrame."""""""
pandas/conftest.py,"@pytest.fixture(params=[True, False, None])
def observed(request):
    return request.param","""""""Pass in the observed keyword to groupby for [True, False]
This indicates whether categoricals should return values for
values which are not in the grouper [False / None], or only values which
appear in the grouper [True]. [None] is supported for future compatibility
if we decide to change the default (and would need to warn if this
parameter is not passed)."""""""
pandas/conftest.py,"@pytest.fixture(params=[True, False, None])
def ordered(request):
    return request.param","""""""Boolean 'ordered' parameter for Categorical."""""""
pandas/conftest.py,"@pytest.fixture(params=[True, False])
def skipna(request):
    return request.param","""""""Boolean 'skipna' parameter."""""""
pandas/conftest.py,"@pytest.fixture(params=['first', 'last', False])
def keep(request):
    return request.param","""""""Valid values for the 'keep' parameter used in
.duplicated or .drop_duplicates"""""""
pandas/conftest.py,"@pytest.fixture(params=['both', 'neither', 'left', 'right'])
def inclusive_endpoints_fixture(request):
    return request.param","""""""Fixture for trying all interval 'inclusive' parameters."""""""
pandas/conftest.py,"@pytest.fixture(params=['left', 'right', 'both', 'neither'])
def closed(request):
    return request.param","""""""Fixture for trying all interval closed parameters."""""""
pandas/conftest.py,"@pytest.fixture(params=['left', 'right', 'both', 'neither'])
def other_closed(request):
    return request.param","""""""Secondary closed fixture to allow parametrizing over all pairs of closed."""""""
pandas/conftest.py,"@pytest.fixture(params=[None, 'gzip', 'bz2', 'zip', 'xz', 'tar', pytest.param('zstd', marks=td.skip_if_no('zstandard'))])
def compression(request):
    return request.param","""""""Fixture for trying common compression types in compression tests."""""""
pandas/conftest.py,"@pytest.fixture(params=['gzip', 'bz2', 'zip', 'xz', 'tar', pytest.param('zstd', marks=td.skip_if_no('zstandard'))])
def compression_only(request):
    return request.param","""""""Fixture for trying common compression types in compression tests excluding
uncompressed case."""""""
pandas/conftest.py,"@pytest.fixture(params=[True, False])
def writable(request):
    return request.param","""""""Fixture that an array is writable."""""""
pandas/conftest.py,"@pytest.fixture(params=['inner', 'outer', 'left', 'right'])
def join_type(request):
    return request.param","""""""Fixture for trying all types of join operations."""""""
pandas/conftest.py,"@pytest.fixture(params=['nlargest', 'nsmallest'])
def nselect_method(request):
    return request.param","""""""Fixture for trying all nselect methods."""""""
pandas/conftest.py,"@pytest.fixture(params=tm.NULL_OBJECTS, ids=lambda x: type(x).__name__)
def nulls_fixture(request):
    return request.param","""""""Fixture for each null type in pandas."""""""
pandas/conftest.py,"@pytest.fixture(params=[None, np.nan, pd.NaT])
def unique_nulls_fixture(request):
    return request.param","""""""Fixture for each null type in pandas, each null type exactly once."""""""
pandas/conftest.py,"@pytest.fixture(params=tm.NP_NAT_OBJECTS, ids=lambda x: type(x).__name__)
def np_nat_fixture(request):
    return request.param","""""""Fixture for each NaT type in numpy."""""""
pandas/conftest.py,"@pytest.fixture(params=[DataFrame, Series])
def frame_or_series(request):
    return request.param","""""""Fixture to parametrize over DataFrame and Series."""""""
pandas/conftest.py,"@pytest.fixture(params=[Index, Series], ids=['index', 'series'])
def index_or_series(request):
    return request.param","""""""Fixture to parametrize over Index and Series, made necessary by a mypy
bug, giving an error:

List item 0 has incompatible type ""Type[Series]""; expected ""Type[PandasObject]""

See GH#29725"""""""
pandas/conftest.py,"@pytest.fixture(params=[Index, Series, pd.array], ids=['index', 'series', 'array'])
def index_or_series_or_array(request):
    return request.param","""""""Fixture to parametrize over Index, Series, and ExtensionArray"""""""
pandas/conftest.py,"@pytest.fixture(params=[Index, Series, DataFrame, pd.array], ids=lambda x: x.__name__)
def box_with_array(request):
    return request.param","""""""Fixture to test behavior for Index, Series, DataFrame, and pandas Array
classes"""""""
pandas/conftest.py,"@pytest.fixture
def dict_subclass() -> type[dict]:

    class TestSubDict(dict):

        def __init__(self, *args, **kwargs) -> None:
            dict.__init__(self, *args, **kwargs)
    return TestSubDict","""""""Fixture for a dictionary subclass."""""""
pandas/conftest.py,"@pytest.fixture
def non_dict_mapping_subclass() -> type[abc.Mapping]:

    class TestNonDictMapping(abc.Mapping):

        def __init__(self, underlying_dict) -> None:
            self._data = underlying_dict

        def __getitem__(self, key):
            return self._data.__getitem__(key)

        def __iter__(self) -> Iterator:
            return self._data.__iter__()

        def __len__(self) -> int:
            return self._data.__len__()
    return TestNonDictMapping","""""""Fixture for a non-mapping dictionary subclass."""""""
pandas/conftest.py,"@pytest.fixture
def multiindex_year_month_day_dataframe_random_data():
    tdf = tm.makeTimeDataFrame(100)
    ymd = tdf.groupby([lambda x: x.year, lambda x: x.month, lambda x: x.day]).sum()
    ymd.index = ymd.index.set_levels([lev.astype('i8') for lev in ymd.index.levels])
    ymd.index.set_names(['year', 'month', 'day'], inplace=True)
    return ymd","""""""DataFrame with 3 level MultiIndex (year, month, day) covering
first 100 business days from 2000-01-01 with random data"""""""
pandas/conftest.py,"@pytest.fixture
def lexsorted_two_level_string_multiindex() -> MultiIndex:
    return MultiIndex(levels=[['foo', 'bar', 'baz', 'qux'], ['one', 'two', 'three']], codes=[[0, 0, 0, 1, 1, 2, 2, 3, 3, 3], [0, 1, 2, 0, 1, 1, 2, 0, 1, 2]], names=['first', 'second'])","""""""2-level MultiIndex, lexsorted, with string names."""""""
pandas/conftest.py,"@pytest.fixture
def multiindex_dataframe_random_data(lexsorted_two_level_string_multiindex) -> DataFrame:
    index = lexsorted_two_level_string_multiindex
    return DataFrame(np.random.default_rng(2).standard_normal((10, 3)), index=index, columns=Index(['A', 'B', 'C'], name='exp'))","""""""DataFrame with 2 level MultiIndex with random data"""""""
pandas/conftest.py,"def _create_multiindex():
    major_axis = Index(['foo', 'bar', 'baz', 'qux'])
    minor_axis = Index(['one', 'two'])
    major_codes = np.array([0, 0, 1, 2, 3, 3])
    minor_codes = np.array([0, 1, 0, 1, 0, 1])
    index_names = ['first', 'second']
    return MultiIndex(levels=[major_axis, minor_axis], codes=[major_codes, minor_codes], names=index_names, verify_integrity=False)","""""""MultiIndex used to test the general functionality of this object"""""""
pandas/conftest.py,"def _create_mi_with_dt64tz_level():
    return MultiIndex.from_product([[1, 2], ['a', 'b'], pd.date_range('20130101', periods=3, tz='US/Eastern')], names=['one', 'two', 'three'])","""""""MultiIndex with a level that is a tzaware DatetimeIndex."""""""
pandas/conftest.py,"@pytest.fixture(params=indices_dict.keys())
def index(request):
    return indices_dict[request.param].copy()","""""""Fixture for many ""simple"" kinds of indices.

These indices are unlikely to cover corner cases, e.g.
    - no names
    - no NaTs/NaNs
    - no values near implementation bounds
    - ..."""""""
pandas/conftest.py,"@pytest.fixture(params=[key for (key, value) in indices_dict.items() if not isinstance(value, MultiIndex)])
def index_flat(request):
    key = request.param
    return indices_dict[key].copy()","""""""index fixture, but excluding MultiIndex cases."""""""
pandas/conftest.py,"@pytest.fixture(params=[key for (key, value) in indices_dict.items() if not (key.startswith(('int', 'uint', 'float')) or key in ['range', 'empty', 'repeats', 'bool-dtype']) and (not isinstance(value, MultiIndex))])
def index_with_missing(request):
    ind = indices_dict[request.param].copy(deep=True)
    vals = ind.values.copy()
    if request.param in ['tuples', 'mi-with-dt64tz-level', 'multi']:
        vals = ind.tolist()
        vals[0] = (None,) + vals[0][1:]
        vals[-1] = (None,) + vals[-1][1:]
        return MultiIndex.from_tuples(vals)
    else:
        vals[0] = None
        vals[-1] = None
        return type(ind)(vals)","""""""Fixture for indices with missing values.

Integer-dtype and empty cases are excluded because they cannot hold missing
values.

MultiIndex is excluded because isna() is not defined for MultiIndex."""""""
pandas/conftest.py,"@pytest.fixture
def string_series() -> Series:
    s = tm.makeStringSeries()
    s.name = 'series'
    return s","""""""Fixture for Series of floats with Index of unique strings"""""""
pandas/conftest.py,"@pytest.fixture
def object_series() -> Series:
    s = tm.makeObjectSeries()
    s.name = 'objects'
    return s","""""""Fixture for Series of dtype object with Index of unique strings"""""""
pandas/conftest.py,"@pytest.fixture
def datetime_series() -> Series:
    s = tm.makeTimeSeries()
    s.name = 'ts'
    return s","""""""Fixture for Series of floats with DatetimeIndex"""""""
pandas/conftest.py,"def _create_series(index):
    size = len(index)
    data = np.random.default_rng(2).standard_normal(size)
    return Series(data, index=index, name='a', copy=False)","""""""Helper for the _series dict"""""""
pandas/conftest.py,"@pytest.fixture
def series_with_simple_index(index) -> Series:
    return _create_series(index)","""""""Fixture for tests on series with changing types of indices."""""""
pandas/conftest.py,"@pytest.fixture
def series_with_multilevel_index() -> Series:
    arrays = [['bar', 'bar', 'baz', 'baz', 'qux', 'qux', 'foo', 'foo'], ['one', 'two', 'one', 'two', 'one', 'two', 'one', 'two']]
    tuples = zip(*arrays)
    index = MultiIndex.from_tuples(tuples)
    data = np.random.default_rng(2).standard_normal(8)
    ser = Series(data, index=index)
    ser.iloc[3] = np.nan
    return ser","""""""Fixture with a Series with a 2-level MultiIndex."""""""
pandas/conftest.py,"@pytest.fixture(params=_index_or_series_objs.keys())
def index_or_series_obj(request):
    return _index_or_series_objs[request.param].copy(deep=True)","""""""Fixture for tests on indexes, series and series with a narrow dtype
copy to avoid mutation, e.g. setting .name"""""""
pandas/conftest.py,"@pytest.fixture(params=_index_or_series_memory_objs.keys())
def index_or_series_memory_obj(request):
    return _index_or_series_memory_objs[request.param].copy(deep=True)","""""""Fixture for tests on indexes, series, series with a narrow dtype and
series with empty objects type
copy to avoid mutation, e.g. setting .name"""""""
pandas/conftest.py,"@pytest.fixture
def int_frame() -> DataFrame:
    return DataFrame(tm.getSeriesData()).astype('int64')","""""""Fixture for DataFrame of ints with index of unique strings

Columns are ['A', 'B', 'C', 'D']

            A  B  C  D
vpBeWjM651  1  0  1  0
5JyxmrP1En -1  0  0  0
qEDaoD49U2 -1  1  0  0
m66TkTfsFe  0  0  0  0
EHPaNzEUFm -1  0 -1  0
fpRJCevQhi  2  0  0  0
OlQvnmfi3Q  0  0 -2  0
...        .. .. .. ..
uB1FPlz4uP  0  0  0  1
EcSe6yNzCU  0  0 -1  0
L50VudaiI8 -1  1 -2  0
y3bpw4nwIp  0 -1  0  0
H0RdLLwrCT  1  1  0  0
rY82K0vMwm  0  0  0  0
1OPIUjnkjk  2  0  0  0

[30 rows x 4 columns]"""""""
pandas/conftest.py,"@pytest.fixture
def datetime_frame() -> DataFrame:
    return DataFrame(tm.getTimeSeriesData())","""""""Fixture for DataFrame of floats with DatetimeIndex

Columns are ['A', 'B', 'C', 'D']

                   A         B         C         D
2000-01-03 -1.122153  0.468535  0.122226  1.693711
2000-01-04  0.189378  0.486100  0.007864 -1.216052
2000-01-05  0.041401 -0.835752 -0.035279 -0.414357
2000-01-06  0.430050  0.894352  0.090719  0.036939
2000-01-07 -0.620982 -0.668211 -0.706153  1.466335
2000-01-10 -0.752633  0.328434 -0.815325  0.699674
2000-01-11 -2.236969  0.615737 -0.829076 -1.196106
...              ...       ...       ...       ...
2000-02-03  1.642618 -0.579288  0.046005  1.385249
2000-02-04 -0.544873 -1.160962 -0.284071 -1.418351
2000-02-07 -2.656149 -0.601387  1.410148  0.444150
2000-02-08 -1.201881 -1.289040  0.772992 -1.445300
2000-02-09  1.377373  0.398619  1.008453 -0.928207
2000-02-10  0.473194 -0.636677  0.984058  0.511519
2000-02-11 -0.965556  0.408313 -1.312844 -0.381948

[30 rows x 4 columns]"""""""
pandas/conftest.py,"@pytest.fixture
def float_frame() -> DataFrame:
    return DataFrame(tm.getSeriesData())","""""""Fixture for DataFrame of floats with index of unique strings

Columns are ['A', 'B', 'C', 'D'].

                   A         B         C         D
P7GACiRnxd -0.465578 -0.361863  0.886172 -0.053465
qZKh6afn8n -0.466693 -0.373773  0.266873  1.673901
tkp0r6Qble  0.148691 -0.059051  0.174817  1.598433
wP70WOCtv8  0.133045 -0.581994 -0.992240  0.261651
M2AeYQMnCz -1.207959 -0.185775  0.588206  0.563938
QEPzyGDYDo -0.381843 -0.758281  0.502575 -0.565053
r78Jwns6dn -0.653707  0.883127  0.682199  0.206159
...              ...       ...       ...       ...
IHEGx9NO0T -0.277360  0.113021 -1.018314  0.196316
lPMj8K27FA -1.313667 -0.604776 -1.305618 -0.863999
qa66YMWQa5  1.110525  0.475310 -0.747865  0.032121
yOa0ATsmcE -0.431457  0.067094  0.096567 -0.264962
65znX3uRNG  1.528446  0.160416 -0.109635 -0.032987
eCOBvKqf3e  0.235281  1.622222  0.781255  0.392871
xSucinXxuV -1.263557  0.252799 -0.552247  0.400426

[30 rows x 4 columns]"""""""
pandas/conftest.py,"@pytest.fixture
def mixed_type_frame() -> DataFrame:
    return DataFrame({'a': 1.0, 'b': 2, 'c': 'foo', 'float32': np.array([1.0] * 10, dtype='float32'), 'int32': np.array([1] * 10, dtype='int32')}, index=np.arange(10))","""""""Fixture for DataFrame of float/int/string columns with RangeIndex
Columns are ['a', 'b', 'c', 'float32', 'int32']."""""""
pandas/conftest.py,"@pytest.fixture
def rand_series_with_duplicate_datetimeindex() -> Series:
    dates = [datetime(2000, 1, 2), datetime(2000, 1, 2), datetime(2000, 1, 2), datetime(2000, 1, 3), datetime(2000, 1, 3), datetime(2000, 1, 3), datetime(2000, 1, 4), datetime(2000, 1, 4), datetime(2000, 1, 4), datetime(2000, 1, 5)]
    return Series(np.random.default_rng(2).standard_normal(len(dates)), index=dates)","""""""Fixture for Series with a DatetimeIndex that has duplicates."""""""
pandas/conftest.py,"@pytest.fixture(params=tm.arithmetic_dunder_methods)
def all_arithmetic_operators(request):
    return request.param","""""""Fixture for dunder names for common arithmetic operations."""""""
pandas/conftest.py,"@pytest.fixture(params=[operator.add, ops.radd, operator.sub, ops.rsub, operator.mul, ops.rmul, operator.truediv, ops.rtruediv, operator.floordiv, ops.rfloordiv, operator.mod, ops.rmod, operator.pow, ops.rpow, operator.eq, operator.ne, operator.lt, operator.le, operator.gt, operator.ge, operator.and_, ops.rand_, operator.xor, ops.rxor, operator.or_, ops.ror_])
def all_binary_operators(request):
    return request.param","""""""Fixture for operator and roperator arithmetic, comparison, and logical ops."""""""
pandas/conftest.py,"@pytest.fixture(params=[operator.add, ops.radd, operator.sub, ops.rsub, operator.mul, ops.rmul, operator.truediv, ops.rtruediv, operator.floordiv, ops.rfloordiv, operator.mod, ops.rmod, operator.pow, ops.rpow])
def all_arithmetic_functions(request):
    return request.param","""""""Fixture for operator and roperator arithmetic functions.

Notes
-----
This includes divmod and rdivmod, whereas all_arithmetic_operators
does not."""""""
pandas/conftest.py,"@pytest.fixture(params=_all_numeric_reductions)
def all_numeric_reductions(request):
    return request.param","""""""Fixture for numeric reduction names."""""""
pandas/conftest.py,"@pytest.fixture(params=_all_boolean_reductions)
def all_boolean_reductions(request):
    return request.param","""""""Fixture for boolean reduction names."""""""
pandas/conftest.py,"@pytest.fixture(params=_all_reductions)
def all_reductions(request):
    return request.param","""""""Fixture for all (boolean + numeric) reduction names."""""""
pandas/conftest.py,"@pytest.fixture(params=[operator.eq, operator.ne, operator.gt, operator.ge, operator.lt, operator.le])
def comparison_op(request):
    return request.param","""""""Fixture for operator module comparison functions."""""""
pandas/conftest.py,"@pytest.fixture(params=['__le__', '__lt__', '__ge__', '__gt__'])
def compare_operators_no_eq_ne(request):
    return request.param","""""""Fixture for dunder names for compare operations except == and !=

* >=
* >
* <
* <="""""""
pandas/conftest.py,"@pytest.fixture(params=['__and__', '__rand__', '__or__', '__ror__', '__xor__', '__rxor__'])
def all_logical_operators(request):
    return request.param","""""""Fixture for dunder names for common logical operations

* |
* &
* ^"""""""
pandas/conftest.py,"@pytest.fixture(params=_all_numeric_accumulations)
def all_numeric_accumulations(request):
    return request.param","""""""Fixture for numeric accumulation names"""""""
pandas/conftest.py,"@pytest.fixture
def strict_data_files(pytestconfig):
    return pytestconfig.getoption('--no-strict-data-files')","""""""Returns the configuration for the test setting `--no-strict-data-files`."""""""
pandas/conftest.py,"@pytest.fixture
def datapath(strict_data_files: str) -> Callable[..., str]:
    BASE_PATH = os.path.join(os.path.dirname(__file__), 'tests')

    def deco(*args):
        path = os.path.join(BASE_PATH, *args)
        if not os.path.exists(path):
            if strict_data_files:
                raise ValueError(f'Could not find file {path} and --no-strict-data-files is not set.')
            pytest.skip(f'Could not find {path}.')
        return path
    return deco","""""""Get the path to a data file.

Parameters
----------
path : str
    Path to the file, relative to ``pandas/tests/``

Returns
-------
path including ``pandas/tests``.

Raises
------
ValueError
    If the path doesn't exist and the --no-strict-data-files option is not set."""""""
pandas/conftest.py,"@pytest.fixture
def iris(datapath) -> DataFrame:
    return pd.read_csv(datapath('io', 'data', 'csv', 'iris.csv'))","""""""The iris dataset as a DataFrame."""""""
pandas/conftest.py,"@td.parametrize_fixture_doc(str(TIMEZONE_IDS))
@pytest.fixture(params=TIMEZONES, ids=TIMEZONE_IDS)
def tz_naive_fixture(request):
    return request.param","""""""Fixture for trying timezones including default (None): {0}"""""""
pandas/conftest.py,"@td.parametrize_fixture_doc(str(TIMEZONE_IDS[1:]))
@pytest.fixture(params=TIMEZONES[1:], ids=TIMEZONE_IDS[1:])
def tz_aware_fixture(request):
    return request.param","""""""Fixture for trying explicit timezones: {0}"""""""
pandas/conftest.py,"@pytest.fixture(params=_UTCS)
def utc_fixture(request):
    return request.param","""""""Fixture to provide variants of UTC timezone strings and tzinfo objects."""""""
pandas/conftest.py,"@pytest.fixture(params=tm.STRING_DTYPES)
def string_dtype(request):
    return request.param","""""""Parametrized fixture for string dtypes.

* str
* 'str'
* 'U'"""""""
pandas/conftest.py,"@pytest.fixture(params=['string[python]', pytest.param('string[pyarrow]', marks=td.skip_if_no('pyarrow'))])
def nullable_string_dtype(request):
    return request.param","""""""Parametrized fixture for string dtypes.

* 'string[python]'
* 'string[pyarrow]'"""""""
pandas/conftest.py,"@pytest.fixture(params=['python', pytest.param('pyarrow', marks=td.skip_if_no('pyarrow')), pytest.param('pyarrow_numpy', marks=td.skip_if_no('pyarrow'))])
def string_storage(request):
    return request.param","""""""Parametrized fixture for pd.options.mode.string_storage.

* 'python'
* 'pyarrow'
* 'pyarrow_numpy'"""""""
pandas/conftest.py,"@pytest.fixture(params=['numpy_nullable', pytest.param('pyarrow', marks=td.skip_if_no('pyarrow'))])
def dtype_backend(request):
    return request.param","""""""Parametrized fixture for pd.options.mode.string_storage.

* 'python'
* 'pyarrow'"""""""
pandas/conftest.py,"@pytest.fixture(params=tm.BYTES_DTYPES)
def bytes_dtype(request):
    return request.param","""""""Parametrized fixture for bytes dtypes.

* bytes
* 'bytes'"""""""
pandas/conftest.py,"@pytest.fixture(params=tm.OBJECT_DTYPES)
def object_dtype(request):
    return request.param","""""""Parametrized fixture for object dtypes.

* object
* 'object'"""""""
pandas/conftest.py,"@pytest.fixture(params=['object', 'string[python]', pytest.param('string[pyarrow]', marks=td.skip_if_no('pyarrow')), pytest.param('string[pyarrow_numpy]', marks=td.skip_if_no('pyarrow'))])
def any_string_dtype(request):
    return request.param","""""""Parametrized fixture for string dtypes.
* 'object'
* 'string[python]'
* 'string[pyarrow]'"""""""
pandas/conftest.py,"@pytest.fixture(params=tm.DATETIME64_DTYPES)
def datetime64_dtype(request):
    return request.param","""""""Parametrized fixture for datetime64 dtypes.

* 'datetime64[ns]'
* 'M8[ns]'"""""""
pandas/conftest.py,"@pytest.fixture(params=tm.TIMEDELTA64_DTYPES)
def timedelta64_dtype(request):
    return request.param","""""""Parametrized fixture for timedelta64 dtypes.

* 'timedelta64[ns]'
* 'm8[ns]'"""""""
pandas/conftest.py,"@pytest.fixture
def fixed_now_ts() -> Timestamp:
    return Timestamp(year=2021, month=1, day=1, hour=12, minute=4, second=13, microsecond=22)","""""""Fixture emits fixed Timestamp.now()"""""""
pandas/conftest.py,"@pytest.fixture(params=tm.FLOAT_NUMPY_DTYPES)
def float_numpy_dtype(request):
    return request.param","""""""Parameterized fixture for float dtypes.

* float
* 'float32'
* 'float64'"""""""
pandas/conftest.py,"@pytest.fixture(params=tm.FLOAT_EA_DTYPES)
def float_ea_dtype(request):
    return request.param","""""""Parameterized fixture for float dtypes.

* 'Float32'
* 'Float64'"""""""
pandas/conftest.py,"@pytest.fixture(params=tm.ALL_FLOAT_DTYPES)
def any_float_dtype(request):
    return request.param","""""""Parameterized fixture for float dtypes.

* float
* 'float32'
* 'float64'
* 'Float32'
* 'Float64'"""""""
pandas/conftest.py,"@pytest.fixture(params=tm.COMPLEX_DTYPES)
def complex_dtype(request):
    return request.param","""""""Parameterized fixture for complex dtypes.

* complex
* 'complex64'
* 'complex128'"""""""
pandas/conftest.py,"@pytest.fixture(params=tm.SIGNED_INT_NUMPY_DTYPES)
def any_signed_int_numpy_dtype(request):
    return request.param","""""""Parameterized fixture for signed integer dtypes.

* int
* 'int8'
* 'int16'
* 'int32'
* 'int64'"""""""
pandas/conftest.py,"@pytest.fixture(params=tm.UNSIGNED_INT_NUMPY_DTYPES)
def any_unsigned_int_numpy_dtype(request):
    return request.param","""""""Parameterized fixture for unsigned integer dtypes.

* 'uint8'
* 'uint16'
* 'uint32'
* 'uint64'"""""""
pandas/conftest.py,"@pytest.fixture(params=tm.ALL_INT_NUMPY_DTYPES)
def any_int_numpy_dtype(request):
    return request.param","""""""Parameterized fixture for any integer dtype.

* int
* 'int8'
* 'uint8'
* 'int16'
* 'uint16'
* 'int32'
* 'uint32'
* 'int64'
* 'uint64'"""""""
pandas/conftest.py,"@pytest.fixture(params=tm.ALL_INT_EA_DTYPES)
def any_int_ea_dtype(request):
    return request.param","""""""Parameterized fixture for any nullable integer dtype.

* 'UInt8'
* 'Int8'
* 'UInt16'
* 'Int16'
* 'UInt32'
* 'Int32'
* 'UInt64'
* 'Int64'"""""""
pandas/conftest.py,"@pytest.fixture(params=tm.ALL_INT_DTYPES)
def any_int_dtype(request):
    return request.param","""""""Parameterized fixture for any nullable integer dtype.

* int
* 'int8'
* 'uint8'
* 'int16'
* 'uint16'
* 'int32'
* 'uint32'
* 'int64'
* 'uint64'
* 'UInt8'
* 'Int8'
* 'UInt16'
* 'Int16'
* 'UInt32'
* 'Int32'
* 'UInt64'
* 'Int64'"""""""
pandas/conftest.py,"@pytest.fixture(params=tm.ALL_INT_EA_DTYPES + tm.FLOAT_EA_DTYPES)
def any_numeric_ea_dtype(request):
    return request.param","""""""Parameterized fixture for any nullable integer dtype and
any float ea dtypes.

* 'UInt8'
* 'Int8'
* 'UInt16'
* 'Int16'
* 'UInt32'
* 'Int32'
* 'UInt64'
* 'Int64'
* 'Float32'
* 'Float64'"""""""
pandas/conftest.py,"@pytest.fixture(params=tm.ALL_INT_EA_DTYPES + tm.FLOAT_EA_DTYPES + tm.ALL_INT_PYARROW_DTYPES_STR_REPR + tm.FLOAT_PYARROW_DTYPES_STR_REPR)
def any_numeric_ea_and_arrow_dtype(request):
    return request.param","""""""Parameterized fixture for any nullable integer dtype and
any float ea dtypes.

* 'UInt8'
* 'Int8'
* 'UInt16'
* 'Int16'
* 'UInt32'
* 'Int32'
* 'UInt64'
* 'Int64'
* 'Float32'
* 'Float64'
* 'uint8[pyarrow]'
* 'int8[pyarrow]'
* 'uint16[pyarrow]'
* 'int16[pyarrow]'
* 'uint32[pyarrow]'
* 'int32[pyarrow]'
* 'uint64[pyarrow]'
* 'int64[pyarrow]'
* 'float32[pyarrow]'
* 'float64[pyarrow]'"""""""
pandas/conftest.py,"@pytest.fixture(params=tm.SIGNED_INT_EA_DTYPES)
def any_signed_int_ea_dtype(request):
    return request.param","""""""Parameterized fixture for any signed nullable integer dtype.

* 'Int8'
* 'Int16'
* 'Int32'
* 'Int64'"""""""
pandas/conftest.py,"@pytest.fixture(params=tm.ALL_REAL_NUMPY_DTYPES)
def any_real_numpy_dtype(request):
    return request.param","""""""Parameterized fixture for any (purely) real numeric dtype.

* int
* 'int8'
* 'uint8'
* 'int16'
* 'uint16'
* 'int32'
* 'uint32'
* 'int64'
* 'uint64'
* float
* 'float32'
* 'float64'"""""""
pandas/conftest.py,"@pytest.fixture(params=tm.ALL_REAL_DTYPES)
def any_real_numeric_dtype(request):
    return request.param","""""""Parameterized fixture for any (purely) real numeric dtype.

* int
* 'int8'
* 'uint8'
* 'int16'
* 'uint16'
* 'int32'
* 'uint32'
* 'int64'
* 'uint64'
* float
* 'float32'
* 'float64'

and associated ea dtypes."""""""
pandas/conftest.py,"@pytest.fixture(params=tm.ALL_NUMPY_DTYPES)
def any_numpy_dtype(request):
    return request.param","""""""Parameterized fixture for all numpy dtypes.

* bool
* 'bool'
* int
* 'int8'
* 'uint8'
* 'int16'
* 'uint16'
* 'int32'
* 'uint32'
* 'int64'
* 'uint64'
* float
* 'float32'
* 'float64'
* complex
* 'complex64'
* 'complex128'
* str
* 'str'
* 'U'
* bytes
* 'bytes'
* 'datetime64[ns]'
* 'M8[ns]'
* 'timedelta64[ns]'
* 'm8[ns]'
* object
* 'object'"""""""
pandas/conftest.py,"@pytest.fixture(params=tm.ALL_NUMERIC_DTYPES)
def any_numeric_dtype(request):
    return request.param","""""""Parameterized fixture for all numeric dtypes.

* int
* 'int8'
* 'uint8'
* 'int16'
* 'uint16'
* 'int32'
* 'uint32'
* 'int64'
* 'uint64'
* float
* 'float32'
* 'float64'
* complex
* 'complex64'
* 'complex128'
* 'UInt8'
* 'Int8'
* 'UInt16'
* 'Int16'
* 'UInt32'
* 'Int32'
* 'UInt64'
* 'Int64'
* 'Float32'
* 'Float64'"""""""
pandas/conftest.py,"@pytest.fixture(params=_any_skipna_inferred_dtype, ids=ids)
def any_skipna_inferred_dtype(request):
    (inferred_dtype, values) = request.param
    values = np.array(values, dtype=object)
    return (inferred_dtype, values)","""""""Fixture for all inferred dtypes from _libs.lib.infer_dtype

The covered (inferred) types are:
* 'string'
* 'empty'
* 'bytes'
* 'mixed'
* 'mixed-integer'
* 'mixed-integer-float'
* 'floating'
* 'integer'
* 'decimal'
* 'boolean'
* 'datetime64'
* 'datetime'
* 'date'
* 'timedelta'
* 'time'
* 'period'
* 'interval'

Returns
-------
inferred_dtype : str
    The string for the inferred dtype from _libs.lib.infer_dtype
values : np.ndarray
    An array of object dtype that will be inferred to have
    `inferred_dtype`

Examples
--------
>>> from pandas._libs import lib
>>>
>>> def test_something(any_skipna_inferred_dtype):
...     inferred_dtype, values = any_skipna_inferred_dtype
...     # will pass
...     assert lib.infer_dtype(values, skipna=True) == inferred_dtype"""""""
pandas/conftest.py,"@pytest.fixture
def ip():
    pytest.importorskip('IPython', minversion='6.0.0')
    from IPython.core.interactiveshell import InteractiveShell
    from traitlets.config import Config
    c = Config()
    c.HistoryManager.hist_file = ':memory:'
    return InteractiveShell(config=c)","""""""Get an instance of IPython.InteractiveShell.

Will raise a skip if IPython is not installed."""""""
pandas/conftest.py,"@pytest.fixture(params=['bsr', 'coo', 'csc', 'csr', 'dia', 'dok', 'lil'])
def spmatrix(request):
    sparse = pytest.importorskip('scipy.sparse')
    return getattr(sparse, request.param + '_matrix')","""""""Yields scipy sparse matrix classes."""""""
pandas/conftest.py,"@pytest.fixture(params=[getattr(pd.offsets, o) for o in pd.offsets.__all__ if issubclass(getattr(pd.offsets, o), pd.offsets.Tick) and o != 'Tick'])
def tick_classes(request):
    return request.param","""""""Fixture for Tick based datetime offsets available for a time series."""""""
pandas/conftest.py,"@pytest.fixture(params=[None, lambda x: x])
def sort_by_key(request):
    return request.param","""""""Simple fixture for testing keys in sorting methods.
Tests None (no key) and the identity key."""""""
pandas/conftest.py,"@pytest.fixture(params=[('foo', None, None), ('Egon', 'Venkman', None), ('NCC1701D', 'NCC1701D', 'NCC1701D'), (np.nan, np.nan, np.nan), (np.nan, pd.NaT, None), (np.nan, pd.NA, None), (pd.NA, pd.NA, pd.NA)])
def names(request) -> tuple[Hashable, Hashable, Hashable]:
    return request.param","""""""A 3-tuple of names, the first two for operands, the last for a result."""""""
pandas/conftest.py,"@pytest.fixture(params=[tm.setitem, tm.loc, tm.iloc])
def indexer_sli(request):
    return request.param","""""""Parametrize over __setitem__, loc.__setitem__, iloc.__setitem__"""""""
pandas/conftest.py,"@pytest.fixture(params=[tm.loc, tm.iloc])
def indexer_li(request):
    return request.param","""""""Parametrize over loc.__getitem__, iloc.__getitem__"""""""
pandas/conftest.py,"@pytest.fixture(params=[tm.setitem, tm.iloc])
def indexer_si(request):
    return request.param","""""""Parametrize over __setitem__, iloc.__setitem__"""""""
pandas/conftest.py,"@pytest.fixture(params=[tm.setitem, tm.loc])
def indexer_sl(request):
    return request.param","""""""Parametrize over __setitem__, loc.__setitem__"""""""
pandas/conftest.py,"@pytest.fixture(params=[tm.at, tm.loc])
def indexer_al(request):
    return request.param","""""""Parametrize over at.__setitem__, loc.__setitem__"""""""
pandas/conftest.py,"@pytest.fixture(params=[tm.iat, tm.iloc])
def indexer_ial(request):
    return request.param","""""""Parametrize over iat.__setitem__, iloc.__setitem__"""""""
pandas/conftest.py,"@pytest.fixture
def using_array_manager() -> bool:
    return _get_option('mode.data_manager', silent=True) == 'array'","""""""Fixture to check if the array manager is being used."""""""
pandas/conftest.py,"@pytest.fixture
def using_copy_on_write() -> bool:
    return pd.options.mode.copy_on_write and _get_option('mode.data_manager', silent=True) == 'block'","""""""Fixture to check if Copy-on-Write is enabled."""""""
pandas/conftest.py,"@pytest.fixture(params=warsaws)
def warsaw(request) -> str:
    return request.param","""""""tzinfo for Europe/Warsaw using pytz, dateutil, or zoneinfo."""""""
pandas/core/_numba/executor.py,"def generate_shared_aggregator(func: Callable[..., Scalar], dtype_mapping: dict[np.dtype, np.dtype], is_grouped_kernel: bool, nopython: bool, nogil: bool, parallel: bool):

    def looper_wrapper(values, start=None, end=None, labels=None, ngroups=None, min_periods: int=0, **kwargs):
        result_dtype = dtype_mapping[values.dtype]
        column_looper = make_looper(func, result_dtype, is_grouped_kernel, nopython, nogil, parallel)
        if is_grouped_kernel:
            (result, na_positions) = column_looper(values, labels, ngroups, min_periods, *kwargs.values())
        else:
            (result, na_positions) = column_looper(values, start, end, min_periods, *kwargs.values())
        if result.dtype.kind == 'i':
            for na_pos in na_positions.values():
                if len(na_pos) > 0:
                    result = result.astype('float64')
                    break
        for (i, na_pos) in na_positions.items():
            if len(na_pos) > 0:
                result[i, na_pos] = np.nan
        return result
    return looper_wrapper","""""""Generate a Numba function that loops over the columns 2D object and applies
a 1D numba kernel over each column.

Parameters
----------
func : function
    aggregation function to be applied to each column
dtype_mapping: dict or None
    If not None, maps a dtype to a result dtype.
    Otherwise, will fall back to default mapping.
is_grouped_kernel: bool, default False
    Whether func operates using the group labels (True)
    or using starts/ends arrays

    If true, you also need to pass the number of groups to this function
nopython : bool
    nopython to be passed into numba.jit
nogil : bool
    nogil to be passed into numba.jit
parallel : bool
    parallel to be passed into numba.jit

Returns
-------
Numba function"""""""
pandas/core/_numba/kernels/shared.py,"@numba.jit(numba.boolean(numba.int64[:]), nopython=True, nogil=True, parallel=False)
def is_monotonic_increasing(bounds: np.ndarray) -> bool:
    n = len(bounds)
    if n < 2:
        return True
    prev = bounds[0]
    for i in range(1, n):
        cur = bounds[i]
        if cur < prev:
            return False
        prev = cur
    return True","""""""Check if int64 values are monotonically increasing."""""""
pandas/core/accessor.py,"def delegate_names(delegate, accessors: list[str], typ: str, overwrite: bool=False, accessor_mapping: Callable[[str], str]=lambda x: x, raise_on_missing: bool=True):

    def add_delegate_accessors(cls):
        cls._add_delegate_accessors(delegate, accessors, typ, overwrite=overwrite, accessor_mapping=accessor_mapping, raise_on_missing=raise_on_missing)
        return cls
    return add_delegate_accessors","""""""Add delegated names to a class using a class decorator.  This provides
an alternative usage to directly calling `_add_delegate_accessors`
below a class definition.

Parameters
----------
delegate : object
    The class to get methods/properties & doc-strings.
accessors : Sequence[str]
    List of accessor to add.
typ : {'property', 'method'}
overwrite : bool, default False
   Overwrite the method/property in the target class if it exists.
accessor_mapping: Callable, default lambda x: x
    Callable to map the delegate's function to the cls' function.
raise_on_missing: bool, default True
    Raise if an accessor does not exist on delegate.
    False skips the missing accessor.

Returns
-------
callable
    A class decorator.

Examples
--------
@delegate_names(Categorical, [""categories"", ""ordered""], ""property"")
class CategoricalAccessor(PandasDelegate):
    [...]"""""""
pandas/core/accessor.py,"@doc(klass='', others='')
def _register_accessor(name: str, cls):

    def decorator(accessor):
        if hasattr(cls, name):
            warnings.warn(f'registration of accessor {repr(accessor)} under name {repr(name)} for type {repr(cls)} is overriding a preexisting attribute with the same name.', UserWarning, stacklevel=find_stack_level())
        setattr(cls, name, CachedAccessor(name, accessor))
        cls._accessors.add(name)
        return accessor
    return decorator","""""""Register a custom accessor on {klass} objects.

Parameters
----------
name : str
    Name under which the accessor should be registered. A warning is issued
    if this name conflicts with a preexisting attribute.

Returns
-------
callable
    A class decorator.

See Also
--------
register_dataframe_accessor : Register a custom accessor on DataFrame objects.
register_series_accessor : Register a custom accessor on Series objects.
register_index_accessor : Register a custom accessor on Index objects.

Notes
-----
When accessed, your accessor will be initialized with the pandas object
the user is interacting with. So the signature must be

.. code-block:: python

    def __init__(self, pandas_object):  # noqa: E999
        ...

For consistency with pandas methods, you should raise an ``AttributeError``
if the data passed to your accessor has an incorrect dtype.

>>> pd.Series(['a', 'b']).dt
Traceback (most recent call last):
...
AttributeError: Can only use .dt accessor with datetimelike values

Examples
--------
In your library code::

    import pandas as pd

    @pd.api.extensions.register_dataframe_accessor(""geo"")
    class GeoAccessor:
        def __init__(self, pandas_obj):
            self._obj = pandas_obj

        @property
        def center(self):
            # return the geographic center point of this DataFrame
            lat = self._obj.latitude
            lon = self._obj.longitude
            return (float(lon.mean()), float(lat.mean()))

        def plot(self):
            # plot this array's data on a map, e.g., using Cartopy
            pass

Back in an interactive IPython session:

    .. code-block:: ipython

        In [1]: ds = pd.DataFrame({{""longitude"": np.linspace(0, 10),
           ...:                    ""latitude"": np.linspace(0, 20)}})
        In [2]: ds.geo.center
        Out[2]: (5.0, 10.0)
        In [3]: ds.geo.plot()  # plots data on a map"""""""
pandas/core/algorithms.py,"def _ensure_data(values: ArrayLike) -> np.ndarray:
    if not isinstance(values, ABCMultiIndex):
        values = extract_array(values, extract_numpy=True)
    if is_object_dtype(values.dtype):
        return ensure_object(np.asarray(values))
    elif isinstance(values.dtype, BaseMaskedDtype):
        values = cast('BaseMaskedArray', values)
        if not values._hasna:
            return _ensure_data(values._data)
        return np.asarray(values)
    elif isinstance(values.dtype, CategoricalDtype):
        values = cast('Categorical', values)
        return values.codes
    elif is_bool_dtype(values.dtype):
        if isinstance(values, np.ndarray):
            return np.asarray(values).view('uint8')
        else:
            return np.asarray(values).astype('uint8', copy=False)
    elif is_integer_dtype(values.dtype):
        return np.asarray(values)
    elif is_float_dtype(values.dtype):
        if values.dtype.itemsize in [2, 12, 16]:
            return ensure_float64(values)
        return np.asarray(values)
    elif is_complex_dtype(values.dtype):
        return cast(np.ndarray, values)
    elif needs_i8_conversion(values.dtype):
        npvalues = values.view('i8')
        npvalues = cast(np.ndarray, npvalues)
        return npvalues
    values = np.asarray(values, dtype=object)
    return ensure_object(values)","""""""routine to ensure that our data is of the correct
input dtype for lower-level routines

This will coerce:
- ints -> int64
- uint -> uint64
- bool -> uint8
- datetimelike -> i8
- datetime64tz -> i8 (in local tz)
- categorical -> codes

Parameters
----------
values : np.ndarray or ExtensionArray

Returns
-------
np.ndarray"""""""
pandas/core/algorithms.py,"def _reconstruct_data(values: ArrayLike, dtype: DtypeObj, original: AnyArrayLike) -> ArrayLike:
    if isinstance(values, ABCExtensionArray) and values.dtype == dtype:
        return values
    if not isinstance(dtype, np.dtype):
        cls = dtype.construct_array_type()
        values = cls._from_sequence(values, dtype=dtype)
    else:
        values = values.astype(dtype, copy=False)
    return values","""""""reverse of _ensure_data

Parameters
----------
values : np.ndarray or ExtensionArray
dtype : np.dtype or ExtensionDtype
original : AnyArrayLike

Returns
-------
ExtensionArray or np.ndarray"""""""
pandas/core/algorithms.py,"def _ensure_arraylike(values, func_name: str) -> ArrayLike:
    if not isinstance(values, (ABCIndex, ABCSeries, ABCExtensionArray, np.ndarray)):
        if func_name != 'isin-targets':
            warnings.warn(f'{func_name} with argument that is not not a Series, Index, ExtensionArray, or np.ndarray is deprecated and will raise in a future version.', FutureWarning, stacklevel=find_stack_level())
        inferred = lib.infer_dtype(values, skipna=False)
        if inferred in ['mixed', 'string', 'mixed-integer']:
            if isinstance(values, tuple):
                values = list(values)
            values = construct_1d_object_array_from_listlike(values)
        else:
            values = np.asarray(values)
    return values","""""""ensure that we are arraylike if not already"""""""
pandas/core/algorithms.py,"def _get_hashtable_algo(values: np.ndarray):
    values = _ensure_data(values)
    ndtype = _check_object_for_strings(values)
    hashtable = _hashtables[ndtype]
    return (hashtable, values)","""""""Parameters
----------
values : np.ndarray

Returns
-------
htable : HashTable subclass
values : ndarray"""""""
pandas/core/algorithms.py,"def _check_object_for_strings(values: np.ndarray) -> str:
    ndtype = values.dtype.name
    if ndtype == 'object':
        if lib.is_string_array(values, skipna=False):
            ndtype = 'string'
    return ndtype","""""""Check if we can use string hashtable instead of object hashtable.

Parameters
----------
values : ndarray

Returns
-------
str"""""""
pandas/core/algorithms.py,"def unique(values):
    return unique_with_mask(values)","""""""Return unique values based on a hash table.

Uniques are returned in order of appearance. This does NOT sort.

Significantly faster than numpy.unique for long enough sequences.
Includes NA values.

Parameters
----------
values : 1d array-like

Returns
-------
numpy.ndarray or ExtensionArray

    The return can be:

    * Index : when the input is an Index
    * Categorical : when the input is a Categorical dtype
    * ndarray : when the input is a Series/ndarray

    Return numpy.ndarray or ExtensionArray.

See Also
--------
Index.unique : Return unique values from an Index.
Series.unique : Return unique values of Series object.

Examples
--------
>>> pd.unique(pd.Series([2, 1, 3, 3]))
array([2, 1, 3])

>>> pd.unique(pd.Series([2] + [1] * 5))
array([2, 1])

>>> pd.unique(pd.Series([pd.Timestamp(""20160101""), pd.Timestamp(""20160101"")]))
array(['2016-01-01T00:00:00.000000000'], dtype='datetime64[ns]')

>>> pd.unique(
...     pd.Series(
...         [
...             pd.Timestamp(""20160101"", tz=""US/Eastern""),
...             pd.Timestamp(""20160101"", tz=""US/Eastern""),
...         ]
...     )
... )
<DatetimeArray>
['2016-01-01 00:00:00-05:00']
Length: 1, dtype: datetime64[ns, US/Eastern]

>>> pd.unique(
...     pd.Index(
...         [
...             pd.Timestamp(""20160101"", tz=""US/Eastern""),
...             pd.Timestamp(""20160101"", tz=""US/Eastern""),
...         ]
...     )
... )
DatetimeIndex(['2016-01-01 00:00:00-05:00'],
        dtype='datetime64[ns, US/Eastern]',
        freq=None)

>>> pd.unique(np.array(list(""baabc""), dtype=""O""))
array(['b', 'a', 'c'], dtype=object)

An unordered Categorical will return categories in the
order of appearance.

>>> pd.unique(pd.Series(pd.Categorical(list(""baabc""))))
['b', 'a', 'c']
Categories (3, object): ['a', 'b', 'c']

>>> pd.unique(pd.Series(pd.Categorical(list(""baabc""), categories=list(""abc""))))
['b', 'a', 'c']
Categories (3, object): ['a', 'b', 'c']

An ordered Categorical preserves the category ordering.

>>> pd.unique(
...     pd.Series(
...         pd.Categorical(list(""baabc""), categories=list(""abc""), ordered=True)
...     )
... )
['b', 'a', 'c']
Categories (3, object): ['a' < 'b' < 'c']

An array of tuples

>>> pd.unique(pd.Series([(""a"", ""b""), (""b"", ""a""), (""a"", ""c""), (""b"", ""a"")]).values)
array([('a', 'b'), ('b', 'a'), ('a', 'c')], dtype=object)"""""""
pandas/core/algorithms.py,"def nunique_ints(values: ArrayLike) -> int:
    if len(values) == 0:
        return 0
    values = _ensure_data(values)
    result = (np.bincount(values.ravel().astype('intp')) != 0).sum()
    return result","""""""Return the number of unique values for integer array-likes.

Significantly faster than pandas.unique for long enough sequences.
No checks are done to ensure input is integral.

Parameters
----------
values : 1d array-like

Returns
-------
int : The number of unique values in ``values``"""""""
pandas/core/algorithms.py,"def unique_with_mask(values, mask: npt.NDArray[np.bool_] | None=None):
    values = _ensure_arraylike(values, func_name='unique')
    if isinstance(values.dtype, ExtensionDtype):
        return values.unique()
    original = values
    (hashtable, values) = _get_hashtable_algo(values)
    table = hashtable(len(values))
    if mask is None:
        uniques = table.unique(values)
        uniques = _reconstruct_data(uniques, original.dtype, original)
        return uniques
    else:
        (uniques, mask) = table.unique(values, mask=mask)
        uniques = _reconstruct_data(uniques, original.dtype, original)
        assert mask is not None
        return (uniques, mask.astype('bool'))","""""""See algorithms.unique for docs. Takes a mask for masked arrays."""""""
pandas/core/algorithms.py,"def isin(comps: ListLike, values: ListLike) -> npt.NDArray[np.bool_]:
    if not is_list_like(comps):
        raise TypeError(f'only list-like objects are allowed to be passed to isin(), you passed a `{type(comps).__name__}`')
    if not is_list_like(values):
        raise TypeError(f'only list-like objects are allowed to be passed to isin(), you passed a `{type(values).__name__}`')
    if not isinstance(values, (ABCIndex, ABCSeries, ABCExtensionArray, np.ndarray)):
        orig_values = list(values)
        values = _ensure_arraylike(orig_values, func_name='isin-targets')
        if len(values) > 0 and values.dtype.kind in 'iufcb' and (not is_signed_integer_dtype(comps)):
            values = construct_1d_object_array_from_listlike(orig_values)
    elif isinstance(values, ABCMultiIndex):
        values = np.array(values)
    else:
        values = extract_array(values, extract_numpy=True, extract_range=True)
    comps_array = _ensure_arraylike(comps, func_name='isin')
    comps_array = extract_array(comps_array, extract_numpy=True)
    if not isinstance(comps_array, np.ndarray):
        return comps_array.isin(values)
    elif needs_i8_conversion(comps_array.dtype):
        return pd_array(comps_array).isin(values)
    elif needs_i8_conversion(values.dtype) and (not is_object_dtype(comps_array.dtype)):
        return np.zeros(comps_array.shape, dtype=bool)
    elif needs_i8_conversion(values.dtype):
        return isin(comps_array, values.astype(object))
    elif isinstance(values.dtype, ExtensionDtype):
        return isin(np.asarray(comps_array), np.asarray(values))
    if len(comps_array) > _MINIMUM_COMP_ARR_LEN and len(values) <= 26 and (comps_array.dtype != object):
        if isna(values).any():

            def f(c, v):
                return np.logical_or(np.isin(c, v).ravel(), np.isnan(c))
        else:
            f = lambda a, b: np.isin(a, b).ravel()
    else:
        common = np_find_common_type(values.dtype, comps_array.dtype)
        values = values.astype(common, copy=False)
        comps_array = comps_array.astype(common, copy=False)
        f = htable.ismember
    return f(comps_array, values)","""""""Compute the isin boolean array.

Parameters
----------
comps : list-like
values : list-like

Returns
-------
ndarray[bool]
    Same length as `comps`."""""""
pandas/core/algorithms.py,"def factorize_array(values: np.ndarray, use_na_sentinel: bool=True, size_hint: int | None=None, na_value: object=None, mask: npt.NDArray[np.bool_] | None=None) -> tuple[npt.NDArray[np.intp], np.ndarray]:
    original = values
    if values.dtype.kind in 'mM':
        na_value = iNaT
    (hash_klass, values) = _get_hashtable_algo(values)
    table = hash_klass(size_hint or len(values))
    (uniques, codes) = table.factorize(values, na_sentinel=-1, na_value=na_value, mask=mask, ignore_na=use_na_sentinel)
    uniques = _reconstruct_data(uniques, original.dtype, original)
    codes = ensure_platform_int(codes)
    return (codes, uniques)","""""""Factorize a numpy array to codes and uniques.

This doesn't do any coercion of types or unboxing before factorization.

Parameters
----------
values : ndarray
use_na_sentinel : bool, default True
    If True, the sentinel -1 will be used for NaN values. If False,
    NaN values will be encoded as non-negative integers and will not drop the
    NaN from the uniques of the values.
size_hint : int, optional
    Passed through to the hashtable's 'get_labels' method
na_value : object, optional
    A value in `values` to consider missing. Note: only use this
    parameter when you know that you don't have any values pandas would
    consider missing in the array (NaN for float data, iNaT for
    datetimes, etc.).
mask : ndarray[bool], optional
    If not None, the mask is used as indicator for missing values
    (True = missing, False = valid) instead of `na_value` or
    condition ""val != val"".

Returns
-------
codes : ndarray[np.intp]
uniques : ndarray"""""""
pandas/core/algorithms.py,"@doc(values=dedent(""    values : sequence\n        A 1-D sequence. Sequences that aren't pandas objects are\n        coerced to ndarrays before factorization.\n    ""), sort=dedent('    sort : bool, default False\n        Sort `uniques` and shuffle `codes` to maintain the\n        relationship.\n    '), size_hint=dedent('    size_hint : int, optional\n        Hint to the hashtable sizer.\n    '))
def factorize(values, sort: bool=False, use_na_sentinel: bool=True, size_hint: int | None=None) -> tuple[np.ndarray, np.ndarray | Index]:
    if isinstance(values, (ABCIndex, ABCSeries)):
        return values.factorize(sort=sort, use_na_sentinel=use_na_sentinel)
    values = _ensure_arraylike(values, func_name='factorize')
    original = values
    if isinstance(values, (ABCDatetimeArray, ABCTimedeltaArray)) and values.freq is not None:
        (codes, uniques) = values.factorize(sort=sort)
        return (codes, uniques)
    elif not isinstance(values, np.ndarray):
        (codes, uniques) = values.factorize(use_na_sentinel=use_na_sentinel)
    else:
        values = np.asarray(values)
        if not use_na_sentinel and values.dtype == object:
            null_mask = isna(values)
            if null_mask.any():
                na_value = na_value_for_dtype(values.dtype, compat=False)
                values = np.where(null_mask, na_value, values)
        (codes, uniques) = factorize_array(values, use_na_sentinel=use_na_sentinel, size_hint=size_hint)
    if sort and len(uniques) > 0:
        (uniques, codes) = safe_sort(uniques, codes, use_na_sentinel=use_na_sentinel, assume_unique=True, verify=False)
    uniques = _reconstruct_data(uniques, original.dtype, original)
    return (codes, uniques)","""""""Encode the object as an enumerated type or categorical variable.

This method is useful for obtaining a numeric representation of an
array when all that matters is identifying distinct values. `factorize`
is available as both a top-level function :func:`pandas.factorize`,
and as a method :meth:`Series.factorize` and :meth:`Index.factorize`.

Parameters
----------
{values}{sort}
use_na_sentinel : bool, default True
    If True, the sentinel -1 will be used for NaN values. If False,
    NaN values will be encoded as non-negative integers and will not drop the
    NaN from the uniques of the values.

    .. versionadded:: 1.5.0
{size_hint}
Returns
-------
codes : ndarray
    An integer ndarray that's an indexer into `uniques`.
    ``uniques.take(codes)`` will have the same values as `values`.
uniques : ndarray, Index, or Categorical
    The unique valid values. When `values` is Categorical, `uniques`
    is a Categorical. When `values` is some other pandas object, an
    `Index` is returned. Otherwise, a 1-D ndarray is returned.

    .. note::

       Even if there's a missing value in `values`, `uniques` will
       *not* contain an entry for it.

See Also
--------
cut : Discretize continuous-valued array.
unique : Find the unique value in an array.

Notes
-----
Reference :ref:`the user guide <reshaping.factorize>` for more examples.

Examples
--------
These examples all show factorize as a top-level method like
``pd.factorize(values)``. The results are identical for methods like
:meth:`Series.factorize`.

>>> codes, uniques = pd.factorize(np.array(['b', 'b', 'a', 'c', 'b'], dtype=""O""))
>>> codes
array([0, 0, 1, 2, 0])
>>> uniques
array(['b', 'a', 'c'], dtype=object)

With ``sort=True``, the `uniques` will be sorted, and `codes` will be
shuffled so that the relationship is the maintained.

>>> codes, uniques = pd.factorize(np.array(['b', 'b', 'a', 'c', 'b'], dtype=""O""),
...                               sort=True)
>>> codes
array([1, 1, 0, 2, 1])
>>> uniques
array(['a', 'b', 'c'], dtype=object)

When ``use_na_sentinel=True`` (the default), missing values are indicated in
the `codes` with the sentinel value ``-1`` and missing values are not
included in `uniques`.

>>> codes, uniques = pd.factorize(np.array(['b', None, 'a', 'c', 'b'], dtype=""O""))
>>> codes
array([ 0, -1,  1,  2,  0])
>>> uniques
array(['b', 'a', 'c'], dtype=object)

Thus far, we've only factorized lists (which are internally coerced to
NumPy arrays). When factorizing pandas objects, the type of `uniques`
will differ. For Categoricals, a `Categorical` is returned.

>>> cat = pd.Categorical(['a', 'a', 'c'], categories=['a', 'b', 'c'])
>>> codes, uniques = pd.factorize(cat)
>>> codes
array([0, 0, 1])
>>> uniques
['a', 'c']
Categories (3, object): ['a', 'b', 'c']

Notice that ``'b'`` is in ``uniques.categories``, despite not being
present in ``cat.values``.

For all other pandas objects, an Index of the appropriate type is
returned.

>>> cat = pd.Series(['a', 'a', 'c'])
>>> codes, uniques = pd.factorize(cat)
>>> codes
array([0, 0, 1])
>>> uniques
Index(['a', 'c'], dtype='object')

If NaN is in the values, and we want to include NaN in the uniques of the
values, it can be achieved by setting ``use_na_sentinel=False``.

>>> values = np.array([1, 2, 1, np.nan])
>>> codes, uniques = pd.factorize(values)  # default: use_na_sentinel=True
>>> codes
array([ 0,  1,  0, -1])
>>> uniques
array([1., 2.])

>>> codes, uniques = pd.factorize(values, use_na_sentinel=False)
>>> codes
array([0, 1, 0, 2])
>>> uniques
array([ 1.,  2., nan])"""""""
pandas/core/algorithms.py,"def value_counts(values, sort: bool=True, ascending: bool=False, normalize: bool=False, bins=None, dropna: bool=True) -> Series:
    warnings.warn('pandas.value_counts is deprecated and will be removed in a future version. Use pd.Series(obj).value_counts() instead.', FutureWarning, stacklevel=find_stack_level())
    return value_counts_internal(values, sort=sort, ascending=ascending, normalize=normalize, bins=bins, dropna=dropna)","""""""Compute a histogram of the counts of non-null values.

Parameters
----------
values : ndarray (1-d)
sort : bool, default True
    Sort by values
ascending : bool, default False
    Sort in ascending order
normalize: bool, default False
    If True then compute a relative histogram
bins : integer, optional
    Rather than count values, group them into half-open bins,
    convenience for pd.cut, only works with numeric data
dropna : bool, default True
    Don't include counts of NaN

Returns
-------
Series"""""""
pandas/core/algorithms.py,"def value_counts_arraylike(values: np.ndarray, dropna: bool, mask: npt.NDArray[np.bool_] | None=None) -> tuple[ArrayLike, npt.NDArray[np.int64], int]:
    original = values
    values = _ensure_data(values)
    (keys, counts, na_counter) = htable.value_count(values, dropna, mask=mask)
    if needs_i8_conversion(original.dtype):
        if dropna:
            mask = keys != iNaT
            (keys, counts) = (keys[mask], counts[mask])
    res_keys = _reconstruct_data(keys, original.dtype, original)
    return (res_keys, counts, na_counter)","""""""Parameters
----------
values : np.ndarray
dropna : bool
mask : np.ndarray[bool] or None, default None

Returns
-------
uniques : np.ndarray
counts : np.ndarray[np.int64]"""""""
pandas/core/algorithms.py,"def duplicated(values: ArrayLike, keep: Literal['first', 'last', False]='first', mask: npt.NDArray[np.bool_] | None=None) -> npt.NDArray[np.bool_]:
    values = _ensure_data(values)
    return htable.duplicated(values, keep=keep, mask=mask)","""""""Return boolean ndarray denoting duplicate values.

Parameters
----------
values : np.ndarray or ExtensionArray
    Array over which to check for duplicate values.
keep : {'first', 'last', False}, default 'first'
    - ``first`` : Mark duplicates as ``True`` except for the first
      occurrence.
    - ``last`` : Mark duplicates as ``True`` except for the last
      occurrence.
    - False : Mark all duplicates as ``True``.
mask : ndarray[bool], optional
    array indicating which elements to exclude from checking

Returns
-------
duplicated : ndarray[bool]"""""""
pandas/core/algorithms.py,"def mode(values: ArrayLike, dropna: bool=True, mask: npt.NDArray[np.bool_] | None=None) -> ArrayLike:
    values = _ensure_arraylike(values, func_name='mode')
    original = values
    if needs_i8_conversion(values.dtype):
        values = ensure_wrapped_if_datetimelike(values)
        values = cast('ExtensionArray', values)
        return values._mode(dropna=dropna)
    values = _ensure_data(values)
    npresult = htable.mode(values, dropna=dropna, mask=mask)
    try:
        npresult = np.sort(npresult)
    except TypeError as err:
        warnings.warn(f'Unable to sort modes: {err}', stacklevel=find_stack_level())
    result = _reconstruct_data(npresult, original.dtype, original)
    return result","""""""Returns the mode(s) of an array.

Parameters
----------
values : array-like
    Array over which to check for duplicate values.
dropna : bool, default True
    Don't consider counts of NaN/NaT.

Returns
-------
np.ndarray or ExtensionArray"""""""
pandas/core/algorithms.py,"def rank(values: ArrayLike, axis: AxisInt=0, method: str='average', na_option: str='keep', ascending: bool=True, pct: bool=False) -> npt.NDArray[np.float64]:
    is_datetimelike = needs_i8_conversion(values.dtype)
    values = _ensure_data(values)
    if values.ndim == 1:
        ranks = algos.rank_1d(values, is_datetimelike=is_datetimelike, ties_method=method, ascending=ascending, na_option=na_option, pct=pct)
    elif values.ndim == 2:
        ranks = algos.rank_2d(values, axis=axis, is_datetimelike=is_datetimelike, ties_method=method, ascending=ascending, na_option=na_option, pct=pct)
    else:
        raise TypeError('Array with ndim > 2 are not supported.')
    return ranks","""""""Rank the values along a given axis.

Parameters
----------
values : np.ndarray or ExtensionArray
    Array whose values will be ranked. The number of dimensions in this
    array must not exceed 2.
axis : int, default 0
    Axis over which to perform rankings.
method : {'average', 'min', 'max', 'first', 'dense'}, default 'average'
    The method by which tiebreaks are broken during the ranking.
na_option : {'keep', 'top'}, default 'keep'
    The method by which NaNs are placed in the ranking.
    - ``keep``: rank each NaN value with a NaN ranking
    - ``top``: replace each NaN with either +/- inf so that they
               there are ranked at the top
ascending : bool, default True
    Whether or not the elements should be ranked in ascending order.
pct : bool, default False
    Whether or not to the display the returned rankings in integer form
    (e.g. 1, 2, 3) or in percentile form (e.g. 0.333..., 0.666..., 1)."""""""
pandas/core/algorithms.py,"def checked_add_with_arr(arr: npt.NDArray[np.int64], b: int | npt.NDArray[np.int64], arr_mask: npt.NDArray[np.bool_] | None=None, b_mask: npt.NDArray[np.bool_] | None=None) -> npt.NDArray[np.int64]:
    b2 = np.broadcast_to(b, arr.shape)
    if b_mask is not None:
        b2_mask = np.broadcast_to(b_mask, arr.shape)
    else:
        b2_mask = None
    if arr_mask is not None and b2_mask is not None:
        not_nan = np.logical_not(arr_mask | b2_mask)
    elif arr_mask is not None:
        not_nan = np.logical_not(arr_mask)
    elif b_mask is not None:
        not_nan = np.logical_not(b2_mask)
    else:
        not_nan = np.empty(arr.shape, dtype=bool)
        not_nan.fill(True)
    i8max = lib.i8max
    i8min = iNaT
    mask1 = b2 > 0
    mask2 = b2 < 0
    if not mask1.any():
        to_raise = ((i8min - b2 > arr) & not_nan).any()
    elif not mask2.any():
        to_raise = ((i8max - b2 < arr) & not_nan).any()
    else:
        to_raise = ((i8max - b2[mask1] < arr[mask1]) & not_nan[mask1]).any() or ((i8min - b2[mask2] > arr[mask2]) & not_nan[mask2]).any()
    if to_raise:
        raise OverflowError('Overflow in int64 addition')
    result = arr + b
    if arr_mask is not None or b2_mask is not None:
        np.putmask(result, ~not_nan, iNaT)
    return result","""""""Perform array addition that checks for underflow and overflow.

Performs the addition of an int64 array and an int64 integer (or array)
but checks that they do not result in overflow first. For elements that
are indicated to be NaN, whether or not there is overflow for that element
is automatically ignored.

Parameters
----------
arr : np.ndarray[int64] addend.
b : array or scalar addend.
arr_mask : np.ndarray[bool] or None, default None
    array indicating which elements to exclude from checking
b_mask : np.ndarray[bool] or None, default None
    array or scalar indicating which element(s) to exclude from checking

Returns
-------
sum : An array for elements x + b for each element x in arr if b is
      a scalar or an array for elements x + y for each element pair
      (x, y) in (arr, b).

Raises
------
OverflowError if any x + y exceeds the maximum or minimum int64 value."""""""
pandas/core/algorithms.py,"def take(arr, indices: TakeIndexer, axis: AxisInt=0, allow_fill: bool=False, fill_value=None):
    if not isinstance(arr, (np.ndarray, ABCExtensionArray, ABCIndex, ABCSeries)):
        warnings.warn('pd.api.extensions.take accepting non-standard inputs is deprecated and will raise in a future version. Pass either a numpy.ndarray, ExtensionArray, Index, or Series instead.', FutureWarning, stacklevel=find_stack_level())
    if not is_array_like(arr):
        arr = np.asarray(arr)
    indices = ensure_platform_int(indices)
    if allow_fill:
        validate_indices(indices, arr.shape[axis])
        result = take_nd(arr, indices, axis=axis, allow_fill=True, fill_value=fill_value)
    else:
        result = arr.take(indices, axis=axis)
    return result","""""""Take elements from an array.

Parameters
----------
arr : array-like or scalar value
    Non array-likes (sequences/scalars without a dtype) are coerced
    to an ndarray.

    .. deprecated:: 2.1.0
        Passing an argument other than a numpy.ndarray, ExtensionArray,
        Index, or Series is deprecated.

indices : sequence of int or one-dimensional np.ndarray of int
    Indices to be taken.
axis : int, default 0
    The axis over which to select values.
allow_fill : bool, default False
    How to handle negative values in `indices`.

    * False: negative values in `indices` indicate positional indices
      from the right (the default). This is similar to :func:`numpy.take`.

    * True: negative values in `indices` indicate
      missing values. These values are set to `fill_value`. Any other
      negative values raise a ``ValueError``.

fill_value : any, optional
    Fill value to use for NA-indices when `allow_fill` is True.
    This may be ``None``, in which case the default NA value for
    the type (``self.dtype.na_value``) is used.

    For multi-dimensional `arr`, each *element* is filled with
    `fill_value`.

Returns
-------
ndarray or ExtensionArray
    Same type as the input.

Raises
------
IndexError
    When `indices` is out of bounds for the array.
ValueError
    When the indexer contains negative values other than ``-1``
    and `allow_fill` is True.

Notes
-----
When `allow_fill` is False, `indices` may be whatever dimensionality
is accepted by NumPy for `arr`.

When `allow_fill` is True, `indices` should be 1-D.

See Also
--------
numpy.take : Take elements from an array along an axis.

Examples
--------
>>> import pandas as pd

With the default ``allow_fill=False``, negative numbers indicate
positional indices from the right.

>>> pd.api.extensions.take(np.array([10, 20, 30]), [0, 0, -1])
array([10, 10, 30])

Setting ``allow_fill=True`` will place `fill_value` in those positions.

>>> pd.api.extensions.take(np.array([10, 20, 30]), [0, 0, -1], allow_fill=True)
array([10., 10., nan])

>>> pd.api.extensions.take(np.array([10, 20, 30]), [0, 0, -1], allow_fill=True,
...      fill_value=-10)
array([ 10,  10, -10])"""""""
pandas/core/algorithms.py,"def searchsorted(arr: ArrayLike, value: NumpyValueArrayLike | ExtensionArray, side: Literal['left', 'right']='left', sorter: NumpySorter | None=None) -> npt.NDArray[np.intp] | np.intp:
    if sorter is not None:
        sorter = ensure_platform_int(sorter)
    if isinstance(arr, np.ndarray) and arr.dtype.kind in 'iu' and (is_integer(value) or is_integer_dtype(value)):
        iinfo = np.iinfo(arr.dtype.type)
        value_arr = np.array([value]) if is_integer(value) else np.array(value)
        if (value_arr >= iinfo.min).all() and (value_arr <= iinfo.max).all():
            dtype = arr.dtype
        else:
            dtype = value_arr.dtype
        if is_integer(value):
            value = cast(int, dtype.type(value))
        else:
            value = pd_array(cast(ArrayLike, value), dtype=dtype)
    else:
        arr = ensure_wrapped_if_datetimelike(arr)
    return arr.searchsorted(value, side=side, sorter=sorter)","""""""Find indices where elements should be inserted to maintain order.

Find the indices into a sorted array `arr` (a) such that, if the
corresponding elements in `value` were inserted before the indices,
the order of `arr` would be preserved.

Assuming that `arr` is sorted:

======  ================================
`side`  returned index `i` satisfies
======  ================================
left    ``arr[i-1] < value <= self[i]``
right   ``arr[i-1] <= value < self[i]``
======  ================================

Parameters
----------
arr: np.ndarray, ExtensionArray, Series
    Input array. If `sorter` is None, then it must be sorted in
    ascending order, otherwise `sorter` must be an array of indices
    that sort it.
value : array-like or scalar
    Values to insert into `arr`.
side : {'left', 'right'}, optional
    If 'left', the index of the first suitable location found is given.
    If 'right', return the last such index.  If there is no suitable
    index, return either 0 or N (where N is the length of `self`).
sorter : 1-D array-like, optional
    Optional array of integer indices that sort array a into ascending
    order. They are typically the result of argsort.

Returns
-------
array of ints or int
    If value is array-like, array of insertion points.
    If value is scalar, a single integer.

See Also
--------
numpy.searchsorted : Similar method from NumPy."""""""
pandas/core/algorithms.py,"def diff(arr, n: int, axis: AxisInt=0):
    n = int(n)
    na = np.nan
    dtype = arr.dtype
    is_bool = is_bool_dtype(dtype)
    if is_bool:
        op = operator.xor
    else:
        op = operator.sub
    if isinstance(dtype, NumpyEADtype):
        arr = arr.to_numpy()
        dtype = arr.dtype
    if not isinstance(arr, np.ndarray):
        if hasattr(arr, f'__{op.__name__}__'):
            if axis != 0:
                raise ValueError(f'cannot diff {type(arr).__name__} on axis={axis}')
            return op(arr, arr.shift(n))
        else:
            raise TypeError(f""{type(arr).__name__} has no 'diff' method. Convert to a suitable dtype prior to calling 'diff'."")
    is_timedelta = False
    if arr.dtype.kind in 'mM':
        dtype = np.int64
        arr = arr.view('i8')
        na = iNaT
        is_timedelta = True
    elif is_bool:
        dtype = np.object_
    elif dtype.kind in 'iu':
        if arr.dtype.name in ['int8', 'int16']:
            dtype = np.float32
        else:
            dtype = np.float64
    orig_ndim = arr.ndim
    if orig_ndim == 1:
        arr = arr.reshape(-1, 1)
    dtype = np.dtype(dtype)
    out_arr = np.empty(arr.shape, dtype=dtype)
    na_indexer = [slice(None)] * 2
    na_indexer[axis] = slice(None, n) if n >= 0 else slice(n, None)
    out_arr[tuple(na_indexer)] = na
    if arr.dtype.name in _diff_special:
        algos.diff_2d(arr, out_arr, n, axis, datetimelike=is_timedelta)
    else:
        _res_indexer = [slice(None)] * 2
        _res_indexer[axis] = slice(n, None) if n >= 0 else slice(None, n)
        res_indexer = tuple(_res_indexer)
        _lag_indexer = [slice(None)] * 2
        _lag_indexer[axis] = slice(None, -n) if n > 0 else slice(-n, None)
        lag_indexer = tuple(_lag_indexer)
        out_arr[res_indexer] = op(arr[res_indexer], arr[lag_indexer])
    if is_timedelta:
        out_arr = out_arr.view('timedelta64[ns]')
    if orig_ndim == 1:
        out_arr = out_arr[:, 0]
    return out_arr","""""""difference of n between self,
analogous to s-s.shift(n)

Parameters
----------
arr : ndarray or ExtensionArray
n : int
    number of periods
axis : {0, 1}
    axis to shift on
stacklevel : int, default 3
    The stacklevel for the lost dtype warning.

Returns
-------
shifted"""""""
pandas/core/algorithms.py,"def safe_sort(values: Index | ArrayLike, codes: npt.NDArray[np.intp] | None=None, use_na_sentinel: bool=True, assume_unique: bool=False, verify: bool=True) -> AnyArrayLike | tuple[AnyArrayLike, np.ndarray]:
    if not isinstance(values, (np.ndarray, ABCExtensionArray, ABCIndex)):
        raise TypeError('Only np.ndarray, ExtensionArray, and Index objects are allowed to be passed to safe_sort as values')
    sorter = None
    ordered: AnyArrayLike
    if not isinstance(values.dtype, ExtensionDtype) and lib.infer_dtype(values, skipna=False) == 'mixed-integer':
        ordered = _sort_mixed(values)
    else:
        try:
            sorter = values.argsort()
            ordered = values.take(sorter)
        except TypeError:
            if values.size and isinstance(values[0], tuple):
                ordered = _sort_tuples(values)
            else:
                ordered = _sort_mixed(values)
    if codes is None:
        return ordered
    if not is_list_like(codes):
        raise TypeError('Only list-like objects or None are allowed to be passed to safe_sort as codes')
    codes = ensure_platform_int(np.asarray(codes))
    if not assume_unique and (not len(unique(values)) == len(values)):
        raise ValueError('values should be unique if codes is not None')
    if sorter is None:
        (hash_klass, values) = _get_hashtable_algo(values)
        t = hash_klass(len(values))
        t.map_locations(values)
        sorter = ensure_platform_int(t.lookup(ordered))
    if use_na_sentinel:
        order2 = sorter.argsort()
        new_codes = take_nd(order2, codes, fill_value=-1)
        if verify:
            mask = (codes < -len(values)) | (codes >= len(values))
        else:
            mask = None
    else:
        reverse_indexer = np.empty(len(sorter), dtype=int)
        reverse_indexer.put(sorter, np.arange(len(sorter)))
        new_codes = reverse_indexer.take(codes, mode='wrap')
        if use_na_sentinel:
            mask = codes == -1
            if verify:
                mask = mask | (codes < -len(values)) | (codes >= len(values))
    if use_na_sentinel and mask is not None:
        np.putmask(new_codes, mask, -1)
    return (ordered, ensure_platform_int(new_codes))","""""""Sort ``values`` and reorder corresponding ``codes``.

``values`` should be unique if ``codes`` is not None.
Safe for use with mixed types (int, str), orders ints before strs.

Parameters
----------
values : list-like
    Sequence; must be unique if ``codes`` is not None.
codes : np.ndarray[intp] or None, default None
    Indices to ``values``. All out of bound indices are treated as
    ""not found"" and will be masked with ``-1``.
use_na_sentinel : bool, default True
    If True, the sentinel -1 will be used for NaN values. If False,
    NaN values will be encoded as non-negative integers and will not drop the
    NaN from the uniques of the values.
assume_unique : bool, default False
    When True, ``values`` are assumed to be unique, which can speed up
    the calculation. Ignored when ``codes`` is None.
verify : bool, default True
    Check if codes are out of bound for the values and put out of bound
    codes equal to ``-1``. If ``verify=False``, it is assumed there
    are no out of bound codes. Ignored when ``codes`` is None.

Returns
-------
ordered : AnyArrayLike
    Sorted ``values``
new_codes : ndarray
    Reordered ``codes``; returned when ``codes`` is not None.

Raises
------
TypeError
    * If ``values`` is not list-like or if ``codes`` is neither None
    nor list-like
    * If ``values`` cannot be sorted
ValueError
    * If ``codes`` is not None and ``values`` contain duplicates."""""""
pandas/core/algorithms.py,"def _sort_mixed(values) -> AnyArrayLike:
    str_pos = np.array([isinstance(x, str) for x in values], dtype=bool)
    null_pos = np.array([isna(x) for x in values], dtype=bool)
    num_pos = ~str_pos & ~null_pos
    str_argsort = np.argsort(values[str_pos])
    num_argsort = np.argsort(values[num_pos])
    str_locs = str_pos.nonzero()[0].take(str_argsort)
    num_locs = num_pos.nonzero()[0].take(num_argsort)
    null_locs = null_pos.nonzero()[0]
    locs = np.concatenate([num_locs, str_locs, null_locs])
    return values.take(locs)","""""""order ints before strings before nulls in 1d arrays"""""""
pandas/core/algorithms.py,"def _sort_tuples(values: np.ndarray) -> np.ndarray:
    from pandas.core.internals.construction import to_arrays
    from pandas.core.sorting import lexsort_indexer
    (arrays, _) = to_arrays(values, None)
    indexer = lexsort_indexer(arrays, orders=True)
    return values[indexer]","""""""Convert array of tuples (1d) to array of arrays (2d).
We need to keep the columns separately as they contain different types and
nans (can't use `np.sort` as it may fail when str and nan are mixed in a
column as types cannot be compared)."""""""
pandas/core/algorithms.py,"def union_with_duplicates(lvals: ArrayLike | Index, rvals: ArrayLike | Index) -> ArrayLike | Index:
    from pandas import Series
    l_count = value_counts_internal(lvals, dropna=False)
    r_count = value_counts_internal(rvals, dropna=False)
    (l_count, r_count) = l_count.align(r_count, fill_value=0)
    final_count = np.maximum(l_count.values, r_count.values)
    final_count = Series(final_count, index=l_count.index, dtype='int', copy=False)
    if isinstance(lvals, ABCMultiIndex) and isinstance(rvals, ABCMultiIndex):
        unique_vals = lvals.append(rvals).unique()
    else:
        if isinstance(lvals, ABCIndex):
            lvals = lvals._values
        if isinstance(rvals, ABCIndex):
            rvals = rvals._values
        combined = concat_compat([lvals, rvals])
        unique_vals = unique(combined)
        unique_vals = ensure_wrapped_if_datetimelike(unique_vals)
    repeats = final_count.reindex(unique_vals).values
    return np.repeat(unique_vals, repeats)","""""""Extracts the union from lvals and rvals with respect to duplicates and nans in
both arrays.

Parameters
----------
lvals: np.ndarray or ExtensionArray
    left values which is ordered in front.
rvals: np.ndarray or ExtensionArray
    right values ordered after lvals.

Returns
-------
np.ndarray or ExtensionArray
    Containing the unsorted union of both arrays.

Notes
-----
Caller is responsible for ensuring lvals.dtype == rvals.dtype."""""""
pandas/core/algorithms.py,"def map_array(arr: ArrayLike, mapper, na_action: Literal['ignore'] | None=None, convert: bool=True) -> np.ndarray | ExtensionArray | Index:
    if na_action not in (None, 'ignore'):
        msg = f""na_action must either be 'ignore' or None, {na_action} was passed""
        raise ValueError(msg)
    if is_dict_like(mapper):
        if isinstance(mapper, dict) and hasattr(mapper, '__missing__'):
            dict_with_default = mapper
            mapper = lambda x: dict_with_default[np.nan if isinstance(x, float) and np.isnan(x) else x]
        else:
            from pandas import Series
            if len(mapper) == 0:
                mapper = Series(mapper, dtype=np.float64)
            else:
                mapper = Series(mapper)
    if isinstance(mapper, ABCSeries):
        if na_action == 'ignore':
            mapper = mapper[mapper.index.notna()]
        indexer = mapper.index.get_indexer(arr)
        new_values = take_nd(mapper._values, indexer)
        return new_values
    if not len(arr):
        return arr.copy()
    values = arr.astype(object, copy=False)
    if na_action is None:
        return lib.map_infer(values, mapper, convert=convert)
    else:
        return lib.map_infer_mask(values, mapper, mask=isna(values).view(np.uint8), convert=convert)","""""""Map values using an input mapping or function.

Parameters
----------
mapper : function, dict, or Series
    Mapping correspondence.
na_action : {None, 'ignore'}, default None
    If 'ignore', propagate NA values, without passing them to the
    mapping correspondence.
convert : bool, default True
    Try to find better dtype for elementwise function results. If
    False, leave as dtype=object.

Returns
-------
Union[ndarray, Index, ExtensionArray]
    The output of the mapping function applied to the array.
    If the function returns a tuple with more than one element
    a MultiIndex will be returned."""""""
pandas/core/apply.py,"def frame_apply(obj: DataFrame, func: AggFuncType, axis: Axis=0, raw: bool=False, result_type: str | None=None, by_row: Literal[False, 'compat']='compat', engine: str='python', engine_kwargs: dict[str, bool] | None=None, args=None, kwargs=None) -> FrameApply:
    axis = obj._get_axis_number(axis)
    klass: type[FrameApply]
    if axis == 0:
        klass = FrameRowApply
    elif axis == 1:
        klass = FrameColumnApply
    (_, func, _, _) = reconstruct_func(func, **kwargs)
    assert func is not None
    return klass(obj, func, raw=raw, result_type=result_type, by_row=by_row, engine=engine, engine_kwargs=engine_kwargs, args=args, kwargs=kwargs)","""""""construct and return a row or column based frame apply object"""""""
pandas/core/apply.py,"def reconstruct_func(func: AggFuncType | None, **kwargs) -> tuple[bool, AggFuncType, list[str] | None, npt.NDArray[np.intp] | None]:
    relabeling = func is None and is_multi_agg_with_relabel(**kwargs)
    columns: list[str] | None = None
    order: npt.NDArray[np.intp] | None = None
    if not relabeling:
        if isinstance(func, list) and len(func) > len(set(func)):
            raise SpecificationError('Function names must be unique if there is no new column names assigned')
        if func is None:
            raise TypeError(""Must provide 'func' or tuples of '(column, aggfunc)."")
    if relabeling:
        (func, columns, order) = normalize_keyword_aggregation(kwargs)
    assert func is not None
    return (relabeling, func, columns, order)","""""""This is the internal function to reconstruct func given if there is relabeling
or not and also normalize the keyword to get new order of columns.

If named aggregation is applied, `func` will be None, and kwargs contains the
column and aggregation function information to be parsed;
If named aggregation is not applied, `func` is either string (e.g. 'min') or
Callable, or list of them (e.g. ['min', np.max]), or the dictionary of column name
and str/Callable/list of them (e.g. {'A': 'min'}, or {'A': [np.min, lambda x: x]})

If relabeling is True, will return relabeling, reconstructed func, column
names, and the reconstructed order of columns.
If relabeling is False, the columns and order will be None.

Parameters
----------
func: agg function (e.g. 'min' or Callable) or list of agg functions
    (e.g. ['min', np.max]) or dictionary (e.g. {'A': ['min', np.max]}).
**kwargs: dict, kwargs used in is_multi_agg_with_relabel and
    normalize_keyword_aggregation function for relabelling

Returns
-------
relabelling: bool, if there is relabelling or not
func: normalized and mangled func
columns: list of column names
order: array of columns indices

Examples
--------
>>> reconstruct_func(None, **{""foo"": (""col"", ""min"")})
(True, defaultdict(<class 'list'>, {'col': ['min']}), ('foo',), array([0]))

>>> reconstruct_func(""min"")
(False, 'min', None, None)"""""""
pandas/core/apply.py,"def is_multi_agg_with_relabel(**kwargs) -> bool:
    return all((isinstance(v, tuple) and len(v) == 2 for v in kwargs.values())) and len(kwargs) > 0","""""""Check whether kwargs passed to .agg look like multi-agg with relabeling.

Parameters
----------
**kwargs : dict

Returns
-------
bool

Examples
--------
>>> is_multi_agg_with_relabel(a=""max"")
False
>>> is_multi_agg_with_relabel(a_max=(""a"", ""max""), a_min=(""a"", ""min""))
True
>>> is_multi_agg_with_relabel()
False"""""""
pandas/core/apply.py,"def normalize_keyword_aggregation(kwargs: dict) -> tuple[dict, list[str], npt.NDArray[np.intp]]:
    from pandas.core.indexes.base import Index
    aggspec: DefaultDict = defaultdict(list)
    order = []
    (columns, pairs) = list(zip(*kwargs.items()))
    for (column, aggfunc) in pairs:
        aggspec[column].append(aggfunc)
        order.append((column, com.get_callable_name(aggfunc) or aggfunc))
    uniquified_order = _make_unique_kwarg_list(order)
    aggspec_order = [(column, com.get_callable_name(aggfunc) or aggfunc) for (column, aggfuncs) in aggspec.items() for aggfunc in aggfuncs]
    uniquified_aggspec = _make_unique_kwarg_list(aggspec_order)
    col_idx_order = Index(uniquified_aggspec).get_indexer(uniquified_order)
    return (aggspec, columns, col_idx_order)","""""""Normalize user-provided ""named aggregation"" kwargs.
Transforms from the new ``Mapping[str, NamedAgg]`` style kwargs
to the old Dict[str, List[scalar]]].

Parameters
----------
kwargs : dict

Returns
-------
aggspec : dict
    The transformed kwargs.
columns : List[str]
    The user-provided keys.
col_idx_order : List[int]
    List of columns indices.

Examples
--------
>>> normalize_keyword_aggregation({""output"": (""input"", ""sum"")})
(defaultdict(<class 'list'>, {'input': ['sum']}), ('output',), array([0]))"""""""
pandas/core/apply.py,"def _make_unique_kwarg_list(seq: Sequence[tuple[Any, Any]]) -> Sequence[tuple[Any, Any]]:
    return [(pair[0], f'{pair[1]}_{seq[:i].count(pair)}') if seq.count(pair) > 1 else pair for (i, pair) in enumerate(seq)]","""""""Uniquify aggfunc name of the pairs in the order list

Examples:
--------
>>> kwarg_list = [('a', '<lambda>'), ('a', '<lambda>'), ('b', '<lambda>')]
>>> _make_unique_kwarg_list(kwarg_list)
[('a', '<lambda>_0'), ('a', '<lambda>_1'), ('b', '<lambda>')]"""""""
pandas/core/apply.py,"def relabel_result(result: DataFrame | Series, func: dict[str, list[Callable | str]], columns: Iterable[Hashable], order: Iterable[int]) -> dict[Hashable, Series]:
    from pandas.core.indexes.base import Index
    reordered_indexes = [pair[0] for pair in sorted(zip(columns, order), key=lambda t: t[1])]
    reordered_result_in_dict: dict[Hashable, Series] = {}
    idx = 0
    reorder_mask = not isinstance(result, ABCSeries) and len(result.columns) > 1
    for (col, fun) in func.items():
        s = result[col].dropna()
        if reorder_mask:
            fun = [com.get_callable_name(f) if not isinstance(f, str) else f for f in fun]
            col_idx_order = Index(s.index).get_indexer(fun)
            s = s.iloc[col_idx_order]
        s.index = reordered_indexes[idx:idx + len(fun)]
        reordered_result_in_dict[col] = s.reindex(columns, copy=False)
        idx = idx + len(fun)
    return reordered_result_in_dict","""""""Internal function to reorder result if relabelling is True for
dataframe.agg, and return the reordered result in dict.

Parameters:
----------
result: Result from aggregation
func: Dict of (column name, funcs)
columns: New columns name for relabelling
order: New order for relabelling

Examples
--------
>>> from pandas.core.apply import relabel_result
>>> result = pd.DataFrame(
...     {""A"": [np.nan, 2, np.nan], ""C"": [6, np.nan, np.nan], ""B"": [np.nan, 4, 2.5]},
...     index=[""max"", ""mean"", ""min""]
... )
>>> funcs = {""A"": [""max""], ""C"": [""max""], ""B"": [""mean"", ""min""]}
>>> columns = (""foo"", ""aab"", ""bar"", ""dat"")
>>> order = [0, 1, 2, 3]
>>> result_in_dict = relabel_result(result, funcs, columns, order)
>>> pd.DataFrame(result_in_dict, index=columns)
       A    C    B
foo  2.0  NaN  NaN
aab  NaN  6.0  NaN
bar  NaN  NaN  4.0
dat  NaN  NaN  2.5"""""""
pandas/core/apply.py,"def _managle_lambda_list(aggfuncs: Sequence[Any]) -> Sequence[Any]:
    if len(aggfuncs) <= 1:
        return aggfuncs
    i = 0
    mangled_aggfuncs = []
    for aggfunc in aggfuncs:
        if com.get_callable_name(aggfunc) == '<lambda>':
            aggfunc = partial(aggfunc)
            aggfunc.__name__ = f'<lambda_{i}>'
            i += 1
        mangled_aggfuncs.append(aggfunc)
    return mangled_aggfuncs","""""""Possibly mangle a list of aggfuncs.

Parameters
----------
aggfuncs : Sequence

Returns
-------
mangled: list-like
    A new AggSpec sequence, where lambdas have been converted
    to have unique names.

Notes
-----
If just one aggfunc is passed, the name will not be mangled."""""""
pandas/core/apply.py,"def maybe_mangle_lambdas(agg_spec: Any) -> Any:
    is_dict = is_dict_like(agg_spec)
    if not (is_dict or is_list_like(agg_spec)):
        return agg_spec
    mangled_aggspec = type(agg_spec)()
    if is_dict:
        for (key, aggfuncs) in agg_spec.items():
            if is_list_like(aggfuncs) and (not is_dict_like(aggfuncs)):
                mangled_aggfuncs = _managle_lambda_list(aggfuncs)
            else:
                mangled_aggfuncs = aggfuncs
            mangled_aggspec[key] = mangled_aggfuncs
    else:
        mangled_aggspec = _managle_lambda_list(agg_spec)
    return mangled_aggspec","""""""Make new lambdas with unique names.

Parameters
----------
agg_spec : Any
    An argument to GroupBy.agg.
    Non-dict-like `agg_spec` are pass through as is.
    For dict-like `agg_spec` a new spec is returned
    with name-mangled lambdas.

Returns
-------
mangled : Any
    Same type as the input.

Examples
--------
>>> maybe_mangle_lambdas('sum')
'sum'
>>> maybe_mangle_lambdas([lambda: 1, lambda: 2])  # doctest: +SKIP
[<function __main__.<lambda_0>,
 <function pandas...._make_lambda.<locals>.f(*args, **kwargs)>]"""""""
pandas/core/apply.py,"def validate_func_kwargs(kwargs: dict) -> tuple[list[str], list[str | Callable[..., Any]]]:
    tuple_given_message = 'func is expected but received {} in **kwargs.'
    columns = list(kwargs)
    func = []
    for col_func in kwargs.values():
        if not (isinstance(col_func, str) or callable(col_func)):
            raise TypeError(tuple_given_message.format(type(col_func).__name__))
        func.append(col_func)
    if not columns:
        no_arg_message = ""Must provide 'func' or named aggregation **kwargs.""
        raise TypeError(no_arg_message)
    return (columns, func)","""""""Validates types of user-provided ""named aggregation"" kwargs.
`TypeError` is raised if aggfunc is not `str` or callable.

Parameters
----------
kwargs : dict

Returns
-------
columns : List[str]
    List of user-provided keys.
func : List[Union[str, callable[...,Any]]]
    List of user-provided aggfuncs

Examples
--------
>>> validate_func_kwargs({'one': 'min', 'two': 'max'})
(['one', 'two'], ['min', 'max'])"""""""
pandas/core/array_algos/datetimelike_accumulations.py,"def _cum_func(func: Callable, values: np.ndarray, *, skipna: bool=True):
    try:
        fill_value = {np.maximum.accumulate: np.iinfo(np.int64).min, np.cumsum: 0, np.minimum.accumulate: np.iinfo(np.int64).max}[func]
    except KeyError:
        raise ValueError(f'No accumulation for {func} implemented on BaseMaskedArray')
    mask = isna(values)
    y = values.view('i8')
    y[mask] = fill_value
    if not skipna:
        mask = np.maximum.accumulate(mask)
    result = func(y)
    result[mask] = iNaT
    if values.dtype.kind in 'mM':
        return result.view(values.dtype.base)
    return result","""""""Accumulations for 1D datetimelike arrays.

Parameters
----------
func : np.cumsum, np.maximum.accumulate, np.minimum.accumulate
values : np.ndarray
    Numpy array with the values (can be of any dtype that support the
    operation). Values is changed is modified inplace.
skipna : bool, default True
    Whether to skip NA."""""""
pandas/core/array_algos/masked_accumulations.py,"def _cum_func(func: Callable, values: np.ndarray, mask: npt.NDArray[np.bool_], *, skipna: bool=True):
    dtype_info: np.iinfo | np.finfo
    if values.dtype.kind == 'f':
        dtype_info = np.finfo(values.dtype.type)
    elif values.dtype.kind in 'iu':
        dtype_info = np.iinfo(values.dtype.type)
    elif values.dtype.kind == 'b':
        dtype_info = np.iinfo(np.uint8)
    else:
        raise NotImplementedError(f'No masked accumulation defined for dtype {values.dtype.type}')
    try:
        fill_value = {np.cumprod: 1, np.maximum.accumulate: dtype_info.min, np.cumsum: 0, np.minimum.accumulate: dtype_info.max}[func]
    except KeyError:
        raise NotImplementedError(f'No accumulation for {func} implemented on BaseMaskedArray')
    values[mask] = fill_value
    if not skipna:
        mask = np.maximum.accumulate(mask)
    values = func(values)
    return (values, mask)","""""""Accumulations for 1D masked array.

We will modify values in place to replace NAs with the appropriate fill value.

Parameters
----------
func : np.cumsum, np.cumprod, np.maximum.accumulate, np.minimum.accumulate
values : np.ndarray
    Numpy array with the values (can be of any dtype that support the
    operation).
mask : np.ndarray
    Boolean numpy array (True values indicate missing values).
skipna : bool, default True
    Whether to skip NA."""""""
pandas/core/array_algos/masked_reductions.py,"def _reductions(func: Callable, values: np.ndarray, mask: npt.NDArray[np.bool_], *, skipna: bool=True, min_count: int=0, axis: AxisInt | None=None, **kwargs):
    if not skipna:
        if mask.any() or check_below_min_count(values.shape, None, min_count):
            return libmissing.NA
        else:
            return func(values, axis=axis, **kwargs)
    else:
        if check_below_min_count(values.shape, mask, min_count) and (axis is None or values.ndim == 1):
            return libmissing.NA
        return func(values, where=~mask, axis=axis, **kwargs)","""""""Sum, mean or product for 1D masked array.

Parameters
----------
func : np.sum or np.prod
values : np.ndarray
    Numpy array with the values (can be of any dtype that support the
    operation).
mask : np.ndarray[bool]
    Boolean numpy array (True values indicate missing values).
skipna : bool, default True
    Whether to skip NA.
min_count : int, default 0
    The required number of valid values to perform the operation. If fewer than
    ``min_count`` non-NA values are present the result will be NA.
axis : int, optional, default None"""""""
pandas/core/array_algos/masked_reductions.py,"def _minmax(func: Callable, values: np.ndarray, mask: npt.NDArray[np.bool_], *, skipna: bool=True, axis: AxisInt | None=None):
    if not skipna:
        if mask.any() or not values.size:
            return libmissing.NA
        else:
            return func(values, axis=axis)
    else:
        subset = values[~mask]
        if subset.size:
            return func(subset, axis=axis)
        else:
            return libmissing.NA","""""""Reduction for 1D masked array.

Parameters
----------
func : np.min or np.max
values : np.ndarray
    Numpy array with the values (can be of any dtype that support the
    operation).
mask : np.ndarray[bool]
    Boolean numpy array (True values indicate missing values).
skipna : bool, default True
    Whether to skip NA.
axis : int, optional, default None"""""""
pandas/core/array_algos/putmask.py,"def putmask_inplace(values: ArrayLike, mask: npt.NDArray[np.bool_], value: Any) -> None:
    if not isinstance(values, np.ndarray) or (values.dtype == object and (not lib.is_scalar(value))) or (isinstance(value, np.ndarray) and (not np.can_cast(value.dtype, values.dtype))):
        if is_list_like(value) and len(value) == len(values):
            values[mask] = value[mask]
        else:
            values[mask] = value
    else:
        np.putmask(values, mask, value)","""""""ExtensionArray-compatible implementation of np.putmask.  The main
difference is we do not handle repeating or truncating like numpy.

Parameters
----------
values: np.ndarray or ExtensionArray
mask : np.ndarray[bool]
    We assume extract_bool_array has already been called.
value : Any"""""""
pandas/core/array_algos/putmask.py,"def putmask_without_repeat(values: np.ndarray, mask: npt.NDArray[np.bool_], new: Any) -> None:
    if getattr(new, 'ndim', 0) >= 1:
        new = new.astype(values.dtype, copy=False)
    nlocs = mask.sum()
    if nlocs > 0 and is_list_like(new) and (getattr(new, 'ndim', 1) == 1):
        shape = np.shape(new)
        if nlocs == shape[-1]:
            np.place(values, mask, new)
        elif mask.shape[-1] == shape[-1] or shape[-1] == 1:
            np.putmask(values, mask, new)
        else:
            raise ValueError('cannot assign mismatch length to masked array')
    else:
        np.putmask(values, mask, new)","""""""np.putmask will truncate or repeat if `new` is a listlike with
len(new) != len(values).  We require an exact match.

Parameters
----------
values : np.ndarray
mask : np.ndarray[bool]
new : Any"""""""
pandas/core/array_algos/putmask.py,"def validate_putmask(values: ArrayLike | MultiIndex, mask: np.ndarray) -> tuple[npt.NDArray[np.bool_], bool]:
    mask = extract_bool_array(mask)
    if mask.shape != values.shape:
        raise ValueError('putmask: mask and data must be the same size')
    noop = not mask.any()
    return (mask, noop)","""""""Validate mask and check if this putmask operation is a no-op."""""""
pandas/core/array_algos/putmask.py,"def extract_bool_array(mask: ArrayLike) -> npt.NDArray[np.bool_]:
    if isinstance(mask, ExtensionArray):
        mask = mask.to_numpy(dtype=bool, na_value=False)
    mask = np.asarray(mask, dtype=bool)
    return mask","""""""If we have a SparseArray or BooleanArray, convert it to ndarray[bool]."""""""
pandas/core/array_algos/putmask.py,"def setitem_datetimelike_compat(values: np.ndarray, num_set: int, other):
    if values.dtype == object:
        (dtype, _) = infer_dtype_from(other)
        if lib.is_np_dtype(dtype, 'mM'):
            if not is_list_like(other):
                other = [other] * num_set
            else:
                other = list(other)
    return other","""""""Parameters
----------
values : np.ndarray
num_set : int
    For putmask, this is mask.sum()
other : Any"""""""
pandas/core/array_algos/quantile.py,"def quantile_compat(values: ArrayLike, qs: npt.NDArray[np.float64], interpolation: str) -> ArrayLike:
    if isinstance(values, np.ndarray):
        fill_value = na_value_for_dtype(values.dtype, compat=False)
        mask = isna(values)
        return quantile_with_mask(values, mask, fill_value, qs, interpolation)
    else:
        return values._quantile(qs, interpolation)","""""""Compute the quantiles of the given values for each quantile in `qs`.

Parameters
----------
values : np.ndarray or ExtensionArray
qs : np.ndarray[float64]
interpolation : str

Returns
-------
np.ndarray or ExtensionArray"""""""
pandas/core/array_algos/quantile.py,"def quantile_with_mask(values: np.ndarray, mask: npt.NDArray[np.bool_], fill_value, qs: npt.NDArray[np.float64], interpolation: str) -> np.ndarray:
    assert values.shape == mask.shape
    if values.ndim == 1:
        values = np.atleast_2d(values)
        mask = np.atleast_2d(mask)
        res_values = quantile_with_mask(values, mask, fill_value, qs, interpolation)
        return res_values[0]
    assert values.ndim == 2
    is_empty = values.shape[1] == 0
    if is_empty:
        flat = np.array([fill_value] * len(qs))
        result = np.repeat(flat, len(values)).reshape(len(values), len(qs))
    else:
        result = _nanpercentile(values, qs * 100.0, na_value=fill_value, mask=mask, interpolation=interpolation)
        result = np.array(result, copy=False)
        result = result.T
    return result","""""""Compute the quantiles of the given values for each quantile in `qs`.

Parameters
----------
values : np.ndarray
    For ExtensionArray, this is _values_for_factorize()[0]
mask : np.ndarray[bool]
    mask = isna(values)
    For ExtensionArray, this is computed before calling _value_for_factorize
fill_value : Scalar
    The value to interpret fill NA entries with
    For ExtensionArray, this is _values_for_factorize()[1]
qs : np.ndarray[float64]
interpolation : str
    Type of interpolation

Returns
-------
np.ndarray

Notes
-----
Assumes values is already 2D.  For ExtensionArray this means np.atleast_2d
has been called on _values_for_factorize()[0]

Quantile is computed along axis=1."""""""
pandas/core/array_algos/quantile.py,"def _nanpercentile_1d(values: np.ndarray, mask: npt.NDArray[np.bool_], qs: npt.NDArray[np.float64], na_value: Scalar, interpolation: str) -> Scalar | np.ndarray:
    values = values[~mask]
    if len(values) == 0:
        return np.full(len(qs), na_value)
    return np.percentile(values, qs, method=interpolation)","""""""Wrapper for np.percentile that skips missing values, specialized to
1-dimensional case.

Parameters
----------
values : array over which to find quantiles
mask : ndarray[bool]
    locations in values that should be considered missing
qs : np.ndarray[float64] of quantile indices to find
na_value : scalar
    value to return for empty or all-null values
interpolation : str

Returns
-------
quantiles : scalar or array"""""""
pandas/core/array_algos/quantile.py,"def _nanpercentile(values: np.ndarray, qs: npt.NDArray[np.float64], *, na_value, mask: npt.NDArray[np.bool_], interpolation: str):
    if values.dtype.kind in 'mM':
        result = _nanpercentile(values.view('i8'), qs=qs, na_value=na_value.view('i8'), mask=mask, interpolation=interpolation)
        return result.astype(values.dtype)
    if mask.any():
        assert mask.shape == values.shape
        result = [_nanpercentile_1d(val, m, qs, na_value, interpolation=interpolation) for (val, m) in zip(list(values), list(mask))]
        if values.dtype.kind == 'f':
            result = np.array(result, dtype=values.dtype, copy=False).T
        else:
            result = np.array(result, copy=False).T
            if result.dtype != values.dtype and (not mask.all()) and (result == result.astype(values.dtype, copy=False)).all():
                result = result.astype(values.dtype, copy=False)
        return result
    else:
        return np.percentile(values, qs, axis=1, method=interpolation)","""""""Wrapper for np.percentile that skips missing values.

Parameters
----------
values : np.ndarray[ndim=2]  over which to find quantiles
qs : np.ndarray[float64] of quantile indices to find
na_value : scalar
    value to return for empty or all-null values
mask : np.ndarray[bool]
    locations in values that should be considered missing
interpolation : str

Returns
-------
quantiles : scalar or array"""""""
pandas/core/array_algos/replace.py,"def should_use_regex(regex: bool, to_replace: Any) -> bool:
    if is_re(to_replace):
        regex = True
    regex = regex and is_re_compilable(to_replace)
    regex = regex and re.compile(to_replace).pattern != ''
    return regex","""""""Decide whether to treat `to_replace` as a regular expression."""""""
pandas/core/array_algos/replace.py,"def compare_or_regex_search(a: ArrayLike, b: Scalar | Pattern, regex: bool, mask: npt.NDArray[np.bool_]) -> ArrayLike:
    if isna(b):
        return ~mask

    def _check_comparison_types(result: ArrayLike | bool, a: ArrayLike, b: Scalar | Pattern):
        """"""
        Raises an error if the two arrays (a,b) cannot be compared.
        Otherwise, returns the comparison result as expected.
        """"""
        if is_bool(result) and isinstance(a, np.ndarray):
            type_names = [type(a).__name__, type(b).__name__]
            type_names[0] = f'ndarray(dtype={a.dtype})'
            raise TypeError(f'Cannot compare types {repr(type_names[0])} and {repr(type_names[1])}')
    if not regex or not should_use_regex(regex, b):
        op = lambda x: operator.eq(x, b)
    else:
        op = np.vectorize(lambda x: bool(re.search(b, x)) if isinstance(x, str) and isinstance(b, (str, Pattern)) else False)
    if isinstance(a, np.ndarray):
        a = a[mask]
    result = op(a)
    if isinstance(result, np.ndarray) and mask is not None:
        tmp = np.zeros(mask.shape, dtype=np.bool_)
        np.place(tmp, mask, result)
        result = tmp
    _check_comparison_types(result, a, b)
    return result","""""""Compare two array-like inputs of the same shape or two scalar values

Calls operator.eq or re.search, depending on regex argument. If regex is
True, perform an element-wise regex matching.

Parameters
----------
a : array-like
b : scalar or regex pattern
regex : bool
mask : np.ndarray[bool]

Returns
-------
mask : array-like of bool"""""""
pandas/core/array_algos/replace.py,"def replace_regex(values: ArrayLike, rx: re.Pattern, value, mask: npt.NDArray[np.bool_] | None) -> None:
    if isna(value) or not isinstance(value, str):

        def re_replacer(s):
            if is_re(rx) and isinstance(s, str):
                return value if rx.search(s) is not None else s
            else:
                return s
    else:

        def re_replacer(s):
            if is_re(rx) and isinstance(s, str):
                return rx.sub(value, s)
            else:
                return s
    f = np.vectorize(re_replacer, otypes=[np.object_])
    if mask is None:
        values[:] = f(values)
    else:
        values[mask] = f(values[mask])","""""""Parameters
----------
values : ArrayLike
    Object dtype.
rx : re.Pattern
value : Any
mask : np.ndarray[bool], optional

Notes
-----
Alters values in-place."""""""
pandas/core/array_algos/take.py,"def take_nd(arr: ArrayLike, indexer, axis: AxisInt=0, fill_value=lib.no_default, allow_fill: bool=True) -> ArrayLike:
    if fill_value is lib.no_default:
        fill_value = na_value_for_dtype(arr.dtype, compat=False)
    elif lib.is_np_dtype(arr.dtype, 'mM'):
        (dtype, fill_value) = maybe_promote(arr.dtype, fill_value)
        if arr.dtype != dtype:
            arr = arr.astype(dtype)
    if not isinstance(arr, np.ndarray):
        if not is_1d_only_ea_dtype(arr.dtype):
            arr = cast('NDArrayBackedExtensionArray', arr)
            return arr.take(indexer, fill_value=fill_value, allow_fill=allow_fill, axis=axis)
        return arr.take(indexer, fill_value=fill_value, allow_fill=allow_fill)
    arr = np.asarray(arr)
    return _take_nd_ndarray(arr, indexer, axis, fill_value, allow_fill)","""""""Specialized Cython take which sets NaN values in one pass

This dispatches to ``take`` defined on ExtensionArrays.

Note: this function assumes that the indexer is a valid(ated) indexer with
no out of bound indices.

Parameters
----------
arr : np.ndarray or ExtensionArray
    Input array.
indexer : ndarray
    1-D array of indices to take, subarrays corresponding to -1 value
    indices are filed with fill_value
axis : int, default 0
    Axis to take from
fill_value : any, default np.nan
    Fill value to replace -1 values with
allow_fill : bool, default True
    If False, indexer is assumed to contain no -1 values so no filling
    will be done.  This short-circuits computation of a mask.  Result is
    undefined if allow_fill == False and -1 is present in indexer.

Returns
-------
subarray : np.ndarray or ExtensionArray
    May be the same type as the input, or cast to an ndarray."""""""
pandas/core/array_algos/take.py,"def take_1d(arr: ArrayLike, indexer: npt.NDArray[np.intp], fill_value=None, allow_fill: bool=True, mask: npt.NDArray[np.bool_] | None=None) -> ArrayLike:
    if not isinstance(arr, np.ndarray):
        return arr.take(indexer, fill_value=fill_value, allow_fill=allow_fill)
    if not allow_fill:
        return arr.take(indexer)
    (dtype, fill_value, mask_info) = _take_preprocess_indexer_and_fill_value(arr, indexer, fill_value, True, mask)
    out = np.empty(indexer.shape, dtype=dtype)
    func = _get_take_nd_function(arr.ndim, arr.dtype, out.dtype, axis=0, mask_info=mask_info)
    func(arr, indexer, out, fill_value)
    return out","""""""Specialized version for 1D arrays. Differences compared to `take_nd`:

- Assumes input array has already been converted to numpy array / EA
- Assumes indexer is already guaranteed to be intp dtype ndarray
- Only works for 1D arrays

To ensure the lowest possible overhead.

Note: similarly to `take_nd`, this function assumes that the indexer is
a valid(ated) indexer with no out of bound indices.

Parameters
----------
arr : np.ndarray or ExtensionArray
    Input array.
indexer : ndarray
    1-D array of indices to take (validated indices, intp dtype).
fill_value : any, default np.nan
    Fill value to replace -1 values with
allow_fill : bool, default True
    If False, indexer is assumed to contain no -1 values so no filling
    will be done.  This short-circuits computation of a mask. Result is
    undefined if allow_fill == False and -1 is present in indexer.
mask : np.ndarray, optional, default None
    If `allow_fill` is True, and the mask (where indexer == -1) is already
    known, it can be passed to avoid recomputation."""""""
pandas/core/array_algos/take.py,"def take_2d_multi(arr: np.ndarray, indexer: tuple[npt.NDArray[np.intp], npt.NDArray[np.intp]], fill_value=np.nan) -> np.ndarray:
    assert indexer is not None
    assert indexer[0] is not None
    assert indexer[1] is not None
    (row_idx, col_idx) = indexer
    row_idx = ensure_platform_int(row_idx)
    col_idx = ensure_platform_int(col_idx)
    indexer = (row_idx, col_idx)
    mask_info = None
    (dtype, fill_value) = maybe_promote(arr.dtype, fill_value)
    if dtype != arr.dtype:
        row_mask = row_idx == -1
        col_mask = col_idx == -1
        row_needs = row_mask.any()
        col_needs = col_mask.any()
        mask_info = ((row_mask, col_mask), (row_needs, col_needs))
        if not (row_needs or col_needs):
            (dtype, fill_value) = (arr.dtype, arr.dtype.type())
    out_shape = (len(row_idx), len(col_idx))
    out = np.empty(out_shape, dtype=dtype)
    func = _take_2d_multi_dict.get((arr.dtype.name, out.dtype.name), None)
    if func is None and arr.dtype != out.dtype:
        func = _take_2d_multi_dict.get((out.dtype.name, out.dtype.name), None)
        if func is not None:
            func = _convert_wrapper(func, out.dtype)
    if func is not None:
        func(arr, indexer, out=out, fill_value=fill_value)
    else:
        _take_2d_multi_object(arr, indexer, out, fill_value=fill_value, mask_info=mask_info)
    return out","""""""Specialized Cython take which sets NaN values in one pass."""""""
pandas/core/array_algos/take.py,"@functools.lru_cache
def _get_take_nd_function_cached(ndim: int, arr_dtype: np.dtype, out_dtype: np.dtype, axis: AxisInt):
    tup = (arr_dtype.name, out_dtype.name)
    if ndim == 1:
        func = _take_1d_dict.get(tup, None)
    elif ndim == 2:
        if axis == 0:
            func = _take_2d_axis0_dict.get(tup, None)
        else:
            func = _take_2d_axis1_dict.get(tup, None)
    if func is not None:
        return func
    tup = (out_dtype.name, out_dtype.name)
    if ndim == 1:
        func = _take_1d_dict.get(tup, None)
    elif ndim == 2:
        if axis == 0:
            func = _take_2d_axis0_dict.get(tup, None)
        else:
            func = _take_2d_axis1_dict.get(tup, None)
    if func is not None:
        func = _convert_wrapper(func, out_dtype)
        return func
    return None","""""""Part of _get_take_nd_function below that doesn't need `mask_info` and thus
can be cached (mask_info potentially contains a numpy ndarray which is not
hashable and thus cannot be used as argument for cached function)."""""""
pandas/core/array_algos/take.py,"def _get_take_nd_function(ndim: int, arr_dtype: np.dtype, out_dtype: np.dtype, axis: AxisInt=0, mask_info=None):
    func = None
    if ndim <= 2:
        func = _get_take_nd_function_cached(ndim, arr_dtype, out_dtype, axis)
    if func is None:

        def func(arr, indexer, out, fill_value=np.nan) -> None:
            indexer = ensure_platform_int(indexer)
            _take_nd_object(arr, indexer, out, axis=axis, fill_value=fill_value, mask_info=mask_info)
    return func","""""""Get the appropriate ""take"" implementation for the given dimension, axis
and dtypes."""""""
pandas/core/arraylike.py,"def array_ufunc(self, ufunc: np.ufunc, method: str, *inputs: Any, **kwargs: Any):
    from pandas.core.frame import DataFrame, Series
    from pandas.core.generic import NDFrame
    from pandas.core.internals import BlockManager
    cls = type(self)
    kwargs = _standardize_out_kwarg(**kwargs)
    result = maybe_dispatch_ufunc_to_dunder_op(self, ufunc, method, *inputs, **kwargs)
    if result is not NotImplemented:
        return result
    no_defer = (np.ndarray.__array_ufunc__, cls.__array_ufunc__)
    for item in inputs:
        higher_priority = hasattr(item, '__array_priority__') and item.__array_priority__ > self.__array_priority__
        has_array_ufunc = hasattr(item, '__array_ufunc__') and type(item).__array_ufunc__ not in no_defer and (not isinstance(item, self._HANDLED_TYPES))
        if higher_priority or has_array_ufunc:
            return NotImplemented
    types = tuple((type(x) for x in inputs))
    alignable = [x for (x, t) in zip(inputs, types) if issubclass(t, NDFrame)]
    if len(alignable) > 1:
        set_types = set(types)
        if len(set_types) > 1 and {DataFrame, Series}.issubset(set_types):
            raise NotImplementedError(f'Cannot apply ufunc {ufunc} to mixed DataFrame and Series inputs.')
        axes = self.axes
        for obj in alignable[1:]:
            for (i, (ax1, ax2)) in enumerate(zip(axes, obj.axes)):
                axes[i] = ax1.union(ax2)
        reconstruct_axes = dict(zip(self._AXIS_ORDERS, axes))
        inputs = tuple((x.reindex(**reconstruct_axes) if issubclass(t, NDFrame) else x for (x, t) in zip(inputs, types)))
    else:
        reconstruct_axes = dict(zip(self._AXIS_ORDERS, self.axes))
    if self.ndim == 1:
        names = [getattr(x, 'name') for x in inputs if hasattr(x, 'name')]
        name = names[0] if len(set(names)) == 1 else None
        reconstruct_kwargs = {'name': name}
    else:
        reconstruct_kwargs = {}

    def reconstruct(result):
        if ufunc.nout > 1:
            return tuple((_reconstruct(x) for x in result))
        return _reconstruct(result)

    def _reconstruct(result):
        if lib.is_scalar(result):
            return result
        if result.ndim != self.ndim:
            if method == 'outer':
                raise NotImplementedError
            return result
        if isinstance(result, BlockManager):
            result = self._constructor_from_mgr(result, axes=result.axes)
        else:
            result = self._constructor(result, **reconstruct_axes, **reconstruct_kwargs, copy=False)
        if len(alignable) == 1:
            result = result.__finalize__(self)
        return result
    if 'out' in kwargs:
        result = dispatch_ufunc_with_out(self, ufunc, method, *inputs, **kwargs)
        return reconstruct(result)
    if method == 'reduce':
        result = dispatch_reduction_ufunc(self, ufunc, method, *inputs, **kwargs)
        if result is not NotImplemented:
            return result
    if self.ndim > 1 and (len(inputs) > 1 or ufunc.nout > 1):
        inputs = tuple((np.asarray(x) for x in inputs))
        result = getattr(ufunc, method)(*inputs, **kwargs)
    elif self.ndim == 1:
        inputs = tuple((extract_array(x, extract_numpy=True) for x in inputs))
        result = getattr(ufunc, method)(*inputs, **kwargs)
    elif method == '__call__' and (not kwargs):
        mgr = inputs[0]._mgr
        result = mgr.apply(getattr(ufunc, method))
    else:
        result = default_array_ufunc(inputs[0], ufunc, method, *inputs, **kwargs)
    result = reconstruct(result)
    return result","""""""Compatibility with numpy ufuncs.

See also
--------
numpy.org/doc/stable/reference/arrays.classes.html#numpy.class.__array_ufunc__"""""""
pandas/core/arraylike.py,"def _standardize_out_kwarg(**kwargs) -> dict:
    if 'out' not in kwargs and 'out1' in kwargs and ('out2' in kwargs):
        out1 = kwargs.pop('out1')
        out2 = kwargs.pop('out2')
        out = (out1, out2)
        kwargs['out'] = out
    return kwargs","""""""If kwargs contain ""out1"" and ""out2"", replace that with a tuple ""out""

np.divmod, np.modf, np.frexp can have either `out=(out1, out2)` or
`out1=out1, out2=out2)`"""""""
pandas/core/arraylike.py,"def dispatch_ufunc_with_out(self, ufunc: np.ufunc, method: str, *inputs, **kwargs):
    out = kwargs.pop('out')
    where = kwargs.pop('where', None)
    result = getattr(ufunc, method)(*inputs, **kwargs)
    if result is NotImplemented:
        return NotImplemented
    if isinstance(result, tuple):
        if not isinstance(out, tuple) or len(out) != len(result):
            raise NotImplementedError
        for (arr, res) in zip(out, result):
            _assign_where(arr, res, where)
        return out
    if isinstance(out, tuple):
        if len(out) == 1:
            out = out[0]
        else:
            raise NotImplementedError
    _assign_where(out, result, where)
    return out","""""""If we have an `out` keyword, then call the ufunc without `out` and then
set the result into the given `out`."""""""
pandas/core/arraylike.py,"def _assign_where(out, result, where) -> None:
    if where is None:
        out[:] = result
    else:
        np.putmask(out, where, result)","""""""Set a ufunc result into 'out', masking with a 'where' argument if necessary."""""""
pandas/core/arraylike.py,"def default_array_ufunc(self, ufunc: np.ufunc, method: str, *inputs, **kwargs):
    if not any((x is self for x in inputs)):
        raise NotImplementedError
    new_inputs = [x if x is not self else np.asarray(x) for x in inputs]
    return getattr(ufunc, method)(*new_inputs, **kwargs)","""""""Fallback to the behavior we would get if we did not define __array_ufunc__.

Notes
-----
We are assuming that `self` is among `inputs`."""""""
pandas/core/arraylike.py,"def dispatch_reduction_ufunc(self, ufunc: np.ufunc, method: str, *inputs, **kwargs):
    assert method == 'reduce'
    if len(inputs) != 1 or inputs[0] is not self:
        return NotImplemented
    if ufunc.__name__ not in REDUCTION_ALIASES:
        return NotImplemented
    method_name = REDUCTION_ALIASES[ufunc.__name__]
    if not hasattr(self, method_name):
        return NotImplemented
    if self.ndim > 1:
        if isinstance(self, ABCNDFrame):
            kwargs['numeric_only'] = False
        if 'axis' not in kwargs:
            kwargs['axis'] = 0
    return getattr(self, method_name)(skipna=False, **kwargs)","""""""Dispatch ufunc reductions to self's reduction methods."""""""
pandas/core/arrays/_mixins.py,"def ravel_compat(meth: F) -> F:

    @wraps(meth)
    def method(self, *args, **kwargs):
        if self.ndim == 1:
            return meth(self, *args, **kwargs)
        flags = self._ndarray.flags
        flat = self.ravel('K')
        result = meth(flat, *args, **kwargs)
        order = 'F' if flags.f_contiguous else 'C'
        return result.reshape(self.shape, order=order)
    return cast(F, method)","""""""Decorator to ravel a 2D array before passing it to a cython operation,
then reshape the result to our own shape."""""""
pandas/core/arrays/_ranges.py,"def generate_regular_range(start: Timestamp | Timedelta | None, end: Timestamp | Timedelta | None, periods: int | None, freq: BaseOffset, unit: str='ns') -> npt.NDArray[np.intp]:
    istart = start._value if start is not None else None
    iend = end._value if end is not None else None
    freq.nanos
    td = Timedelta(freq)
    b: int | np.int64 | np.uint64
    e: int | np.int64 | np.uint64
    try:
        td = td.as_unit(unit, round_ok=False)
    except ValueError as err:
        raise ValueError(f'freq={freq} is incompatible with unit={unit}. Use a lower freq or a higher unit instead.') from err
    stride = int(td._value)
    if periods is None and istart is not None and (iend is not None):
        b = istart
        e = b + (iend - b) // stride * stride + stride // 2 + 1
    elif istart is not None and periods is not None:
        b = istart
        e = _generate_range_overflow_safe(b, periods, stride, side='start')
    elif iend is not None and periods is not None:
        e = iend + stride
        b = _generate_range_overflow_safe(e, periods, stride, side='end')
    else:
        raise ValueError(""at least 'start' or 'end' should be specified if a 'period' is given."")
    with np.errstate(over='raise'):
        try:
            values = np.arange(b, e, stride, dtype=np.int64)
        except FloatingPointError:
            xdr = [b]
            while xdr[-1] != e:
                xdr.append(xdr[-1] + stride)
            values = np.array(xdr[:-1], dtype=np.int64)
    return values","""""""Generate a range of dates or timestamps with the spans between dates
described by the given `freq` DateOffset.

Parameters
----------
start : Timedelta, Timestamp or None
    First point of produced date range.
end : Timedelta, Timestamp or None
    Last point of produced date range.
periods : int or None
    Number of periods in produced date range.
freq : Tick
    Describes space between dates in produced date range.
unit : str, default ""ns""
    The resolution the output is meant to represent.

Returns
-------
ndarray[np.int64]
    Representing the given resolution."""""""
pandas/core/arrays/_ranges.py,"def _generate_range_overflow_safe(endpoint: int, periods: int, stride: int, side: str='start') -> np.int64 | np.uint64:
    assert side in ['start', 'end']
    i64max = np.uint64(i8max)
    msg = f'Cannot generate range with {side}={endpoint} and periods={periods}'
    with np.errstate(over='raise'):
        try:
            addend = np.uint64(periods) * np.uint64(np.abs(stride))
        except FloatingPointError as err:
            raise OutOfBoundsDatetime(msg) from err
    if np.abs(addend) <= i64max:
        return _generate_range_overflow_safe_signed(endpoint, periods, stride, side)
    elif endpoint > 0 and side == 'start' and (stride > 0) or (endpoint < 0 < stride and side == 'end'):
        raise OutOfBoundsDatetime(msg)
    elif side == 'end' and endpoint - stride <= i64max < endpoint:
        return _generate_range_overflow_safe(endpoint - stride, periods - 1, stride, side)
    mid_periods = periods // 2
    remaining = periods - mid_periods
    assert 0 < remaining < periods, (remaining, periods, endpoint, stride)
    midpoint = int(_generate_range_overflow_safe(endpoint, mid_periods, stride, side))
    return _generate_range_overflow_safe(midpoint, remaining, stride, side)","""""""Calculate the second endpoint for passing to np.arange, checking
to avoid an integer overflow.  Catch OverflowError and re-raise
as OutOfBoundsDatetime.

Parameters
----------
endpoint : int
    nanosecond timestamp of the known endpoint of the desired range
periods : int
    number of periods in the desired range
stride : int
    nanoseconds between periods in the desired range
side : {'start', 'end'}
    which end of the range `endpoint` refers to

Returns
-------
other_end : np.int64 | np.uint64

Raises
------
OutOfBoundsDatetime"""""""
pandas/core/arrays/_ranges.py,"def _generate_range_overflow_safe_signed(endpoint: int, periods: int, stride: int, side: str) -> np.int64 | np.uint64:
    assert side in ['start', 'end']
    if side == 'end':
        stride *= -1
    with np.errstate(over='raise'):
        addend = np.int64(periods) * np.int64(stride)
        try:
            result = np.int64(endpoint) + addend
            if result == iNaT:
                raise OverflowError
            return result
        except (FloatingPointError, OverflowError):
            pass
        assert stride > 0 and endpoint >= 0 or (stride < 0 and endpoint <= 0)
        if stride > 0:
            uresult = np.uint64(endpoint) + np.uint64(addend)
            i64max = np.uint64(i8max)
            assert uresult > i64max
            if uresult <= i64max + np.uint64(stride):
                return uresult
    raise OutOfBoundsDatetime(f'Cannot generate range with {side}={endpoint} and periods={periods}')","""""""A special case for _generate_range_overflow_safe where `periods * stride`
can be calculated without overflowing int64 bounds."""""""
pandas/core/arrays/arrow/_arrow_utils.py,"def fallback_performancewarning(version: str | None=None) -> None:
    msg = 'Falling back on a non-pyarrow code path which may decrease performance.'
    if version is not None:
        msg += f' Upgrade to pyarrow >={version} to possibly suppress this warning.'
    warnings.warn(msg, PerformanceWarning, stacklevel=find_stack_level())","""""""Raise a PerformanceWarning for falling back to ExtensionArray's
non-pyarrow method"""""""
pandas/core/arrays/arrow/_arrow_utils.py,"def pyarrow_array_to_numpy_and_mask(arr, dtype: np.dtype) -> tuple[np.ndarray, np.ndarray]:
    dtype = np.dtype(dtype)
    if pyarrow.types.is_null(arr.type):
        data = np.empty(len(arr), dtype=dtype)
        mask = np.zeros(len(arr), dtype=bool)
        return (data, mask)
    buflist = arr.buffers()
    offset = arr.offset * dtype.itemsize
    length = len(arr) * dtype.itemsize
    data_buf = buflist[1][offset:offset + length]
    data = np.frombuffer(data_buf, dtype=dtype)
    bitmask = buflist[0]
    if bitmask is not None:
        mask = pyarrow.BooleanArray.from_buffers(pyarrow.bool_(), len(arr), [None, bitmask], offset=arr.offset)
        mask = np.asarray(mask)
    else:
        mask = np.ones(len(arr), dtype=bool)
    return (data, mask)","""""""Convert a primitive pyarrow.Array to a numpy array and boolean mask based
on the buffers of the Array.

At the moment pyarrow.BooleanArray is not supported.

Parameters
----------
arr : pyarrow.Array
dtype : numpy.dtype

Returns
-------
(data, mask)
    Tuple of two numpy arrays with the raw data (with specified dtype) and
    a boolean mask (validity mask, so False means missing)"""""""
pandas/core/arrays/arrow/array.py,"def to_pyarrow_type(dtype: ArrowDtype | pa.DataType | Dtype | None) -> pa.DataType | None:
    if isinstance(dtype, ArrowDtype):
        return dtype.pyarrow_dtype
    elif isinstance(dtype, pa.DataType):
        return dtype
    elif isinstance(dtype, DatetimeTZDtype):
        return pa.timestamp(dtype.unit, dtype.tz)
    elif dtype:
        try:
            return pa.from_numpy_dtype(dtype)
        except pa.ArrowNotImplementedError:
            pass
    return None","""""""Convert dtype to a pyarrow type instance."""""""
pandas/core/arrays/arrow/array.py,"def transpose_homogeneous_pyarrow(arrays: Sequence[ArrowExtensionArray]) -> list[ArrowExtensionArray]:
    arrays = list(arrays)
    (nrows, ncols) = (len(arrays[0]), len(arrays))
    indices = np.arange(nrows * ncols).reshape(ncols, nrows).T.flatten()
    arr = pa.chunked_array([chunk for arr in arrays for chunk in arr._pa_array.chunks])
    arr = arr.take(indices)
    return [ArrowExtensionArray(arr.slice(i * ncols, ncols)) for i in range(nrows)]","""""""Transpose arrow extension arrays in a list, but faster.

Input should be a list of arrays of equal length and all have the same
dtype. The caller is responsible for ensuring validity of input data."""""""
pandas/core/arrays/boolean.py,"def coerce_to_array(values, mask=None, copy: bool=False) -> tuple[np.ndarray, np.ndarray]:
    if isinstance(values, BooleanArray):
        if mask is not None:
            raise ValueError('cannot pass mask for BooleanArray input')
        (values, mask) = (values._data, values._mask)
        if copy:
            values = values.copy()
            mask = mask.copy()
        return (values, mask)
    mask_values = None
    if isinstance(values, np.ndarray) and values.dtype == np.bool_:
        if copy:
            values = values.copy()
    elif isinstance(values, np.ndarray) and values.dtype.kind in 'iufcb':
        mask_values = isna(values)
        values_bool = np.zeros(len(values), dtype=bool)
        values_bool[~mask_values] = values[~mask_values].astype(bool)
        if not np.all(values_bool[~mask_values].astype(values.dtype) == values[~mask_values]):
            raise TypeError('Need to pass bool-like values')
        values = values_bool
    else:
        values_object = np.asarray(values, dtype=object)
        inferred_dtype = lib.infer_dtype(values_object, skipna=True)
        integer_like = ('floating', 'integer', 'mixed-integer-float')
        if inferred_dtype not in ('boolean', 'empty') + integer_like:
            raise TypeError('Need to pass bool-like values')
        mask_values = cast('npt.NDArray[np.bool_]', isna(values_object))
        values = np.zeros(len(values), dtype=bool)
        values[~mask_values] = values_object[~mask_values].astype(bool)
        if inferred_dtype in integer_like and (not np.all(values[~mask_values].astype(float) == values_object[~mask_values].astype(float))):
            raise TypeError('Need to pass bool-like values')
    if mask is None and mask_values is None:
        mask = np.zeros(values.shape, dtype=bool)
    elif mask is None:
        mask = mask_values
    elif isinstance(mask, np.ndarray) and mask.dtype == np.bool_:
        if mask_values is not None:
            mask = mask | mask_values
        elif copy:
            mask = mask.copy()
    else:
        mask = np.array(mask, dtype=bool)
        if mask_values is not None:
            mask = mask | mask_values
    if values.shape != mask.shape:
        raise ValueError('values.shape and mask.shape must match')
    return (values, mask)","""""""Coerce the input values array to numpy arrays with a mask.

Parameters
----------
values : 1D list-like
mask : bool 1D array, optional
copy : bool, default False
    if True, copy the input

Returns
-------
tuple of (values, mask)"""""""
pandas/core/arrays/categorical.py,"def contains(cat, key, container) -> bool:
    hash(key)
    try:
        loc = cat.categories.get_loc(key)
    except (KeyError, TypeError):
        return False
    if is_scalar(loc):
        return loc in container
    else:
        return any((loc_ in container for loc_ in loc))","""""""Helper for membership check for ``key`` in ``cat``.

This is a helper method for :method:`__contains__`
and :class:`CategoricalIndex.__contains__`.

Returns True if ``key`` is in ``cat.categories`` and the
location of ``key`` in ``categories`` is in ``container``.

Parameters
----------
cat : :class:`Categorical`or :class:`categoricalIndex`
key : a hashable object
    The key to check membership for.
container : Container (e.g. list-like or mapping)
    The container to check for membership in.

Returns
-------
is_in : bool
    True if ``key`` is in ``self.categories`` and location of
    ``key`` in ``categories`` is in ``container``, else False.

Notes
-----
This method does not check for NaN values. Do that separately
before calling this method."""""""
pandas/core/arrays/categorical.py,"def _get_codes_for_values(values: Index | Series | ExtensionArray | np.ndarray, categories: Index) -> np.ndarray:
    codes = categories.get_indexer_for(values)
    return coerce_indexer_dtype(codes, categories)","""""""utility routine to turn values into codes given the specified categories

If `values` is known to be a Categorical, use recode_for_categories instead."""""""
pandas/core/arrays/categorical.py,"def recode_for_categories(codes: np.ndarray, old_categories, new_categories, copy: bool=True) -> np.ndarray:
    if len(old_categories) == 0:
        if copy:
            return codes.copy()
        return codes
    elif new_categories.equals(old_categories):
        if copy:
            return codes.copy()
        return codes
    indexer = coerce_indexer_dtype(new_categories.get_indexer_for(old_categories), new_categories)
    new_codes = take_nd(indexer, codes, fill_value=-1)
    return new_codes","""""""Convert a set of codes for to a new set of categories

Parameters
----------
codes : np.ndarray
old_categories, new_categories : Index
copy: bool, default True
    Whether to copy if the codes are unchanged.

Returns
-------
new_codes : np.ndarray[np.int64]

Examples
--------
>>> old_cat = pd.Index(['b', 'a', 'c'])
>>> new_cat = pd.Index(['a', 'b'])
>>> codes = np.array([0, 1, 1, 2])
>>> recode_for_categories(codes, old_cat, new_cat)
array([ 1,  0,  0, -1], dtype=int8)"""""""
pandas/core/arrays/categorical.py,"def factorize_from_iterable(values) -> tuple[np.ndarray, Index]:
    from pandas import CategoricalIndex
    if not is_list_like(values):
        raise TypeError('Input must be list-like')
    categories: Index
    vdtype = getattr(values, 'dtype', None)
    if isinstance(vdtype, CategoricalDtype):
        values = extract_array(values)
        cat_codes = np.arange(len(values.categories), dtype=values.codes.dtype)
        cat = Categorical.from_codes(cat_codes, dtype=values.dtype, validate=False)
        categories = CategoricalIndex(cat)
        codes = values.codes
    else:
        cat = Categorical(values, ordered=False)
        categories = cat.categories
        codes = cat.codes
    return (codes, categories)","""""""Factorize an input `values` into `categories` and `codes`. Preserves
categorical dtype in `categories`.

Parameters
----------
values : list-like

Returns
-------
codes : ndarray
categories : Index
    If `values` has a categorical dtype, then `categories` is
    a CategoricalIndex keeping the categories and order of `values`."""""""
pandas/core/arrays/categorical.py,"def factorize_from_iterables(iterables) -> tuple[list[np.ndarray], list[Index]]:
    if len(iterables) == 0:
        return ([], [])
    (codes, categories) = zip(*(factorize_from_iterable(it) for it in iterables))
    return (list(codes), list(categories))","""""""A higher-level wrapper over `factorize_from_iterable`.

Parameters
----------
iterables : list-like of list-likes

Returns
-------
codes : list of ndarrays
categories : list of Indexes

Notes
-----
See `factorize_from_iterable` for more info."""""""
pandas/core/arrays/datetimelike.py,"def _period_dispatch(meth: F) -> F:

    @wraps(meth)
    def new_meth(self, *args, **kwargs):
        if not isinstance(self.dtype, PeriodDtype):
            return meth(self, *args, **kwargs)
        arr = self.view('M8[ns]')
        result = meth(arr, *args, **kwargs)
        if result is NaT:
            return NaT
        elif isinstance(result, Timestamp):
            return self._box_func(result._value)
        res_i8 = result.view('i8')
        return self._from_backing_data(res_i8)
    return cast(F, new_meth)","""""""For PeriodArray methods, dispatch to DatetimeArray and re-wrap the results
in PeriodArray.  We cannot use ._ndarray directly for the affected
methods because the i8 data has different semantics on NaT values."""""""
pandas/core/arrays/datetimelike.py,"def validate_periods(periods: int | float | None) -> int | None:
    if periods is not None:
        if lib.is_float(periods):
            periods = int(periods)
        elif not lib.is_integer(periods):
            raise TypeError(f'periods must be a number, got {periods}')
    return periods","""""""If a `periods` argument is passed to the Datetime/Timedelta Array/Index
constructor, cast it to an integer.

Parameters
----------
periods : None, float, int

Returns
-------
periods : None or int

Raises
------
TypeError
    if periods is None, float, or int"""""""
pandas/core/arrays/datetimelike.py,"def validate_inferred_freq(freq, inferred_freq, freq_infer) -> tuple[BaseOffset | None, bool]:
    if inferred_freq is not None:
        if freq is not None and freq != inferred_freq:
            raise ValueError(f'Inferred frequency {inferred_freq} from passed values does not conform to passed frequency {freq.freqstr}')
        if freq is None:
            freq = inferred_freq
        freq_infer = False
    return (freq, freq_infer)","""""""If the user passes a freq and another freq is inferred from passed data,
require that they match.

Parameters
----------
freq : DateOffset or None
inferred_freq : DateOffset or None
freq_infer : bool

Returns
-------
freq : DateOffset or None
freq_infer : bool

Notes
-----
We assume at this point that `maybe_infer_freq` has been called, so
`freq` is either a DateOffset object or None."""""""
pandas/core/arrays/datetimelike.py,"def maybe_infer_freq(freq):
    freq_infer = False
    if not isinstance(freq, BaseOffset):
        if freq != 'infer':
            freq = to_offset(freq)
        else:
            freq_infer = True
            freq = None
    return (freq, freq_infer)","""""""Comparing a DateOffset to the string ""infer"" raises, so we need to
be careful about comparisons.  Make a dummy variable `freq_infer` to
signify the case where the given freq is ""infer"" and set freq to None
to avoid comparison trouble later on.

Parameters
----------
freq : {DateOffset, None, str}

Returns
-------
freq : {DateOffset, None}
freq_infer : bool
    Whether we should inherit the freq of passed data."""""""
pandas/core/arrays/datetimelike.py,"def dtype_to_unit(dtype: DatetimeTZDtype | np.dtype) -> str:
    if isinstance(dtype, DatetimeTZDtype):
        return dtype.unit
    return np.datetime_data(dtype)[0]","""""""Return the unit str corresponding to the dtype's resolution.

Parameters
----------
dtype : DatetimeTZDtype or np.dtype
    If np.dtype, we assume it is a datetime64 dtype.

Returns
-------
str"""""""
pandas/core/arrays/datetimes.py,"def tz_to_dtype(tz: tzinfo | None, unit: str='ns') -> np.dtype[np.datetime64] | DatetimeTZDtype:
    if tz is None:
        return np.dtype(f'M8[{unit}]')
    else:
        return DatetimeTZDtype(tz=tz, unit=unit)","""""""Return a datetime64[ns] dtype appropriate for the given timezone.

Parameters
----------
tz : tzinfo or None
unit : str, default ""ns""

Returns
-------
np.dtype or Datetime64TZDType"""""""
pandas/core/arrays/datetimes.py,"def _sequence_to_dt64ns(data, *, copy: bool=False, tz: tzinfo | None=None, dayfirst: bool=False, yearfirst: bool=False, ambiguous: TimeAmbiguous='raise', out_unit: str | None=None):
    inferred_freq = None
    (data, copy) = dtl.ensure_arraylike_for_datetimelike(data, copy, cls_name='DatetimeArray')
    if isinstance(data, DatetimeArray):
        inferred_freq = data.freq
    (data, copy) = maybe_convert_dtype(data, copy, tz=tz)
    data_dtype = getattr(data, 'dtype', None)
    out_dtype = DT64NS_DTYPE
    if out_unit is not None:
        out_dtype = np.dtype(f'M8[{out_unit}]')
    if data_dtype == object or is_string_dtype(data_dtype):
        copy = False
        if lib.infer_dtype(data, skipna=False) == 'integer':
            data = data.astype(np.int64)
        elif tz is not None and ambiguous == 'raise':
            obj_data = np.asarray(data, dtype=object)
            i8data = tslib.array_to_datetime_with_tz(obj_data, tz)
            return (i8data.view(DT64NS_DTYPE), tz, None)
        else:
            (data, inferred_tz) = objects_to_datetime64ns(data, dayfirst=dayfirst, yearfirst=yearfirst, allow_object=False)
            if tz and inferred_tz:
                assert data.dtype == 'i8'
                return (data.view(DT64NS_DTYPE), tz, None)
            elif inferred_tz:
                tz = inferred_tz
        data_dtype = data.dtype
    if isinstance(data_dtype, DatetimeTZDtype):
        tz = _maybe_infer_tz(tz, data.tz)
        result = data._ndarray
    elif lib.is_np_dtype(data_dtype, 'M'):
        data = getattr(data, '_ndarray', data)
        new_dtype = data.dtype
        data_unit = get_unit_from_dtype(new_dtype)
        if not is_supported_unit(data_unit):
            new_reso = get_supported_reso(data_unit)
            new_unit = npy_unit_to_abbrev(new_reso)
            new_dtype = np.dtype(f'M8[{new_unit}]')
            data = astype_overflowsafe(data, dtype=new_dtype, copy=False)
            data_unit = get_unit_from_dtype(new_dtype)
            copy = False
        if data.dtype.byteorder == '>':
            data = data.astype(data.dtype.newbyteorder('<'))
            new_dtype = data.dtype
            copy = False
        if tz is not None:
            shape = data.shape
            if data.ndim > 1:
                data = data.ravel()
            data = tzconversion.tz_localize_to_utc(data.view('i8'), tz, ambiguous=ambiguous, creso=data_unit)
            data = data.view(new_dtype)
            data = data.reshape(shape)
        assert data.dtype == new_dtype, data.dtype
        result = data
    else:
        if data.dtype != INT64_DTYPE:
            data = data.astype(np.int64, copy=False)
        result = data.view(out_dtype)
    if copy:
        result = result.copy()
    assert isinstance(result, np.ndarray), type(result)
    assert result.dtype.kind == 'M'
    assert result.dtype != 'M8'
    assert is_supported_unit(get_unit_from_dtype(result.dtype))
    return (result, tz, inferred_freq)","""""""Parameters
----------
data : list-like
copy : bool, default False
tz : tzinfo or None, default None
dayfirst : bool, default False
yearfirst : bool, default False
ambiguous : str, bool, or arraylike, default 'raise'
    See pandas._libs.tslibs.tzconversion.tz_localize_to_utc.
out_unit : str or None, default None
    Desired output resolution.

Returns
-------
result : numpy.ndarray
    The sequence converted to a numpy array with dtype ``datetime64[ns]``.
tz : tzinfo or None
    Either the user-provided tzinfo or one inferred from the data.
inferred_freq : Tick or None
    The inferred frequency of the sequence.

Raises
------
TypeError : PeriodDType data is passed"""""""
pandas/core/arrays/datetimes.py,"def objects_to_datetime64ns(data: np.ndarray, dayfirst, yearfirst, utc: bool=False, errors: DateTimeErrorChoices='raise', allow_object: bool=False):
    assert errors in ['raise', 'ignore', 'coerce']
    data = np.array(data, copy=False, dtype=np.object_)
    (result, tz_parsed) = tslib.array_to_datetime(data, errors=errors, utc=utc, dayfirst=dayfirst, yearfirst=yearfirst)
    if tz_parsed is not None:
        return (result.view('i8'), tz_parsed)
    elif result.dtype.kind == 'M':
        return (result, tz_parsed)
    elif result.dtype == object:
        if allow_object:
            return (result, tz_parsed)
        raise TypeError('DatetimeIndex has mixed timezones')
    else:
        raise TypeError(result)","""""""Convert data to array of timestamps.

Parameters
----------
data : np.ndarray[object]
dayfirst : bool
yearfirst : bool
utc : bool, default False
    Whether to convert/localize timestamps to UTC.
errors : {'raise', 'ignore', 'coerce'}
allow_object : bool
    Whether to return an object-dtype ndarray instead of raising if the
    data contains more than one timezone.

Returns
-------
result : ndarray
    np.int64 dtype if returned values represent UTC timestamps
    np.datetime64[ns] if returned values represent wall times
    object if mixed timezones
inferred_tz : tzinfo or None

Raises
------
ValueError : if data cannot be converted to datetimes"""""""
pandas/core/arrays/datetimes.py,"def maybe_convert_dtype(data, copy: bool, tz: tzinfo | None=None):
    if not hasattr(data, 'dtype'):
        return (data, copy)
    if is_float_dtype(data.dtype):
        data = data.astype(DT64NS_DTYPE).view('i8')
        copy = False
    elif lib.is_np_dtype(data.dtype, 'm') or is_bool_dtype(data.dtype):
        raise TypeError(f'dtype {data.dtype} cannot be converted to datetime64[ns]')
    elif isinstance(data.dtype, PeriodDtype):
        raise TypeError('Passing PeriodDtype data is invalid. Use `data.to_timestamp()` instead')
    elif isinstance(data.dtype, ExtensionDtype) and (not isinstance(data.dtype, DatetimeTZDtype)):
        data = np.array(data, dtype=np.object_)
        copy = False
    return (data, copy)","""""""Convert data based on dtype conventions, issuing
errors where appropriate.

Parameters
----------
data : np.ndarray or pd.Index
copy : bool
tz : tzinfo or None, default None

Returns
-------
data : np.ndarray or pd.Index
copy : bool

Raises
------
TypeError : PeriodDType data is passed"""""""
pandas/core/arrays/datetimes.py,"def _maybe_infer_tz(tz: tzinfo | None, inferred_tz: tzinfo | None) -> tzinfo | None:
    if tz is None:
        tz = inferred_tz
    elif inferred_tz is None:
        pass
    elif not timezones.tz_compare(tz, inferred_tz):
        raise TypeError(f'data is already tz-aware {inferred_tz}, unable to set specified tz: {tz}')
    return tz","""""""If a timezone is inferred from data, check that it is compatible with
the user-provided timezone, if any.

Parameters
----------
tz : tzinfo or None
inferred_tz : tzinfo or None

Returns
-------
tz : tzinfo or None

Raises
------
TypeError : if both timezones are present but do not match"""""""
pandas/core/arrays/datetimes.py,"def _validate_dt64_dtype(dtype):
    if dtype is not None:
        dtype = pandas_dtype(dtype)
        if dtype == np.dtype('M8'):
            msg = ""Passing in 'datetime64' dtype with no precision is not allowed. Please pass in 'datetime64[ns]' instead.""
            raise ValueError(msg)
        if isinstance(dtype, np.dtype) and (dtype.kind != 'M' or not is_supported_unit(get_unit_from_dtype(dtype))) or not isinstance(dtype, (np.dtype, DatetimeTZDtype)):
            raise ValueError(f""Unexpected value for 'dtype': '{dtype}'. Must be 'datetime64[s]', 'datetime64[ms]', 'datetime64[us]', 'datetime64[ns]' or DatetimeTZDtype'."")
        if getattr(dtype, 'tz', None):
            dtype = cast(DatetimeTZDtype, dtype)
            dtype = DatetimeTZDtype(unit=dtype.unit, tz=timezones.tz_standardize(dtype.tz))
    return dtype","""""""Check that a dtype, if passed, represents either a numpy datetime64[ns]
dtype or a pandas DatetimeTZDtype.

Parameters
----------
dtype : object

Returns
-------
dtype : None, numpy.dtype, or DatetimeTZDtype

Raises
------
ValueError : invalid dtype

Notes
-----
Unlike _validate_tz_from_dtype, this does _not_ allow non-existent
tz errors to go through"""""""
pandas/core/arrays/datetimes.py,"def _validate_tz_from_dtype(dtype, tz: tzinfo | None, explicit_tz_none: bool=False) -> tzinfo | None:
    if dtype is not None:
        if isinstance(dtype, str):
            try:
                dtype = DatetimeTZDtype.construct_from_string(dtype)
            except TypeError:
                pass
        dtz = getattr(dtype, 'tz', None)
        if dtz is not None:
            if tz is not None and (not timezones.tz_compare(tz, dtz)):
                raise ValueError('cannot supply both a tz and a dtype with a tz')
            if explicit_tz_none:
                raise ValueError('Cannot pass both a timezone-aware dtype and tz=None')
            tz = dtz
        if tz is not None and lib.is_np_dtype(dtype, 'M'):
            if tz is not None and (not timezones.tz_compare(tz, dtz)):
                raise ValueError('cannot supply both a tz and a timezone-naive dtype (i.e. datetime64[ns])')
    return tz","""""""If the given dtype is a DatetimeTZDtype, extract the implied
tzinfo object from it and check that it does not conflict with the given
tz.

Parameters
----------
dtype : dtype, str
tz : None, tzinfo
explicit_tz_none : bool, default False
    Whether tz=None was passed explicitly, as opposed to lib.no_default.

Returns
-------
tz : consensus tzinfo

Raises
------
ValueError : on tzinfo mismatch"""""""
pandas/core/arrays/datetimes.py,"def _infer_tz_from_endpoints(start: Timestamp, end: Timestamp, tz: tzinfo | None) -> tzinfo | None:
    try:
        inferred_tz = timezones.infer_tzinfo(start, end)
    except AssertionError as err:
        raise TypeError('Start and end cannot both be tz-aware with different timezones') from err
    inferred_tz = timezones.maybe_get_tz(inferred_tz)
    tz = timezones.maybe_get_tz(tz)
    if tz is not None and inferred_tz is not None:
        if not timezones.tz_compare(inferred_tz, tz):
            raise AssertionError('Inferred time zone not equal to passed time zone')
    elif inferred_tz is not None:
        tz = inferred_tz
    return tz","""""""If a timezone is not explicitly given via `tz`, see if one can
be inferred from the `start` and `end` endpoints.  If more than one
of these inputs provides a timezone, require that they all agree.

Parameters
----------
start : Timestamp
end : Timestamp
tz : tzinfo or None

Returns
-------
tz : tzinfo or None

Raises
------
TypeError : if start and end timezones do not agree"""""""
pandas/core/arrays/datetimes.py,"def _maybe_localize_point(ts, is_none, is_not_none, freq, tz, ambiguous, nonexistent):
    if is_none is None and is_not_none is not None:
        ambiguous = ambiguous if ambiguous != 'infer' else False
        localize_args = {'ambiguous': ambiguous, 'nonexistent': nonexistent, 'tz': None}
        if isinstance(freq, Tick) or freq is None:
            localize_args['tz'] = tz
        ts = ts.tz_localize(**localize_args)
    return ts","""""""Localize a start or end Timestamp to the timezone of the corresponding
start or end Timestamp

Parameters
----------
ts : start or end Timestamp to potentially localize
is_none : argument that should be None
is_not_none : argument that should not be None
freq : Tick, DateOffset, or None
tz : str, timezone object or None
ambiguous: str, localization behavior for ambiguous times
nonexistent: str, localization behavior for nonexistent times

Returns
-------
ts : Timestamp"""""""
pandas/core/arrays/datetimes.py,"def _generate_range(start: Timestamp | None, end: Timestamp | None, periods: int | None, offset: BaseOffset, *, unit: str):
    offset = to_offset(offset)
    start = Timestamp(start)
    if start is not NaT:
        start = start.as_unit(unit)
    else:
        start = None
    end = Timestamp(end)
    if end is not NaT:
        end = end.as_unit(unit)
    else:
        end = None
    if start and (not offset.is_on_offset(start)):
        start = offset.rollforward(start)
    elif end and (not offset.is_on_offset(end)):
        end = offset.rollback(end)
    if periods is None and end < start and (offset.n >= 0):
        end = None
        periods = 0
    if end is None:
        end = start + (periods - 1) * offset
    if start is None:
        start = end - (periods - 1) * offset
    start = cast(Timestamp, start)
    end = cast(Timestamp, end)
    cur = start
    if offset.n >= 0:
        while cur <= end:
            yield cur
            if cur == end:
                break
            with warnings.catch_warnings():
                warnings.filterwarnings('ignore', 'Discarding nonzero nanoseconds in conversion', category=UserWarning)
                next_date = offset._apply(cur)
            next_date = next_date.as_unit(unit)
            if next_date <= cur:
                raise ValueError(f'Offset {offset} did not increment date')
            cur = next_date
    else:
        while cur >= end:
            yield cur
            if cur == end:
                break
            with warnings.catch_warnings():
                warnings.filterwarnings('ignore', 'Discarding nonzero nanoseconds in conversion', category=UserWarning)
                next_date = offset._apply(cur)
            next_date = next_date.as_unit(unit)
            if next_date >= cur:
                raise ValueError(f'Offset {offset} did not decrement date')
            cur = next_date","""""""Generates a sequence of dates corresponding to the specified time
offset. Similar to dateutil.rrule except uses pandas DateOffset
objects to represent time increments.

Parameters
----------
start : Timestamp or None
end : Timestamp or None
periods : int or None
offset : DateOffset
unit : str

Notes
-----
* This method is faster for generating weekdays than dateutil.rrule
* At least two of (start, end, periods) must be specified.
* If both start and end are specified, the returned dates will
satisfy start <= date <= end.

Returns
-------
dates : generator object"""""""
pandas/core/arrays/interval.py,"def _maybe_convert_platform_interval(values) -> ArrayLike:
    if isinstance(values, (list, tuple)) and len(values) == 0:
        return np.array([], dtype=np.int64)
    elif not is_list_like(values) or isinstance(values, ABCDataFrame):
        return values
    elif isinstance(getattr(values, 'dtype', None), CategoricalDtype):
        values = np.asarray(values)
    elif not hasattr(values, 'dtype') and (not isinstance(values, (list, tuple, range))):
        return values
    else:
        values = extract_array(values, extract_numpy=True)
    if not hasattr(values, 'dtype'):
        values = np.asarray(values)
        if values.dtype.kind in 'iu' and values.dtype != np.int64:
            values = values.astype(np.int64)
    return values","""""""Try to do platform conversion, with special casing for IntervalArray.
Wrapper around maybe_convert_platform that alters the default return
dtype in certain cases to be compatible with IntervalArray.  For example,
empty lists return with integer dtype instead of object dtype, which is
prohibited for IntervalArray.

Parameters
----------
values : array-like

Returns
-------
array"""""""
pandas/core/arrays/masked.py,"def transpose_homogeneous_masked_arrays(masked_arrays: Sequence[BaseMaskedArray]) -> list[BaseMaskedArray]:
    masked_arrays = list(masked_arrays)
    values = [arr._data.reshape(1, -1) for arr in masked_arrays]
    transposed_values = np.concatenate(values, axis=0)
    masks = [arr._mask.reshape(1, -1) for arr in masked_arrays]
    transposed_masks = np.concatenate(masks, axis=0)
    dtype = masked_arrays[0].dtype
    arr_type = dtype.construct_array_type()
    transposed_arrays: list[BaseMaskedArray] = []
    for i in range(transposed_values.shape[1]):
        transposed_arr = arr_type(transposed_values[:, i], mask=transposed_masks[:, i])
        transposed_arrays.append(transposed_arr)
    return transposed_arrays","""""""Transpose masked arrays in a list, but faster.

Input should be a list of 1-dim masked arrays of equal length and all have the
same dtype. The caller is responsible for ensuring validity of input data."""""""
pandas/core/arrays/period.py,"def raise_on_incompatible(left, right) -> IncompatibleFrequency:
    if isinstance(right, (np.ndarray, ABCTimedeltaArray)) or right is None:
        other_freq = None
    elif isinstance(right, BaseOffset):
        other_freq = freq_to_period_freqstr(right.n, right.name)
    elif isinstance(right, (ABCPeriodIndex, PeriodArray, Period)):
        other_freq = right.freqstr
    else:
        other_freq = delta_to_tick(Timedelta(right)).freqstr
    own_freq = freq_to_period_freqstr(left.freq.n, left.freq.name)
    msg = DIFFERENT_FREQ.format(cls=type(left).__name__, own_freq=own_freq, other_freq=other_freq)
    return IncompatibleFrequency(msg)","""""""Helper function to render a consistent error message when raising
IncompatibleFrequency.

Parameters
----------
left : PeriodArray
right : None, DateOffset, Period, ndarray, or timedelta-like

Returns
-------
IncompatibleFrequency
    Exception to be raised by the caller."""""""
pandas/core/arrays/period.py,"def period_array(data: Sequence[Period | str | None] | AnyArrayLike, freq: str | Tick | BaseOffset | None=None, copy: bool=False) -> PeriodArray:
    data_dtype = getattr(data, 'dtype', None)
    if lib.is_np_dtype(data_dtype, 'M'):
        return PeriodArray._from_datetime64(data, freq)
    if isinstance(data_dtype, PeriodDtype):
        out = PeriodArray(data)
        if freq is not None:
            if freq == data_dtype.freq:
                return out
            return out.asfreq(freq)
        return out
    if not isinstance(data, (np.ndarray, list, tuple, ABCSeries)):
        data = list(data)
    arrdata = np.asarray(data)
    dtype: PeriodDtype | None
    if freq:
        dtype = PeriodDtype(freq)
    else:
        dtype = None
    if arrdata.dtype.kind == 'f' and len(arrdata) > 0:
        raise TypeError('PeriodIndex does not allow floating point in construction')
    if arrdata.dtype.kind in 'iu':
        arr = arrdata.astype(np.int64, copy=False)
        ordinals = libperiod.from_ordinals(arr, freq)
        return PeriodArray(ordinals, dtype=dtype)
    data = ensure_object(arrdata)
    return PeriodArray._from_sequence(data, dtype=dtype)","""""""Construct a new PeriodArray from a sequence of Period scalars.

Parameters
----------
data : Sequence of Period objects
    A sequence of Period objects. These are required to all have
    the same ``freq.`` Missing values can be indicated by ``None``
    or ``pandas.NaT``.
freq : str, Tick, or Offset
    The frequency of every element of the array. This can be specified
    to avoid inferring the `freq` from `data`.
copy : bool, default False
    Whether to ensure a copy of the data is made.

Returns
-------
PeriodArray

See Also
--------
PeriodArray
pandas.PeriodIndex

Examples
--------
>>> period_array([pd.Period('2017', freq='A'),
...               pd.Period('2018', freq='A')])
<PeriodArray>
['2017', '2018']
Length: 2, dtype: period[A-DEC]

>>> period_array([pd.Period('2017', freq='A'),
...               pd.Period('2018', freq='A'),
...               pd.NaT])
<PeriodArray>
['2017', '2018', 'NaT']
Length: 3, dtype: period[A-DEC]

Integers that look like years are handled

>>> period_array([2000, 2001, 2002], freq='D')
<PeriodArray>
['2000-01-01', '2001-01-01', '2002-01-01']
Length: 3, dtype: period[D]

Datetime-like strings may also be passed

>>> period_array(['2000-Q1', '2000-Q2', '2000-Q3', '2000-Q4'], freq='Q')
<PeriodArray>
['2000Q1', '2000Q2', '2000Q3', '2000Q4']
Length: 4, dtype: period[Q-DEC]"""""""
pandas/core/arrays/period.py,"def validate_dtype_freq(dtype, freq: BaseOffsetT | BaseOffset | timedelta | str | None) -> BaseOffsetT:
    if freq is not None:
        freq = to_offset(freq, is_period=True)
    if dtype is not None:
        dtype = pandas_dtype(dtype)
        if not isinstance(dtype, PeriodDtype):
            raise ValueError('dtype must be PeriodDtype')
        if freq is None:
            freq = dtype.freq
        elif freq != dtype.freq:
            raise IncompatibleFrequency('specified freq and dtype are different')
    return freq","""""""If both a dtype and a freq are available, ensure they match.  If only
dtype is available, extract the implied freq.

Parameters
----------
dtype : dtype
freq : DateOffset or None

Returns
-------
freq : DateOffset

Raises
------
ValueError : non-period dtype
IncompatibleFrequency : mismatch between dtype and freq"""""""
pandas/core/arrays/period.py,"def dt64arr_to_periodarr(data, freq, tz=None) -> tuple[npt.NDArray[np.int64], BaseOffset]:
    if not isinstance(data.dtype, np.dtype) or data.dtype.kind != 'M':
        raise ValueError(f'Wrong dtype: {data.dtype}')
    if freq is None:
        if isinstance(data, ABCIndex):
            (data, freq) = (data._values, data.freq)
        elif isinstance(data, ABCSeries):
            (data, freq) = (data._values, data.dt.freq)
    elif isinstance(data, (ABCIndex, ABCSeries)):
        data = data._values
    reso = get_unit_from_dtype(data.dtype)
    freq = Period._maybe_convert_freq(freq)
    base = freq._period_dtype_code
    return (c_dt64arr_to_periodarr(data.view('i8'), base, tz, reso=reso), freq)","""""""Convert an datetime-like array to values Period ordinals.

Parameters
----------
data : Union[Series[datetime64[ns]], DatetimeIndex, ndarray[datetime64ns]]
freq : Optional[Union[str, Tick]]
    Must match the `freq` on the `data` if `data` is a DatetimeIndex
    or Series.
tz : Optional[tzinfo]

Returns
-------
ordinals : ndarray[int64]
freq : Tick
    The frequency extracted from the Series or DatetimeIndex if that's
    used."""""""
pandas/core/arrays/sparse/array.py,"def _get_fill(arr: SparseArray) -> np.ndarray:
    try:
        return np.asarray(arr.fill_value, dtype=arr.dtype.subtype)
    except ValueError:
        return np.asarray(arr.fill_value)","""""""Create a 0-dim ndarray containing the fill value

Parameters
----------
arr : SparseArray

Returns
-------
fill_value : ndarray
    0-dim ndarray with just the fill value.

Notes
-----
coerce fill_value to arr dtype if possible
int64 SparseArray can have NaN as fill_value if there is no missing"""""""
pandas/core/arrays/sparse/array.py,"def _sparse_array_op(left: SparseArray, right: SparseArray, op: Callable, name: str) -> SparseArray:
    if name.startswith('__'):
        name = name[2:-2]
    ltype = left.dtype.subtype
    rtype = right.dtype.subtype
    if ltype != rtype:
        subtype = find_common_type([ltype, rtype])
        ltype = SparseDtype(subtype, left.fill_value)
        rtype = SparseDtype(subtype, right.fill_value)
        left = left.astype(ltype, copy=False)
        right = right.astype(rtype, copy=False)
        dtype = ltype.subtype
    else:
        dtype = ltype
    result_dtype = None
    if left.sp_index.ngaps == 0 or right.sp_index.ngaps == 0:
        with np.errstate(all='ignore'):
            result = op(left.to_dense(), right.to_dense())
            fill = op(_get_fill(left), _get_fill(right))
        if left.sp_index.ngaps == 0:
            index = left.sp_index
        else:
            index = right.sp_index
    elif left.sp_index.equals(right.sp_index):
        with np.errstate(all='ignore'):
            result = op(left.sp_values, right.sp_values)
            fill = op(_get_fill(left), _get_fill(right))
        index = left.sp_index
    else:
        if name[0] == 'r':
            (left, right) = (right, left)
            name = name[1:]
        if name in ('and', 'or', 'xor') and dtype == 'bool':
            opname = f'sparse_{name}_uint8'
            left_sp_values = left.sp_values.view(np.uint8)
            right_sp_values = right.sp_values.view(np.uint8)
            result_dtype = bool
        else:
            opname = f'sparse_{name}_{dtype}'
            left_sp_values = left.sp_values
            right_sp_values = right.sp_values
        if name in ['floordiv', 'mod'] and (right == 0).any() and (left.dtype.kind in 'iu'):
            opname = f'sparse_{name}_float64'
            left_sp_values = left_sp_values.astype('float64')
            right_sp_values = right_sp_values.astype('float64')
        sparse_op = getattr(splib, opname)
        with np.errstate(all='ignore'):
            (result, index, fill) = sparse_op(left_sp_values, left.sp_index, left.fill_value, right_sp_values, right.sp_index, right.fill_value)
    if name == 'divmod':
        return (_wrap_result(name, result[0], index, fill[0], dtype=result_dtype), _wrap_result(name, result[1], index, fill[1], dtype=result_dtype))
    if result_dtype is None:
        result_dtype = result.dtype
    return _wrap_result(name, result, index, fill, dtype=result_dtype)","""""""Perform a binary operation between two arrays.

Parameters
----------
left : Union[SparseArray, ndarray]
right : Union[SparseArray, ndarray]
op : Callable
    The binary operation to perform
name str
    Name of the callable.

Returns
-------
SparseArray"""""""
pandas/core/arrays/sparse/array.py,"def _wrap_result(name: str, data, sparse_index, fill_value, dtype: Dtype | None=None) -> SparseArray:
    if name.startswith('__'):
        name = name[2:-2]
    if name in ('eq', 'ne', 'lt', 'gt', 'le', 'ge'):
        dtype = bool
    fill_value = lib.item_from_zerodim(fill_value)
    if is_bool_dtype(dtype):
        fill_value = bool(fill_value)
    return SparseArray(data, sparse_index=sparse_index, fill_value=fill_value, dtype=dtype)","""""""wrap op result to have correct dtype"""""""
pandas/core/arrays/sparse/array.py,"def _make_sparse(arr: np.ndarray, kind: SparseIndexKind='block', fill_value=None, dtype: np.dtype | None=None):
    assert isinstance(arr, np.ndarray)
    if arr.ndim > 1:
        raise TypeError('expected dimension <= 1 data')
    if fill_value is None:
        fill_value = na_value_for_dtype(arr.dtype)
    if isna(fill_value):
        mask = notna(arr)
    else:
        if is_string_dtype(arr.dtype):
            arr = arr.astype(object)
        if is_object_dtype(arr.dtype):
            mask = splib.make_mask_object_ndarray(arr, fill_value)
        else:
            mask = arr != fill_value
    length = len(arr)
    if length != len(mask):
        indices = mask.sp_index.indices
    else:
        indices = mask.nonzero()[0].astype(np.int32)
    index = make_sparse_index(length, indices, kind)
    sparsified_values = arr[mask]
    if dtype is not None:
        sparsified_values = ensure_wrapped_if_datetimelike(sparsified_values)
        sparsified_values = astype_array(sparsified_values, dtype=dtype)
        sparsified_values = np.asarray(sparsified_values)
    return (sparsified_values, index, fill_value)","""""""Convert ndarray to sparse format

Parameters
----------
arr : ndarray
kind : {'block', 'integer'}
fill_value : NaN or another value
dtype : np.dtype, optional
copy : bool, default False

Returns
-------
(sparse_values, index, fill_value) : (ndarray, SparseIndex, Scalar)"""""""
pandas/core/arrays/sparse/scipy_sparse.py,"def _levels_to_axis(ss, levels: tuple[int] | list[int], valid_ilocs: npt.NDArray[np.intp], sort_labels: bool=False) -> tuple[npt.NDArray[np.intp], list[IndexLabel]]:
    if sort_labels and len(levels) == 1:
        ax_coords = ss.index.codes[levels[0]][valid_ilocs]
        ax_labels = ss.index.levels[levels[0]]
    else:
        levels_values = lib.fast_zip([ss.index.get_level_values(lvl).to_numpy() for lvl in levels])
        (codes, ax_labels) = factorize(levels_values, sort=sort_labels)
        ax_coords = codes[valid_ilocs]
    ax_labels = ax_labels.tolist()
    return (ax_coords, ax_labels)","""""""For a MultiIndexed sparse Series `ss`, return `ax_coords` and `ax_labels`,
where `ax_coords` are the coordinates along one of the two axes of the
destination sparse matrix, and `ax_labels` are the labels from `ss`' Index
which correspond to these coordinates.

Parameters
----------
ss : Series
levels : tuple/list
valid_ilocs : numpy.ndarray
    Array of integer positions of valid values for the sparse matrix in ss.
sort_labels : bool, default False
    Sort the axis labels before forming the sparse matrix. When `levels`
    refers to a single level, set to True for a faster execution.

Returns
-------
ax_coords : numpy.ndarray (axis coordinates)
ax_labels : list (axis labels)"""""""
pandas/core/arrays/sparse/scipy_sparse.py,"def _to_ijv(ss, row_levels: tuple[int] | list[int]=(0,), column_levels: tuple[int] | list[int]=(1,), sort_labels: bool=False) -> tuple[np.ndarray, npt.NDArray[np.intp], npt.NDArray[np.intp], list[IndexLabel], list[IndexLabel]]:
    _check_is_partition([row_levels, column_levels], range(ss.index.nlevels))
    sp_vals = ss.array.sp_values
    na_mask = notna(sp_vals)
    values = sp_vals[na_mask]
    valid_ilocs = ss.array.sp_index.indices[na_mask]
    (i_coords, i_labels) = _levels_to_axis(ss, row_levels, valid_ilocs, sort_labels=sort_labels)
    (j_coords, j_labels) = _levels_to_axis(ss, column_levels, valid_ilocs, sort_labels=sort_labels)
    return (values, i_coords, j_coords, i_labels, j_labels)","""""""For an arbitrary MultiIndexed sparse Series return (v, i, j, ilabels,
jlabels) where (v, (i, j)) is suitable for passing to scipy.sparse.coo
constructor, and ilabels and jlabels are the row and column labels
respectively.

Parameters
----------
ss : Series
row_levels : tuple/list
column_levels : tuple/list
sort_labels : bool, default False
    Sort the row and column labels before forming the sparse matrix.
    When `row_levels` and/or `column_levels` refer to a single level,
    set to `True` for a faster execution.

Returns
-------
values : numpy.ndarray
    Valid values to populate a sparse matrix, extracted from
    ss.
i_coords : numpy.ndarray (row coordinates of the values)
j_coords : numpy.ndarray (column coordinates of the values)
i_labels : list (row labels)
j_labels : list (column labels)"""""""
pandas/core/arrays/sparse/scipy_sparse.py,"def sparse_series_to_coo(ss: Series, row_levels: Iterable[int]=(0,), column_levels: Iterable[int]=(1,), sort_labels: bool=False) -> tuple[scipy.sparse.coo_matrix, list[IndexLabel], list[IndexLabel]]:
    import scipy.sparse
    if ss.index.nlevels < 2:
        raise ValueError('to_coo requires MultiIndex with nlevels >= 2.')
    if not ss.index.is_unique:
        raise ValueError('Duplicate index entries are not allowed in to_coo transformation.')
    row_levels = [ss.index._get_level_number(x) for x in row_levels]
    column_levels = [ss.index._get_level_number(x) for x in column_levels]
    (v, i, j, rows, columns) = _to_ijv(ss, row_levels=row_levels, column_levels=column_levels, sort_labels=sort_labels)
    sparse_matrix = scipy.sparse.coo_matrix((v, (i, j)), shape=(len(rows), len(columns)))
    return (sparse_matrix, rows, columns)","""""""Convert a sparse Series to a scipy.sparse.coo_matrix using index
levels row_levels, column_levels as the row and column
labels respectively. Returns the sparse_matrix, row and column labels."""""""
pandas/core/arrays/sparse/scipy_sparse.py,"def coo_to_sparse_series(A: scipy.sparse.coo_matrix, dense_index: bool=False) -> Series:
    from pandas import SparseDtype
    try:
        ser = Series(A.data, MultiIndex.from_arrays((A.row, A.col)), copy=False)
    except AttributeError as err:
        raise TypeError(f'Expected coo_matrix. Got {type(A).__name__} instead.') from err
    ser = ser.sort_index()
    ser = ser.astype(SparseDtype(ser.dtype))
    if dense_index:
        ind = MultiIndex.from_product([A.row, A.col])
        ser = ser.reindex(ind)
    return ser","""""""Convert a scipy.sparse.coo_matrix to a Series with type sparse.

Parameters
----------
A : scipy.sparse.coo_matrix
dense_index : bool, default False

Returns
-------
Series

Raises
------
TypeError if A is not a coo_matrix"""""""
pandas/core/arrays/timedeltas.py,"def sequence_to_td64ns(data, copy: bool=False, unit=None, errors: DateTimeErrorChoices='raise') -> tuple[np.ndarray, Tick | None]:
    assert unit not in ['Y', 'y', 'M']
    inferred_freq = None
    if unit is not None:
        unit = parse_timedelta_unit(unit)
    (data, copy) = dtl.ensure_arraylike_for_datetimelike(data, copy, cls_name='TimedeltaArray')
    if isinstance(data, TimedeltaArray):
        inferred_freq = data.freq
    if data.dtype == object or is_string_dtype(data.dtype):
        data = _objects_to_td64ns(data, unit=unit, errors=errors)
        copy = False
    elif is_integer_dtype(data.dtype):
        (data, copy_made) = _ints_to_td64ns(data, unit=unit)
        copy = copy and (not copy_made)
    elif is_float_dtype(data.dtype):
        if isinstance(data.dtype, ExtensionDtype):
            mask = data._mask
            data = data._data
        else:
            mask = np.isnan(data)
        (m, p) = precision_from_unit(unit or 'ns')
        with warnings.catch_warnings():
            warnings.filterwarnings('ignore', 'invalid value encountered in cast', RuntimeWarning)
            base = data.astype(np.int64)
        frac = data - base
        if p:
            frac = np.round(frac, p)
        with warnings.catch_warnings():
            warnings.filterwarnings('ignore', 'invalid value encountered in cast', RuntimeWarning)
            data = (base * m + (frac * m).astype(np.int64)).view('timedelta64[ns]')
        data[mask] = iNaT
        copy = False
    elif lib.is_np_dtype(data.dtype, 'm'):
        data_unit = get_unit_from_dtype(data.dtype)
        if not is_supported_unit(data_unit):
            new_reso = get_supported_reso(data_unit)
            new_unit = npy_unit_to_abbrev(new_reso)
            new_dtype = np.dtype(f'm8[{new_unit}]')
            data = astype_overflowsafe(data, dtype=new_dtype, copy=False)
            copy = False
    else:
        raise TypeError(f'dtype {data.dtype} cannot be converted to timedelta64[ns]')
    data = np.array(data, copy=copy)
    assert data.dtype.kind == 'm'
    assert data.dtype != 'm8'
    return (data, inferred_freq)","""""""Parameters
----------
data : list-like
copy : bool, default False
unit : str, optional
    The timedelta unit to treat integers as multiples of. For numeric
    data this defaults to ``'ns'``.
    Must be un-specified if the data contains a str and ``errors==""raise""``.
errors : {""raise"", ""coerce"", ""ignore""}, default ""raise""
    How to handle elements that cannot be converted to timedelta64[ns].
    See ``pandas.to_timedelta`` for details.

Returns
-------
converted : numpy.ndarray
    The sequence converted to a numpy array with dtype ``timedelta64[ns]``.
inferred_freq : Tick or None
    The inferred frequency of the sequence.

Raises
------
ValueError : Data cannot be converted to timedelta64[ns].

Notes
-----
Unlike `pandas.to_timedelta`, if setting ``errors=ignore`` will not cause
errors to be ignored; they are caught and subsequently ignored at a
higher level."""""""
pandas/core/arrays/timedeltas.py,"def _ints_to_td64ns(data, unit: str='ns'):
    copy_made = False
    unit = unit if unit is not None else 'ns'
    if data.dtype != np.int64:
        data = data.astype(np.int64)
        copy_made = True
    if unit != 'ns':
        dtype_str = f'timedelta64[{unit}]'
        data = data.view(dtype_str)
        data = astype_overflowsafe(data, dtype=TD64NS_DTYPE)
        copy_made = True
    else:
        data = data.view('timedelta64[ns]')
    return (data, copy_made)","""""""Convert an ndarray with integer-dtype to timedelta64[ns] dtype, treating
the integers as multiples of the given timedelta unit.

Parameters
----------
data : numpy.ndarray with integer-dtype
unit : str, default ""ns""
    The timedelta unit to treat integers as multiples of.

Returns
-------
numpy.ndarray : timedelta64[ns] array converted from data
bool : whether a copy was made"""""""
pandas/core/arrays/timedeltas.py,"def _objects_to_td64ns(data, unit=None, errors: DateTimeErrorChoices='raise'):
    values = np.array(data, dtype=np.object_, copy=False)
    result = array_to_timedelta64(values, unit=unit, errors=errors)
    return result.view('timedelta64[ns]')","""""""Convert a object-dtyped or string-dtyped array into an
timedelta64[ns]-dtyped array.

Parameters
----------
data : ndarray or Index
unit : str, default ""ns""
    The timedelta unit to treat integers as multiples of.
    Must not be specified if the data contains a str.
errors : {""raise"", ""coerce"", ""ignore""}, default ""raise""
    How to handle elements that cannot be converted to timedelta64[ns].
    See ``pandas.to_timedelta`` for details.

Returns
-------
numpy.ndarray : timedelta64[ns] array converted from data

Raises
------
ValueError : Data cannot be converted to timedelta64[ns].

Notes
-----
Unlike `pandas.to_timedelta`, if setting `errors=ignore` will not cause
errors to be ignored; they are caught and subsequently ignored at a
higher level."""""""
pandas/core/common.py,"def flatten(line):
    for element in line:
        if iterable_not_string(element):
            yield from flatten(element)
        else:
            yield element","""""""Flatten an arbitrarily nested sequence.

Parameters
----------
line : sequence
    The non string sequence to flatten

Notes
-----
This doesn't consider strings sequences.

Returns
-------
flattened : generator"""""""
pandas/core/common.py,"def is_bool_indexer(key: Any) -> bool:
    if isinstance(key, (ABCSeries, np.ndarray, ABCIndex, ABCExtensionArray)):
        if key.dtype == np.object_:
            key_array = np.asarray(key)
            if not lib.is_bool_array(key_array):
                na_msg = 'Cannot mask with non-boolean array containing NA / NaN values'
                if lib.is_bool_array(key_array, skipna=True):
                    raise ValueError(na_msg)
                return False
            return True
        elif is_bool_dtype(key.dtype):
            return True
    elif isinstance(key, list):
        if len(key) > 0:
            if type(key) is not list:
                key = list(key)
            return lib.is_bool_list(key)
    return False","""""""Check whether `key` is a valid boolean indexer.

Parameters
----------
key : Any
    Only list-likes may be considered boolean indexers.
    All other types are not considered a boolean indexer.
    For array-like input, boolean ndarrays or ExtensionArrays
    with ``_is_boolean`` set are considered boolean indexers.

Returns
-------
bool
    Whether `key` is a valid boolean indexer.

Raises
------
ValueError
    When the array is an object-dtype ndarray or ExtensionArray
    and contains missing values.

See Also
--------
check_array_indexer : Check that `key` is a valid array to index,
    and convert to an ndarray."""""""
pandas/core/common.py,"def cast_scalar_indexer(val):
    if lib.is_float(val) and val.is_integer():
        raise IndexError('Indexing with a float is no longer supported. Manually convert to an integer key instead.')
    return val","""""""Disallow indexing with a float key, even if that key is a round number.

Parameters
----------
val : scalar

Returns
-------
outval : scalar"""""""
pandas/core/common.py,"def not_none(*args):
    return (arg for arg in args if arg is not None)","""""""Returns a generator consisting of the arguments that are not None."""""""
pandas/core/common.py,"def any_none(*args) -> bool:
    return any((arg is None for arg in args))","""""""Returns a boolean indicating if any argument is None."""""""
pandas/core/common.py,"def all_none(*args) -> bool:
    return all((arg is None for arg in args))","""""""Returns a boolean indicating if all arguments are None."""""""
pandas/core/common.py,"def any_not_none(*args) -> bool:
    return any((arg is not None for arg in args))","""""""Returns a boolean indicating if any argument is not None."""""""
pandas/core/common.py,"def all_not_none(*args) -> bool:
    return all((arg is not None for arg in args))","""""""Returns a boolean indicating if all arguments are not None."""""""
pandas/core/common.py,"def count_not_none(*args) -> int:
    return sum((x is not None for x in args))","""""""Returns the count of arguments that are not None."""""""
pandas/core/common.py,"def index_labels_to_array(labels: np.ndarray | Iterable, dtype: NpDtype | None=None) -> np.ndarray:
    if isinstance(labels, (str, tuple)):
        labels = [labels]
    if not isinstance(labels, (list, np.ndarray)):
        try:
            labels = list(labels)
        except TypeError:
            labels = [labels]
    labels = asarray_tuplesafe(labels, dtype=dtype)
    return labels","""""""Transform label or iterable of labels to array, for use in Index.

Parameters
----------
dtype : dtype
    If specified, use as dtype of the resulting array, otherwise infer.

Returns
-------
array"""""""
pandas/core/common.py,"def maybe_iterable_to_list(obj: Iterable[T] | T) -> Collection[T] | T:
    if isinstance(obj, abc.Iterable) and (not isinstance(obj, abc.Sized)):
        return list(obj)
    obj = cast(Collection, obj)
    return obj","""""""If obj is Iterable but not list-like, consume into list."""""""
pandas/core/common.py,"def is_null_slice(obj) -> bool:
    return isinstance(obj, slice) and obj.start is None and (obj.stop is None) and (obj.step is None)","""""""We have a null slice."""""""
pandas/core/common.py,"def is_empty_slice(obj) -> bool:
    return isinstance(obj, slice) and obj.start is not None and (obj.stop is not None) and (obj.start == obj.stop)","""""""We have an empty slice, e.g. no values are selected."""""""
pandas/core/common.py,"def is_true_slices(line) -> list[bool]:
    return [isinstance(k, slice) and (not is_null_slice(k)) for k in line]","""""""Find non-trivial slices in ""line"": return a list of booleans with same length."""""""
pandas/core/common.py,"def is_full_slice(obj, line: int) -> bool:
    return isinstance(obj, slice) and obj.start == 0 and (obj.stop == line) and (obj.step is None)","""""""We have a full length slice."""""""
pandas/core/common.py,"def apply_if_callable(maybe_callable, obj, **kwargs):
    if callable(maybe_callable):
        return maybe_callable(obj, **kwargs)
    return maybe_callable","""""""Evaluate possibly callable input using obj and kwargs if it is callable,
otherwise return as it is.

Parameters
----------
maybe_callable : possibly a callable
obj : NDFrame
**kwargs"""""""
pandas/core/common.py,"def standardize_mapping(into):
    if not inspect.isclass(into):
        if isinstance(into, defaultdict):
            return partial(defaultdict, into.default_factory)
        into = type(into)
    if not issubclass(into, abc.Mapping):
        raise TypeError(f'unsupported type: {into}')
    if into == defaultdict:
        raise TypeError('to_dict() only accepts initialized defaultdicts')
    return into","""""""Helper function to standardize a supplied mapping.

Parameters
----------
into : instance or subclass of collections.abc.Mapping
    Must be a class, an initialized collections.defaultdict,
    or an instance of a collections.abc.Mapping subclass.

Returns
-------
mapping : a collections.abc.Mapping subclass or other constructor
    a callable object that can accept an iterator to create
    the desired Mapping.

See Also
--------
DataFrame.to_dict
Series.to_dict"""""""
pandas/core/common.py,"def random_state(state: RandomState | None=None):
    if is_integer(state) or isinstance(state, (np.ndarray, np.random.BitGenerator)):
        return np.random.RandomState(state)
    elif isinstance(state, np.random.RandomState):
        return state
    elif isinstance(state, np.random.Generator):
        return state
    elif state is None:
        return np.random
    else:
        raise ValueError('random_state must be an integer, array-like, a BitGenerator, Generator, a numpy RandomState, or None')","""""""Helper function for processing random_state arguments.

Parameters
----------
state : int, array-like, BitGenerator, Generator, np.random.RandomState, None.
    If receives an int, array-like, or BitGenerator, passes to
    np.random.RandomState() as seed.
    If receives an np.random RandomState or Generator, just returns that unchanged.
    If receives `None`, returns np.random.
    If receives anything else, raises an informative ValueError.

    Default None.

Returns
-------
np.random.RandomState or np.random.Generator. If state is None, returns np.random"""""""
pandas/core/common.py,"def pipe(obj, func: Callable[..., T] | tuple[Callable[..., T], str], *args, **kwargs) -> T:
    if isinstance(func, tuple):
        (func, target) = func
        if target in kwargs:
            msg = f'{target} is both the pipe target and a keyword argument'
            raise ValueError(msg)
        kwargs[target] = obj
        return func(*args, **kwargs)
    else:
        return func(obj, *args, **kwargs)","""""""Apply a function ``func`` to object ``obj`` either by passing obj as the
first argument to the function or, in the case that the func is a tuple,
interpret the first element of the tuple as a function and pass the obj to
that function as a keyword argument whose key is the value of the second
element of the tuple.

Parameters
----------
func : callable or tuple of (callable, str)
    Function to apply to this object or, alternatively, a
    ``(callable, data_keyword)`` tuple where ``data_keyword`` is a
    string indicating the keyword of ``callable`` that expects the
    object.
*args : iterable, optional
    Positional arguments passed into ``func``.
**kwargs : dict, optional
    A dictionary of keyword arguments passed into ``func``.

Returns
-------
object : the return type of ``func``."""""""
pandas/core/common.py,"def get_rename_function(mapper):

    def f(x):
        if x in mapper:
            return mapper[x]
        else:
            return x
    return f if isinstance(mapper, (abc.Mapping, ABCSeries)) else mapper","""""""Returns a function that will map names/labels, dependent if mapper
is a dict, Series or just a function."""""""
pandas/core/common.py,"def convert_to_list_like(values: Hashable | Iterable | AnyArrayLike) -> list | AnyArrayLike:
    if isinstance(values, (list, np.ndarray, ABCIndex, ABCSeries, ABCExtensionArray)):
        return values
    elif isinstance(values, abc.Iterable) and (not isinstance(values, str)):
        return list(values)
    return [values]","""""""Convert list-like or scalar input to list-like. List, numpy and pandas array-like
inputs are returned unmodified whereas others are converted to list."""""""
pandas/core/common.py,"@contextlib.contextmanager
def temp_setattr(obj, attr: str, value, condition: bool=True) -> Generator[None, None, None]:
    if condition:
        old_value = getattr(obj, attr)
        setattr(obj, attr, value)
    try:
        yield obj
    finally:
        if condition:
            setattr(obj, attr, old_value)","""""""Temporarily set attribute on an object.

Parameters
----------
obj : object
    Object whose attribute will be modified.
attr : str
    Attribute to modify.
value : Any
    Value to temporarily set attribute to.
condition : bool, default True
    Whether to set the attribute. Provided in order to not have to
    conditionally use this context manager.

Yields
------
object : obj with modified attribute."""""""
pandas/core/common.py,"def require_length_match(data, index: Index) -> None:
    if len(data) != len(index):
        raise ValueError(f'Length of values ({len(data)}) does not match length of index ({len(index)})')","""""""Check the length of data matches the length of the index."""""""
pandas/core/common.py,"def get_cython_func(arg: Callable) -> str | None:
    return _cython_table.get(arg)","""""""if we define an internal function for this argument, return it"""""""
pandas/core/common.py,"def is_builtin_func(arg):
    return _builtin_table.get(arg, arg)","""""""if we define a builtin function for this argument, return it,
otherwise return the arg"""""""
pandas/core/common.py,"def fill_missing_names(names: Sequence[Hashable | None]) -> list[Hashable]:
    return [f'level_{i}' if name is None else name for (i, name) in enumerate(names)]","""""""If a name is missing then replace it by level_n, where n is the count

.. versionadded:: 1.4.0

Parameters
----------
names : list-like
    list of column names or None values.

Returns
-------
list
    list of column names with the None values replaced."""""""
pandas/core/computation/align.py,"def _any_pandas_objects(terms) -> bool:
    return any((isinstance(term.value, PandasObject) for term in terms))","""""""Check a sequence of terms for instances of PandasObject."""""""
pandas/core/computation/align.py,"def align_terms(terms):
    try:
        terms = list(com.flatten(terms))
    except TypeError:
        if isinstance(terms.value, (ABCSeries, ABCDataFrame)):
            typ = type(terms.value)
            return (typ, _zip_axes_from_type(typ, terms.value.axes))
        return (np.result_type(terms.type), None)
    if all((term.is_scalar for term in terms)):
        return (result_type_many(*(term.value for term in terms)).type, None)
    (typ, axes) = _align_core(terms)
    return (typ, axes)","""""""Align a set of terms."""""""
pandas/core/computation/align.py,"def reconstruct_object(typ, obj, axes, dtype):
    try:
        typ = typ.type
    except AttributeError:
        pass
    res_t = np.result_type(obj.dtype, dtype)
    if not isinstance(typ, partial) and issubclass(typ, PandasObject):
        return typ(obj, dtype=res_t, **axes)
    if hasattr(res_t, 'type') and typ == np.bool_ and (res_t != np.bool_):
        ret_value = res_t.type(obj)
    else:
        ret_value = typ(obj).astype(res_t)
        if len(obj.shape) == 1 and len(obj) == 1 and (not isinstance(ret_value, np.ndarray)):
            ret_value = np.array([ret_value]).astype(res_t)
    return ret_value","""""""Reconstruct an object given its type, raw value, and possibly empty
(None) axes.

Parameters
----------
typ : object
    A type
obj : object
    The value to use in the type constructor
axes : dict
    The axes to use to construct the resulting pandas object

Returns
-------
ret : typ
    An object of type ``typ`` with the value `obj` and possible axes
    `axes`."""""""
pandas/core/computation/common.py,"def ensure_decoded(s) -> str:
    if isinstance(s, (np.bytes_, bytes)):
        s = s.decode(get_option('display.encoding'))
    return s","""""""If we have bytes, decode them to unicode."""""""
pandas/core/computation/common.py,"def result_type_many(*arrays_and_dtypes):
    try:
        return np.result_type(*arrays_and_dtypes)
    except ValueError:
        return reduce(np.result_type, arrays_and_dtypes)
    except TypeError:
        from pandas.core.dtypes.cast import find_common_type
        from pandas.core.dtypes.common import is_extension_array_dtype
        arr_and_dtypes = list(arrays_and_dtypes)
        (ea_dtypes, non_ea_dtypes) = ([], [])
        for arr_or_dtype in arr_and_dtypes:
            if is_extension_array_dtype(arr_or_dtype):
                ea_dtypes.append(arr_or_dtype)
            else:
                non_ea_dtypes.append(arr_or_dtype)
        if non_ea_dtypes:
            try:
                np_dtype = np.result_type(*non_ea_dtypes)
            except ValueError:
                np_dtype = reduce(np.result_type, arrays_and_dtypes)
            return find_common_type(ea_dtypes + [np_dtype])
        return find_common_type(ea_dtypes)","""""""Wrapper around numpy.result_type which overcomes the NPY_MAXARGS (32)
argument limit."""""""
pandas/core/computation/engines.py,"def _check_ne_builtin_clash(expr: Expr) -> None:
    names = expr.names
    overlap = names & _ne_builtins
    if overlap:
        s = ', '.join([repr(x) for x in overlap])
        raise NumExprClobberingError(f'Variables in expression ""{expr}"" overlap with builtins: ({s})')","""""""Attempt to prevent foot-shooting in a helpful way.

Parameters
----------
expr : Expr
    Terms can contain"""""""
pandas/core/computation/eval.py,"def _check_engine(engine: str | None) -> str:
    from pandas.core.computation.check import NUMEXPR_INSTALLED
    from pandas.core.computation.expressions import USE_NUMEXPR
    if engine is None:
        engine = 'numexpr' if USE_NUMEXPR else 'python'
    if engine not in ENGINES:
        valid_engines = list(ENGINES.keys())
        raise KeyError(f""Invalid engine '{engine}' passed, valid engines are {valid_engines}"")
    if engine == 'numexpr' and (not NUMEXPR_INSTALLED):
        raise ImportError(""'numexpr' is not installed or an unsupported version. Cannot use engine='numexpr' for query/eval if 'numexpr' is not installed"")
    return engine","""""""Make sure a valid engine is passed.

Parameters
----------
engine : str
    String to validate.

Raises
------
KeyError
  * If an invalid engine is passed.
ImportError
  * If numexpr was requested but doesn't exist.

Returns
-------
str
    Engine name."""""""
pandas/core/computation/eval.py,"def _check_parser(parser: str):
    if parser not in PARSERS:
        raise KeyError(f""Invalid parser '{parser}' passed, valid parsers are {PARSERS.keys()}"")","""""""Make sure a valid parser is passed.

Parameters
----------
parser : str

Raises
------
KeyError
  * If an invalid parser is passed"""""""
pandas/core/computation/eval.py,"def _check_expression(expr):
    if not expr:
        raise ValueError('expr cannot be an empty string')","""""""Make sure an expression is not an empty string

Parameters
----------
expr : object
    An object that can be converted to a string

Raises
------
ValueError
  * If expr is an empty string"""""""
pandas/core/computation/eval.py,"def _convert_expression(expr) -> str:
    s = pprint_thing(expr)
    _check_expression(s)
    return s","""""""Convert an object to an expression.

This function converts an object to an expression (a unicode string) and
checks to make sure it isn't empty after conversion. This is used to
convert operators to their string representation for recursive calls to
:func:`~pandas.eval`.

Parameters
----------
expr : object
    The object to be converted to a string.

Returns
-------
str
    The string representation of an object.

Raises
------
ValueError
  * If the expression is empty."""""""
pandas/core/computation/eval.py,"def eval(expr: str | BinOp, parser: str='pandas', engine: str | None=None, local_dict=None, global_dict=None, resolvers=(), level: int=0, target=None, inplace: bool=False):
    inplace = validate_bool_kwarg(inplace, 'inplace')
    exprs: list[str | BinOp]
    if isinstance(expr, str):
        _check_expression(expr)
        exprs = [e.strip() for e in expr.splitlines() if e.strip() != '']
    else:
        exprs = [expr]
    multi_line = len(exprs) > 1
    if multi_line and target is None:
        raise ValueError('multi-line expressions are only valid in the context of data, use DataFrame.eval')
    engine = _check_engine(engine)
    _check_parser(parser)
    _check_resolvers(resolvers)
    ret = None
    first_expr = True
    target_modified = False
    for expr in exprs:
        expr = _convert_expression(expr)
        _check_for_locals(expr, level, parser)
        env = ensure_scope(level + 1, global_dict=global_dict, local_dict=local_dict, resolvers=resolvers, target=target)
        parsed_expr = Expr(expr, engine=engine, parser=parser, env=env)
        if engine == 'numexpr' and (is_extension_array_dtype(parsed_expr.terms.return_type) or (getattr(parsed_expr.terms, 'operand_types', None) is not None and any((is_extension_array_dtype(elem) for elem in parsed_expr.terms.operand_types)))):
            warnings.warn(""Engine has switched to 'python' because numexpr does not support extension array dtypes. Please set your engine to python manually."", RuntimeWarning, stacklevel=find_stack_level())
            engine = 'python'
        eng = ENGINES[engine]
        eng_inst = eng(parsed_expr)
        ret = eng_inst.evaluate()
        if parsed_expr.assigner is None:
            if multi_line:
                raise ValueError('Multi-line expressions are only valid if all expressions contain an assignment')
            if inplace:
                raise ValueError('Cannot operate inplace if there is no assignment')
        assigner = parsed_expr.assigner
        if env.target is not None and assigner is not None:
            target_modified = True
            if not inplace and first_expr:
                try:
                    target = env.target
                    if isinstance(target, NDFrame):
                        target = target.copy(deep=None)
                    else:
                        target = target.copy()
                except AttributeError as err:
                    raise ValueError('Cannot return a copy of the target') from err
            else:
                target = env.target
            try:
                with warnings.catch_warnings(record=True):
                    if inplace and isinstance(target, NDFrame):
                        target.loc[:, assigner] = ret
                    else:
                        target[assigner] = ret
            except (TypeError, IndexError) as err:
                raise ValueError('Cannot assign expression output to target') from err
            if not resolvers:
                resolvers = ({assigner: ret},)
            else:
                for resolver in resolvers:
                    if assigner in resolver:
                        resolver[assigner] = ret
                        break
                else:
                    resolvers += ({assigner: ret},)
            ret = None
            first_expr = False
    if inplace is False:
        return target if target_modified else ret","""""""Evaluate a Python expression as a string using various backends.

The following arithmetic operations are supported: ``+``, ``-``, ``*``,
``/``, ``**``, ``%``, ``//`` (python engine only) along with the following
boolean operations: ``|`` (or), ``&`` (and), and ``~`` (not).
Additionally, the ``'pandas'`` parser allows the use of :keyword:`and`,
:keyword:`or`, and :keyword:`not` with the same semantics as the
corresponding bitwise operators.  :class:`~pandas.Series` and
:class:`~pandas.DataFrame` objects are supported and behave as they would
with plain ol' Python evaluation.

Parameters
----------
expr : str
    The expression to evaluate. This string cannot contain any Python
    `statements
    <https://docs.python.org/3/reference/simple_stmts.html#simple-statements>`__,
    only Python `expressions
    <https://docs.python.org/3/reference/simple_stmts.html#expression-statements>`__.
parser : {'pandas', 'python'}, default 'pandas'
    The parser to use to construct the syntax tree from the expression. The
    default of ``'pandas'`` parses code slightly different than standard
    Python. Alternatively, you can parse an expression using the
    ``'python'`` parser to retain strict Python semantics.  See the
    :ref:`enhancing performance <enhancingperf.eval>` documentation for
    more details.
engine : {'python', 'numexpr'}, default 'numexpr'

    The engine used to evaluate the expression. Supported engines are

    - None : tries to use ``numexpr``, falls back to ``python``
    - ``'numexpr'`` : This default engine evaluates pandas objects using
      numexpr for large speed ups in complex expressions with large frames.
    - ``'python'`` : Performs operations as if you had ``eval``'d in top
      level python. This engine is generally not that useful.

    More backends may be available in the future.
local_dict : dict or None, optional
    A dictionary of local variables, taken from locals() by default.
global_dict : dict or None, optional
    A dictionary of global variables, taken from globals() by default.
resolvers : list of dict-like or None, optional
    A list of objects implementing the ``__getitem__`` special method that
    you can use to inject an additional collection of namespaces to use for
    variable lookup. For example, this is used in the
    :meth:`~DataFrame.query` method to inject the
    ``DataFrame.index`` and ``DataFrame.columns``
    variables that refer to their respective :class:`~pandas.DataFrame`
    instance attributes.
level : int, optional
    The number of prior stack frames to traverse and add to the current
    scope. Most users will **not** need to change this parameter.
target : object, optional, default None
    This is the target object for assignment. It is used when there is
    variable assignment in the expression. If so, then `target` must
    support item assignment with string keys, and if a copy is being
    returned, it must also support `.copy()`.
inplace : bool, default False
    If `target` is provided, and the expression mutates `target`, whether
    to modify `target` inplace. Otherwise, return a copy of `target` with
    the mutation.

Returns
-------
ndarray, numeric scalar, DataFrame, Series, or None
    The completion value of evaluating the given code or None if ``inplace=True``.

Raises
------
ValueError
    There are many instances where such an error can be raised:

    - `target=None`, but the expression is multiline.
    - The expression is multiline, but not all them have item assignment.
      An example of such an arrangement is this:

      a = b + 1
      a + 2

      Here, there are expressions on different lines, making it multiline,
      but the last line has no variable assigned to the output of `a + 2`.
    - `inplace=True`, but the expression is missing item assignment.
    - Item assignment is provided, but the `target` does not support
      string item assignment.
    - Item assignment is provided and `inplace=False`, but the `target`
      does not support the `.copy()` method

See Also
--------
DataFrame.query : Evaluates a boolean expression to query the columns
        of a frame.
DataFrame.eval : Evaluate a string describing operations on
        DataFrame columns.

Notes
-----
The ``dtype`` of any objects involved in an arithmetic ``%`` operation are
recursively cast to ``float64``.

See the :ref:`enhancing performance <enhancingperf.eval>` documentation for
more details.

Examples
--------
>>> df = pd.DataFrame({""animal"": [""dog"", ""pig""], ""age"": [10, 20]})
>>> df
  animal  age
0    dog   10
1    pig   20

We can add a new column using ``pd.eval``:

>>> pd.eval(""double_age = df.age * 2"", target=df)
  animal  age  double_age
0    dog   10          20
1    pig   20          40"""""""
pandas/core/computation/expr.py,"def _rewrite_assign(tok: tuple[int, str]) -> tuple[int, str]:
    (toknum, tokval) = tok
    return (toknum, '==' if tokval == '=' else tokval)","""""""Rewrite the assignment operator for PyTables expressions that use ``=``
as a substitute for ``==``.

Parameters
----------
tok : tuple of int, str
    ints correspond to the all caps constants in the tokenize module

Returns
-------
tuple of int, str
    Either the input or token or the replacement values"""""""
pandas/core/computation/expr.py,"def _replace_booleans(tok: tuple[int, str]) -> tuple[int, str]:
    (toknum, tokval) = tok
    if toknum == tokenize.OP:
        if tokval == '&':
            return (tokenize.NAME, 'and')
        elif tokval == '|':
            return (tokenize.NAME, 'or')
        return (toknum, tokval)
    return (toknum, tokval)","""""""Replace ``&`` with ``and`` and ``|`` with ``or`` so that bitwise
precedence is changed to boolean precedence.

Parameters
----------
tok : tuple of int, str
    ints correspond to the all caps constants in the tokenize module

Returns
-------
tuple of int, str
    Either the input or token or the replacement values"""""""
pandas/core/computation/expr.py,"def _replace_locals(tok: tuple[int, str]) -> tuple[int, str]:
    (toknum, tokval) = tok
    if toknum == tokenize.OP and tokval == '@':
        return (tokenize.OP, LOCAL_TAG)
    return (toknum, tokval)","""""""Replace local variables with a syntactically valid name.

Parameters
----------
tok : tuple of int, str
    ints correspond to the all caps constants in the tokenize module

Returns
-------
tuple of int, str
    Either the input or token or the replacement values

Notes
-----
This is somewhat of a hack in that we rewrite a string such as ``'@a'`` as
``'__pd_eval_local_a'`` by telling the tokenizer that ``__pd_eval_local_``
is a ``tokenize.OP`` and to replace the ``'@'`` symbol with it."""""""
pandas/core/computation/expr.py,"def _compose2(f, g):
    return lambda *args, **kwargs: f(g(*args, **kwargs))","""""""Compose 2 callables."""""""
pandas/core/computation/expr.py,"def _compose(*funcs):
    assert len(funcs) > 1, 'At least 2 callables must be passed to compose'
    return reduce(_compose2, funcs)","""""""Compose 2 or more callables."""""""
pandas/core/computation/expr.py,"def _preparse(source: str, f=_compose(_replace_locals, _replace_booleans, _rewrite_assign, clean_backtick_quoted_toks)) -> str:
    assert callable(f), 'f must be callable'
    return tokenize.untokenize((f(x) for x in tokenize_string(source)))","""""""Compose a collection of tokenization functions.

Parameters
----------
source : str
    A Python source code string
f : callable
    This takes a tuple of (toknum, tokval) as its argument and returns a
    tuple with the same structure but possibly different elements. Defaults
    to the composition of ``_rewrite_assign``, ``_replace_booleans``, and
    ``_replace_locals``.

Returns
-------
str
    Valid Python source code

Notes
-----
The `f` parameter can be any callable that takes *and* returns input of the
form ``(toknum, tokval)``, where ``toknum`` is one of the constants from
the ``tokenize`` module and ``tokval`` is a string."""""""
pandas/core/computation/expr.py,"def _is_type(t):
    return lambda x: isinstance(x.value, t)","""""""Factory for a type checking function of type ``t`` or tuple of types."""""""
pandas/core/computation/expr.py,"def _filter_nodes(superclass, all_nodes=_all_nodes):
    node_names = (node.__name__ for node in all_nodes if issubclass(node, superclass))
    return frozenset(node_names)","""""""Filter out AST nodes that are subclasses of ``superclass``."""""""
pandas/core/computation/expr.py,"def _node_not_implemented(node_name: str) -> Callable[..., None]:

    def f(self, *args, **kwargs):
        raise NotImplementedError(f""'{node_name}' nodes are not implemented"")
    return f","""""""Return a function that raises a NotImplementedError with a passed node name."""""""
pandas/core/computation/expr.py,"def disallow(nodes: set[str]) -> Callable[[type[_T]], type[_T]]:

    def disallowed(cls: type[_T]) -> type[_T]:
        cls.unsupported_nodes = ()
        for node in nodes:
            new_method = _node_not_implemented(node)
            name = f'visit_{node}'
            cls.unsupported_nodes += (name,)
            setattr(cls, name, new_method)
        return cls
    return disallowed","""""""Decorator to disallow certain nodes from parsing. Raises a
NotImplementedError instead.

Returns
-------
callable"""""""
pandas/core/computation/expr.py,"def _op_maker(op_class, op_symbol):

    def f(self, node, *args, **kwargs):
        """"""
        Return a partial function with an Op subclass with an operator already passed.

        Returns
        -------
        callable
        """"""
        return partial(op_class, op_symbol, *args, **kwargs)
    return f","""""""Return a function to create an op class with its symbol already passed.

Returns
-------
callable"""""""
pandas/core/computation/expr.py,"def add_ops(op_classes):

    def f(cls):
        for (op_attr_name, op_class) in op_classes.items():
            ops = getattr(cls, f'{op_attr_name}_ops')
            ops_map = getattr(cls, f'{op_attr_name}_op_nodes_map')
            for op in ops:
                op_node = ops_map[op]
                if op_node is not None:
                    made_op = _op_maker(op_class, op)
                    setattr(cls, f'visit_{op_node}', made_op)
        return cls
    return f","""""""Decorator to add default implementation of ops."""""""
pandas/core/computation/expressions.py,"def _evaluate_standard(op, op_str, a, b):
    if _TEST_MODE:
        _store_test_result(False)
    return op(a, b)","""""""Standard evaluation."""""""
pandas/core/computation/expressions.py,"def _can_use_numexpr(op, op_str, a, b, dtype_check) -> bool:
    if op_str is not None:
        if a.size > _MIN_ELEMENTS:
            dtypes: set[str] = set()
            for o in [a, b]:
                if hasattr(o, 'dtype'):
                    dtypes |= {o.dtype.name}
            if not len(dtypes) or _ALLOWED_DTYPES[dtype_check] >= dtypes:
                return True
    return False","""""""return a boolean if we WILL be using numexpr"""""""
pandas/core/computation/expressions.py,"def _bool_arith_fallback(op_str, a, b) -> bool:
    if _has_bool_dtype(a) and _has_bool_dtype(b):
        if op_str in _BOOL_OP_UNSUPPORTED:
            warnings.warn(f'evaluating in Python space because the {repr(op_str)} operator is not supported by numexpr for the bool dtype, use {repr(_BOOL_OP_UNSUPPORTED[op_str])} instead.', stacklevel=find_stack_level())
            return True
    return False","""""""Check if we should fallback to the python `_evaluate_standard` in case
of an unsupported operation by numexpr, which is the case for some
boolean ops."""""""
pandas/core/computation/expressions.py,"def evaluate(op, a, b, use_numexpr: bool=True):
    op_str = _op_str_mapping[op]
    if op_str is not None:
        if use_numexpr:
            return _evaluate(op, op_str, a, b)
    return _evaluate_standard(op, op_str, a, b)","""""""Evaluate and return the expression of the op on a and b.

Parameters
----------
op : the actual operand
a : left operand
b : right operand
use_numexpr : bool, default True
    Whether to try to use numexpr."""""""
pandas/core/computation/expressions.py,"def where(cond, a, b, use_numexpr: bool=True):
    assert _where is not None
    return _where(cond, a, b) if use_numexpr else _where_standard(cond, a, b)","""""""Evaluate the where condition cond on a and b.

Parameters
----------
cond : np.ndarray[bool]
a : return if cond is True
b : return if cond is False
use_numexpr : bool, default True
    Whether to try to use numexpr."""""""
pandas/core/computation/expressions.py,"def set_test_mode(v: bool=True) -> None:
    global _TEST_MODE, _TEST_RESULT
    _TEST_MODE = v
    _TEST_RESULT = []","""""""Keeps track of whether numexpr was used.

Stores an additional ``True`` for every successful use of evaluate with
numexpr since the last ``get_test_result``."""""""
pandas/core/computation/expressions.py,"def get_test_result() -> list[bool]:
    global _TEST_RESULT
    res = _TEST_RESULT
    _TEST_RESULT = []
    return res","""""""Get test result and reset test_results."""""""
pandas/core/computation/ops.py,"def _in(x, y):
    try:
        return x.isin(y)
    except AttributeError:
        if is_list_like(x):
            try:
                return y.isin(x)
            except AttributeError:
                pass
        return x in y","""""""Compute the vectorized membership of ``x in y`` if possible, otherwise
use Python."""""""
pandas/core/computation/ops.py,"def _not_in(x, y):
    try:
        return ~x.isin(y)
    except AttributeError:
        if is_list_like(x):
            try:
                return ~y.isin(x)
            except AttributeError:
                pass
        return x not in y","""""""Compute the vectorized membership of ``x not in y`` if possible,
otherwise use Python."""""""
pandas/core/computation/ops.py,"def _cast_inplace(terms, acceptable_dtypes, dtype) -> None:
    dt = np.dtype(dtype)
    for term in terms:
        if term.type in acceptable_dtypes:
            continue
        try:
            new_value = term.value.astype(dt)
        except AttributeError:
            new_value = dt.type(term.value)
        term.update(new_value)","""""""Cast an expression inplace.

Parameters
----------
terms : Op
    The expression that should cast.
acceptable_dtypes : list of acceptable numpy.dtype
    Will not cast if term's dtype in this list.
dtype : str or numpy.dtype
    The dtype to cast to."""""""
pandas/core/computation/parsing.py,"def create_valid_python_identifier(name: str) -> str:
    if name.isidentifier() and (not iskeyword(name)):
        return name
    special_characters_replacements = {char: f'_{token.tok_name[tokval]}_' for (char, tokval) in tokenize.EXACT_TOKEN_TYPES.items()}
    special_characters_replacements.update({' ': '_', '?': '_QUESTIONMARK_', '!': '_EXCLAMATIONMARK_', '$': '_DOLLARSIGN_', '': '_EUROSIGN_', '': '_DEGREESIGN_', ""'"": '_SINGLEQUOTE_', '""': '_DOUBLEQUOTE_'})
    name = ''.join([special_characters_replacements.get(char, char) for char in name])
    name = f'BACKTICK_QUOTED_STRING_{name}'
    if not name.isidentifier():
        raise SyntaxError(f""Could not convert '{name}' to a valid Python identifier."")
    return name","""""""Create valid Python identifiers from any string.

Check if name contains any special characters. If it contains any
special characters, the special characters will be replaced by
a special string and a prefix is added.

Raises
------
SyntaxError
    If the returned name is not a Python valid identifier, raise an exception.
    This can happen if there is a hashtag in the name, as the tokenizer will
    than terminate and not find the backtick.
    But also for characters that fall out of the range of (U+0001..U+007F)."""""""
pandas/core/computation/parsing.py,"def clean_backtick_quoted_toks(tok: tuple[int, str]) -> tuple[int, str]:
    (toknum, tokval) = tok
    if toknum == BACKTICK_QUOTED_STRING:
        return (tokenize.NAME, create_valid_python_identifier(tokval))
    return (toknum, tokval)","""""""Clean up a column name if surrounded by backticks.

Backtick quoted string are indicated by a certain tokval value. If a string
is a backtick quoted token it will processed by
:func:`_create_valid_python_identifier` so that the parser can find this
string when the query is executed.
In this case the tok will get the NAME tokval.

Parameters
----------
tok : tuple of int, str
    ints correspond to the all caps constants in the tokenize module

Returns
-------
tok : Tuple[int, str]
    Either the input or token or the replacement values"""""""
pandas/core/computation/parsing.py,"def clean_column_name(name: Hashable) -> Hashable:
    try:
        tokenized = tokenize_string(f'`{name}`')
        tokval = next(tokenized)[1]
        return create_valid_python_identifier(tokval)
    except SyntaxError:
        return name","""""""Function to emulate the cleaning of a backtick quoted name.

The purpose for this function is to see what happens to the name of
identifier if it goes to the process of being parsed a Python code
inside a backtick quoted string and than being cleaned
(removed of any special characters).

Parameters
----------
name : hashable
    Name to be cleaned.

Returns
-------
name : hashable
    Returns the name after tokenizing and cleaning.

Notes
-----
    For some cases, a name cannot be converted to a valid Python identifier.
    In that case :func:`tokenize_string` raises a SyntaxError.
    In that case, we just return the name unmodified.

    If this name was used in the query string (this makes the query call impossible)
    an error will be raised by :func:`tokenize_backtick_quoted_string` instead,
    which is not caught and propagates to the user level."""""""
pandas/core/computation/parsing.py,"def tokenize_backtick_quoted_string(token_generator: Iterator[tokenize.TokenInfo], source: str, string_start: int) -> tuple[int, str]:
    for (_, tokval, start, _, _) in token_generator:
        if tokval == '`':
            string_end = start[1]
            break
    return (BACKTICK_QUOTED_STRING, source[string_start:string_end])","""""""Creates a token from a backtick quoted string.

Moves the token_generator forwards till right after the next backtick.

Parameters
----------
token_generator : Iterator[tokenize.TokenInfo]
    The generator that yields the tokens of the source string (Tuple[int, str]).
    The generator is at the first token after the backtick (`)

source : str
    The Python source code string.

string_start : int
    This is the start of backtick quoted string inside the source string.

Returns
-------
tok: Tuple[int, str]
    The token that represents the backtick quoted string.
    The integer is equal to BACKTICK_QUOTED_STRING (100)."""""""
pandas/core/computation/parsing.py,"def tokenize_string(source: str) -> Iterator[tuple[int, str]]:
    line_reader = StringIO(source).readline
    token_generator = tokenize.generate_tokens(line_reader)
    for (toknum, tokval, start, _, _) in token_generator:
        if tokval == '`':
            try:
                yield tokenize_backtick_quoted_string(token_generator, source, string_start=start[1] + 1)
            except Exception as err:
                raise SyntaxError(f""Failed to parse backticks in '{source}'."") from err
        else:
            yield (toknum, tokval)","""""""Tokenize a Python source code string.

Parameters
----------
source : str
    The Python source code string.

Returns
-------
tok_generator : Iterator[Tuple[int, str]]
    An iterator yielding all tokens with only toknum and tokval (Tuple[ing, str])."""""""
pandas/core/computation/pytables.py,"def _validate_where(w):
    if not (isinstance(w, (PyTablesExpr, str)) or is_list_like(w)):
        raise TypeError('where must be passed as a string, PyTablesExpr, or list-like of PyTablesExpr')
    return w","""""""Validate that the where statement is of the right type.

The type may either be String, Expr, or list-like of Exprs.

Parameters
----------
w : String term expression, Expr, or list-like of Exprs.

Returns
-------
where : The original where clause if the check was successful.

Raises
------
TypeError : An invalid data type was passed in for w (e.g. dict)."""""""
pandas/core/computation/pytables.py,"def maybe_expression(s) -> bool:
    if not isinstance(s, str):
        return False
    operations = PyTablesExprVisitor.binary_ops + PyTablesExprVisitor.unary_ops + ('=',)
    return any((op in s for op in operations))","""""""loose checking if s is a pytables-acceptable expression"""""""
pandas/core/computation/scope.py,"def ensure_scope(level: int, global_dict=None, local_dict=None, resolvers=(), target=None) -> Scope:
    return Scope(level + 1, global_dict=global_dict, local_dict=local_dict, resolvers=resolvers, target=target)","""""""Ensure that we are grabbing the correct scope."""""""
pandas/core/computation/scope.py,"def _replacer(x) -> str:
    try:
        hexin = ord(x)
    except TypeError:
        hexin = x
    return hex(hexin)","""""""Replace a number with its hexadecimal representation. Used to tag
temporary variables with their calling scope's id."""""""
pandas/core/computation/scope.py,"def _raw_hex_id(obj) -> str:
    packed = struct.pack('@P', id(obj))
    return ''.join([_replacer(x) for x in packed])","""""""Return the padded hexadecimal id of ``obj``."""""""
pandas/core/computation/scope.py,"def _get_pretty_string(obj) -> str:
    sio = StringIO()
    pprint.pprint(obj, stream=sio)
    return sio.getvalue()","""""""Return a prettier version of obj.

Parameters
----------
obj : object
    Object to pretty print

Returns
-------
str
    Pretty print object repr"""""""
pandas/core/config_init.py,"def is_terminal() -> bool:
    try:
        ip = get_ipython()
    except NameError:
        return True
    else:
        if hasattr(ip, 'kernel'):
            return False
        else:
            return True","""""""Detect if Python is running in a terminal.

Returns True if Python is running in a terminal or False if not."""""""
pandas/core/construction.py,"def array(data: Sequence[object] | AnyArrayLike, dtype: Dtype | None=None, copy: bool=True) -> ExtensionArray:
    from pandas.core.arrays import BooleanArray, DatetimeArray, ExtensionArray, FloatingArray, IntegerArray, IntervalArray, NumpyExtensionArray, PeriodArray, TimedeltaArray
    from pandas.core.arrays.string_ import StringDtype
    if lib.is_scalar(data):
        msg = f""Cannot pass scalar '{data}' to 'pandas.array'.""
        raise ValueError(msg)
    elif isinstance(data, ABCDataFrame):
        raise TypeError(""Cannot pass DataFrame to 'pandas.array'"")
    if dtype is None and isinstance(data, (ABCSeries, ABCIndex, ExtensionArray)):
        dtype = data.dtype
    data = extract_array(data, extract_numpy=True)
    if dtype is not None:
        dtype = pandas_dtype(dtype)
    if isinstance(data, ExtensionArray) and (dtype is None or data.dtype == dtype):
        if copy:
            return data.copy()
        return data
    if isinstance(dtype, ExtensionDtype):
        cls = dtype.construct_array_type()
        return cls._from_sequence(data, dtype=dtype, copy=copy)
    if dtype is None:
        inferred_dtype = lib.infer_dtype(data, skipna=True)
        if inferred_dtype == 'period':
            period_data = cast(Union[Sequence[Optional[Period]], AnyArrayLike], data)
            return PeriodArray._from_sequence(period_data, copy=copy)
        elif inferred_dtype == 'interval':
            return IntervalArray(data, copy=copy)
        elif inferred_dtype.startswith('datetime'):
            try:
                return DatetimeArray._from_sequence(data, copy=copy)
            except ValueError:
                pass
        elif inferred_dtype.startswith('timedelta'):
            return TimedeltaArray._from_sequence(data, copy=copy)
        elif inferred_dtype == 'string':
            return StringDtype().construct_array_type()._from_sequence(data, copy=copy)
        elif inferred_dtype == 'integer':
            return IntegerArray._from_sequence(data, copy=copy)
        elif inferred_dtype == 'empty' and (not hasattr(data, 'dtype')) and (not len(data)):
            return FloatingArray._from_sequence(data, copy=copy)
        elif inferred_dtype in ('floating', 'mixed-integer-float') and getattr(data, 'dtype', None) != np.float16:
            return FloatingArray._from_sequence(data, copy=copy)
        elif inferred_dtype == 'boolean':
            return BooleanArray._from_sequence(data, copy=copy)
    if lib.is_np_dtype(dtype, 'M') and is_supported_unit(get_unit_from_dtype(dtype)):
        return DatetimeArray._from_sequence(data, dtype=dtype, copy=copy)
    if lib.is_np_dtype(dtype, 'm') and is_supported_unit(get_unit_from_dtype(dtype)):
        return TimedeltaArray._from_sequence(data, dtype=dtype, copy=copy)
    elif lib.is_np_dtype(dtype, 'mM'):
        warnings.warn(""datetime64 and timedelta64 dtype resolutions other than 's', 'ms', 'us', and 'ns' are deprecated. In future releases passing unsupported resolutions will raise an exception."", FutureWarning, stacklevel=find_stack_level())
    return NumpyExtensionArray._from_sequence(data, dtype=dtype, copy=copy)","""""""Create an array.

Parameters
----------
data : Sequence of objects
    The scalars inside `data` should be instances of the
    scalar type for `dtype`. It's expected that `data`
    represents a 1-dimensional array of data.

    When `data` is an Index or Series, the underlying array
    will be extracted from `data`.

dtype : str, np.dtype, or ExtensionDtype, optional
    The dtype to use for the array. This may be a NumPy
    dtype or an extension type registered with pandas using
    :meth:`pandas.api.extensions.register_extension_dtype`.

    If not specified, there are two possibilities:

    1. When `data` is a :class:`Series`, :class:`Index`, or
       :class:`ExtensionArray`, the `dtype` will be taken
       from the data.
    2. Otherwise, pandas will attempt to infer the `dtype`
       from the data.

    Note that when `data` is a NumPy array, ``data.dtype`` is
    *not* used for inferring the array type. This is because
    NumPy cannot represent all the types of data that can be
    held in extension arrays.

    Currently, pandas will infer an extension dtype for sequences of

    ============================== =======================================
    Scalar Type                    Array Type
    ============================== =======================================
    :class:`pandas.Interval`       :class:`pandas.arrays.IntervalArray`
    :class:`pandas.Period`         :class:`pandas.arrays.PeriodArray`
    :class:`datetime.datetime`     :class:`pandas.arrays.DatetimeArray`
    :class:`datetime.timedelta`    :class:`pandas.arrays.TimedeltaArray`
    :class:`int`                   :class:`pandas.arrays.IntegerArray`
    :class:`float`                 :class:`pandas.arrays.FloatingArray`
    :class:`str`                   :class:`pandas.arrays.StringArray` or
                                   :class:`pandas.arrays.ArrowStringArray`
    :class:`bool`                  :class:`pandas.arrays.BooleanArray`
    ============================== =======================================

    The ExtensionArray created when the scalar type is :class:`str` is determined by
    ``pd.options.mode.string_storage`` if the dtype is not explicitly given.

    For all other cases, NumPy's usual inference rules will be used.

    .. versionchanged:: 1.2.0

        Pandas now also infers nullable-floating dtype for float-like
        input data

copy : bool, default True
    Whether to copy the data, even if not necessary. Depending
    on the type of `data`, creating the new array may require
    copying data, even if ``copy=False``.

Returns
-------
ExtensionArray
    The newly created array.

Raises
------
ValueError
    When `data` is not 1-dimensional.

See Also
--------
numpy.array : Construct a NumPy array.
Series : Construct a pandas Series.
Index : Construct a pandas Index.
arrays.NumpyExtensionArray : ExtensionArray wrapping a NumPy array.
Series.array : Extract the array stored within a Series.

Notes
-----
Omitting the `dtype` argument means pandas will attempt to infer the
best array type from the values in the data. As new array types are
added by pandas and 3rd party libraries, the ""best"" array type may
change. We recommend specifying `dtype` to ensure that

1. the correct array type for the data is returned
2. the returned array type doesn't change as new extension types
   are added by pandas and third-party libraries

Additionally, if the underlying memory representation of the returned
array matters, we recommend specifying the `dtype` as a concrete object
rather than a string alias or allowing it to be inferred. For example,
a future version of pandas or a 3rd-party library may include a
dedicated ExtensionArray for string data. In this event, the following
would no longer return a :class:`arrays.NumpyExtensionArray` backed by a
NumPy array.

>>> pd.array(['a', 'b'], dtype=str)
<NumpyExtensionArray>
['a', 'b']
Length: 2, dtype: str32

This would instead return the new ExtensionArray dedicated for string
data. If you really need the new array to be backed by a  NumPy array,
specify that in the dtype.

>>> pd.array(['a', 'b'], dtype=np.dtype(""<U1""))
<NumpyExtensionArray>
['a', 'b']
Length: 2, dtype: str32

Finally, Pandas has arrays that mostly overlap with NumPy

  * :class:`arrays.DatetimeArray`
  * :class:`arrays.TimedeltaArray`

When data with a ``datetime64[ns]`` or ``timedelta64[ns]`` dtype is
passed, pandas will always return a ``DatetimeArray`` or ``TimedeltaArray``
rather than a ``NumpyExtensionArray``. This is for symmetry with the case of
timezone-aware data, which NumPy does not natively support.

>>> pd.array(['2015', '2016'], dtype='datetime64[ns]')
<DatetimeArray>
['2015-01-01 00:00:00', '2016-01-01 00:00:00']
Length: 2, dtype: datetime64[ns]

>>> pd.array([""1H"", ""2H""], dtype='timedelta64[ns]')
<TimedeltaArray>
['0 days 01:00:00', '0 days 02:00:00']
Length: 2, dtype: timedelta64[ns]

Examples
--------
If a dtype is not specified, pandas will infer the best dtype from the values.
See the description of `dtype` for the types pandas infers for.

>>> pd.array([1, 2])
<IntegerArray>
[1, 2]
Length: 2, dtype: Int64

>>> pd.array([1, 2, np.nan])
<IntegerArray>
[1, 2, <NA>]
Length: 3, dtype: Int64

>>> pd.array([1.1, 2.2])
<FloatingArray>
[1.1, 2.2]
Length: 2, dtype: Float64

>>> pd.array([""a"", None, ""c""])
<StringArray>
['a', <NA>, 'c']
Length: 3, dtype: string

>>> with pd.option_context(""string_storage"", ""pyarrow""):
...     arr = pd.array([""a"", None, ""c""])
...
>>> arr
<ArrowStringArray>
['a', <NA>, 'c']
Length: 3, dtype: string

>>> pd.array([pd.Period('2000', freq=""D""), pd.Period(""2000"", freq=""D"")])
<PeriodArray>
['2000-01-01', '2000-01-01']
Length: 2, dtype: period[D]

You can use the string alias for `dtype`

>>> pd.array(['a', 'b', 'a'], dtype='category')
['a', 'b', 'a']
Categories (2, object): ['a', 'b']

Or specify the actual dtype

>>> pd.array(['a', 'b', 'a'],
...          dtype=pd.CategoricalDtype(['a', 'b', 'c'], ordered=True))
['a', 'b', 'a']
Categories (3, object): ['a' < 'b' < 'c']

If pandas does not infer a dedicated extension type a
:class:`arrays.NumpyExtensionArray` is returned.

>>> pd.array([1 + 1j, 3 + 2j])
<NumpyExtensionArray>
[(1+1j), (3+2j)]
Length: 2, dtype: complex128

As mentioned in the ""Notes"" section, new extension types may be added
in the future (by pandas or 3rd party libraries), causing the return
value to no longer be a :class:`arrays.NumpyExtensionArray`. Specify the
`dtype` as a NumPy dtype if you need to ensure there's no future change in
behavior.

>>> pd.array([1, 2], dtype=np.dtype(""int32""))
<NumpyExtensionArray>
[1, 2]
Length: 2, dtype: int32

`data` must be 1-dimensional. A ValueError is raised when the input
has the wrong dimensionality.

>>> pd.array(1)
Traceback (most recent call last):
  ...
ValueError: Cannot pass scalar '1' to 'pandas.array'."""""""
pandas/core/construction.py,"def extract_array(obj: T, extract_numpy: bool=False, extract_range: bool=False) -> T | ArrayLike:
    typ = getattr(obj, '_typ', None)
    if typ in _typs:
        if typ == 'rangeindex':
            if extract_range:
                return obj._values
            return obj
        return obj._values
    elif extract_numpy and typ == 'npy_extension':
        return obj.to_numpy()
    return obj","""""""Extract the ndarray or ExtensionArray from a Series or Index.

For all other types, `obj` is just returned as is.

Parameters
----------
obj : object
    For Series / Index, the underlying ExtensionArray is unboxed.

extract_numpy : bool, default False
    Whether to extract the ndarray from a NumpyExtensionArray.

extract_range : bool, default False
    If we have a RangeIndex, return range._values if True
    (which is a materialized integer ndarray), otherwise return unchanged.

Returns
-------
arr : object

Examples
--------
>>> extract_array(pd.Series(['a', 'b', 'c'], dtype='category'))
['a', 'b', 'c']
Categories (3, object): ['a', 'b', 'c']

Other objects like lists, arrays, and DataFrames are just passed through.

>>> extract_array([1, 2, 3])
[1, 2, 3]

For an ndarray-backed Series / Index the ndarray is returned.

>>> extract_array(pd.Series([1, 2, 3]))
array([1, 2, 3])

To extract all the way down to the ndarray, pass ``extract_numpy=True``.

>>> extract_array(pd.Series([1, 2, 3]), extract_numpy=True)
array([1, 2, 3])"""""""
pandas/core/construction.py,"def ensure_wrapped_if_datetimelike(arr):
    if isinstance(arr, np.ndarray):
        if arr.dtype.kind == 'M':
            from pandas.core.arrays import DatetimeArray
            return DatetimeArray._from_sequence(arr)
        elif arr.dtype.kind == 'm':
            from pandas.core.arrays import TimedeltaArray
            return TimedeltaArray._from_sequence(arr)
    return arr","""""""Wrap datetime64 and timedelta64 ndarrays in DatetimeArray/TimedeltaArray."""""""
pandas/core/construction.py,"def sanitize_masked_array(data: ma.MaskedArray) -> np.ndarray:
    mask = ma.getmaskarray(data)
    if mask.any():
        (dtype, fill_value) = maybe_promote(data.dtype, np.nan)
        dtype = cast(np.dtype, dtype)
        data = ma.asarray(data.astype(dtype, copy=True))
        data.soften_mask()
        data[mask] = fill_value
    else:
        data = data.copy()
    return data","""""""Convert numpy MaskedArray to ensure mask is softened."""""""
pandas/core/construction.py,"def sanitize_array(data, index: Index | None, dtype: DtypeObj | None=None, copy: bool=False, *, allow_2d: bool=False) -> ArrayLike:
    if isinstance(data, ma.MaskedArray):
        data = sanitize_masked_array(data)
    if isinstance(dtype, NumpyEADtype):
        dtype = dtype.numpy_dtype
    data = extract_array(data, extract_numpy=True, extract_range=True)
    if isinstance(data, np.ndarray) and data.ndim == 0:
        if dtype is None:
            dtype = data.dtype
        data = lib.item_from_zerodim(data)
    elif isinstance(data, range):
        data = range_to_ndarray(data)
        copy = False
    if not is_list_like(data):
        if index is None:
            raise ValueError('index must be specified when data is not list-like')
        if isinstance(data, str) and using_pyarrow_string_dtype():
            from pandas.core.arrays.string_ import StringDtype
            dtype = StringDtype('pyarrow_numpy')
        data = construct_1d_arraylike_from_scalar(data, len(index), dtype)
        return data
    elif isinstance(data, ABCExtensionArray):
        if dtype is not None:
            subarr = data.astype(dtype, copy=copy)
        elif copy:
            subarr = data.copy()
        else:
            subarr = data
    elif isinstance(dtype, ExtensionDtype):
        _sanitize_non_ordered(data)
        cls = dtype.construct_array_type()
        subarr = cls._from_sequence(data, dtype=dtype, copy=copy)
    elif isinstance(data, np.ndarray):
        if isinstance(data, np.matrix):
            data = data.A
        if dtype is None:
            subarr = data
            if data.dtype == object:
                subarr = maybe_infer_to_datetimelike(data)
            elif data.dtype.kind == 'U' and using_pyarrow_string_dtype():
                from pandas.core.arrays.string_ import StringDtype
                dtype = StringDtype(storage='pyarrow_numpy')
                subarr = dtype.construct_array_type()._from_sequence(data, dtype=dtype)
            if subarr is data and copy:
                subarr = subarr.copy()
        else:
            subarr = _try_cast(data, dtype, copy)
    elif hasattr(data, '__array__'):
        data = np.array(data, copy=copy)
        return sanitize_array(data, index=index, dtype=dtype, copy=False, allow_2d=allow_2d)
    else:
        _sanitize_non_ordered(data)
        data = list(data)
        if len(data) == 0 and dtype is None:
            subarr = np.array([], dtype=np.float64)
        elif dtype is not None:
            subarr = _try_cast(data, dtype, copy)
        else:
            subarr = maybe_convert_platform(data)
            if subarr.dtype == object:
                subarr = cast(np.ndarray, subarr)
                subarr = maybe_infer_to_datetimelike(subarr)
    subarr = _sanitize_ndim(subarr, data, dtype, index, allow_2d=allow_2d)
    if isinstance(subarr, np.ndarray):
        dtype = cast(np.dtype, dtype)
        subarr = _sanitize_str_dtypes(subarr, data, dtype, copy)
    return subarr","""""""Sanitize input data to an ndarray or ExtensionArray, copy if specified,
coerce to the dtype if specified.

Parameters
----------
data : Any
index : Index or None, default None
dtype : np.dtype, ExtensionDtype, or None, default None
copy : bool, default False
allow_2d : bool, default False
    If False, raise if we have a 2D Arraylike.

Returns
-------
np.ndarray or ExtensionArray"""""""
pandas/core/construction.py,"def range_to_ndarray(rng: range) -> np.ndarray:
    try:
        arr = np.arange(rng.start, rng.stop, rng.step, dtype='int64')
    except OverflowError:
        if rng.start >= 0 and rng.step > 0 or rng.step < 0 <= rng.stop:
            try:
                arr = np.arange(rng.start, rng.stop, rng.step, dtype='uint64')
            except OverflowError:
                arr = construct_1d_object_array_from_listlike(list(rng))
        else:
            arr = construct_1d_object_array_from_listlike(list(rng))
    return arr","""""""Cast a range object to ndarray."""""""
pandas/core/construction.py,"def _sanitize_non_ordered(data) -> None:
    if isinstance(data, (set, frozenset)):
        raise TypeError(f""'{type(data).__name__}' type is unordered"")","""""""Raise only for unordered sets, e.g., not for dict_keys"""""""
pandas/core/construction.py,"def _sanitize_ndim(result: ArrayLike, data, dtype: DtypeObj | None, index: Index | None, *, allow_2d: bool=False) -> ArrayLike:
    if getattr(result, 'ndim', 0) == 0:
        raise ValueError('result should be arraylike with ndim > 0')
    if result.ndim == 1:
        result = _maybe_repeat(result, index)
    elif result.ndim > 1:
        if isinstance(data, np.ndarray):
            if allow_2d:
                return result
            raise ValueError(f'Data must be 1-dimensional, got ndarray of shape {data.shape} instead')
        if is_object_dtype(dtype) and isinstance(dtype, ExtensionDtype):
            result = com.asarray_tuplesafe(data, dtype=np.dtype('object'))
            cls = dtype.construct_array_type()
            result = cls._from_sequence(result, dtype=dtype)
        else:
            result = com.asarray_tuplesafe(data, dtype=dtype)
    return result","""""""Ensure we have a 1-dimensional result array."""""""
pandas/core/construction.py,"def _sanitize_str_dtypes(result: np.ndarray, data, dtype: np.dtype | None, copy: bool) -> np.ndarray:
    if issubclass(result.dtype.type, str):
        if not lib.is_scalar(data):
            if not np.all(isna(data)):
                data = np.array(data, dtype=dtype, copy=False)
            result = np.array(data, dtype=object, copy=copy)
    return result","""""""Ensure we have a dtype that is supported by pandas."""""""
pandas/core/construction.py,"def _maybe_repeat(arr: ArrayLike, index: Index | None) -> ArrayLike:
    if index is not None:
        if 1 == len(arr) != len(index):
            arr = arr.repeat(len(index))
    return arr","""""""If we have a length-1 array and an index describing how long we expect
the result to be, repeat the array."""""""
pandas/core/construction.py,"def _try_cast(arr: list | np.ndarray, dtype: np.dtype, copy: bool) -> ArrayLike:
    is_ndarray = isinstance(arr, np.ndarray)
    if dtype == object:
        if not is_ndarray:
            subarr = construct_1d_object_array_from_listlike(arr)
            return subarr
        return ensure_wrapped_if_datetimelike(arr).astype(dtype, copy=copy)
    elif dtype.kind == 'U':
        if is_ndarray:
            arr = cast(np.ndarray, arr)
            shape = arr.shape
            if arr.ndim > 1:
                arr = arr.ravel()
        else:
            shape = (len(arr),)
        return lib.ensure_string_array(arr, convert_na_value=False, copy=copy).reshape(shape)
    elif dtype.kind in 'mM':
        return maybe_cast_to_datetime(arr, dtype)
    elif dtype.kind in 'iu':
        subarr = maybe_cast_to_integer_array(arr, dtype)
    else:
        subarr = np.array(arr, dtype=dtype, copy=copy)
    return subarr","""""""Convert input to numpy ndarray and optionally cast to a given dtype.

Parameters
----------
arr : ndarray or list
    Excludes: ExtensionArray, Series, Index.
dtype : np.dtype
copy : bool
    If False, don't copy the data if not needed.

Returns
-------
np.ndarray or ExtensionArray"""""""
pandas/core/dtypes/astype.py,"def _astype_nansafe(arr: np.ndarray, dtype: DtypeObj, copy: bool=True, skipna: bool=False) -> ArrayLike:
    if isinstance(dtype, ExtensionDtype):
        return dtype.construct_array_type()._from_sequence(arr, dtype=dtype, copy=copy)
    elif not isinstance(dtype, np.dtype):
        raise ValueError('dtype must be np.dtype or ExtensionDtype')
    if arr.dtype.kind in 'mM':
        from pandas.core.construction import ensure_wrapped_if_datetimelike
        arr = ensure_wrapped_if_datetimelike(arr)
        res = arr.astype(dtype, copy=copy)
        return np.asarray(res)
    if issubclass(dtype.type, str):
        shape = arr.shape
        if arr.ndim > 1:
            arr = arr.ravel()
        return lib.ensure_string_array(arr, skipna=skipna, convert_na_value=False).reshape(shape)
    elif np.issubdtype(arr.dtype, np.floating) and dtype.kind in 'iu':
        return _astype_float_to_int_nansafe(arr, dtype, copy)
    elif arr.dtype == object:
        if lib.is_np_dtype(dtype, 'M'):
            from pandas import to_datetime
            dti = to_datetime(arr.ravel())
            dta = dti._data.reshape(arr.shape)
            return dta.astype(dtype, copy=False)._ndarray
        elif lib.is_np_dtype(dtype, 'm'):
            from pandas.core.construction import ensure_wrapped_if_datetimelike
            tdvals = array_to_timedelta64(arr).view('m8[ns]')
            tda = ensure_wrapped_if_datetimelike(tdvals)
            return tda.astype(dtype, copy=False)._ndarray
    if dtype.name in ('datetime64', 'timedelta64'):
        msg = f""The '{dtype.name}' dtype has no unit. Please pass in '{dtype.name}[ns]' instead.""
        raise ValueError(msg)
    if copy or arr.dtype == object or dtype == object:
        return arr.astype(dtype, copy=True)
    return arr.astype(dtype, copy=copy)","""""""Cast the elements of an array to a given dtype a nan-safe manner.

Parameters
----------
arr : ndarray
dtype : np.dtype or ExtensionDtype
copy : bool, default True
    If False, a view will be attempted but may fail, if
    e.g. the item sizes don't align.
skipna: bool, default False
    Whether or not we should skip NaN when casting as a string-type.

Raises
------
ValueError
    The dtype was a datetime64/timedelta64 dtype, but it had no unit."""""""
pandas/core/dtypes/astype.py,"def _astype_float_to_int_nansafe(values: np.ndarray, dtype: np.dtype, copy: bool) -> np.ndarray:
    if not np.isfinite(values).all():
        raise IntCastingNaNError('Cannot convert non-finite values (NA or inf) to integer')
    if dtype.kind == 'u':
        if not (values >= 0).all():
            raise ValueError(f'Cannot losslessly cast from {values.dtype} to {dtype}')
    with warnings.catch_warnings():
        warnings.filterwarnings('ignore', category=RuntimeWarning)
        return values.astype(dtype, copy=copy)","""""""astype with a check preventing converting NaN to an meaningless integer value."""""""
pandas/core/dtypes/astype.py,"def astype_array(values: ArrayLike, dtype: DtypeObj, copy: bool=False) -> ArrayLike:
    if values.dtype == dtype:
        if copy:
            return values.copy()
        return values
    if not isinstance(values, np.ndarray):
        values = values.astype(dtype, copy=copy)
    else:
        values = _astype_nansafe(values, dtype, copy=copy)
    if isinstance(dtype, np.dtype) and issubclass(values.dtype.type, str):
        values = np.array(values, dtype=object)
    return values","""""""Cast array (ndarray or ExtensionArray) to the new dtype.

Parameters
----------
values : ndarray or ExtensionArray
dtype : dtype object
copy : bool, default False
    copy if indicated

Returns
-------
ndarray or ExtensionArray"""""""
pandas/core/dtypes/astype.py,"def astype_array_safe(values: ArrayLike, dtype, copy: bool=False, errors: IgnoreRaise='raise') -> ArrayLike:
    errors_legal_values = ('raise', 'ignore')
    if errors not in errors_legal_values:
        invalid_arg = f""Expected value of kwarg 'errors' to be one of {list(errors_legal_values)}. Supplied value is '{errors}'""
        raise ValueError(invalid_arg)
    if inspect.isclass(dtype) and issubclass(dtype, ExtensionDtype):
        msg = f""Expected an instance of {dtype.__name__}, but got the class instead. Try instantiating 'dtype'.""
        raise TypeError(msg)
    dtype = pandas_dtype(dtype)
    if isinstance(dtype, NumpyEADtype):
        dtype = dtype.numpy_dtype
    try:
        new_values = astype_array(values, dtype, copy=copy)
    except (ValueError, TypeError):
        if errors == 'ignore':
            new_values = values
        else:
            raise
    return new_values","""""""Cast array (ndarray or ExtensionArray) to the new dtype.

This basically is the implementation for DataFrame/Series.astype and
includes all custom logic for pandas (NaN-safety, converting str to object,
not allowing )

Parameters
----------
values : ndarray or ExtensionArray
dtype : str, dtype convertible
copy : bool, default False
    copy if indicated
errors : str, {'raise', 'ignore'}, default 'raise'
    - ``raise`` : allow exceptions to be raised
    - ``ignore`` : suppress exceptions. On error return original object

Returns
-------
ndarray or ExtensionArray"""""""
pandas/core/dtypes/astype.py,"def astype_is_view(dtype: DtypeObj, new_dtype: DtypeObj) -> bool:
    if isinstance(dtype, np.dtype) and (not isinstance(new_dtype, np.dtype)):
        (new_dtype, dtype) = (dtype, new_dtype)
    if dtype == new_dtype:
        return True
    elif isinstance(dtype, np.dtype) and isinstance(new_dtype, np.dtype):
        return False
    elif is_string_dtype(dtype) and is_string_dtype(new_dtype):
        return True
    elif is_object_dtype(dtype) and new_dtype.kind == 'O':
        return True
    elif dtype.kind in 'mM' and new_dtype.kind in 'mM':
        dtype = getattr(dtype, 'numpy_dtype', dtype)
        new_dtype = getattr(new_dtype, 'numpy_dtype', new_dtype)
        return getattr(dtype, 'unit', None) == getattr(new_dtype, 'unit', None)
    numpy_dtype = getattr(dtype, 'numpy_dtype', None)
    new_numpy_dtype = getattr(new_dtype, 'numpy_dtype', None)
    if numpy_dtype is None and isinstance(dtype, np.dtype):
        numpy_dtype = dtype
    if new_numpy_dtype is None and isinstance(new_dtype, np.dtype):
        new_numpy_dtype = new_dtype
    if numpy_dtype is not None and new_numpy_dtype is not None:
        return numpy_dtype == new_numpy_dtype
    return True","""""""Checks if astype avoided copying the data.

Parameters
----------
dtype : Original dtype
new_dtype : target dtype

Returns
-------
True if new data is a view or not guaranteed to be a copy, False otherwise"""""""
pandas/core/dtypes/base.py,"def register_extension_dtype(cls: type_t[ExtensionDtypeT]) -> type_t[ExtensionDtypeT]:
    _registry.register(cls)
    return cls","""""""Register an ExtensionType with pandas as class decorator.

This enables operations like ``.astype(name)`` for the name
of the ExtensionDtype.

Returns
-------
callable
    A class decorator.

Examples
--------
>>> from pandas.api.extensions import register_extension_dtype, ExtensionDtype
>>> @register_extension_dtype
... class MyExtensionDtype(ExtensionDtype):
...     name = ""myextension"""""""""
pandas/core/dtypes/cast.py,"def maybe_convert_platform(values: list | tuple | range | np.ndarray | ExtensionArray) -> ArrayLike:
    arr: ArrayLike
    if isinstance(values, (list, tuple, range)):
        arr = construct_1d_object_array_from_listlike(values)
    else:
        arr = values
    if arr.dtype == _dtype_obj:
        arr = cast(np.ndarray, arr)
        arr = lib.maybe_convert_objects(arr)
    return arr","""""""try to do platform conversion, allow ndarray or list here"""""""
pandas/core/dtypes/cast.py,"def is_nested_object(obj) -> bool:
    return bool(isinstance(obj, ABCSeries) and is_object_dtype(obj.dtype) and any((isinstance(v, ABCSeries) for v in obj._values)))","""""""return a boolean if we have a nested object, e.g. a Series with 1 or
more Series elements

This may not be necessarily be performant."""""""
pandas/core/dtypes/cast.py,"def maybe_box_datetimelike(value: Scalar, dtype: Dtype | None=None) -> Scalar:
    if dtype == _dtype_obj:
        pass
    elif isinstance(value, (np.datetime64, dt.datetime)):
        value = Timestamp(value)
    elif isinstance(value, (np.timedelta64, dt.timedelta)):
        value = Timedelta(value)
    return value","""""""Cast scalar to Timestamp or Timedelta if scalar is datetime-like
and dtype is not object.

Parameters
----------
value : scalar
dtype : Dtype, optional

Returns
-------
scalar"""""""
pandas/core/dtypes/cast.py,"def maybe_box_native(value: Scalar | None | NAType) -> Scalar | None | NAType:
    if is_float(value):
        value = float(value)
    elif is_integer(value):
        value = int(value)
    elif is_bool(value):
        value = bool(value)
    elif isinstance(value, (np.datetime64, np.timedelta64)):
        value = maybe_box_datetimelike(value)
    elif value is NA:
        value = None
    return value","""""""If passed a scalar cast the scalar to a python native type.

Parameters
----------
value : scalar or Series

Returns
-------
scalar or Series"""""""
pandas/core/dtypes/cast.py,"def _maybe_unbox_datetimelike(value: Scalar, dtype: DtypeObj) -> Scalar:
    if is_valid_na_for_dtype(value, dtype):
        value = dtype.type('NaT', 'ns')
    elif isinstance(value, Timestamp):
        if value.tz is None:
            value = value.to_datetime64()
        elif not isinstance(dtype, DatetimeTZDtype):
            raise TypeError('Cannot unbox tzaware Timestamp to tznaive dtype')
    elif isinstance(value, Timedelta):
        value = value.to_timedelta64()
    _disallow_mismatched_datetimelike(value, dtype)
    return value","""""""Convert a Timedelta or Timestamp to timedelta64 or datetime64 for setting
into a numpy array.  Failing to unbox would risk dropping nanoseconds.

Notes
-----
Caller is responsible for checking dtype.kind in ""mM"""""""""
pandas/core/dtypes/cast.py,"def _disallow_mismatched_datetimelike(value, dtype: DtypeObj):
    vdtype = getattr(value, 'dtype', None)
    if vdtype is None:
        return
    elif vdtype.kind == 'm' and dtype.kind == 'M' or (vdtype.kind == 'M' and dtype.kind == 'm'):
        raise TypeError(f'Cannot cast {repr(value)} to {dtype}')","""""""numpy allows np.array(dt64values, dtype=""timedelta64[ns]"") and
vice-versa, but we do not want to allow this, so we need to
check explicitly"""""""
pandas/core/dtypes/cast.py,"def maybe_downcast_to_dtype(result: ArrayLike, dtype: str | np.dtype) -> ArrayLike:
    do_round = False
    if isinstance(dtype, str):
        if dtype == 'infer':
            inferred_type = lib.infer_dtype(result, skipna=False)
            if inferred_type == 'boolean':
                dtype = 'bool'
            elif inferred_type == 'integer':
                dtype = 'int64'
            elif inferred_type == 'datetime64':
                dtype = 'datetime64[ns]'
            elif inferred_type in ['timedelta', 'timedelta64']:
                dtype = 'timedelta64[ns]'
            elif inferred_type == 'floating':
                dtype = 'int64'
                if issubclass(result.dtype.type, np.number):
                    do_round = True
            else:
                dtype = 'object'
        dtype = np.dtype(dtype)
    if not isinstance(dtype, np.dtype):
        raise TypeError(dtype)
    converted = maybe_downcast_numeric(result, dtype, do_round)
    if converted is not result:
        return converted
    if dtype.kind in 'mM' and result.dtype.kind in 'if':
        result = result.astype(dtype)
    elif dtype.kind == 'm' and result.dtype == _dtype_obj:
        result = cast(np.ndarray, result)
        result = array_to_timedelta64(result)
    elif dtype == np.dtype('M8[ns]') and result.dtype == _dtype_obj:
        result = cast(np.ndarray, result)
        return np.asarray(maybe_cast_to_datetime(result, dtype=dtype))
    return result","""""""try to cast to the specified dtype (e.g. convert back to bool/int
or could be an astype of float64->float32"""""""
pandas/core/dtypes/cast.py,"def maybe_downcast_numeric(result: ArrayLike, dtype: DtypeObj, do_round: bool=False) -> ArrayLike:
    if not isinstance(dtype, np.dtype) or not isinstance(result.dtype, np.dtype):
        return result

    def trans(x):
        if do_round:
            return x.round()
        return x
    if dtype.kind == result.dtype.kind:
        if result.dtype.itemsize <= dtype.itemsize and result.size:
            return result
    if dtype.kind in 'biu':
        if not result.size:
            return trans(result).astype(dtype)
        r = result.ravel()
        arr = np.array([r[0]])
        if isna(arr).any():
            return result
        elif not isinstance(r[0], (np.integer, np.floating, int, float, bool)):
            return result
        if issubclass(result.dtype.type, (np.object_, np.number)) and notna(result).all():
            new_result = trans(result).astype(dtype)
            if new_result.dtype.kind == 'O' or result.dtype.kind == 'O':
                if (new_result == result).all():
                    return new_result
            elif np.allclose(new_result, result, rtol=0):
                return new_result
    elif issubclass(dtype.type, np.floating) and result.dtype.kind != 'b' and (not is_string_dtype(result.dtype)):
        with warnings.catch_warnings():
            warnings.filterwarnings('ignore', 'overflow encountered in cast', RuntimeWarning)
            new_result = result.astype(dtype)
        size_tols = {4: 0.0005, 8: 5e-08, 16: 5e-16}
        atol = size_tols.get(new_result.dtype.itemsize, 0.0)
        if np.allclose(new_result, result, equal_nan=True, rtol=0.0, atol=atol):
            return new_result
    elif dtype.kind == result.dtype.kind == 'c':
        new_result = result.astype(dtype)
        if np.array_equal(new_result, result, equal_nan=True):
            return new_result
    return result","""""""Subset of maybe_downcast_to_dtype restricted to numeric dtypes.

Parameters
----------
result : ndarray or ExtensionArray
dtype : np.dtype or ExtensionDtype
do_round : bool

Returns
-------
ndarray or ExtensionArray"""""""
pandas/core/dtypes/cast.py,"def maybe_upcast_numeric_to_64bit(arr: NumpyIndexT) -> NumpyIndexT:
    dtype = arr.dtype
    if dtype.kind == 'i' and dtype != np.int64:
        return arr.astype(np.int64)
    elif dtype.kind == 'u' and dtype != np.uint64:
        return arr.astype(np.uint64)
    elif dtype.kind == 'f' and dtype != np.float64:
        return arr.astype(np.float64)
    else:
        return arr","""""""If array is a int/uint/float bit size lower than 64 bit, upcast it to 64 bit.

Parameters
----------
arr : ndarray or ExtensionArray

Returns
-------
ndarray or ExtensionArray"""""""
pandas/core/dtypes/cast.py,"def maybe_cast_pointwise_result(result: ArrayLike, dtype: DtypeObj, numeric_only: bool=False, same_dtype: bool=True) -> ArrayLike:
    if isinstance(dtype, ExtensionDtype):
        if not isinstance(dtype, (CategoricalDtype, DatetimeTZDtype)):
            cls = dtype.construct_array_type()
            if same_dtype:
                result = _maybe_cast_to_extension_array(cls, result, dtype=dtype)
            else:
                result = _maybe_cast_to_extension_array(cls, result)
    elif numeric_only and dtype.kind in 'iufcb' or not numeric_only:
        result = maybe_downcast_to_dtype(result, dtype)
    return result","""""""Try casting result of a pointwise operation back to the original dtype if
appropriate.

Parameters
----------
result : array-like
    Result to cast.
dtype : np.dtype or ExtensionDtype
    Input Series from which result was calculated.
numeric_only : bool, default False
    Whether to cast only numerics or datetimes as well.
same_dtype : bool, default True
    Specify dtype when calling _from_sequence

Returns
-------
result : array-like
    result maybe casted to the dtype."""""""
pandas/core/dtypes/cast.py,"def _maybe_cast_to_extension_array(cls: type[ExtensionArray], obj: ArrayLike, dtype: ExtensionDtype | None=None) -> ArrayLike:
    from pandas.core.arrays.string_ import BaseStringArray
    if issubclass(cls, BaseStringArray) and lib.infer_dtype(obj) != 'string':
        return obj
    try:
        result = cls._from_sequence(obj, dtype=dtype)
    except Exception:
        result = obj
    return result","""""""Call to `_from_sequence` that returns the object unchanged on Exception.

Parameters
----------
cls : class, subclass of ExtensionArray
obj : arraylike
    Values to pass to cls._from_sequence
dtype : ExtensionDtype, optional

Returns
-------
ExtensionArray or obj"""""""
pandas/core/dtypes/cast.py,"def ensure_dtype_can_hold_na(dtype: DtypeObj) -> DtypeObj:
    if isinstance(dtype, ExtensionDtype):
        if dtype._can_hold_na:
            return dtype
        elif isinstance(dtype, IntervalDtype):
            return IntervalDtype(np.float64, closed=dtype.closed)
        return _dtype_obj
    elif dtype.kind == 'b':
        return _dtype_obj
    elif dtype.kind in 'iu':
        return np.dtype(np.float64)
    return dtype","""""""If we have a dtype that cannot hold NA values, find the best match that can."""""""
pandas/core/dtypes/cast.py,"def maybe_promote(dtype: np.dtype, fill_value=np.nan):
    orig = fill_value
    orig_is_nat = False
    if checknull(fill_value):
        if fill_value is not NA:
            try:
                orig_is_nat = np.isnat(fill_value)
            except TypeError:
                pass
        fill_value = _canonical_nans.get(type(fill_value), fill_value)
    try:
        (dtype, fill_value) = _maybe_promote_cached(dtype, fill_value, type(fill_value))
    except TypeError:
        (dtype, fill_value) = _maybe_promote(dtype, fill_value)
    if dtype == _dtype_obj and orig is not None or (orig_is_nat and np.datetime_data(orig)[0] != 'ns'):
        fill_value = orig
    return (dtype, fill_value)","""""""Find the minimal dtype that can hold both the given dtype and fill_value.

Parameters
----------
dtype : np.dtype
fill_value : scalar, default np.nan

Returns
-------
dtype
    Upcasted from dtype argument if necessary.
fill_value
    Upcasted from fill_value argument if necessary.

Raises
------
ValueError
    If fill_value is a non-scalar and dtype is not object."""""""
pandas/core/dtypes/cast.py,"def _ensure_dtype_type(value, dtype: np.dtype):
    if dtype == _dtype_obj:
        return value
    return dtype.type(value)","""""""Ensure that the given value is an instance of the given dtype.

e.g. if out dtype is np.complex64_, we should have an instance of that
as opposed to a python complex object.

Parameters
----------
value : object
dtype : np.dtype

Returns
-------
object"""""""
pandas/core/dtypes/cast.py,"def infer_dtype_from(val) -> tuple[DtypeObj, Any]:
    if not is_list_like(val):
        return infer_dtype_from_scalar(val)
    return infer_dtype_from_array(val)","""""""Interpret the dtype from a scalar or array.

Parameters
----------
val : object"""""""
pandas/core/dtypes/cast.py,"def infer_dtype_from_scalar(val) -> tuple[DtypeObj, Any]:
    dtype: DtypeObj = _dtype_obj
    if isinstance(val, np.ndarray):
        if val.ndim != 0:
            msg = 'invalid ndarray passed to infer_dtype_from_scalar'
            raise ValueError(msg)
        dtype = val.dtype
        val = lib.item_from_zerodim(val)
    elif isinstance(val, str):
        dtype = _dtype_obj
        if using_pyarrow_string_dtype():
            from pandas.core.arrays.string_ import StringDtype
            dtype = StringDtype(storage='pyarrow_numpy')
    elif isinstance(val, (np.datetime64, dt.datetime)):
        try:
            val = Timestamp(val)
        except OutOfBoundsDatetime:
            return (_dtype_obj, val)
        if val is NaT or val.tz is None:
            val = val.to_datetime64()
            dtype = val.dtype
        else:
            dtype = DatetimeTZDtype(unit=val.unit, tz=val.tz)
    elif isinstance(val, (np.timedelta64, dt.timedelta)):
        try:
            val = Timedelta(val)
        except (OutOfBoundsTimedelta, OverflowError):
            dtype = _dtype_obj
        else:
            if val is NaT:
                val = np.timedelta64('NaT', 'ns')
            else:
                val = val.asm8
            dtype = val.dtype
    elif is_bool(val):
        dtype = np.dtype(np.bool_)
    elif is_integer(val):
        if isinstance(val, np.integer):
            dtype = np.dtype(type(val))
        else:
            dtype = np.dtype(np.int64)
        try:
            np.array(val, dtype=dtype)
        except OverflowError:
            dtype = np.array(val).dtype
    elif is_float(val):
        if isinstance(val, np.floating):
            dtype = np.dtype(type(val))
        else:
            dtype = np.dtype(np.float64)
    elif is_complex(val):
        dtype = np.dtype(np.complex128)
    if lib.is_period(val):
        dtype = PeriodDtype(freq=val.freq)
    elif lib.is_interval(val):
        subtype = infer_dtype_from_scalar(val.left)[0]
        dtype = IntervalDtype(subtype=subtype, closed=val.closed)
    return (dtype, val)","""""""Interpret the dtype from a scalar.

Parameters
----------
val : object"""""""
pandas/core/dtypes/cast.py,"def dict_compat(d: dict[Scalar, Scalar]) -> dict[Scalar, Scalar]:
    return {maybe_box_datetimelike(key): value for (key, value) in d.items()}","""""""Convert datetimelike-keyed dicts to a Timestamp-keyed dict.

Parameters
----------
d: dict-like object

Returns
-------
dict"""""""
pandas/core/dtypes/cast.py,"def infer_dtype_from_array(arr) -> tuple[DtypeObj, ArrayLike]:
    if isinstance(arr, np.ndarray):
        return (arr.dtype, arr)
    if not is_list_like(arr):
        raise TypeError(""'arr' must be list-like"")
    arr_dtype = getattr(arr, 'dtype', None)
    if isinstance(arr_dtype, ExtensionDtype):
        return (arr.dtype, arr)
    elif isinstance(arr, ABCSeries):
        return (arr.dtype, np.asarray(arr))
    inferred = lib.infer_dtype(arr, skipna=False)
    if inferred in ['string', 'bytes', 'mixed', 'mixed-integer']:
        return (np.dtype(np.object_), arr)
    arr = np.asarray(arr)
    return (arr.dtype, arr)","""""""Infer the dtype from an array.

Parameters
----------
arr : array

Returns
-------
tuple (pandas-compat dtype, array)


Examples
--------
>>> np.asarray([1, '1'])
array(['1', '1'], dtype='<U21')

>>> infer_dtype_from_array([1, '1'])
(dtype('O'), [1, '1'])"""""""
pandas/core/dtypes/cast.py,"def _maybe_infer_dtype_type(element):
    tipo = None
    if hasattr(element, 'dtype'):
        tipo = element.dtype
    elif is_list_like(element):
        element = np.asarray(element)
        tipo = element.dtype
    return tipo","""""""Try to infer an object's dtype, for use in arithmetic ops.

Uses `element.dtype` if that's available.
Objects implementing the iterator protocol are cast to a NumPy array,
and from there the array's type is used.

Parameters
----------
element : object
    Possibly has a `.dtype` attribute, and possibly the iterator
    protocol.

Returns
-------
tipo : type

Examples
--------
>>> from collections import namedtuple
>>> Foo = namedtuple(""Foo"", ""dtype"")
>>> _maybe_infer_dtype_type(Foo(np.dtype(""i8"")))
dtype('int64')"""""""
pandas/core/dtypes/cast.py,"def invalidate_string_dtypes(dtype_set: set[DtypeObj]) -> None:
    non_string_dtypes = dtype_set - {np.dtype('S').type, np.dtype('<U').type}
    if non_string_dtypes != dtype_set:
        raise TypeError(""string dtypes are not allowed, use 'object' instead"")","""""""Change string like dtypes to object for
``DataFrame.select_dtypes()``."""""""
pandas/core/dtypes/cast.py,"def coerce_indexer_dtype(indexer, categories) -> np.ndarray:
    length = len(categories)
    if length < _int8_max:
        return ensure_int8(indexer)
    elif length < _int16_max:
        return ensure_int16(indexer)
    elif length < _int32_max:
        return ensure_int32(indexer)
    return ensure_int64(indexer)","""""""coerce the indexer input array to the smallest dtype possible"""""""
pandas/core/dtypes/cast.py,"def convert_dtypes(input_array: ArrayLike, convert_string: bool=True, convert_integer: bool=True, convert_boolean: bool=True, convert_floating: bool=True, infer_objects: bool=False, dtype_backend: Literal['numpy_nullable', 'pyarrow']='numpy_nullable') -> DtypeObj:
    inferred_dtype: str | DtypeObj
    if (convert_string or convert_integer or convert_boolean or convert_floating) and isinstance(input_array, np.ndarray):
        if input_array.dtype == object:
            inferred_dtype = lib.infer_dtype(input_array)
        else:
            inferred_dtype = input_array.dtype
        if is_string_dtype(inferred_dtype):
            if not convert_string or inferred_dtype == 'bytes':
                inferred_dtype = input_array.dtype
            else:
                inferred_dtype = pandas_dtype_func('string')
        if convert_integer:
            target_int_dtype = pandas_dtype_func('Int64')
            if input_array.dtype.kind in 'iu':
                from pandas.core.arrays.integer import NUMPY_INT_TO_DTYPE
                inferred_dtype = NUMPY_INT_TO_DTYPE.get(input_array.dtype, target_int_dtype)
            elif input_array.dtype.kind in 'fcb':
                arr = input_array[notna(input_array)]
                if (arr.astype(int) == arr).all():
                    inferred_dtype = target_int_dtype
                else:
                    inferred_dtype = input_array.dtype
            elif infer_objects and input_array.dtype == object and (isinstance(inferred_dtype, str) and inferred_dtype == 'integer'):
                inferred_dtype = target_int_dtype
        if convert_floating:
            if input_array.dtype.kind in 'fcb':
                from pandas.core.arrays.floating import NUMPY_FLOAT_TO_DTYPE
                inferred_float_dtype: DtypeObj = NUMPY_FLOAT_TO_DTYPE.get(input_array.dtype, pandas_dtype_func('Float64'))
                if convert_integer:
                    arr = input_array[notna(input_array)]
                    if (arr.astype(int) == arr).all():
                        inferred_dtype = pandas_dtype_func('Int64')
                    else:
                        inferred_dtype = inferred_float_dtype
                else:
                    inferred_dtype = inferred_float_dtype
            elif infer_objects and input_array.dtype == object and (isinstance(inferred_dtype, str) and inferred_dtype == 'mixed-integer-float'):
                inferred_dtype = pandas_dtype_func('Float64')
        if convert_boolean:
            if input_array.dtype.kind == 'b':
                inferred_dtype = pandas_dtype_func('boolean')
            elif isinstance(inferred_dtype, str) and inferred_dtype == 'boolean':
                inferred_dtype = pandas_dtype_func('boolean')
        if isinstance(inferred_dtype, str):
            inferred_dtype = input_array.dtype
    else:
        inferred_dtype = input_array.dtype
    if dtype_backend == 'pyarrow':
        from pandas.core.arrays.arrow.array import to_pyarrow_type
        from pandas.core.arrays.string_ import StringDtype
        assert not isinstance(inferred_dtype, str)
        if convert_integer and inferred_dtype.kind in 'iu' or (convert_floating and inferred_dtype.kind in 'fc') or (convert_boolean and inferred_dtype.kind == 'b') or (convert_string and isinstance(inferred_dtype, StringDtype)) or (inferred_dtype.kind not in 'iufcb' and (not isinstance(inferred_dtype, StringDtype))):
            if isinstance(inferred_dtype, PandasExtensionDtype) and (not isinstance(inferred_dtype, DatetimeTZDtype)):
                base_dtype = inferred_dtype.base
            elif isinstance(inferred_dtype, (BaseMaskedDtype, ArrowDtype)):
                base_dtype = inferred_dtype.numpy_dtype
            elif isinstance(inferred_dtype, StringDtype):
                base_dtype = np.dtype(str)
            else:
                base_dtype = inferred_dtype
            if base_dtype.kind == 'O' and len(input_array) > 0 and isna(input_array).all():
                import pyarrow as pa
                pa_type = pa.null()
            else:
                pa_type = to_pyarrow_type(base_dtype)
            if pa_type is not None:
                inferred_dtype = ArrowDtype(pa_type)
    elif dtype_backend == 'numpy_nullable' and isinstance(inferred_dtype, ArrowDtype):
        inferred_dtype = _arrow_dtype_mapping()[inferred_dtype.pyarrow_dtype]
    return inferred_dtype","""""""Convert objects to best possible type, and optionally,
to types supporting ``pd.NA``.

Parameters
----------
input_array : ExtensionArray or np.ndarray
convert_string : bool, default True
    Whether object dtypes should be converted to ``StringDtype()``.
convert_integer : bool, default True
    Whether, if possible, conversion can be done to integer extension types.
convert_boolean : bool, defaults True
    Whether object dtypes should be converted to ``BooleanDtypes()``.
convert_floating : bool, defaults True
    Whether, if possible, conversion can be done to floating extension types.
    If `convert_integer` is also True, preference will be give to integer
    dtypes if the floats can be faithfully casted to integers.
infer_objects : bool, defaults False
    Whether to also infer objects to float/int if possible. Is only hit if the
    object array contains pd.NA.
dtype_backend : {'numpy_nullable', 'pyarrow'}, default 'numpy_nullable'
    Back-end data type applied to the resultant :class:`DataFrame`
    (still experimental). Behaviour is as follows:

    * ``""numpy_nullable""``: returns nullable-dtype-backed :class:`DataFrame`
      (default).
    * ``""pyarrow""``: returns pyarrow-backed nullable :class:`ArrowDtype`
      DataFrame.

    .. versionadded:: 2.0

Returns
-------
np.dtype, or ExtensionDtype"""""""
pandas/core/dtypes/cast.py,"def maybe_infer_to_datetimelike(value: npt.NDArray[np.object_]) -> np.ndarray | DatetimeArray | TimedeltaArray | PeriodArray | IntervalArray:
    if not isinstance(value, np.ndarray) or value.dtype != object:
        raise TypeError(type(value))
    if value.ndim != 1:
        raise ValueError(value.ndim)
    if not len(value):
        return value
    return lib.maybe_convert_objects(value, convert_numeric=False, convert_non_numeric=True, dtype_if_all_nat=np.dtype('M8[ns]'))","""""""we might have a array (or single object) that is datetime like,
and no dtype is passed don't change the value unless we find a
datetime/timedelta set

this is pretty strict in that a datetime/timedelta is REQUIRED
in addition to possible nulls/string likes

Parameters
----------
value : np.ndarray[object]

Returns
-------
np.ndarray, DatetimeArray, TimedeltaArray, PeriodArray, or IntervalArray"""""""
pandas/core/dtypes/cast.py,"def maybe_cast_to_datetime(value: np.ndarray | list, dtype: np.dtype) -> ExtensionArray | np.ndarray:
    from pandas.core.arrays.datetimes import DatetimeArray
    from pandas.core.arrays.timedeltas import TimedeltaArray
    assert dtype.kind in 'mM'
    if not is_list_like(value):
        raise TypeError('value must be listlike')
    _ensure_nanosecond_dtype(dtype)
    if lib.is_np_dtype(dtype, 'm'):
        res = TimedeltaArray._from_sequence(value, dtype=dtype)
        return res
    else:
        try:
            dta = DatetimeArray._from_sequence(value, dtype=dtype)
        except ValueError as err:
            if 'cannot supply both a tz and a timezone-naive dtype' in str(err):
                raise ValueError('Cannot convert timezone-aware data to timezone-naive dtype. Use pd.Series(values).dt.tz_localize(None) instead.') from err
            raise
        return dta","""""""try to cast the array/value to a datetimelike dtype, converting float
nan to iNaT

Caller is responsible for handling ExtensionDtype cases and non dt64/td64
cases."""""""
pandas/core/dtypes/cast.py,"def _ensure_nanosecond_dtype(dtype: DtypeObj) -> None:
    msg = f""The '{dtype.name}' dtype has no unit. Please pass in '{dtype.name}[ns]' instead.""
    dtype = getattr(dtype, 'subtype', dtype)
    if not isinstance(dtype, np.dtype):
        pass
    elif dtype.kind in 'mM':
        reso = get_unit_from_dtype(dtype)
        if not is_supported_unit(reso):
            if dtype.name in ['datetime64', 'timedelta64']:
                raise ValueError(msg)
            raise TypeError(f""dtype={dtype} is not supported. Supported resolutions are 's', 'ms', 'us', and 'ns'"")","""""""Convert dtypes with granularity less than nanosecond to nanosecond

>>> _ensure_nanosecond_dtype(np.dtype(""M8[us]""))

>>> _ensure_nanosecond_dtype(np.dtype(""M8[D]""))
Traceback (most recent call last):
    ...
TypeError: dtype=datetime64[D] is not supported. Supported resolutions are 's', 'ms', 'us', and 'ns'

>>> _ensure_nanosecond_dtype(np.dtype(""m8[ps]""))
Traceback (most recent call last):
    ...
TypeError: dtype=timedelta64[ps] is not supported. Supported resolutions are 's', 'ms', 'us', and 'ns'"""""""
pandas/core/dtypes/cast.py,"def find_result_type(left_dtype: DtypeObj, right: Any) -> DtypeObj:
    new_dtype: DtypeObj
    if isinstance(left_dtype, np.dtype) and left_dtype.kind in 'iuc' and (lib.is_integer(right) or lib.is_float(right)):
        if lib.is_float(right) and right.is_integer() and (left_dtype.kind != 'f'):
            right = int(right)
        new_dtype = np.result_type(left_dtype, right)
    elif is_valid_na_for_dtype(right, left_dtype):
        new_dtype = ensure_dtype_can_hold_na(left_dtype)
    else:
        (dtype, _) = infer_dtype_from(right)
        new_dtype = find_common_type([left_dtype, dtype])
    return new_dtype","""""""Find the type/dtype for the result of an operation between objects.

This is similar to find_common_type, but looks at the right object instead
of just its dtype. This can be useful in particular when the right
object does not have a `dtype`.

Parameters
----------
left_dtype : np.dtype or ExtensionDtype
right : Any

Returns
-------
np.dtype or ExtensionDtype

See also
--------
find_common_type
numpy.result_type"""""""
pandas/core/dtypes/cast.py,"def common_dtype_categorical_compat(objs: Sequence[Index | ArrayLike], dtype: DtypeObj) -> DtypeObj:
    if lib.is_np_dtype(dtype, 'iu'):
        for obj in objs:
            obj_dtype = getattr(obj, 'dtype', None)
            if isinstance(obj_dtype, CategoricalDtype):
                if isinstance(obj, ABCIndex):
                    hasnas = obj.hasnans
                else:
                    hasnas = cast('Categorical', obj)._hasna
                if hasnas:
                    dtype = np.dtype(np.float64)
                    break
    return dtype","""""""Update the result of find_common_type to account for NAs in a Categorical.

Parameters
----------
objs : list[np.ndarray | ExtensionArray | Index]
dtype : np.dtype or ExtensionDtype

Returns
-------
np.dtype or ExtensionDtype"""""""
pandas/core/dtypes/cast.py,"def np_find_common_type(*dtypes: np.dtype) -> np.dtype:
    try:
        common_dtype = np.result_type(*dtypes)
        if common_dtype.kind in 'mMSU':
            common_dtype = np.dtype('O')
    except TypeError:
        common_dtype = np.dtype('O')
    return common_dtype","""""""np.find_common_type implementation pre-1.25 deprecation using np.result_type
https://github.com/pandas-dev/pandas/pull/49569#issuecomment-1308300065

Parameters
----------
dtypes : np.dtypes

Returns
-------
np.dtype"""""""
pandas/core/dtypes/cast.py,"def find_common_type(types):
    if not types:
        raise ValueError('no types given')
    first = types[0]
    if lib.dtypes_all_equal(list(types)):
        return first
    types = list(dict.fromkeys(types).keys())
    if any((isinstance(t, ExtensionDtype) for t in types)):
        for t in types:
            if isinstance(t, ExtensionDtype):
                res = t._get_common_dtype(types)
                if res is not None:
                    return res
        return np.dtype('object')
    if all((lib.is_np_dtype(t, 'M') for t in types)):
        return np.dtype(max(types))
    if all((lib.is_np_dtype(t, 'm') for t in types)):
        return np.dtype(max(types))
    has_bools = any((t.kind == 'b' for t in types))
    if has_bools:
        for t in types:
            if t.kind in 'iufc':
                return np.dtype('object')
    return np_find_common_type(*types)","""""""Find a common data type among the given dtypes.

Parameters
----------
types : list of dtypes

Returns
-------
pandas extension or numpy dtype

See Also
--------
numpy.find_common_type"""""""
pandas/core/dtypes/cast.py,"def construct_1d_arraylike_from_scalar(value: Scalar, length: int, dtype: DtypeObj | None) -> ArrayLike:
    if dtype is None:
        try:
            (dtype, value) = infer_dtype_from_scalar(value)
        except OutOfBoundsDatetime:
            dtype = _dtype_obj
    if isinstance(dtype, ExtensionDtype):
        cls = dtype.construct_array_type()
        seq = [] if length == 0 else [value]
        subarr = cls._from_sequence(seq, dtype=dtype).repeat(length)
    else:
        if length and dtype.kind in 'iu' and isna(value):
            dtype = np.dtype('float64')
        elif lib.is_np_dtype(dtype, 'US'):
            dtype = np.dtype('object')
            if not isna(value):
                value = ensure_str(value)
        elif dtype.kind in 'mM':
            value = _maybe_box_and_unbox_datetimelike(value, dtype)
        subarr = np.empty(length, dtype=dtype)
        if length:
            subarr.fill(value)
    return subarr","""""""create a np.ndarray / pandas type of specified shape and dtype
filled with values

Parameters
----------
value : scalar value
length : int
dtype : pandas_dtype or np.dtype

Returns
-------
np.ndarray / pandas type of length, filled with value"""""""
pandas/core/dtypes/cast.py,"def construct_1d_object_array_from_listlike(values: Sized) -> np.ndarray:
    result = np.empty(len(values), dtype='object')
    result[:] = values
    return result","""""""Transform any list-like object in a 1-dimensional numpy array of object
dtype.

Parameters
----------
values : any iterable which has a len()

Raises
------
TypeError
    * If `values` does not have a len()

Returns
-------
1-dimensional numpy array of dtype object"""""""
pandas/core/dtypes/cast.py,"def maybe_cast_to_integer_array(arr: list | np.ndarray, dtype: np.dtype) -> np.ndarray:
    assert dtype.kind in 'iu'
    try:
        if not isinstance(arr, np.ndarray):
            with warnings.catch_warnings():
                warnings.filterwarnings('ignore', 'NumPy will stop allowing conversion of out-of-bound Python int', DeprecationWarning)
                casted = np.array(arr, dtype=dtype, copy=False)
        else:
            with warnings.catch_warnings():
                warnings.filterwarnings('ignore', category=RuntimeWarning)
                casted = arr.astype(dtype, copy=False)
    except OverflowError as err:
        raise OverflowError(f'The elements provided in the data cannot all be casted to the dtype {dtype}') from err
    if isinstance(arr, np.ndarray) and arr.dtype == dtype:
        return casted
    with warnings.catch_warnings():
        warnings.filterwarnings('ignore', category=RuntimeWarning)
        warnings.filterwarnings('ignore', 'elementwise comparison failed', FutureWarning)
        if np.array_equal(arr, casted):
            return casted
    arr = np.asarray(arr)
    if np.issubdtype(arr.dtype, str):
        if (casted.astype(str) == arr).all():
            return casted
        raise ValueError(f'string values cannot be losslessly cast to {dtype}')
    if dtype.kind == 'u' and (arr < 0).any():
        raise OverflowError('Trying to coerce negative values to unsigned integers')
    if arr.dtype.kind == 'f':
        if not np.isfinite(arr).all():
            raise IntCastingNaNError('Cannot convert non-finite values (NA or inf) to integer')
        raise ValueError('Trying to coerce float values to integers')
    if arr.dtype == object:
        raise ValueError('Trying to coerce float values to integers')
    if casted.dtype < arr.dtype:
        raise ValueError(f'Values are too large to be losslessly converted to {dtype}. To cast anyway, use pd.Series(values).astype({dtype})')
    if arr.dtype.kind in 'mM':
        raise TypeError(f'Constructing a Series or DataFrame from {arr.dtype} values and dtype={dtype} is not supported. Use values.view({dtype}) instead.')
    raise ValueError(f'values cannot be losslessly cast to {dtype}')","""""""Takes any dtype and returns the casted version, raising for when data is
incompatible with integer/unsigned integer dtypes.

Parameters
----------
arr : np.ndarray or list
    The array to cast.
dtype : np.dtype
    The integer dtype to cast the array to.

Returns
-------
ndarray
    Array of integer or unsigned integer dtype.

Raises
------
OverflowError : the dtype is incompatible with the data
ValueError : loss of precision has occurred during casting

Examples
--------
If you try to coerce negative values to unsigned integers, it raises:

>>> pd.Series([-1], dtype=""uint64"")
Traceback (most recent call last):
    ...
OverflowError: Trying to coerce negative values to unsigned integers

Also, if you try to coerce float values to integers, it raises:

>>> maybe_cast_to_integer_array([1, 2, 3.5], dtype=np.dtype(""int64""))
Traceback (most recent call last):
    ...
ValueError: Trying to coerce float values to integers"""""""
pandas/core/dtypes/cast.py,"def can_hold_element(arr: ArrayLike, element: Any) -> bool:
    dtype = arr.dtype
    if not isinstance(dtype, np.dtype) or dtype.kind in 'mM':
        if isinstance(dtype, (PeriodDtype, IntervalDtype, DatetimeTZDtype, np.dtype)):
            arr = cast('PeriodArray | DatetimeArray | TimedeltaArray | IntervalArray', arr)
            try:
                arr._validate_setitem_value(element)
                return True
            except (ValueError, TypeError):
                return False
        return True
    try:
        np_can_hold_element(dtype, element)
        return True
    except (TypeError, LossySetitemError):
        return False","""""""Can we do an inplace setitem with this element in an array with this dtype?

Parameters
----------
arr : np.ndarray or ExtensionArray
element : Any

Returns
-------
bool"""""""
pandas/core/dtypes/cast.py,"def np_can_hold_element(dtype: np.dtype, element: Any) -> Any:
    if dtype == _dtype_obj:
        return element
    tipo = _maybe_infer_dtype_type(element)
    if dtype.kind in 'iu':
        if isinstance(element, range):
            if _dtype_can_hold_range(element, dtype):
                return element
            raise LossySetitemError
        if is_integer(element) or (is_float(element) and element.is_integer()):
            info = np.iinfo(dtype)
            if info.min <= element <= info.max:
                return dtype.type(element)
            raise LossySetitemError
        if tipo is not None:
            if tipo.kind not in 'iu':
                if isinstance(element, np.ndarray) and element.dtype.kind == 'f':
                    with np.errstate(invalid='ignore'):
                        casted = element.astype(dtype)
                    comp = casted == element
                    if comp.all():
                        return casted
                    raise LossySetitemError
                elif isinstance(element, ABCExtensionArray) and isinstance(element.dtype, CategoricalDtype):
                    try:
                        casted = element.astype(dtype)
                    except (ValueError, TypeError):
                        raise LossySetitemError
                    comp = casted == element
                    if not comp.all():
                        raise LossySetitemError
                    return casted
                raise LossySetitemError
            if dtype.kind == 'u' and isinstance(element, np.ndarray) and (element.dtype.kind == 'i'):
                casted = element.astype(dtype)
                if (casted == element).all():
                    return casted
                raise LossySetitemError
            if dtype.itemsize < tipo.itemsize:
                raise LossySetitemError
            if not isinstance(tipo, np.dtype):
                if element._hasna:
                    raise LossySetitemError
                return element
            return element
        raise LossySetitemError
    if dtype.kind == 'f':
        if lib.is_integer(element) or lib.is_float(element):
            casted = dtype.type(element)
            if np.isnan(casted) or casted == element:
                return casted
            raise LossySetitemError
        if tipo is not None:
            if tipo.kind not in 'iuf':
                raise LossySetitemError
            if not isinstance(tipo, np.dtype):
                if element._hasna:
                    raise LossySetitemError
                return element
            elif tipo.itemsize > dtype.itemsize or tipo.kind != dtype.kind:
                if isinstance(element, np.ndarray):
                    casted = element.astype(dtype)
                    if np.array_equal(casted, element, equal_nan=True):
                        return casted
                    raise LossySetitemError
            return element
        raise LossySetitemError
    if dtype.kind == 'c':
        if lib.is_integer(element) or lib.is_complex(element) or lib.is_float(element):
            if np.isnan(element):
                return dtype.type(element)
            with warnings.catch_warnings():
                warnings.filterwarnings('ignore')
                casted = dtype.type(element)
            if casted == element:
                return casted
            raise LossySetitemError
        if tipo is not None:
            if tipo.kind in 'iufc':
                return element
            raise LossySetitemError
        raise LossySetitemError
    if dtype.kind == 'b':
        if tipo is not None:
            if tipo.kind == 'b':
                if not isinstance(tipo, np.dtype):
                    if element._hasna:
                        raise LossySetitemError
                return element
            raise LossySetitemError
        if lib.is_bool(element):
            return element
        raise LossySetitemError
    if dtype.kind == 'S':
        if tipo is not None:
            if tipo.kind == 'S' and tipo.itemsize <= dtype.itemsize:
                return element
            raise LossySetitemError
        if isinstance(element, bytes) and len(element) <= dtype.itemsize:
            return element
        raise LossySetitemError
    if dtype.kind == 'V':
        raise LossySetitemError
    raise NotImplementedError(dtype)","""""""Raise if we cannot losslessly set this element into an ndarray with this dtype.

Specifically about places where we disagree with numpy.  i.e. there are
cases where numpy will raise in doing the setitem that we do not check
for here, e.g. setting str ""X"" into a numeric ndarray.

Returns
-------
Any
    The element, potentially cast to the dtype.

Raises
------
ValueError : If we cannot losslessly store this element with this dtype."""""""
pandas/core/dtypes/cast.py,"def _dtype_can_hold_range(rng: range, dtype: np.dtype) -> bool:
    if not len(rng):
        return True
    return np.can_cast(rng[0], dtype) and np.can_cast(rng[-1], dtype)","""""""_maybe_infer_dtype_type infers to int64 (and float64 for very large endpoints),
but in many cases a range can be held by a smaller integer dtype.
Check if this is one of those cases."""""""
pandas/core/dtypes/common.py,"def ensure_str(value: bytes | Any) -> str:
    if isinstance(value, bytes):
        value = value.decode('utf-8')
    elif not isinstance(value, str):
        value = str(value)
    return value","""""""Ensure that bytes and non-strings get converted into ``str`` objects."""""""
pandas/core/dtypes/common.py,"def ensure_python_int(value: int | np.integer) -> int:
    if not (is_integer(value) or is_float(value)):
        if not is_scalar(value):
            raise TypeError(f'Value needs to be a scalar value, was type {type(value).__name__}')
        raise TypeError(f'Wrong type {type(value)} for value {value}')
    try:
        new_value = int(value)
        assert new_value == value
    except (TypeError, ValueError, AssertionError) as err:
        raise TypeError(f'Wrong type {type(value)} for value {value}') from err
    return new_value","""""""Ensure that a value is a python int.

Parameters
----------
value: int or numpy.integer

Returns
-------
int

Raises
------
TypeError: if the value isn't an int or can't be converted to one."""""""
pandas/core/dtypes/common.py,"def classes(*klasses) -> Callable:
    return lambda tipo: issubclass(tipo, klasses)","""""""Evaluate if the tipo is a subclass of the klasses."""""""
pandas/core/dtypes/common.py,"def _classes_and_not_datetimelike(*klasses) -> Callable:
    return lambda tipo: issubclass(tipo, klasses) and (not issubclass(tipo, (np.datetime64, np.timedelta64)))","""""""Evaluate if the tipo is a subclass of the klasses
and not a datetimelike."""""""
pandas/core/dtypes/common.py,"def is_object_dtype(arr_or_dtype) -> bool:
    return _is_dtype_type(arr_or_dtype, classes(np.object_))","""""""Check whether an array-like or dtype is of the object dtype.

Parameters
----------
arr_or_dtype : array-like or dtype
    The array-like or dtype to check.

Returns
-------
boolean
    Whether or not the array-like or dtype is of the object dtype.

Examples
--------
>>> from pandas.api.types import is_object_dtype
>>> is_object_dtype(object)
True
>>> is_object_dtype(int)
False
>>> is_object_dtype(np.array([], dtype=object))
True
>>> is_object_dtype(np.array([], dtype=int))
False
>>> is_object_dtype([1, 2, 3])
False"""""""
pandas/core/dtypes/common.py,"def is_sparse(arr) -> bool:
    warnings.warn('is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.', FutureWarning, stacklevel=find_stack_level())
    dtype = getattr(arr, 'dtype', arr)
    return isinstance(dtype, SparseDtype)","""""""Check whether an array-like is a 1-D pandas sparse array.

Check that the one-dimensional array-like is a pandas sparse array.
Returns True if it is a pandas sparse array, not another type of
sparse array.

Parameters
----------
arr : array-like
    Array-like to check.

Returns
-------
bool
    Whether or not the array-like is a pandas sparse array.

Examples
--------
Returns `True` if the parameter is a 1-D pandas sparse array.

>>> from pandas.api.types import is_sparse
>>> is_sparse(pd.arrays.SparseArray([0, 0, 1, 0]))
True
>>> is_sparse(pd.Series(pd.arrays.SparseArray([0, 0, 1, 0])))
True

Returns `False` if the parameter is not sparse.

>>> is_sparse(np.array([0, 0, 1, 0]))
False
>>> is_sparse(pd.Series([0, 1, 0, 0]))
False

Returns `False` if the parameter is not a pandas sparse array.

>>> from scipy.sparse import bsr_matrix
>>> is_sparse(bsr_matrix([0, 1, 0, 0]))
False

Returns `False` if the parameter has more than one dimension."""""""
pandas/core/dtypes/common.py,"def is_scipy_sparse(arr) -> bool:
    global _is_scipy_sparse
    if _is_scipy_sparse is None:
        try:
            from scipy.sparse import issparse as _is_scipy_sparse
        except ImportError:
            _is_scipy_sparse = lambda _: False
    assert _is_scipy_sparse is not None
    return _is_scipy_sparse(arr)","""""""Check whether an array-like is a scipy.sparse.spmatrix instance.

Parameters
----------
arr : array-like
    The array-like to check.

Returns
-------
boolean
    Whether or not the array-like is a scipy.sparse.spmatrix instance.

Notes
-----
If scipy is not installed, this function will always return False.

Examples
--------
>>> from scipy.sparse import bsr_matrix
>>> is_scipy_sparse(bsr_matrix([1, 2, 3]))
True
>>> is_scipy_sparse(pd.arrays.SparseArray([1, 2, 3]))
False"""""""
pandas/core/dtypes/common.py,"def is_datetime64_dtype(arr_or_dtype) -> bool:
    if isinstance(arr_or_dtype, np.dtype):
        return arr_or_dtype.kind == 'M'
    return _is_dtype_type(arr_or_dtype, classes(np.datetime64))","""""""Check whether an array-like or dtype is of the datetime64 dtype.

Parameters
----------
arr_or_dtype : array-like or dtype
    The array-like or dtype to check.

Returns
-------
boolean
    Whether or not the array-like or dtype is of the datetime64 dtype.

Examples
--------
>>> from pandas.api.types import is_datetime64_dtype
>>> is_datetime64_dtype(object)
False
>>> is_datetime64_dtype(np.datetime64)
True
>>> is_datetime64_dtype(np.array([], dtype=int))
False
>>> is_datetime64_dtype(np.array([], dtype=np.datetime64))
True
>>> is_datetime64_dtype([1, 2, 3])
False"""""""
pandas/core/dtypes/common.py,"def is_datetime64tz_dtype(arr_or_dtype) -> bool:
    warnings.warn('is_datetime64tz_dtype is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.DatetimeTZDtype)` instead.', FutureWarning, stacklevel=find_stack_level())
    if isinstance(arr_or_dtype, DatetimeTZDtype):
        return True
    if arr_or_dtype is None:
        return False
    return DatetimeTZDtype.is_dtype(arr_or_dtype)","""""""Check whether an array-like or dtype is of a DatetimeTZDtype dtype.

Parameters
----------
arr_or_dtype : array-like or dtype
    The array-like or dtype to check.

Returns
-------
boolean
    Whether or not the array-like or dtype is of a DatetimeTZDtype dtype.

Examples
--------
>>> from pandas.api.types import is_datetime64tz_dtype
>>> is_datetime64tz_dtype(object)
False
>>> is_datetime64tz_dtype([1, 2, 3])
False
>>> is_datetime64tz_dtype(pd.DatetimeIndex([1, 2, 3]))  # tz-naive
False
>>> is_datetime64tz_dtype(pd.DatetimeIndex([1, 2, 3], tz=""US/Eastern""))
True

>>> from pandas.core.dtypes.dtypes import DatetimeTZDtype
>>> dtype = DatetimeTZDtype(""ns"", tz=""US/Eastern"")
>>> s = pd.Series([], dtype=dtype)
>>> is_datetime64tz_dtype(dtype)
True
>>> is_datetime64tz_dtype(s)
True"""""""
pandas/core/dtypes/common.py,"def is_timedelta64_dtype(arr_or_dtype) -> bool:
    if isinstance(arr_or_dtype, np.dtype):
        return arr_or_dtype.kind == 'm'
    return _is_dtype_type(arr_or_dtype, classes(np.timedelta64))","""""""Check whether an array-like or dtype is of the timedelta64 dtype.

Parameters
----------
arr_or_dtype : array-like or dtype
    The array-like or dtype to check.

Returns
-------
boolean
    Whether or not the array-like or dtype is of the timedelta64 dtype.

Examples
--------
>>> from pandas.core.dtypes.common import is_timedelta64_dtype
>>> is_timedelta64_dtype(object)
False
>>> is_timedelta64_dtype(np.timedelta64)
True
>>> is_timedelta64_dtype([1, 2, 3])
False
>>> is_timedelta64_dtype(pd.Series([], dtype=""timedelta64[ns]""))
True
>>> is_timedelta64_dtype('0 days')
False"""""""
pandas/core/dtypes/common.py,"def is_period_dtype(arr_or_dtype) -> bool:
    warnings.warn('is_period_dtype is deprecated and will be removed in a future version. Use `isinstance(dtype, pd.PeriodDtype)` instead', FutureWarning, stacklevel=find_stack_level())
    if isinstance(arr_or_dtype, ExtensionDtype):
        return arr_or_dtype.type is Period
    if arr_or_dtype is None:
        return False
    return PeriodDtype.is_dtype(arr_or_dtype)","""""""Check whether an array-like or dtype is of the Period dtype.

Parameters
----------
arr_or_dtype : array-like or dtype
    The array-like or dtype to check.

Returns
-------
boolean
    Whether or not the array-like or dtype is of the Period dtype.

Examples
--------
>>> from pandas.core.dtypes.common import is_period_dtype
>>> is_period_dtype(object)
False
>>> is_period_dtype(pd.PeriodDtype(freq=""D""))
True
>>> is_period_dtype([1, 2, 3])
False
>>> is_period_dtype(pd.Period(""2017-01-01""))
False
>>> is_period_dtype(pd.PeriodIndex([], freq=""A""))
True"""""""
pandas/core/dtypes/common.py,"def is_interval_dtype(arr_or_dtype) -> bool:
    warnings.warn('is_interval_dtype is deprecated and will be removed in a future version. Use `isinstance(dtype, pd.IntervalDtype)` instead', FutureWarning, stacklevel=find_stack_level())
    if isinstance(arr_or_dtype, ExtensionDtype):
        return arr_or_dtype.type is Interval
    if arr_or_dtype is None:
        return False
    return IntervalDtype.is_dtype(arr_or_dtype)","""""""Check whether an array-like or dtype is of the Interval dtype.

Parameters
----------
arr_or_dtype : array-like or dtype
    The array-like or dtype to check.

Returns
-------
boolean
    Whether or not the array-like or dtype is of the Interval dtype.

Examples
--------
>>> from pandas.core.dtypes.common import is_interval_dtype
>>> is_interval_dtype(object)
False
>>> is_interval_dtype(pd.IntervalDtype())
True
>>> is_interval_dtype([1, 2, 3])
False
>>>
>>> interval = pd.Interval(1, 2, closed=""right"")
>>> is_interval_dtype(interval)
False
>>> is_interval_dtype(pd.IntervalIndex([interval]))
True"""""""
pandas/core/dtypes/common.py,"def is_categorical_dtype(arr_or_dtype) -> bool:
    warnings.warn('is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, pd.CategoricalDtype) instead', FutureWarning, stacklevel=find_stack_level())
    if isinstance(arr_or_dtype, ExtensionDtype):
        return arr_or_dtype.name == 'category'
    if arr_or_dtype is None:
        return False
    return CategoricalDtype.is_dtype(arr_or_dtype)","""""""Check whether an array-like or dtype is of the Categorical dtype.

Parameters
----------
arr_or_dtype : array-like or dtype
    The array-like or dtype to check.

Returns
-------
boolean
    Whether or not the array-like or dtype is of the Categorical dtype.

Examples
--------
>>> from pandas.api.types import is_categorical_dtype
>>> from pandas import CategoricalDtype
>>> is_categorical_dtype(object)
False
>>> is_categorical_dtype(CategoricalDtype())
True
>>> is_categorical_dtype([1, 2, 3])
False
>>> is_categorical_dtype(pd.Categorical([1, 2, 3]))
True
>>> is_categorical_dtype(pd.CategoricalIndex([1, 2, 3]))
True"""""""
pandas/core/dtypes/common.py,"def is_string_or_object_np_dtype(dtype: np.dtype) -> bool:
    return dtype == object or dtype.kind in 'SU'","""""""Faster alternative to is_string_dtype, assumes we have a np.dtype object."""""""
pandas/core/dtypes/common.py,"def is_string_dtype(arr_or_dtype) -> bool:
    if hasattr(arr_or_dtype, 'dtype') and _get_dtype(arr_or_dtype).kind == 'O':
        return is_all_strings(arr_or_dtype)

    def condition(dtype) -> bool:
        if is_string_or_object_np_dtype(dtype):
            return True
        try:
            return dtype == 'string'
        except TypeError:
            return False
    return _is_dtype(arr_or_dtype, condition)","""""""Check whether the provided array or dtype is of the string dtype.

If an array is passed with an object dtype, the elements must be
inferred as strings.

Parameters
----------
arr_or_dtype : array-like or dtype
    The array or dtype to check.

Returns
-------
boolean
    Whether or not the array or dtype is of the string dtype.

Examples
--------
>>> from pandas.api.types import is_string_dtype
>>> is_string_dtype(str)
True
>>> is_string_dtype(object)
True
>>> is_string_dtype(int)
False
>>> is_string_dtype(np.array(['a', 'b']))
True
>>> is_string_dtype(pd.Series([1, 2]))
False
>>> is_string_dtype(pd.Series([1, 2], dtype=object))
False"""""""
pandas/core/dtypes/common.py,"def is_dtype_equal(source, target) -> bool:
    if isinstance(target, str):
        if not isinstance(source, str):
            try:
                src = _get_dtype(source)
                if isinstance(src, ExtensionDtype):
                    return src == target
            except (TypeError, AttributeError, ImportError):
                return False
    elif isinstance(source, str):
        return is_dtype_equal(target, source)
    try:
        source = _get_dtype(source)
        target = _get_dtype(target)
        return source == target
    except (TypeError, AttributeError, ImportError):
        return False","""""""Check if two dtypes are equal.

Parameters
----------
source : The first dtype to compare
target : The second dtype to compare

Returns
-------
boolean
    Whether or not the two dtypes are equal.

Examples
--------
>>> is_dtype_equal(int, float)
False
>>> is_dtype_equal(""int"", int)
True
>>> is_dtype_equal(object, ""category"")
False
>>> is_dtype_equal(CategoricalDtype(), ""category"")
True
>>> is_dtype_equal(DatetimeTZDtype(tz=""UTC""), ""datetime64"")
False"""""""
pandas/core/dtypes/common.py,"def is_integer_dtype(arr_or_dtype) -> bool:
    return _is_dtype_type(arr_or_dtype, _classes_and_not_datetimelike(np.integer)) or _is_dtype(arr_or_dtype, lambda typ: isinstance(typ, ExtensionDtype) and typ.kind in 'iu')","""""""Check whether the provided array or dtype is of an integer dtype.

Unlike in `is_any_int_dtype`, timedelta64 instances will return False.

The nullable Integer dtypes (e.g. pandas.Int64Dtype) are also considered
as integer by this function.

Parameters
----------
arr_or_dtype : array-like or dtype
    The array or dtype to check.

Returns
-------
boolean
    Whether or not the array or dtype is of an integer dtype and
    not an instance of timedelta64.

Examples
--------
>>> from pandas.api.types import is_integer_dtype
>>> is_integer_dtype(str)
False
>>> is_integer_dtype(int)
True
>>> is_integer_dtype(float)
False
>>> is_integer_dtype(np.uint64)
True
>>> is_integer_dtype('int8')
True
>>> is_integer_dtype('Int8')
True
>>> is_integer_dtype(pd.Int8Dtype)
True
>>> is_integer_dtype(np.datetime64)
False
>>> is_integer_dtype(np.timedelta64)
False
>>> is_integer_dtype(np.array(['a', 'b']))
False
>>> is_integer_dtype(pd.Series([1, 2]))
True
>>> is_integer_dtype(np.array([], dtype=np.timedelta64))
False
>>> is_integer_dtype(pd.Index([1, 2.]))  # float
False"""""""
pandas/core/dtypes/common.py,"def is_signed_integer_dtype(arr_or_dtype) -> bool:
    return _is_dtype_type(arr_or_dtype, _classes_and_not_datetimelike(np.signedinteger)) or _is_dtype(arr_or_dtype, lambda typ: isinstance(typ, ExtensionDtype) and typ.kind == 'i')","""""""Check whether the provided array or dtype is of a signed integer dtype.

Unlike in `is_any_int_dtype`, timedelta64 instances will return False.

The nullable Integer dtypes (e.g. pandas.Int64Dtype) are also considered
as integer by this function.

Parameters
----------
arr_or_dtype : array-like or dtype
    The array or dtype to check.

Returns
-------
boolean
    Whether or not the array or dtype is of a signed integer dtype
    and not an instance of timedelta64.

Examples
--------
>>> from pandas.core.dtypes.common import is_signed_integer_dtype
>>> is_signed_integer_dtype(str)
False
>>> is_signed_integer_dtype(int)
True
>>> is_signed_integer_dtype(float)
False
>>> is_signed_integer_dtype(np.uint64)  # unsigned
False
>>> is_signed_integer_dtype('int8')
True
>>> is_signed_integer_dtype('Int8')
True
>>> is_signed_integer_dtype(pd.Int8Dtype)
True
>>> is_signed_integer_dtype(np.datetime64)
False
>>> is_signed_integer_dtype(np.timedelta64)
False
>>> is_signed_integer_dtype(np.array(['a', 'b']))
False
>>> is_signed_integer_dtype(pd.Series([1, 2]))
True
>>> is_signed_integer_dtype(np.array([], dtype=np.timedelta64))
False
>>> is_signed_integer_dtype(pd.Index([1, 2.]))  # float
False
>>> is_signed_integer_dtype(np.array([1, 2], dtype=np.uint32))  # unsigned
False"""""""
pandas/core/dtypes/common.py,"def is_unsigned_integer_dtype(arr_or_dtype) -> bool:
    return _is_dtype_type(arr_or_dtype, _classes_and_not_datetimelike(np.unsignedinteger)) or _is_dtype(arr_or_dtype, lambda typ: isinstance(typ, ExtensionDtype) and typ.kind == 'u')","""""""Check whether the provided array or dtype is of an unsigned integer dtype.

The nullable Integer dtypes (e.g. pandas.UInt64Dtype) are also
considered as integer by this function.

Parameters
----------
arr_or_dtype : array-like or dtype
    The array or dtype to check.

Returns
-------
boolean
    Whether or not the array or dtype is of an unsigned integer dtype.

Examples
--------
>>> from pandas.api.types import is_unsigned_integer_dtype
>>> is_unsigned_integer_dtype(str)
False
>>> is_unsigned_integer_dtype(int)  # signed
False
>>> is_unsigned_integer_dtype(float)
False
>>> is_unsigned_integer_dtype(np.uint64)
True
>>> is_unsigned_integer_dtype('uint8')
True
>>> is_unsigned_integer_dtype('UInt8')
True
>>> is_unsigned_integer_dtype(pd.UInt8Dtype)
True
>>> is_unsigned_integer_dtype(np.array(['a', 'b']))
False
>>> is_unsigned_integer_dtype(pd.Series([1, 2]))  # signed
False
>>> is_unsigned_integer_dtype(pd.Index([1, 2.]))  # float
False
>>> is_unsigned_integer_dtype(np.array([1, 2], dtype=np.uint32))
True"""""""
pandas/core/dtypes/common.py,"def is_int64_dtype(arr_or_dtype) -> bool:
    warnings.warn('is_int64_dtype is deprecated and will be removed in a future version. Use dtype == np.int64 instead.', FutureWarning, stacklevel=find_stack_level())
    return _is_dtype_type(arr_or_dtype, classes(np.int64))","""""""Check whether the provided array or dtype is of the int64 dtype.

.. deprecated:: 2.1.0

   is_int64_dtype is deprecated and will be removed in a future
   version. Use dtype == np.int64 instead.

Parameters
----------
arr_or_dtype : array-like or dtype
    The array or dtype to check.

Returns
-------
boolean
    Whether or not the array or dtype is of the int64 dtype.

Notes
-----
Depending on system architecture, the return value of `is_int64_dtype(
int)` will be True if the OS uses 64-bit integers and False if the OS
uses 32-bit integers.

Examples
--------
>>> from pandas.api.types import is_int64_dtype
>>> is_int64_dtype(str)  # doctest: +SKIP
False
>>> is_int64_dtype(np.int32)  # doctest: +SKIP
False
>>> is_int64_dtype(np.int64)  # doctest: +SKIP
True
>>> is_int64_dtype('int8')  # doctest: +SKIP
False
>>> is_int64_dtype('Int8')  # doctest: +SKIP
False
>>> is_int64_dtype(pd.Int64Dtype)  # doctest: +SKIP
True
>>> is_int64_dtype(float)  # doctest: +SKIP
False
>>> is_int64_dtype(np.uint64)  # unsigned  # doctest: +SKIP
False
>>> is_int64_dtype(np.array(['a', 'b']))  # doctest: +SKIP
False
>>> is_int64_dtype(np.array([1, 2], dtype=np.int64))  # doctest: +SKIP
True
>>> is_int64_dtype(pd.Index([1, 2.]))  # float  # doctest: +SKIP
False
>>> is_int64_dtype(np.array([1, 2], dtype=np.uint32))  # unsigned  # doctest: +SKIP
False"""""""
pandas/core/dtypes/common.py,"def is_datetime64_any_dtype(arr_or_dtype) -> bool:
    if isinstance(arr_or_dtype, (np.dtype, ExtensionDtype)):
        return arr_or_dtype.kind == 'M'
    if arr_or_dtype is None:
        return False
    try:
        tipo = _get_dtype(arr_or_dtype)
    except TypeError:
        return False
    return lib.is_np_dtype(tipo, 'M') or isinstance(tipo, DatetimeTZDtype)","""""""Check whether the provided array or dtype is of the datetime64 dtype.

Parameters
----------
arr_or_dtype : array-like or dtype
    The array or dtype to check.

Returns
-------
bool
    Whether or not the array or dtype is of the datetime64 dtype.

Examples
--------
>>> from pandas.api.types import is_datetime64_any_dtype
>>> from pandas.core.dtypes.dtypes import DatetimeTZDtype
>>> is_datetime64_any_dtype(str)
False
>>> is_datetime64_any_dtype(int)
False
>>> is_datetime64_any_dtype(np.datetime64)  # can be tz-naive
True
>>> is_datetime64_any_dtype(DatetimeTZDtype(""ns"", ""US/Eastern""))
True
>>> is_datetime64_any_dtype(np.array(['a', 'b']))
False
>>> is_datetime64_any_dtype(np.array([1, 2]))
False
>>> is_datetime64_any_dtype(np.array([], dtype=""datetime64[ns]""))
True
>>> is_datetime64_any_dtype(pd.DatetimeIndex([1, 2, 3], dtype=""datetime64[ns]""))
True"""""""
pandas/core/dtypes/common.py,"def is_datetime64_ns_dtype(arr_or_dtype) -> bool:
    if arr_or_dtype is None:
        return False
    try:
        tipo = _get_dtype(arr_or_dtype)
    except TypeError:
        return False
    return tipo == DT64NS_DTYPE or (isinstance(tipo, DatetimeTZDtype) and tipo.unit == 'ns')","""""""Check whether the provided array or dtype is of the datetime64[ns] dtype.

Parameters
----------
arr_or_dtype : array-like or dtype
    The array or dtype to check.

Returns
-------
bool
    Whether or not the array or dtype is of the datetime64[ns] dtype.

Examples
--------
>>> from pandas.api.types import is_datetime64_ns_dtype
>>> from pandas.core.dtypes.dtypes import DatetimeTZDtype
>>> is_datetime64_ns_dtype(str)
False
>>> is_datetime64_ns_dtype(int)
False
>>> is_datetime64_ns_dtype(np.datetime64)  # no unit
False
>>> is_datetime64_ns_dtype(DatetimeTZDtype(""ns"", ""US/Eastern""))
True
>>> is_datetime64_ns_dtype(np.array(['a', 'b']))
False
>>> is_datetime64_ns_dtype(np.array([1, 2]))
False
>>> is_datetime64_ns_dtype(np.array([], dtype=""datetime64""))  # no unit
False
>>> is_datetime64_ns_dtype(np.array([], dtype=""datetime64[ps]""))  # wrong unit
False
>>> is_datetime64_ns_dtype(pd.DatetimeIndex([1, 2, 3], dtype=""datetime64[ns]""))
True"""""""
pandas/core/dtypes/common.py,"def is_timedelta64_ns_dtype(arr_or_dtype) -> bool:
    return _is_dtype(arr_or_dtype, lambda dtype: dtype == TD64NS_DTYPE)","""""""Check whether the provided array or dtype is of the timedelta64[ns] dtype.

This is a very specific dtype, so generic ones like `np.timedelta64`
will return False if passed into this function.

Parameters
----------
arr_or_dtype : array-like or dtype
    The array or dtype to check.

Returns
-------
boolean
    Whether or not the array or dtype is of the timedelta64[ns] dtype.

Examples
--------
>>> from pandas.core.dtypes.common import is_timedelta64_ns_dtype
>>> is_timedelta64_ns_dtype(np.dtype('m8[ns]'))
True
>>> is_timedelta64_ns_dtype(np.dtype('m8[ps]'))  # Wrong frequency
False
>>> is_timedelta64_ns_dtype(np.array([1, 2], dtype='m8[ns]'))
True
>>> is_timedelta64_ns_dtype(np.array([1, 2], dtype=np.timedelta64))
False"""""""
pandas/core/dtypes/common.py,"def is_numeric_v_string_like(a: ArrayLike, b) -> bool:
    is_a_array = isinstance(a, np.ndarray)
    is_b_array = isinstance(b, np.ndarray)
    is_a_numeric_array = is_a_array and a.dtype.kind in ('u', 'i', 'f', 'c', 'b')
    is_b_numeric_array = is_b_array and b.dtype.kind in ('u', 'i', 'f', 'c', 'b')
    is_a_string_array = is_a_array and a.dtype.kind in ('S', 'U')
    is_b_string_array = is_b_array and b.dtype.kind in ('S', 'U')
    is_b_scalar_string_like = not is_b_array and isinstance(b, str)
    return is_a_numeric_array and is_b_scalar_string_like or (is_a_numeric_array and is_b_string_array) or (is_b_numeric_array and is_a_string_array)","""""""Check if we are comparing a string-like object to a numeric ndarray.
NumPy doesn't like to compare such objects, especially numeric arrays
and scalar string-likes.

Parameters
----------
a : array-like, scalar
    The first object to check.
b : array-like, scalar
    The second object to check.

Returns
-------
boolean
    Whether we return a comparing a string-like object to a numeric array.

Examples
--------
>>> is_numeric_v_string_like(np.array([1]), ""foo"")
True
>>> is_numeric_v_string_like(np.array([1, 2]), np.array([""foo""]))
True
>>> is_numeric_v_string_like(np.array([""foo""]), np.array([1, 2]))
True
>>> is_numeric_v_string_like(np.array([1]), np.array([2]))
False
>>> is_numeric_v_string_like(np.array([""foo""]), np.array([""foo""]))
False"""""""
pandas/core/dtypes/common.py,"def needs_i8_conversion(dtype: DtypeObj | None) -> bool:
    if isinstance(dtype, np.dtype):
        return dtype.kind in 'mM'
    return isinstance(dtype, (PeriodDtype, DatetimeTZDtype))","""""""Check whether the dtype should be converted to int64.

Dtype ""needs"" such a conversion if the dtype is of a datetime-like dtype

Parameters
----------
dtype : np.dtype, ExtensionDtype, or None

Returns
-------
boolean
    Whether or not the dtype should be converted to int64.

Examples
--------
>>> needs_i8_conversion(str)
False
>>> needs_i8_conversion(np.int64)
False
>>> needs_i8_conversion(np.datetime64)
False
>>> needs_i8_conversion(np.dtype(np.datetime64))
True
>>> needs_i8_conversion(np.array(['a', 'b']))
False
>>> needs_i8_conversion(pd.Series([1, 2]))
False
>>> needs_i8_conversion(pd.Series([], dtype=""timedelta64[ns]""))
False
>>> needs_i8_conversion(pd.DatetimeIndex([1, 2, 3], tz=""US/Eastern""))
False
>>> needs_i8_conversion(pd.DatetimeIndex([1, 2, 3], tz=""US/Eastern"").dtype)
True"""""""
pandas/core/dtypes/common.py,"def is_numeric_dtype(arr_or_dtype) -> bool:
    return _is_dtype_type(arr_or_dtype, _classes_and_not_datetimelike(np.number, np.bool_)) or _is_dtype(arr_or_dtype, lambda typ: isinstance(typ, ExtensionDtype) and typ._is_numeric)","""""""Check whether the provided array or dtype is of a numeric dtype.

Parameters
----------
arr_or_dtype : array-like or dtype
    The array or dtype to check.

Returns
-------
boolean
    Whether or not the array or dtype is of a numeric dtype.

Examples
--------
>>> from pandas.api.types import is_numeric_dtype
>>> is_numeric_dtype(str)
False
>>> is_numeric_dtype(int)
True
>>> is_numeric_dtype(float)
True
>>> is_numeric_dtype(np.uint64)
True
>>> is_numeric_dtype(np.datetime64)
False
>>> is_numeric_dtype(np.timedelta64)
False
>>> is_numeric_dtype(np.array(['a', 'b']))
False
>>> is_numeric_dtype(pd.Series([1, 2]))
True
>>> is_numeric_dtype(pd.Index([1, 2.]))
True
>>> is_numeric_dtype(np.array([], dtype=np.timedelta64))
False"""""""
pandas/core/dtypes/common.py,"def is_any_real_numeric_dtype(arr_or_dtype) -> bool:
    return is_numeric_dtype(arr_or_dtype) and (not is_complex_dtype(arr_or_dtype)) and (not is_bool_dtype(arr_or_dtype))","""""""Check whether the provided array or dtype is of a real number dtype.

Parameters
----------
arr_or_dtype : array-like or dtype
    The array or dtype to check.

Returns
-------
boolean
    Whether or not the array or dtype is of a real number dtype.

Examples
--------
>>> from pandas.api.types import is_any_real_numeric_dtype
>>> is_any_real_numeric_dtype(int)
True
>>> is_any_real_numeric_dtype(float)
True
>>> is_any_real_numeric_dtype(object)
False
>>> is_any_real_numeric_dtype(str)
False
>>> is_any_real_numeric_dtype(complex(1, 2))
False
>>> is_any_real_numeric_dtype(bool)
False"""""""
pandas/core/dtypes/common.py,"def is_float_dtype(arr_or_dtype) -> bool:
    return _is_dtype_type(arr_or_dtype, classes(np.floating)) or _is_dtype(arr_or_dtype, lambda typ: isinstance(typ, ExtensionDtype) and typ.kind in 'f')","""""""Check whether the provided array or dtype is of a float dtype.

Parameters
----------
arr_or_dtype : array-like or dtype
    The array or dtype to check.

Returns
-------
boolean
    Whether or not the array or dtype is of a float dtype.

Examples
--------
>>> from pandas.api.types import is_float_dtype
>>> is_float_dtype(str)
False
>>> is_float_dtype(int)
False
>>> is_float_dtype(float)
True
>>> is_float_dtype(np.array(['a', 'b']))
False
>>> is_float_dtype(pd.Series([1, 2]))
False
>>> is_float_dtype(pd.Index([1, 2.]))
True"""""""
pandas/core/dtypes/common.py,"def is_bool_dtype(arr_or_dtype) -> bool:
    if arr_or_dtype is None:
        return False
    try:
        dtype = _get_dtype(arr_or_dtype)
    except (TypeError, ValueError):
        return False
    if isinstance(dtype, CategoricalDtype):
        arr_or_dtype = dtype.categories
    if isinstance(arr_or_dtype, ABCIndex):
        if arr_or_dtype.inferred_type == 'boolean':
            if not is_bool_dtype(arr_or_dtype.dtype):
                warnings.warn('The behavior of is_bool_dtype with an object-dtype Index of bool objects is deprecated. In a future version, this will return False. Cast the Index to a bool dtype instead.', FutureWarning, stacklevel=find_stack_level())
            return True
        return False
    elif isinstance(dtype, ExtensionDtype):
        return getattr(dtype, '_is_boolean', False)
    return issubclass(dtype.type, np.bool_)","""""""Check whether the provided array or dtype is of a boolean dtype.

Parameters
----------
arr_or_dtype : array-like or dtype
    The array or dtype to check.

Returns
-------
boolean
    Whether or not the array or dtype is of a boolean dtype.

Notes
-----
An ExtensionArray is considered boolean when the ``_is_boolean``
attribute is set to True.

Examples
--------
>>> from pandas.api.types import is_bool_dtype
>>> is_bool_dtype(str)
False
>>> is_bool_dtype(int)
False
>>> is_bool_dtype(bool)
True
>>> is_bool_dtype(np.bool_)
True
>>> is_bool_dtype(np.array(['a', 'b']))
False
>>> is_bool_dtype(pd.Series([1, 2]))
False
>>> is_bool_dtype(np.array([True, False]))
True
>>> is_bool_dtype(pd.Categorical([True, False]))
True
>>> is_bool_dtype(pd.arrays.SparseArray([True, False]))
True"""""""
pandas/core/dtypes/common.py,"def is_1d_only_ea_dtype(dtype: DtypeObj | None) -> bool:
    return isinstance(dtype, ExtensionDtype) and (not dtype._supports_2d)","""""""Analogue to is_extension_array_dtype but excluding DatetimeTZDtype."""""""
pandas/core/dtypes/common.py,"def is_extension_array_dtype(arr_or_dtype) -> bool:
    dtype = getattr(arr_or_dtype, 'dtype', arr_or_dtype)
    if isinstance(dtype, ExtensionDtype):
        return True
    elif isinstance(dtype, np.dtype):
        return False
    else:
        return registry.find(dtype) is not None","""""""Check if an object is a pandas extension array type.

See the :ref:`Use Guide <extending.extension-types>` for more.

Parameters
----------
arr_or_dtype : object
    For array-like input, the ``.dtype`` attribute will
    be extracted.

Returns
-------
bool
    Whether the `arr_or_dtype` is an extension array type.

Notes
-----
This checks whether an object implements the pandas extension
array interface. In pandas, this includes:

* Categorical
* Sparse
* Interval
* Period
* DatetimeArray
* TimedeltaArray

Third-party libraries may implement arrays or types satisfying
this interface as well.

Examples
--------
>>> from pandas.api.types import is_extension_array_dtype
>>> arr = pd.Categorical(['a', 'b'])
>>> is_extension_array_dtype(arr)
True
>>> is_extension_array_dtype(arr.dtype)
True

>>> arr = np.array(['a', 'b'])
>>> is_extension_array_dtype(arr.dtype)
False"""""""
pandas/core/dtypes/common.py,"def is_ea_or_datetimelike_dtype(dtype: DtypeObj | None) -> bool:
    return isinstance(dtype, ExtensionDtype) or lib.is_np_dtype(dtype, 'mM')","""""""Check for ExtensionDtype, datetime64 dtype, or timedelta64 dtype.

Notes
-----
Checks only for dtype objects, not dtype-castable strings or types."""""""
pandas/core/dtypes/common.py,"def is_complex_dtype(arr_or_dtype) -> bool:
    return _is_dtype_type(arr_or_dtype, classes(np.complexfloating))","""""""Check whether the provided array or dtype is of a complex dtype.

Parameters
----------
arr_or_dtype : array-like or dtype
    The array or dtype to check.

Returns
-------
boolean
    Whether or not the array or dtype is of a complex dtype.

Examples
--------
>>> from pandas.api.types import is_complex_dtype
>>> is_complex_dtype(str)
False
>>> is_complex_dtype(int)
False
>>> is_complex_dtype(np.complex128)
True
>>> is_complex_dtype(np.array(['a', 'b']))
False
>>> is_complex_dtype(pd.Series([1, 2]))
False
>>> is_complex_dtype(np.array([1 + 1j, 5]))
True"""""""
pandas/core/dtypes/common.py,"def _is_dtype(arr_or_dtype, condition) -> bool:
    if arr_or_dtype is None:
        return False
    try:
        dtype = _get_dtype(arr_or_dtype)
    except (TypeError, ValueError):
        return False
    return condition(dtype)","""""""Return true if the condition is satisfied for the arr_or_dtype.

Parameters
----------
arr_or_dtype : array-like, str, np.dtype, or ExtensionArrayType
    The array-like or dtype object whose dtype we want to extract.
condition : callable[Union[np.dtype, ExtensionDtype]]

Returns
-------
bool"""""""
pandas/core/dtypes/common.py,"def _get_dtype(arr_or_dtype) -> DtypeObj:
    if arr_or_dtype is None:
        raise TypeError('Cannot deduce dtype from null object')
    if isinstance(arr_or_dtype, np.dtype):
        return arr_or_dtype
    elif isinstance(arr_or_dtype, type):
        return np.dtype(arr_or_dtype)
    elif hasattr(arr_or_dtype, 'dtype'):
        arr_or_dtype = arr_or_dtype.dtype
    return pandas_dtype(arr_or_dtype)","""""""Get the dtype instance associated with an array
or dtype object.

Parameters
----------
arr_or_dtype : array-like or dtype
    The array-like or dtype object whose dtype we want to extract.

Returns
-------
obj_dtype : The extract dtype instance from the
            passed in array or dtype object.

Raises
------
TypeError : The passed in object is None."""""""
pandas/core/dtypes/common.py,"def _is_dtype_type(arr_or_dtype, condition) -> bool:
    if arr_or_dtype is None:
        return condition(type(None))
    if isinstance(arr_or_dtype, np.dtype):
        return condition(arr_or_dtype.type)
    elif isinstance(arr_or_dtype, type):
        if issubclass(arr_or_dtype, ExtensionDtype):
            arr_or_dtype = arr_or_dtype.type
        return condition(np.dtype(arr_or_dtype).type)
    if hasattr(arr_or_dtype, 'dtype'):
        arr_or_dtype = arr_or_dtype.dtype
    elif is_list_like(arr_or_dtype):
        return condition(type(None))
    try:
        tipo = pandas_dtype(arr_or_dtype).type
    except (TypeError, ValueError):
        if is_scalar(arr_or_dtype):
            return condition(type(None))
        return False
    return condition(tipo)","""""""Return true if the condition is satisfied for the arr_or_dtype.

Parameters
----------
arr_or_dtype : array-like or dtype
    The array-like or dtype object whose dtype we want to extract.
condition : callable[Union[np.dtype, ExtensionDtypeType]]

Returns
-------
bool : if the condition is satisfied for the arr_or_dtype"""""""
pandas/core/dtypes/common.py,"def infer_dtype_from_object(dtype) -> type:
    if isinstance(dtype, type) and issubclass(dtype, np.generic):
        return dtype
    elif isinstance(dtype, (np.dtype, ExtensionDtype)):
        try:
            _validate_date_like_dtype(dtype)
        except TypeError:
            pass
        if hasattr(dtype, 'numpy_dtype'):
            return dtype.numpy_dtype.type
        return dtype.type
    try:
        dtype = pandas_dtype(dtype)
    except TypeError:
        pass
    if isinstance(dtype, ExtensionDtype):
        return dtype.type
    elif isinstance(dtype, str):
        if dtype in ['datetimetz', 'datetime64tz']:
            return DatetimeTZDtype.type
        elif dtype in ['period']:
            raise NotImplementedError
        if dtype in ['datetime', 'timedelta']:
            dtype += '64'
        try:
            return infer_dtype_from_object(getattr(np, dtype))
        except (AttributeError, TypeError):
            pass
    return infer_dtype_from_object(np.dtype(dtype))","""""""Get a numpy dtype.type-style object for a dtype object.

This methods also includes handling of the datetime64[ns] and
datetime64[ns, TZ] objects.

If no dtype can be found, we return ``object``.

Parameters
----------
dtype : dtype, type
    The dtype object whose numpy dtype.type-style
    object we want to extract.

Returns
-------
type"""""""
pandas/core/dtypes/common.py,"def _validate_date_like_dtype(dtype) -> None:
    try:
        typ = np.datetime_data(dtype)[0]
    except ValueError as e:
        raise TypeError(e) from e
    if typ not in ['generic', 'ns']:
        raise ValueError(f'{repr(dtype.name)} is too specific of a frequency, try passing {repr(dtype.type.__name__)}')","""""""Check whether the dtype is a date-like dtype. Raises an error if invalid.

Parameters
----------
dtype : dtype, type
    The dtype to check.

Raises
------
TypeError : The dtype could not be casted to a date-like dtype.
ValueError : The dtype is an illegal date-like dtype (e.g. the
             frequency provided is too specific)"""""""
pandas/core/dtypes/common.py,"def validate_all_hashable(*args, error_name: str | None=None) -> None:
    if not all((is_hashable(arg) for arg in args)):
        if error_name:
            raise TypeError(f'{error_name} must be a hashable type')
        raise TypeError('All elements must be hashable')","""""""Return None if all args are hashable, else raise a TypeError.

Parameters
----------
*args
    Arguments to validate.
error_name : str, optional
    The name to use if error

Raises
------
TypeError : If an argument is not hashable

Returns
-------
None"""""""
pandas/core/dtypes/common.py,"def pandas_dtype(dtype) -> DtypeObj:
    if isinstance(dtype, np.ndarray):
        return dtype.dtype
    elif isinstance(dtype, (np.dtype, ExtensionDtype)):
        return dtype
    result = registry.find(dtype)
    if result is not None:
        if isinstance(result, type):
            warnings.warn(f'Instantiating {result.__name__} without any arguments.Pass a {result.__name__} instance to silence this warning.', UserWarning, stacklevel=find_stack_level())
            result = result()
        return result
    try:
        with warnings.catch_warnings():
            warnings.simplefilter('always', DeprecationWarning)
            npdtype = np.dtype(dtype)
    except SyntaxError as err:
        raise TypeError(f""data type '{dtype}' not understood"") from err
    if is_hashable(dtype) and dtype in [object, np.object_, 'object', 'O', 'object_']:
        return npdtype
    elif npdtype.kind == 'O':
        raise TypeError(f""dtype '{dtype}' not understood"")
    return npdtype","""""""Convert input into a pandas only dtype object or a numpy dtype object.

Parameters
----------
dtype : object to be converted

Returns
-------
np.dtype or a pandas dtype

Raises
------
TypeError if not a dtype

Examples
--------
>>> pd.api.types.pandas_dtype(int)
dtype('int64')"""""""
pandas/core/dtypes/common.py,"def is_all_strings(value: ArrayLike) -> bool:
    dtype = value.dtype
    if isinstance(dtype, np.dtype):
        if len(value) == 0:
            return dtype == np.dtype('object')
        else:
            return dtype == np.dtype('object') and lib.is_string_array(np.asarray(value), skipna=False)
    elif isinstance(dtype, CategoricalDtype):
        return dtype.categories.inferred_type == 'string'
    return dtype == 'string'","""""""Check if this is an array of strings that we should try parsing.

Includes object-dtype ndarray containing all-strings, StringArray,
and Categorical with all-string categories.
Does not include numpy string dtypes."""""""
pandas/core/dtypes/concat.py,"def concat_compat(to_concat: Sequence[ArrayLike], axis: AxisInt=0, ea_compat_axis: bool=False) -> ArrayLike:
    if len(to_concat) and lib.dtypes_all_equal([obj.dtype for obj in to_concat]):
        obj = to_concat[0]
        if isinstance(obj, np.ndarray):
            to_concat_arrs = cast('Sequence[np.ndarray]', to_concat)
            return np.concatenate(to_concat_arrs, axis=axis)
        to_concat_eas = cast('Sequence[ExtensionArray]', to_concat)
        if ea_compat_axis:
            return obj._concat_same_type(to_concat_eas)
        elif axis == 0:
            return obj._concat_same_type(to_concat_eas)
        else:
            return obj._concat_same_type(to_concat_eas, axis=axis)
    orig = to_concat
    non_empties = [x for x in to_concat if _is_nonempty(x, axis)]
    if non_empties and axis == 0 and (not ea_compat_axis):
        to_concat = non_empties
    (any_ea, kinds, target_dtype) = _get_result_dtype(to_concat, non_empties)
    if len(to_concat) < len(orig):
        (_, _, alt_dtype) = _get_result_dtype(orig, non_empties)
        if alt_dtype != target_dtype:
            warnings.warn('The behavior of array concatenation with empty entries is deprecated. In a future version, this will no longer exclude empty items when determining the result dtype. To retain the old behavior, exclude the empty entries before the concat operation.', FutureWarning, stacklevel=find_stack_level())
    if target_dtype is not None:
        to_concat = [astype_array(arr, target_dtype, copy=False) for arr in to_concat]
    if not isinstance(to_concat[0], np.ndarray):
        to_concat_eas = cast('Sequence[ExtensionArray]', to_concat)
        cls = type(to_concat[0])
        return cls._concat_same_type(to_concat_eas)
    else:
        to_concat_arrs = cast('Sequence[np.ndarray]', to_concat)
        result = np.concatenate(to_concat_arrs, axis=axis)
        if not any_ea and 'b' in kinds and (result.dtype.kind in 'iuf'):
            result = result.astype(object, copy=False)
    return result","""""""provide concatenation of an array of arrays each of which is a single
'normalized' dtypes (in that for example, if it's object, then it is a
non-datetimelike and provide a combined dtype for the resulting array that
preserves the overall dtype if possible)

Parameters
----------
to_concat : sequence of arrays
axis : axis to provide concatenation
ea_compat_axis : bool, default False
    For ExtensionArray compat, behave as if axis == 1 when determining
    whether to drop empty arrays.

Returns
-------
a single array, preserving the combined dtypes"""""""
pandas/core/dtypes/concat.py,"def union_categoricals(to_union, sort_categories: bool=False, ignore_order: bool=False) -> Categorical:
    from pandas import Categorical
    from pandas.core.arrays.categorical import recode_for_categories
    if len(to_union) == 0:
        raise ValueError('No Categoricals to union')

    def _maybe_unwrap(x):
        if isinstance(x, (ABCCategoricalIndex, ABCSeries)):
            return x._values
        elif isinstance(x, Categorical):
            return x
        else:
            raise TypeError('all components to combine must be Categorical')
    to_union = [_maybe_unwrap(x) for x in to_union]
    first = to_union[0]
    if not lib.dtypes_all_equal([obj.categories.dtype for obj in to_union]):
        raise TypeError('dtype of categories must be the same')
    ordered = False
    if all((first._categories_match_up_to_permutation(other) for other in to_union[1:])):
        categories = first.categories
        ordered = first.ordered
        all_codes = [first._encode_with_my_categories(x)._codes for x in to_union]
        new_codes = np.concatenate(all_codes)
        if sort_categories and (not ignore_order) and ordered:
            raise TypeError('Cannot use sort_categories=True with ordered Categoricals')
        if sort_categories and (not categories.is_monotonic_increasing):
            categories = categories.sort_values()
            indexer = categories.get_indexer(first.categories)
            from pandas.core.algorithms import take_nd
            new_codes = take_nd(indexer, new_codes, fill_value=-1)
    elif ignore_order or all((not c.ordered for c in to_union)):
        cats = first.categories.append([c.categories for c in to_union[1:]])
        categories = cats.unique()
        if sort_categories:
            categories = categories.sort_values()
        new_codes = [recode_for_categories(c.codes, c.categories, categories) for c in to_union]
        new_codes = np.concatenate(new_codes)
    else:
        if all((c.ordered for c in to_union)):
            msg = 'to union ordered Categoricals, all categories must be the same'
            raise TypeError(msg)
        raise TypeError('Categorical.ordered must be the same')
    if ignore_order:
        ordered = False
    dtype = CategoricalDtype(categories=categories, ordered=ordered)
    return Categorical._simple_new(new_codes, dtype=dtype)","""""""Combine list-like of Categorical-like, unioning categories.

All categories must have the same dtype.

Parameters
----------
to_union : list-like
    Categorical, CategoricalIndex, or Series with dtype='category'.
sort_categories : bool, default False
    If true, resulting categories will be lexsorted, otherwise
    they will be ordered as they appear in the data.
ignore_order : bool, default False
    If true, the ordered attribute of the Categoricals will be ignored.
    Results in an unordered categorical.

Returns
-------
Categorical

Raises
------
TypeError
    - all inputs do not have the same dtype
    - all inputs do not have the same ordered property
    - all inputs are ordered and their categories are not identical
    - sort_categories=True and Categoricals are ordered
ValueError
    Empty list of categoricals passed

Notes
-----
To learn more about categories, see `link
<https://pandas.pydata.org/pandas-docs/stable/user_guide/categorical.html#unioning>`__

Examples
--------
If you want to combine categoricals that do not necessarily have
the same categories, `union_categoricals` will combine a list-like
of categoricals. The new categories will be the union of the
categories being combined.

>>> a = pd.Categorical([""b"", ""c""])
>>> b = pd.Categorical([""a"", ""b""])
>>> pd.api.types.union_categoricals([a, b])
['b', 'c', 'a', 'b']
Categories (3, object): ['b', 'c', 'a']

By default, the resulting categories will be ordered as they appear
in the `categories` of the data. If you want the categories to be
lexsorted, use `sort_categories=True` argument.

>>> pd.api.types.union_categoricals([a, b], sort_categories=True)
['b', 'c', 'a', 'b']
Categories (3, object): ['a', 'b', 'c']

`union_categoricals` also works with the case of combining two
categoricals of the same categories and order information (e.g. what
you could also `append` for).

>>> a = pd.Categorical([""a"", ""b""], ordered=True)
>>> b = pd.Categorical([""a"", ""b"", ""a""], ordered=True)
>>> pd.api.types.union_categoricals([a, b])
['a', 'b', 'a', 'b', 'a']
Categories (2, object): ['a' < 'b']

Raises `TypeError` because the categories are ordered and not identical.

>>> a = pd.Categorical([""a"", ""b""], ordered=True)
>>> b = pd.Categorical([""a"", ""b"", ""c""], ordered=True)
>>> pd.api.types.union_categoricals([a, b])
Traceback (most recent call last):
    ...
TypeError: to union ordered Categoricals, all categories must be the same

Ordered categoricals with different categories or orderings can be
combined by using the `ignore_ordered=True` argument.

>>> a = pd.Categorical([""a"", ""b"", ""c""], ordered=True)
>>> b = pd.Categorical([""c"", ""b"", ""a""], ordered=True)
>>> pd.api.types.union_categoricals([a, b], ignore_order=True)
['a', 'b', 'c', 'c', 'b', 'a']
Categories (3, object): ['a', 'b', 'c']

`union_categoricals` also works with a `CategoricalIndex`, or `Series`
containing categorical data, but note that the resulting array will
always be a plain `Categorical`

>>> a = pd.Series([""b"", ""c""], dtype='category')
>>> b = pd.Series([""a"", ""b""], dtype='category')
>>> pd.api.types.union_categoricals([a, b])
['b', 'c', 'a', 'b']
Categories (3, object): ['b', 'c', 'a']"""""""
pandas/core/dtypes/inference.py,"def is_number(obj) -> TypeGuard[Number | np.number]:
    return isinstance(obj, (Number, np.number))","""""""Check if the object is a number.

Returns True when the object is a number, and False if is not.

Parameters
----------
obj : any type
    The object to check if is a number.

Returns
-------
bool
    Whether `obj` is a number or not.

See Also
--------
api.types.is_integer: Checks a subgroup of numbers.

Examples
--------
>>> from pandas.api.types import is_number
>>> is_number(1)
True
>>> is_number(7.15)
True

Booleans are valid because they are int subclass.

>>> is_number(False)
True

>>> is_number(""foo"")
False
>>> is_number(""5"")
False"""""""
pandas/core/dtypes/inference.py,"def iterable_not_string(obj) -> bool:
    return isinstance(obj, abc.Iterable) and (not isinstance(obj, str))","""""""Check if the object is an iterable but not a string.

Parameters
----------
obj : The object to check.

Returns
-------
is_iter_not_string : bool
    Whether `obj` is a non-string iterable.

Examples
--------
>>> iterable_not_string([1, 2, 3])
True
>>> iterable_not_string(""foo"")
False
>>> iterable_not_string(1)
False"""""""
pandas/core/dtypes/inference.py,"def is_file_like(obj) -> bool:
    if not (hasattr(obj, 'read') or hasattr(obj, 'write')):
        return False
    return bool(hasattr(obj, '__iter__'))","""""""Check if the object is a file-like object.

For objects to be considered file-like, they must
be an iterator AND have either a `read` and/or `write`
method as an attribute.

Note: file-like objects must be iterable, but
iterable objects need not be file-like.

Parameters
----------
obj : The object to check

Returns
-------
bool
    Whether `obj` has file-like properties.

Examples
--------
>>> import io
>>> from pandas.api.types import is_file_like
>>> buffer = io.StringIO(""data"")
>>> is_file_like(buffer)
True
>>> is_file_like([1, 2, 3])
False"""""""
pandas/core/dtypes/inference.py,"def is_re(obj) -> TypeGuard[Pattern]:
    return isinstance(obj, Pattern)","""""""Check if the object is a regex pattern instance.

Parameters
----------
obj : The object to check

Returns
-------
bool
    Whether `obj` is a regex pattern.

Examples
--------
>>> from pandas.api.types import is_re
>>> import re
>>> is_re(re.compile("".*""))
True
>>> is_re(""foo"")
False"""""""
pandas/core/dtypes/inference.py,"def is_re_compilable(obj) -> bool:
    try:
        re.compile(obj)
    except TypeError:
        return False
    else:
        return True","""""""Check if the object can be compiled into a regex pattern instance.

Parameters
----------
obj : The object to check

Returns
-------
bool
    Whether `obj` can be compiled as a regex pattern.

Examples
--------
>>> from pandas.api.types import is_re_compilable
>>> is_re_compilable("".*"")
True
>>> is_re_compilable(1)
False"""""""
pandas/core/dtypes/inference.py,"def is_array_like(obj) -> bool:
    return is_list_like(obj) and hasattr(obj, 'dtype')","""""""Check if the object is array-like.

For an object to be considered array-like, it must be list-like and
have a `dtype` attribute.

Parameters
----------
obj : The object to check

Returns
-------
is_array_like : bool
    Whether `obj` has array-like properties.

Examples
--------
>>> is_array_like(np.array([1, 2, 3]))
True
>>> is_array_like(pd.Series([""a"", ""b""]))
True
>>> is_array_like(pd.Index([""2016-01-01""]))
True
>>> is_array_like([1, 2, 3])
False
>>> is_array_like((""a"", ""b""))
False"""""""
pandas/core/dtypes/inference.py,"def is_nested_list_like(obj) -> bool:
    return is_list_like(obj) and hasattr(obj, '__len__') and (len(obj) > 0) and all((is_list_like(item) for item in obj))","""""""Check if the object is list-like, and that all of its elements
are also list-like.

Parameters
----------
obj : The object to check

Returns
-------
is_list_like : bool
    Whether `obj` has list-like properties.

Examples
--------
>>> is_nested_list_like([[1, 2, 3]])
True
>>> is_nested_list_like([{1, 2, 3}, {1, 2, 3}])
True
>>> is_nested_list_like([""foo""])
False
>>> is_nested_list_like([])
False
>>> is_nested_list_like([[1, 2, 3], 1])
False

Notes
-----
This won't reliably detect whether a consumable iterator (e. g.
a generator) is a nested-list-like without consuming the iterator.
To avoid consuming it, we always return False if the outer container
doesn't define `__len__`.

See Also
--------
is_list_like"""""""
pandas/core/dtypes/inference.py,"def is_dict_like(obj) -> bool:
    dict_like_attrs = ('__getitem__', 'keys', '__contains__')
    return all((hasattr(obj, attr) for attr in dict_like_attrs)) and (not isinstance(obj, type))","""""""Check if the object is dict-like.

Parameters
----------
obj : The object to check

Returns
-------
bool
    Whether `obj` has dict-like properties.

Examples
--------
>>> from pandas.api.types import is_dict_like
>>> is_dict_like({1: 2})
True
>>> is_dict_like([1, 2, 3])
False
>>> is_dict_like(dict)
False
>>> is_dict_like(dict())
True"""""""
pandas/core/dtypes/inference.py,"def is_named_tuple(obj) -> bool:
    return isinstance(obj, abc.Sequence) and hasattr(obj, '_fields')","""""""Check if the object is a named tuple.

Parameters
----------
obj : The object to check

Returns
-------
bool
    Whether `obj` is a named tuple.

Examples
--------
>>> from collections import namedtuple
>>> from pandas.api.types import is_named_tuple
>>> Point = namedtuple(""Point"", [""x"", ""y""])
>>> p = Point(1, 2)
>>>
>>> is_named_tuple(p)
True
>>> is_named_tuple((1, 2))
False"""""""
pandas/core/dtypes/inference.py,"def is_hashable(obj) -> TypeGuard[Hashable]:
    try:
        hash(obj)
    except TypeError:
        return False
    else:
        return True","""""""Return True if hash(obj) will succeed, False otherwise.

Some types will pass a test against collections.abc.Hashable but fail when
they are actually hashed with hash().

Distinguish between these and other types by trying the call to hash() and
seeing if they raise TypeError.

Returns
-------
bool

Examples
--------
>>> import collections
>>> from pandas.api.types import is_hashable
>>> a = ([],)
>>> isinstance(a, collections.abc.Hashable)
True
>>> is_hashable(a)
False"""""""
pandas/core/dtypes/inference.py,"def is_sequence(obj) -> bool:
    try:
        iter(obj)
        len(obj)
        return not isinstance(obj, (str, bytes))
    except (TypeError, AttributeError):
        return False","""""""Check if the object is a sequence of objects.
String types are not included as sequences here.

Parameters
----------
obj : The object to check

Returns
-------
is_sequence : bool
    Whether `obj` is a sequence of objects.

Examples
--------
>>> l = [1, 2, 3]
>>>
>>> is_sequence(l)
True
>>> is_sequence(iter(l))
False"""""""
pandas/core/dtypes/inference.py,"def is_dataclass(item) -> bool:
    try:
        import dataclasses
        return dataclasses.is_dataclass(item) and (not isinstance(item, type))
    except ImportError:
        return False","""""""Checks if the object is a data-class instance

Parameters
----------
item : object

Returns
--------
is_dataclass : bool
    True if the item is an instance of a data-class,
    will return false if you pass the data class itself

Examples
--------
>>> from dataclasses import dataclass
>>> @dataclass
... class Point:
...     x: int
...     y: int

>>> is_dataclass(Point)
False
>>> is_dataclass(Point(0,2))
True"""""""
pandas/core/dtypes/missing.py,"def isna(obj: object) -> bool | npt.NDArray[np.bool_] | NDFrame:
    return _isna(obj)","""""""Detect missing values for an array-like object.

This function takes a scalar or array-like object and indicates
whether values are missing (``NaN`` in numeric arrays, ``None`` or ``NaN``
in object arrays, ``NaT`` in datetimelike).

Parameters
----------
obj : scalar or array-like
    Object to check for null or missing values.

Returns
-------
bool or array-like of bool
    For scalar input, returns a scalar boolean.
    For array input, returns an array of boolean indicating whether each
    corresponding element is missing.

See Also
--------
notna : Boolean inverse of pandas.isna.
Series.isna : Detect missing values in a Series.
DataFrame.isna : Detect missing values in a DataFrame.
Index.isna : Detect missing values in an Index.

Examples
--------
Scalar arguments (including strings) result in a scalar boolean.

>>> pd.isna('dog')
False

>>> pd.isna(pd.NA)
True

>>> pd.isna(np.nan)
True

ndarrays result in an ndarray of booleans.

>>> array = np.array([[1, np.nan, 3], [4, 5, np.nan]])
>>> array
array([[ 1., nan,  3.],
       [ 4.,  5., nan]])
>>> pd.isna(array)
array([[False,  True, False],
       [False, False,  True]])

For indexes, an ndarray of booleans is returned.

>>> index = pd.DatetimeIndex([""2017-07-05"", ""2017-07-06"", None,
...                           ""2017-07-08""])
>>> index
DatetimeIndex(['2017-07-05', '2017-07-06', 'NaT', '2017-07-08'],
              dtype='datetime64[ns]', freq=None)
>>> pd.isna(index)
array([False, False,  True, False])

For Series and DataFrame, the same type is returned, containing booleans.

>>> df = pd.DataFrame([['ant', 'bee', 'cat'], ['dog', None, 'fly']])
>>> df
     0     1    2
0  ant   bee  cat
1  dog  None  fly
>>> pd.isna(df)
       0      1      2
0  False  False  False
1  False   True  False

>>> pd.isna(df[1])
0    False
1     True
Name: 1, dtype: bool"""""""
pandas/core/dtypes/missing.py,"def _isna(obj, inf_as_na: bool=False):
    if is_scalar(obj):
        return libmissing.checknull(obj, inf_as_na=inf_as_na)
    elif isinstance(obj, ABCMultiIndex):
        raise NotImplementedError('isna is not defined for MultiIndex')
    elif isinstance(obj, type):
        return False
    elif isinstance(obj, (np.ndarray, ABCExtensionArray)):
        return _isna_array(obj, inf_as_na=inf_as_na)
    elif isinstance(obj, ABCIndex):
        if not obj._can_hold_na:
            return obj.isna()
        return _isna_array(obj._values, inf_as_na=inf_as_na)
    elif isinstance(obj, ABCSeries):
        result = _isna_array(obj._values, inf_as_na=inf_as_na)
        result = obj._constructor(result, index=obj.index, name=obj.name, copy=False)
        return result
    elif isinstance(obj, ABCDataFrame):
        return obj.isna()
    elif isinstance(obj, list):
        return _isna_array(np.asarray(obj, dtype=object), inf_as_na=inf_as_na)
    elif hasattr(obj, '__array__'):
        return _isna_array(np.asarray(obj), inf_as_na=inf_as_na)
    else:
        return False","""""""Detect missing values, treating None, NaN or NA as null. Infinite
values will also be treated as null if inf_as_na is True.

Parameters
----------
obj: ndarray or object value
    Input array or scalar value.
inf_as_na: bool
    Whether to treat infinity as null.

Returns
-------
boolean ndarray or boolean"""""""
pandas/core/dtypes/missing.py,"def _use_inf_as_na(key) -> None:
    inf_as_na = get_option(key)
    globals()['_isna'] = partial(_isna, inf_as_na=inf_as_na)
    if inf_as_na:
        globals()['nan_checker'] = lambda x: ~np.isfinite(x)
        globals()['INF_AS_NA'] = True
    else:
        globals()['nan_checker'] = np.isnan
        globals()['INF_AS_NA'] = False","""""""Option change callback for na/inf behaviour.

Choose which replacement for numpy.isnan / -numpy.isfinite is used.

Parameters
----------
flag: bool
    True means treat None, NaN, INF, -INF as null (old way),
    False means None and NaN are null, but INF, -INF are not null
    (new way).

Notes
-----
This approach to setting global module values is discussed and
approved here:

* https://stackoverflow.com/questions/4859217/
  programmatically-creating-variables-in-python/4859312#4859312"""""""
pandas/core/dtypes/missing.py,"def _isna_array(values: ArrayLike, inf_as_na: bool=False):
    dtype = values.dtype
    if not isinstance(values, np.ndarray):
        if inf_as_na and isinstance(dtype, CategoricalDtype):
            result = libmissing.isnaobj(values.to_numpy(), inf_as_na=inf_as_na)
        else:
            result = values.isna()
    elif isinstance(values, np.rec.recarray):
        result = _isna_recarray_dtype(values, inf_as_na=inf_as_na)
    elif is_string_or_object_np_dtype(values.dtype):
        result = _isna_string_dtype(values, inf_as_na=inf_as_na)
    elif dtype.kind in 'mM':
        result = values.view('i8') == iNaT
    elif inf_as_na:
        result = ~np.isfinite(values)
    else:
        result = np.isnan(values)
    return result","""""""Return an array indicating which values of the input array are NaN / NA.

Parameters
----------
obj: ndarray or ExtensionArray
    The input array whose elements are to be checked.
inf_as_na: bool
    Whether or not to treat infinite values as NA.

Returns
-------
array-like
    Array of boolean values denoting the NA status of each element."""""""
pandas/core/dtypes/missing.py,"def notna(obj: object) -> bool | npt.NDArray[np.bool_] | NDFrame:
    res = isna(obj)
    if isinstance(res, bool):
        return not res
    return ~res","""""""Detect non-missing values for an array-like object.

This function takes a scalar or array-like object and indicates
whether values are valid (not missing, which is ``NaN`` in numeric
arrays, ``None`` or ``NaN`` in object arrays, ``NaT`` in datetimelike).

Parameters
----------
obj : array-like or object value
    Object to check for *not* null or *non*-missing values.

Returns
-------
bool or array-like of bool
    For scalar input, returns a scalar boolean.
    For array input, returns an array of boolean indicating whether each
    corresponding element is valid.

See Also
--------
isna : Boolean inverse of pandas.notna.
Series.notna : Detect valid values in a Series.
DataFrame.notna : Detect valid values in a DataFrame.
Index.notna : Detect valid values in an Index.

Examples
--------
Scalar arguments (including strings) result in a scalar boolean.

>>> pd.notna('dog')
True

>>> pd.notna(pd.NA)
False

>>> pd.notna(np.nan)
False

ndarrays result in an ndarray of booleans.

>>> array = np.array([[1, np.nan, 3], [4, 5, np.nan]])
>>> array
array([[ 1., nan,  3.],
       [ 4.,  5., nan]])
>>> pd.notna(array)
array([[ True, False,  True],
       [ True,  True, False]])

For indexes, an ndarray of booleans is returned.

>>> index = pd.DatetimeIndex([""2017-07-05"", ""2017-07-06"", None,
...                          ""2017-07-08""])
>>> index
DatetimeIndex(['2017-07-05', '2017-07-06', 'NaT', '2017-07-08'],
              dtype='datetime64[ns]', freq=None)
>>> pd.notna(index)
array([ True,  True, False,  True])

For Series and DataFrame, the same type is returned, containing booleans.

>>> df = pd.DataFrame([['ant', 'bee', 'cat'], ['dog', None, 'fly']])
>>> df
     0     1    2
0  ant   bee  cat
1  dog  None  fly
>>> pd.notna(df)
      0      1     2
0  True   True  True
1  True  False  True

>>> pd.notna(df[1])
0     True
1    False
Name: 1, dtype: bool"""""""
pandas/core/dtypes/missing.py,"def array_equivalent(left, right, strict_nan: bool=False, dtype_equal: bool=False) -> bool:
    (left, right) = (np.asarray(left), np.asarray(right))
    if left.shape != right.shape:
        return False
    if dtype_equal:
        if left.dtype.kind in 'fc':
            return _array_equivalent_float(left, right)
        elif left.dtype.kind in 'mM':
            return _array_equivalent_datetimelike(left, right)
        elif is_string_or_object_np_dtype(left.dtype):
            return _array_equivalent_object(left, right, strict_nan)
        else:
            return np.array_equal(left, right)
    if left.dtype.kind in 'OSU' or right.dtype.kind in 'OSU':
        return _array_equivalent_object(left, right, strict_nan)
    if left.dtype.kind in 'fc':
        if not (left.size and right.size):
            return True
        return ((left == right) | isna(left) & isna(right)).all()
    elif left.dtype.kind in 'mM' or right.dtype.kind in 'mM':
        if left.dtype != right.dtype:
            return False
        left = left.view('i8')
        right = right.view('i8')
    if (left.dtype.type is np.void or right.dtype.type is np.void) and left.dtype != right.dtype:
        return False
    return np.array_equal(left, right)","""""""True if two arrays, left and right, have equal non-NaN elements, and NaNs
in corresponding locations.  False otherwise. It is assumed that left and
right are NumPy arrays of the same dtype. The behavior of this function
(particularly with respect to NaNs) is not defined if the dtypes are
different.

Parameters
----------
left, right : ndarrays
strict_nan : bool, default False
    If True, consider NaN and None to be different.
dtype_equal : bool, default False
    Whether `left` and `right` are known to have the same dtype
    according to `is_dtype_equal`. Some methods like `BlockManager.equals`.
    require that the dtypes match. Setting this to ``True`` can improve
    performance, but will give different results for arrays that are
    equal but different dtypes.

Returns
-------
b : bool
    Returns True if the arrays are equivalent.

Examples
--------
>>> array_equivalent(
...     np.array([1, 2, np.nan]),
...     np.array([1, 2, np.nan]))
True
>>> array_equivalent(
...     np.array([1, np.nan, 2]),
...     np.array([1, 2, np.nan]))
False"""""""
pandas/core/dtypes/missing.py,"def array_equals(left: ArrayLike, right: ArrayLike) -> bool:
    if left.dtype != right.dtype:
        return False
    elif isinstance(left, ABCExtensionArray):
        return left.equals(right)
    else:
        return array_equivalent(left, right, dtype_equal=True)","""""""ExtensionArray-compatible implementation of array_equivalent."""""""
pandas/core/dtypes/missing.py,"def infer_fill_value(val):
    if not is_list_like(val):
        val = [val]
    val = np.array(val, copy=False)
    if val.dtype.kind in 'mM':
        return np.array('NaT', dtype=val.dtype)
    elif val.dtype == object:
        dtype = lib.infer_dtype(ensure_object(val), skipna=False)
        if dtype in ['datetime', 'datetime64']:
            return np.array('NaT', dtype=DT64NS_DTYPE)
        elif dtype in ['timedelta', 'timedelta64']:
            return np.array('NaT', dtype=TD64NS_DTYPE)
        return np.array(np.nan, dtype=object)
    elif val.dtype.kind == 'U':
        return np.array(np.nan, dtype=val.dtype)
    return np.nan","""""""infer the fill value for the nan/NaT from the provided
scalar/ndarray/list-like if we are a NaT, return the correct dtyped
element to provide proper block construction"""""""
pandas/core/dtypes/missing.py,"def maybe_fill(arr: np.ndarray) -> np.ndarray:
    if arr.dtype.kind not in 'iub':
        arr.fill(np.nan)
    return arr","""""""Fill numpy.ndarray with NaN, unless we have a integer or boolean dtype."""""""
pandas/core/dtypes/missing.py,"def na_value_for_dtype(dtype: DtypeObj, compat: bool=True):
    if isinstance(dtype, ExtensionDtype):
        return dtype.na_value
    elif dtype.kind in 'mM':
        return dtype.type('NaT', 'ns')
    elif dtype.kind == 'f':
        return np.nan
    elif dtype.kind in 'iu':
        if compat:
            return 0
        return np.nan
    elif dtype.kind == 'b':
        if compat:
            return False
        return np.nan
    return np.nan","""""""Return a dtype compat na value

Parameters
----------
dtype : string / dtype
compat : bool, default True

Returns
-------
np.dtype or a pandas dtype

Examples
--------
>>> na_value_for_dtype(np.dtype('int64'))
0
>>> na_value_for_dtype(np.dtype('int64'), compat=False)
nan
>>> na_value_for_dtype(np.dtype('float64'))
nan
>>> na_value_for_dtype(np.dtype('bool'))
False
>>> na_value_for_dtype(np.dtype('datetime64[ns]'))
numpy.datetime64('NaT')"""""""
pandas/core/dtypes/missing.py,"def remove_na_arraylike(arr: Series | Index | np.ndarray):
    if isinstance(arr.dtype, ExtensionDtype):
        return arr[notna(arr)]
    else:
        return arr[notna(np.asarray(arr))]","""""""Return array-like containing only true/non-NaN values, possibly empty."""""""
pandas/core/dtypes/missing.py,"def is_valid_na_for_dtype(obj, dtype: DtypeObj) -> bool:
    if not lib.is_scalar(obj) or not isna(obj):
        return False
    elif dtype.kind == 'M':
        if isinstance(dtype, np.dtype):
            return not isinstance(obj, (np.timedelta64, Decimal))
        return not isinstance(obj, (np.timedelta64, np.datetime64, Decimal))
    elif dtype.kind == 'm':
        return not isinstance(obj, (np.datetime64, Decimal))
    elif dtype.kind in 'iufc':
        return obj is not NaT and (not isinstance(obj, (np.datetime64, np.timedelta64)))
    elif dtype.kind == 'b':
        return lib.is_float(obj) or obj is None or obj is libmissing.NA
    elif dtype == _dtype_str:
        return not isinstance(obj, (np.datetime64, np.timedelta64, Decimal, float))
    elif dtype == _dtype_object:
        return True
    elif isinstance(dtype, PeriodDtype):
        return not isinstance(obj, (np.datetime64, np.timedelta64, Decimal))
    elif isinstance(dtype, IntervalDtype):
        return lib.is_float(obj) or obj is None or obj is libmissing.NA
    elif isinstance(dtype, CategoricalDtype):
        return is_valid_na_for_dtype(obj, dtype.categories.dtype)
    return not isinstance(obj, (np.datetime64, np.timedelta64, Decimal))","""""""isna check that excludes incompatible dtypes

Parameters
----------
obj : object
dtype : np.datetime64, np.timedelta64, DatetimeTZDtype, or PeriodDtype

Returns
-------
bool"""""""
pandas/core/dtypes/missing.py,"def isna_all(arr: ArrayLike) -> bool:
    total_len = len(arr)
    chunk_len = max(total_len // 40, 1000)
    dtype = arr.dtype
    if lib.is_np_dtype(dtype, 'f'):
        checker = nan_checker
    elif lib.is_np_dtype(dtype, 'mM') or isinstance(dtype, (DatetimeTZDtype, PeriodDtype)):
        checker = lambda x: np.asarray(x.view('i8')) == iNaT
    else:
        checker = lambda x: _isna_array(x, inf_as_na=INF_AS_NA)
    return all((checker(arr[i:i + chunk_len]).all() for i in range(0, total_len, chunk_len)))","""""""Optimized equivalent to isna(arr).all()"""""""
pandas/core/generic.py,"def make_doc(name: str, ndim: int) -> str:
    if ndim == 1:
        name1 = 'scalar'
        name2 = 'Series'
        axis_descr = '{index (0)}'
    else:
        name1 = 'Series'
        name2 = 'DataFrame'
        axis_descr = '{index (0), columns (1)}'
    if name == 'any':
        base_doc = _bool_doc
        desc = _any_desc
        see_also = _any_see_also
        examples = _any_examples
        kwargs = {'empty_value': 'False'}
    elif name == 'all':
        base_doc = _bool_doc
        desc = _all_desc
        see_also = _all_see_also
        examples = _all_examples
        kwargs = {'empty_value': 'True'}
    elif name == 'min':
        base_doc = _num_doc
        desc = 'Return the minimum of the values over the requested axis.\n\nIf you want the *index* of the minimum, use ``idxmin``. This is the equivalent of the ``numpy.ndarray`` method ``argmin``.'
        see_also = _stat_func_see_also
        examples = _min_examples
        kwargs = {'min_count': ''}
    elif name == 'max':
        base_doc = _num_doc
        desc = 'Return the maximum of the values over the requested axis.\n\nIf you want the *index* of the maximum, use ``idxmax``. This is the equivalent of the ``numpy.ndarray`` method ``argmax``.'
        see_also = _stat_func_see_also
        examples = _max_examples
        kwargs = {'min_count': ''}
    elif name == 'sum':
        base_doc = _sum_prod_doc
        desc = 'Return the sum of the values over the requested axis.\n\nThis is equivalent to the method ``numpy.sum``.'
        see_also = _stat_func_see_also
        examples = _sum_examples
        kwargs = {'min_count': _min_count_stub}
    elif name == 'prod':
        base_doc = _sum_prod_doc
        desc = 'Return the product of the values over the requested axis.'
        see_also = _stat_func_see_also
        examples = _prod_examples
        kwargs = {'min_count': _min_count_stub}
    elif name == 'median':
        base_doc = _num_doc
        desc = 'Return the median of the values over the requested axis.'
        see_also = ''
        examples = ""\n\n            Examples\n            --------\n            >>> s = pd.Series([1, 2, 3])\n            >>> s.median()\n            2.0\n\n            With a DataFrame\n\n            >>> df = pd.DataFrame({'a': [1, 2], 'b': [2, 3]}, index=['tiger', 'zebra'])\n            >>> df\n                   a   b\n            tiger  1   2\n            zebra  2   3\n            >>> df.median()\n            a   1.5\n            b   2.5\n            dtype: float64\n\n            Using axis=1\n\n            >>> df.median(axis=1)\n            tiger   1.5\n            zebra   2.5\n            dtype: float64\n\n            In this case, `numeric_only` should be set to `True`\n            to avoid getting an error.\n\n            >>> df = pd.DataFrame({'a': [1, 2], 'b': ['T', 'Z']},\n            ...                   index=['tiger', 'zebra'])\n            >>> df.median(numeric_only=True)\n            a   1.5\n            dtype: float64""
        kwargs = {'min_count': ''}
    elif name == 'mean':
        base_doc = _num_doc
        desc = 'Return the mean of the values over the requested axis.'
        see_also = ''
        examples = ""\n\n            Examples\n            --------\n            >>> s = pd.Series([1, 2, 3])\n            >>> s.mean()\n            2.0\n\n            With a DataFrame\n\n            >>> df = pd.DataFrame({'a': [1, 2], 'b': [2, 3]}, index=['tiger', 'zebra'])\n            >>> df\n                   a   b\n            tiger  1   2\n            zebra  2   3\n            >>> df.mean()\n            a   1.5\n            b   2.5\n            dtype: float64\n\n            Using axis=1\n\n            >>> df.mean(axis=1)\n            tiger   1.5\n            zebra   2.5\n            dtype: float64\n\n            In this case, `numeric_only` should be set to `True` to avoid\n            getting an error.\n\n            >>> df = pd.DataFrame({'a': [1, 2], 'b': ['T', 'Z']},\n            ...                   index=['tiger', 'zebra'])\n            >>> df.mean(numeric_only=True)\n            a   1.5\n            dtype: float64""
        kwargs = {'min_count': ''}
    elif name == 'var':
        base_doc = _num_ddof_doc
        desc = 'Return unbiased variance over requested axis.\n\nNormalized by N-1 by default. This can be changed using the ddof argument.'
        examples = _var_examples
        see_also = ''
        kwargs = {'notes': ''}
    elif name == 'std':
        base_doc = _num_ddof_doc
        desc = 'Return sample standard deviation over requested axis.\n\nNormalized by N-1 by default. This can be changed using the ddof argument.'
        examples = _std_examples
        see_also = ''
        kwargs = {'notes': _std_notes}
    elif name == 'sem':
        base_doc = _num_ddof_doc
        desc = 'Return unbiased standard error of the mean over requested axis.\n\nNormalized by N-1 by default. This can be changed using the ddof argument'
        examples = ""\n\n            Examples\n            --------\n            >>> s = pd.Series([1, 2, 3])\n            >>> s.sem().round(6)\n            0.57735\n\n            With a DataFrame\n\n            >>> df = pd.DataFrame({'a': [1, 2], 'b': [2, 3]}, index=['tiger', 'zebra'])\n            >>> df\n                   a   b\n            tiger  1   2\n            zebra  2   3\n            >>> df.sem()\n            a   0.5\n            b   0.5\n            dtype: float64\n\n            Using axis=1\n\n            >>> df.sem(axis=1)\n            tiger   0.5\n            zebra   0.5\n            dtype: float64\n\n            In this case, `numeric_only` should be set to `True`\n            to avoid getting an error.\n\n            >>> df = pd.DataFrame({'a': [1, 2], 'b': ['T', 'Z']},\n            ...                   index=['tiger', 'zebra'])\n            >>> df.sem(numeric_only=True)\n            a   0.5\n            dtype: float64""
        see_also = ''
        kwargs = {'notes': ''}
    elif name == 'skew':
        base_doc = _num_doc
        desc = 'Return unbiased skew over requested axis.\n\nNormalized by N-1.'
        see_also = ''
        examples = ""\n\n            Examples\n            --------\n            >>> s = pd.Series([1, 2, 3])\n            >>> s.skew()\n            0.0\n\n            With a DataFrame\n\n            >>> df = pd.DataFrame({'a': [1, 2, 3], 'b': [2, 3, 4], 'c': [1, 3, 5]},\n            ...                   index=['tiger', 'zebra', 'cow'])\n            >>> df\n                    a   b   c\n            tiger   1   2   1\n            zebra   2   3   3\n            cow     3   4   5\n            >>> df.skew()\n            a   0.0\n            b   0.0\n            c   0.0\n            dtype: float64\n\n            Using axis=1\n\n            >>> df.skew(axis=1)\n            tiger   1.732051\n            zebra  -1.732051\n            cow     0.000000\n            dtype: float64\n\n            In this case, `numeric_only` should be set to `True` to avoid\n            getting an error.\n\n            >>> df = pd.DataFrame({'a': [1, 2, 3], 'b': ['T', 'Z', 'X']},\n            ...                   index=['tiger', 'zebra', 'cow'])\n            >>> df.skew(numeric_only=True)\n            a   0.0\n            dtype: float64""
        kwargs = {'min_count': ''}
    elif name == 'kurt':
        base_doc = _num_doc
        desc = ""Return unbiased kurtosis over requested axis.\n\nKurtosis obtained using Fisher's definition of\nkurtosis (kurtosis of normal == 0.0). Normalized by N-1.""
        see_also = ''
        examples = ""\n\n            Examples\n            --------\n            >>> s = pd.Series([1, 2, 2, 3], index=['cat', 'dog', 'dog', 'mouse'])\n            >>> s\n            cat    1\n            dog    2\n            dog    2\n            mouse  3\n            dtype: int64\n            >>> s.kurt()\n            1.5\n\n            With a DataFrame\n\n            >>> df = pd.DataFrame({'a': [1, 2, 2, 3], 'b': [3, 4, 4, 4]},\n            ...                   index=['cat', 'dog', 'dog', 'mouse'])\n            >>> df\n                   a   b\n              cat  1   3\n              dog  2   4\n              dog  2   4\n            mouse  3   4\n            >>> df.kurt()\n            a   1.5\n            b   4.0\n            dtype: float64\n\n            With axis=None\n\n            >>> df.kurt(axis=None).round(6)\n            -0.988693\n\n            Using axis=1\n\n            >>> df = pd.DataFrame({'a': [1, 2], 'b': [3, 4], 'c': [3, 4], 'd': [1, 2]},\n            ...                   index=['cat', 'dog'])\n            >>> df.kurt(axis=1)\n            cat   -6.0\n            dog   -6.0\n            dtype: float64""
        kwargs = {'min_count': ''}
    elif name == 'cumsum':
        base_doc = _cnum_doc
        desc = 'sum'
        see_also = ''
        examples = _cumsum_examples
        kwargs = {'accum_func_name': 'sum'}
    elif name == 'cumprod':
        base_doc = _cnum_doc
        desc = 'product'
        see_also = ''
        examples = _cumprod_examples
        kwargs = {'accum_func_name': 'prod'}
    elif name == 'cummin':
        base_doc = _cnum_doc
        desc = 'minimum'
        see_also = ''
        examples = _cummin_examples
        kwargs = {'accum_func_name': 'min'}
    elif name == 'cummax':
        base_doc = _cnum_doc
        desc = 'maximum'
        see_also = ''
        examples = _cummax_examples
        kwargs = {'accum_func_name': 'max'}
    else:
        raise NotImplementedError
    docstr = base_doc.format(desc=desc, name=name, name1=name1, name2=name2, axis_descr=axis_descr, see_also=see_also, examples=examples, **kwargs)
    return docstr","""""""Generate the docstring for a Series/DataFrame reduction."""""""
pandas/core/groupby/categorical.py,"def recode_for_groupby(c: Categorical, sort: bool, observed: bool) -> tuple[Categorical, Categorical | None]:
    if observed:
        unique_codes = unique1d(c.codes)
        take_codes = unique_codes[unique_codes != -1]
        if sort:
            take_codes = np.sort(take_codes)
        categories = c.categories.take(take_codes)
        codes = recode_for_categories(c.codes, c.categories, categories)
        dtype = CategoricalDtype(categories, ordered=c.ordered)
        return (Categorical._simple_new(codes, dtype=dtype), c)
    if sort:
        return (c, None)
    all_codes = np.arange(c.categories.nunique())
    unique_notnan_codes = unique1d(c.codes[c.codes != -1])
    if sort:
        unique_notnan_codes = np.sort(unique_notnan_codes)
    if len(all_codes) > len(unique_notnan_codes):
        missing_codes = np.setdiff1d(all_codes, unique_notnan_codes, assume_unique=True)
        take_codes = np.concatenate((unique_notnan_codes, missing_codes))
    else:
        take_codes = unique_notnan_codes
    return (Categorical(c, c.unique().categories.take(take_codes)), None)","""""""Code the categories to ensure we can groupby for categoricals.

If observed=True, we return a new Categorical with the observed
categories only.

If sort=False, return a copy of self, coded with categories as
returned by .unique(), followed by any categories not appearing in
the data. If sort=True, return self.

This method is needed solely to ensure the categorical index of the
GroupBy result has categories in the order of appearance in the data
(GH-8868).

Parameters
----------
c : Categorical
sort : bool
    The value of the sort parameter groupby was called with.
observed : bool
    Account only for the observed values

Returns
-------
Categorical
    If sort=False, the new categories are set to the order of
    appearance in codes (unless ordered=True, in which case the
    original order is preserved), followed by any unrepresented
    categories in the original order.
Categorical or None
    If we are observed, return the original categorical, otherwise None"""""""
pandas/core/groupby/groupby.py,"def _insert_quantile_level(idx: Index, qs: npt.NDArray[np.float64]) -> MultiIndex:
    nqs = len(qs)
    (lev_codes, lev) = Index(qs).factorize()
    lev_codes = coerce_indexer_dtype(lev_codes, lev)
    if idx._is_multi:
        idx = cast(MultiIndex, idx)
        levels = list(idx.levels) + [lev]
        codes = [np.repeat(x, nqs) for x in idx.codes] + [np.tile(lev_codes, len(idx))]
        mi = MultiIndex(levels=levels, codes=codes, names=idx.names + [None])
    else:
        nidx = len(idx)
        idx_codes = coerce_indexer_dtype(np.arange(nidx), idx)
        levels = [idx, lev]
        codes = [np.repeat(idx_codes, nqs), np.tile(lev_codes, nidx)]
        mi = MultiIndex(levels=levels, codes=codes, names=[idx.name, None])
    return mi","""""""Insert the sequence 'qs' of quantiles as the inner-most level of a MultiIndex.

The quantile level in the MultiIndex is a repeated copy of 'qs'.

Parameters
----------
idx : Index
qs : np.ndarray[float64]

Returns
-------
MultiIndex"""""""
pandas/core/groupby/grouper.py,"def get_grouper(obj: NDFrameT, key=None, axis: Axis=0, level=None, sort: bool=True, observed: bool=False, validate: bool=True, dropna: bool=True) -> tuple[ops.BaseGrouper, frozenset[Hashable], NDFrameT]:
    group_axis = obj._get_axis(axis)
    if level is not None:
        if isinstance(group_axis, MultiIndex):
            if is_list_like(level) and len(level) == 1:
                level = level[0]
            if key is None and is_scalar(level):
                key = group_axis.get_level_values(level)
                level = None
        else:
            if is_list_like(level):
                nlevels = len(level)
                if nlevels == 1:
                    level = level[0]
                elif nlevels == 0:
                    raise ValueError('No group keys passed!')
                else:
                    raise ValueError('multiple levels only valid with MultiIndex')
            if isinstance(level, str):
                if obj._get_axis(axis).name != level:
                    raise ValueError(f'level name {level} is not the name of the {obj._get_axis_name(axis)}')
            elif level > 0 or level < -1:
                raise ValueError('level > 0 or level < -1 only valid with MultiIndex')
            level = None
            key = group_axis
    if isinstance(key, Grouper):
        (grouper, obj) = key._get_grouper(obj, validate=False)
        if key.key is None:
            return (grouper, frozenset(), obj)
        else:
            return (grouper, frozenset({key.key}), obj)
    elif isinstance(key, ops.BaseGrouper):
        return (key, frozenset(), obj)
    if not isinstance(key, list):
        keys = [key]
        match_axis_length = False
    else:
        keys = key
        match_axis_length = len(keys) == len(group_axis)
    any_callable = any((callable(g) or isinstance(g, dict) for g in keys))
    any_groupers = any((isinstance(g, (Grouper, Grouping)) for g in keys))
    any_arraylike = any((isinstance(g, (list, tuple, Series, Index, np.ndarray)) for g in keys))
    if not any_callable and (not any_arraylike) and (not any_groupers) and match_axis_length and (level is None):
        if isinstance(obj, DataFrame):
            all_in_columns_index = all((g in obj.columns or g in obj.index.names for g in keys))
        else:
            assert isinstance(obj, Series)
            all_in_columns_index = all((g in obj.index.names for g in keys))
        if not all_in_columns_index:
            keys = [com.asarray_tuplesafe(keys)]
    if isinstance(level, (tuple, list)):
        if key is None:
            keys = [None] * len(level)
        levels = level
    else:
        levels = [level] * len(keys)
    groupings: list[Grouping] = []
    exclusions: set[Hashable] = set()

    def is_in_axis(key) -> bool:
        if not _is_label_like(key):
            if obj.ndim == 1:
                return False
            items = obj.axes[-1]
            try:
                items.get_loc(key)
            except (KeyError, TypeError, InvalidIndexError):
                return False
        return True

    def is_in_obj(gpr) -> bool:
        if not hasattr(gpr, 'name'):
            return False
        if using_copy_on_write():
            try:
                obj_gpr_column = obj[gpr.name]
            except (KeyError, IndexError, InvalidIndexError, OutOfBoundsDatetime):
                return False
            if isinstance(gpr, Series) and isinstance(obj_gpr_column, Series):
                return gpr._mgr.references_same_values(obj_gpr_column._mgr, 0)
            return False
        try:
            return gpr is obj[gpr.name]
        except (KeyError, IndexError, InvalidIndexError, OutOfBoundsDatetime):
            return False
    for (gpr, level) in zip(keys, levels):
        if is_in_obj(gpr):
            in_axis = True
            exclusions.add(gpr.name)
        elif is_in_axis(gpr):
            if obj.ndim != 1 and gpr in obj:
                if validate:
                    obj._check_label_or_level_ambiguity(gpr, axis=axis)
                (in_axis, name, gpr) = (True, gpr, obj[gpr])
                if gpr.ndim != 1:
                    raise ValueError(f""Grouper for '{name}' not 1-dimensional"")
                exclusions.add(name)
            elif obj._is_level_reference(gpr, axis=axis):
                (in_axis, level, gpr) = (False, gpr, None)
            else:
                raise KeyError(gpr)
        elif isinstance(gpr, Grouper) and gpr.key is not None:
            exclusions.add(gpr.key)
            in_axis = True
        else:
            in_axis = False
        ping = Grouping(group_axis, gpr, obj=obj, level=level, sort=sort, observed=observed, in_axis=in_axis, dropna=dropna) if not isinstance(gpr, Grouping) else gpr
        groupings.append(ping)
    if len(groupings) == 0 and len(obj):
        raise ValueError('No group keys passed!')
    if len(groupings) == 0:
        groupings.append(Grouping(Index([], dtype='int'), np.array([], dtype=np.intp)))
    grouper = ops.BaseGrouper(group_axis, groupings, sort=sort, dropna=dropna)
    return (grouper, frozenset(exclusions), obj)","""""""Create and return a BaseGrouper, which is an internal
mapping of how to create the grouper indexers.
This may be composed of multiple Grouping objects, indicating
multiple groupers

Groupers are ultimately index mappings. They can originate as:
index mappings, keys to columns, functions, or Groupers

Groupers enable local references to axis,level,sort, while
the passed in axis, level, and sort are 'global'.

This routine tries to figure out what the passing in references
are and then creates a Grouping for each one, combined into
a BaseGrouper.

If observed & we have a categorical grouper, only show the observed
values.

If validate, then check for key/level overlaps."""""""
pandas/core/groupby/numba_.py,"def validate_udf(func: Callable) -> None:
    if not callable(func):
        raise NotImplementedError('Numba engine can only be used with a single function.')
    udf_signature = list(inspect.signature(func).parameters.keys())
    expected_args = ['values', 'index']
    min_number_args = len(expected_args)
    if len(udf_signature) < min_number_args or udf_signature[:min_number_args] != expected_args:
        raise NumbaUtilError(f'The first {min_number_args} arguments to {func.__name__} must be {expected_args}')","""""""Validate user defined function for ops when using Numba with groupby ops.

The first signature arguments should include:

def f(values, index, ...):
    ...

Parameters
----------
func : function, default False
    user defined function

Returns
-------
None

Raises
------
NumbaUtilError"""""""
pandas/core/groupby/numba_.py,"@functools.cache
def generate_numba_agg_func(func: Callable[..., Scalar], nopython: bool, nogil: bool, parallel: bool) -> Callable[[np.ndarray, np.ndarray, np.ndarray, np.ndarray, int, Any], np.ndarray]:
    numba_func = jit_user_function(func)
    if TYPE_CHECKING:
        import numba
    else:
        numba = import_optional_dependency('numba')

    @numba.jit(nopython=nopython, nogil=nogil, parallel=parallel)
    def group_agg(values: np.ndarray, index: np.ndarray, begin: np.ndarray, end: np.ndarray, num_columns: int, *args: Any) -> np.ndarray:
        assert len(begin) == len(end)
        num_groups = len(begin)
        result = np.empty((num_groups, num_columns))
        for i in numba.prange(num_groups):
            group_index = index[begin[i]:end[i]]
            for j in numba.prange(num_columns):
                group = values[begin[i]:end[i], j]
                result[i, j] = numba_func(group, group_index, *args)
        return result
    return group_agg","""""""Generate a numba jitted agg function specified by values from engine_kwargs.

1. jit the user's function
2. Return a groupby agg function with the jitted function inline

Configurations specified in engine_kwargs apply to both the user's
function _AND_ the groupby evaluation loop.

Parameters
----------
func : function
    function to be applied to each group and will be JITed
nopython : bool
    nopython to be passed into numba.jit
nogil : bool
    nogil to be passed into numba.jit
parallel : bool
    parallel to be passed into numba.jit

Returns
-------
Numba function"""""""
pandas/core/groupby/numba_.py,"@functools.cache
def generate_numba_transform_func(func: Callable[..., np.ndarray], nopython: bool, nogil: bool, parallel: bool) -> Callable[[np.ndarray, np.ndarray, np.ndarray, np.ndarray, int, Any], np.ndarray]:
    numba_func = jit_user_function(func)
    if TYPE_CHECKING:
        import numba
    else:
        numba = import_optional_dependency('numba')

    @numba.jit(nopython=nopython, nogil=nogil, parallel=parallel)
    def group_transform(values: np.ndarray, index: np.ndarray, begin: np.ndarray, end: np.ndarray, num_columns: int, *args: Any) -> np.ndarray:
        assert len(begin) == len(end)
        num_groups = len(begin)
        result = np.empty((len(values), num_columns))
        for i in numba.prange(num_groups):
            group_index = index[begin[i]:end[i]]
            for j in numba.prange(num_columns):
                group = values[begin[i]:end[i], j]
                result[begin[i]:end[i], j] = numba_func(group, group_index, *args)
        return result
    return group_transform","""""""Generate a numba jitted transform function specified by values from engine_kwargs.

1. jit the user's function
2. Return a groupby transform function with the jitted function inline

Configurations specified in engine_kwargs apply to both the user's
function _AND_ the groupby evaluation loop.

Parameters
----------
func : function
    function to be applied to each window and will be JITed
nopython : bool
    nopython to be passed into numba.jit
nogil : bool
    nogil to be passed into numba.jit
parallel : bool
    parallel to be passed into numba.jit

Returns
-------
Numba function"""""""
pandas/core/groupby/ops.py,"def extract_result(res):
    if hasattr(res, '_values'):
        res = res._values
        if res.ndim == 1 and len(res) == 1:
            res = res[0]
    return res","""""""Extract the result object, it might be a 0-dim ndarray
or a len-1 0-dim, or a scalar"""""""
pandas/core/indexers/utils.py,"def is_valid_positional_slice(slc: slice) -> bool:
    return lib.is_int_or_none(slc.start) and lib.is_int_or_none(slc.stop) and lib.is_int_or_none(slc.step)","""""""Check if a slice object can be interpreted as a positional indexer.

Parameters
----------
slc : slice

Returns
-------
bool

Notes
-----
A valid positional slice may also be interpreted as a label-based slice
depending on the index being sliced."""""""
pandas/core/indexers/utils.py,"def is_list_like_indexer(key) -> bool:
    return is_list_like(key) and (not (isinstance(key, tuple) and type(key) is not tuple))","""""""Check if we have a list-like indexer that is *not* a NamedTuple.

Parameters
----------
key : object

Returns
-------
bool"""""""
pandas/core/indexers/utils.py,"def is_scalar_indexer(indexer, ndim: int) -> bool:
    if ndim == 1 and is_integer(indexer):
        return True
    if isinstance(indexer, tuple) and len(indexer) == ndim:
        return all((is_integer(x) for x in indexer))
    return False","""""""Return True if we are all scalar indexers.

Parameters
----------
indexer : object
ndim : int
    Number of dimensions in the object being indexed.

Returns
-------
bool"""""""
pandas/core/indexers/utils.py,"def is_empty_indexer(indexer) -> bool:
    if is_list_like(indexer) and (not len(indexer)):
        return True
    if not isinstance(indexer, tuple):
        indexer = (indexer,)
    return any((isinstance(idx, np.ndarray) and len(idx) == 0 for idx in indexer))","""""""Check if we have an empty indexer.

Parameters
----------
indexer : object

Returns
-------
bool"""""""
pandas/core/indexers/utils.py,"def check_setitem_lengths(indexer, value, values) -> bool:
    no_op = False
    if isinstance(indexer, (np.ndarray, list)):
        if is_list_like(value):
            if len(indexer) != len(value) and values.ndim == 1:
                if isinstance(indexer, list):
                    indexer = np.array(indexer)
                if not (isinstance(indexer, np.ndarray) and indexer.dtype == np.bool_ and (indexer.sum() == len(value))):
                    raise ValueError('cannot set using a list-like indexer with a different length than the value')
            if not len(indexer):
                no_op = True
    elif isinstance(indexer, slice):
        if is_list_like(value):
            if len(value) != length_of_indexer(indexer, values) and values.ndim == 1:
                raise ValueError('cannot set using a slice indexer with a different length than the value')
            if not len(value):
                no_op = True
    return no_op","""""""Validate that value and indexer are the same length.

An special-case is allowed for when the indexer is a boolean array
and the number of true values equals the length of ``value``. In
this case, no exception is raised.

Parameters
----------
indexer : sequence
    Key for the setitem.
value : array-like
    Value for the setitem.
values : array-like
    Values being set into.

Returns
-------
bool
    Whether this is an empty listlike setting which is a no-op.

Raises
------
ValueError
    When the indexer is an ndarray or list and the lengths don't match."""""""
pandas/core/indexers/utils.py,"def validate_indices(indices: np.ndarray, n: int) -> None:
    if len(indices):
        min_idx = indices.min()
        if min_idx < -1:
            msg = f""'indices' contains values less than allowed ({min_idx} < -1)""
            raise ValueError(msg)
        max_idx = indices.max()
        if max_idx >= n:
            raise IndexError('indices are out-of-bounds')","""""""Perform bounds-checking for an indexer.

-1 is allowed for indicating missing values.

Parameters
----------
indices : ndarray
n : int
    Length of the array being indexed.

Raises
------
ValueError

Examples
--------
>>> validate_indices(np.array([1, 2]), 3) # OK

>>> validate_indices(np.array([1, -2]), 3)
Traceback (most recent call last):
    ...
ValueError: negative dimensions are not allowed

>>> validate_indices(np.array([1, 2, 3]), 3)
Traceback (most recent call last):
    ...
IndexError: indices are out-of-bounds

>>> validate_indices(np.array([-1, -1]), 0) # OK

>>> validate_indices(np.array([0, 1]), 0)
Traceback (most recent call last):
    ...
IndexError: indices are out-of-bounds"""""""
pandas/core/indexers/utils.py,"def maybe_convert_indices(indices, n: int, verify: bool=True) -> np.ndarray:
    if isinstance(indices, list):
        indices = np.array(indices)
        if len(indices) == 0:
            return np.empty(0, dtype=np.intp)
    mask = indices < 0
    if mask.any():
        indices = indices.copy()
        indices[mask] += n
    if verify:
        mask = (indices >= n) | (indices < 0)
        if mask.any():
            raise IndexError('indices are out-of-bounds')
    return indices","""""""Attempt to convert indices into valid, positive indices.

If we have negative indices, translate to positive here.
If we have indices that are out-of-bounds, raise an IndexError.

Parameters
----------
indices : array-like
    Array of indices that we are to convert.
n : int
    Number of elements in the array that we are indexing.
verify : bool, default True
    Check that all entries are between 0 and n - 1, inclusive.

Returns
-------
array-like
    An array-like of positive indices that correspond to the ones
    that were passed in initially to this function.

Raises
------
IndexError
    One of the converted indices either exceeded the number of,
    elements (specified by `n`), or was still negative."""""""
pandas/core/indexers/utils.py,"def length_of_indexer(indexer, target=None) -> int:
    if target is not None and isinstance(indexer, slice):
        target_len = len(target)
        start = indexer.start
        stop = indexer.stop
        step = indexer.step
        if start is None:
            start = 0
        elif start < 0:
            start += target_len
        if stop is None or stop > target_len:
            stop = target_len
        elif stop < 0:
            stop += target_len
        if step is None:
            step = 1
        elif step < 0:
            (start, stop) = (stop + 1, start + 1)
            step = -step
        return (stop - start + step - 1) // step
    elif isinstance(indexer, (ABCSeries, ABCIndex, np.ndarray, list)):
        if isinstance(indexer, list):
            indexer = np.array(indexer)
        if indexer.dtype == bool:
            return indexer.sum()
        return len(indexer)
    elif isinstance(indexer, range):
        return (indexer.stop - indexer.start) // indexer.step
    elif not is_list_like_indexer(indexer):
        return 1
    raise AssertionError('cannot find the length of the indexer')","""""""Return the expected length of target[indexer]

Returns
-------
int"""""""
pandas/core/indexers/utils.py,"def disallow_ndim_indexing(result) -> None:
    if np.ndim(result) > 1:
        raise ValueError('Multi-dimensional indexing (e.g. `obj[:, None]`) is no longer supported. Convert to a numpy array before indexing instead.')","""""""Helper function to disallow multi-dimensional indexing on 1D Series/Index.

GH#27125 indexer like idx[:, None] expands dim, but we cannot do that
and keep an index, so we used to return ndarray, which was deprecated
in GH#30588."""""""
pandas/core/indexers/utils.py,"def unpack_1tuple(tup):
    if len(tup) == 1 and isinstance(tup[0], slice):
        if isinstance(tup, list):
            raise ValueError('Indexing with a single-item list containing a slice is not allowed. Pass a tuple instead.')
        return tup[0]
    return tup","""""""If we have a length-1 tuple/list that contains a slice, unpack to just
the slice.

Notes
-----
The list case is deprecated."""""""
pandas/core/indexers/utils.py,"def check_key_length(columns: Index, key, value: DataFrame) -> None:
    if columns.is_unique:
        if len(value.columns) != len(key):
            raise ValueError('Columns must be same length as key')
    elif len(columns.get_indexer_non_unique(key)[0]) != len(value.columns):
        raise ValueError('Columns must be same length as key')","""""""Checks if a key used as indexer has the same length as the columns it is
associated with.

Parameters
----------
columns : Index The columns of the DataFrame to index.
key : A list-like of keys to index with.
value : DataFrame The value to set for the keys.

Raises
------
ValueError: If the length of key is not equal to the number of columns in value
            or if the number of columns referenced by key is not equal to number
            of columns."""""""
pandas/core/indexers/utils.py,"def unpack_tuple_and_ellipses(item: tuple):
    if len(item) > 1:
        if item[0] is Ellipsis:
            item = item[1:]
        elif item[-1] is Ellipsis:
            item = item[:-1]
    if len(item) > 1:
        raise IndexError('too many indices for array.')
    item = item[0]
    return item","""""""Possibly unpack arr[..., n] to arr[n]"""""""
pandas/core/indexers/utils.py,"def check_array_indexer(array: AnyArrayLike, indexer: Any) -> Any:
    from pandas.core.construction import array as pd_array
    if is_list_like(indexer):
        if isinstance(indexer, tuple):
            return indexer
    else:
        return indexer
    if not is_array_like(indexer):
        indexer = pd_array(indexer)
        if len(indexer) == 0:
            indexer = np.array([], dtype=np.intp)
    dtype = indexer.dtype
    if is_bool_dtype(dtype):
        if isinstance(dtype, ExtensionDtype):
            indexer = indexer.to_numpy(dtype=bool, na_value=False)
        else:
            indexer = np.asarray(indexer, dtype=bool)
        if len(indexer) != len(array):
            raise IndexError(f'Boolean index has wrong length: {len(indexer)} instead of {len(array)}')
    elif is_integer_dtype(dtype):
        try:
            indexer = np.asarray(indexer, dtype=np.intp)
        except ValueError as err:
            raise ValueError('Cannot index with an integer indexer containing NA values') from err
    else:
        raise IndexError('arrays used as indices must be of integer or boolean type')
    return indexer","""""""Check if `indexer` is a valid array indexer for `array`.

For a boolean mask, `array` and `indexer` are checked to have the same
length. The dtype is validated, and if it is an integer or boolean
ExtensionArray, it is checked if there are missing values present, and
it is converted to the appropriate numpy array. Other dtypes will raise
an error.

Non-array indexers (integer, slice, Ellipsis, tuples, ..) are passed
through as is.

Parameters
----------
array : array-like
    The array that is being indexed (only used for the length).
indexer : array-like or list-like
    The array-like that's used to index. List-like input that is not yet
    a numpy array or an ExtensionArray is converted to one. Other input
    types are passed through as is.

Returns
-------
numpy.ndarray
    The validated indexer as a numpy array that can be used to index.

Raises
------
IndexError
    When the lengths don't match.
ValueError
    When `indexer` cannot be converted to a numpy ndarray to index
    (e.g. presence of missing values).

See Also
--------
api.types.is_bool_dtype : Check if `key` is of boolean dtype.

Examples
--------
When checking a boolean mask, a boolean ndarray is returned when the
arguments are all valid.

>>> mask = pd.array([True, False])
>>> arr = pd.array([1, 2])
>>> pd.api.indexers.check_array_indexer(arr, mask)
array([ True, False])

An IndexError is raised when the lengths don't match.

>>> mask = pd.array([True, False, True])
>>> pd.api.indexers.check_array_indexer(arr, mask)
Traceback (most recent call last):
...
IndexError: Boolean index has wrong length: 3 instead of 2.

NA values in a boolean array are treated as False.

>>> mask = pd.array([True, pd.NA])
>>> pd.api.indexers.check_array_indexer(arr, mask)
array([ True, False])

A numpy boolean mask will get passed through (if the length is correct):

>>> mask = np.array([True, False])
>>> pd.api.indexers.check_array_indexer(arr, mask)
array([ True, False])

Similarly for integer indexers, an integer ndarray is returned when it is
a valid indexer, otherwise an error is  (for integer indexers, a matching
length is not required):

>>> indexer = pd.array([0, 2], dtype=""Int64"")
>>> arr = pd.array([1, 2, 3])
>>> pd.api.indexers.check_array_indexer(arr, indexer)
array([0, 2])

>>> indexer = pd.array([0, pd.NA], dtype=""Int64"")
>>> pd.api.indexers.check_array_indexer(arr, indexer)
Traceback (most recent call last):
...
ValueError: Cannot index with an integer indexer containing NA values

For non-integer/boolean dtypes, an appropriate error is raised:

>>> indexer = np.array([0., 2.], dtype=""float64"")
>>> pd.api.indexers.check_array_indexer(arr, indexer)
Traceback (most recent call last):
...
IndexError: arrays used as indices must be of integer or boolean type"""""""
pandas/core/indexes/api.py,"def get_objs_combined_axis(objs, intersect: bool=False, axis: Axis=0, sort: bool=True, copy: bool=False) -> Index:
    obs_idxes = [obj._get_axis(axis) for obj in objs]
    return _get_combined_index(obs_idxes, intersect=intersect, sort=sort, copy=copy)","""""""Extract combined index: return intersection or union (depending on the
value of ""intersect"") of indexes on given axis, or None if all objects
lack indexes (e.g. they are numpy arrays).

Parameters
----------
objs : list
    Series or DataFrame objects, may be mix of the two.
intersect : bool, default False
    If True, calculate the intersection between indexes. Otherwise,
    calculate the union.
axis : {0 or 'index', 1 or 'outer'}, default 0
    The axis to extract indexes from.
sort : bool, default True
    Whether the result index should come out sorted or not.
copy : bool, default False
    If True, return a copy of the combined index.

Returns
-------
Index"""""""
pandas/core/indexes/api.py,"def _get_distinct_objs(objs: list[Index]) -> list[Index]:
    ids: set[int] = set()
    res = []
    for obj in objs:
        if id(obj) not in ids:
            ids.add(id(obj))
            res.append(obj)
    return res","""""""Return a list with distinct elements of ""objs"" (different ids).
Preserves order."""""""
pandas/core/indexes/api.py,"def _get_combined_index(indexes: list[Index], intersect: bool=False, sort: bool=False, copy: bool=False) -> Index:
    indexes = _get_distinct_objs(indexes)
    if len(indexes) == 0:
        index = Index([])
    elif len(indexes) == 1:
        index = indexes[0]
    elif intersect:
        index = indexes[0]
        for other in indexes[1:]:
            index = index.intersection(other)
    else:
        index = union_indexes(indexes, sort=False)
        index = ensure_index(index)
    if sort:
        index = safe_sort_index(index)
    if copy:
        index = index.copy()
    return index","""""""Return the union or intersection of indexes.

Parameters
----------
indexes : list of Index or list objects
    When intersect=True, do not accept list of lists.
intersect : bool, default False
    If True, calculate the intersection between indexes. Otherwise,
    calculate the union.
sort : bool, default False
    Whether the result index should come out sorted or not.
copy : bool, default False
    If True, return a copy of the combined index.

Returns
-------
Index"""""""
pandas/core/indexes/api.py,"def safe_sort_index(index: Index) -> Index:
    if index.is_monotonic_increasing:
        return index
    try:
        array_sorted = safe_sort(index)
    except TypeError:
        pass
    else:
        if isinstance(array_sorted, Index):
            return array_sorted
        array_sorted = cast(np.ndarray, array_sorted)
        if isinstance(index, MultiIndex):
            index = MultiIndex.from_tuples(array_sorted, names=index.names)
        else:
            index = Index(array_sorted, name=index.name, dtype=index.dtype)
    return index","""""""Returns the sorted index

We keep the dtypes and the name attributes.

Parameters
----------
index : an Index

Returns
-------
Index"""""""
pandas/core/indexes/api.py,"def union_indexes(indexes, sort: bool | None=True) -> Index:
    if len(indexes) == 0:
        raise AssertionError('Must have at least 1 Index to union')
    if len(indexes) == 1:
        result = indexes[0]
        if isinstance(result, list):
            result = Index(sorted(result))
        return result
    (indexes, kind) = _sanitize_and_check(indexes)

    def _unique_indices(inds, dtype) -> Index:
        """"""
        Concatenate indices and remove duplicates.

        Parameters
        ----------
        inds : list of Index or list objects
        dtype : dtype to set for the resulting Index

        Returns
        -------
        Index
        """"""
        if all((isinstance(ind, Index) for ind in inds)):
            inds = [ind.astype(dtype, copy=False) for ind in inds]
            result = inds[0].unique()
            other = inds[1].append(inds[2:])
            diff = other[result.get_indexer_for(other) == -1]
            if len(diff):
                result = result.append(diff.unique())
            if sort:
                result = result.sort_values()
            return result

        def conv(i):
            if isinstance(i, Index):
                i = i.tolist()
            return i
        return Index(lib.fast_unique_multiple_list([conv(i) for i in inds], sort=sort), dtype=dtype)

    def _find_common_index_dtype(inds):
        """"""
        Finds a common type for the indexes to pass through to resulting index.

        Parameters
        ----------
        inds: list of Index or list objects

        Returns
        -------
        The common type or None if no indexes were given
        """"""
        dtypes = [idx.dtype for idx in indexes if isinstance(idx, Index)]
        if dtypes:
            dtype = find_common_type(dtypes)
        else:
            dtype = None
        return dtype
    if kind == 'special':
        result = indexes[0]
        dtis = [x for x in indexes if isinstance(x, DatetimeIndex)]
        dti_tzs = [x for x in dtis if x.tz is not None]
        if len(dti_tzs) not in [0, len(dtis)]:
            raise TypeError('Cannot join tz-naive with tz-aware DatetimeIndex')
        if len(dtis) == len(indexes):
            result = indexes[0]
        elif len(dtis) > 1:
            sort = False
            indexes = [x.astype(object, copy=False) for x in indexes]
            result = indexes[0]
        for other in indexes[1:]:
            result = result.union(other, sort=None if sort else False)
        return result
    elif kind == 'array':
        dtype = _find_common_index_dtype(indexes)
        index = indexes[0]
        if not all((index.equals(other) for other in indexes[1:])):
            index = _unique_indices(indexes, dtype)
        name = get_unanimous_names(*indexes)[0]
        if name != index.name:
            index = index.rename(name)
        return index
    else:
        dtype = _find_common_index_dtype(indexes)
        return _unique_indices(indexes, dtype)","""""""Return the union of indexes.

The behavior of sort and names is not consistent.

Parameters
----------
indexes : list of Index or list objects
sort : bool, default True
    Whether the result index should come out sorted or not.

Returns
-------
Index"""""""
pandas/core/indexes/api.py,"def _sanitize_and_check(indexes):
    kinds = list({type(index) for index in indexes})
    if list in kinds:
        if len(kinds) > 1:
            indexes = [Index(list(x)) if not isinstance(x, Index) else x for x in indexes]
            kinds.remove(list)
        else:
            return (indexes, 'list')
    if len(kinds) > 1 or Index not in kinds:
        return (indexes, 'special')
    else:
        return (indexes, 'array')","""""""Verify the type of indexes and convert lists to Index.

Cases:

- [list, list, ...]: Return ([list, list, ...], 'list')
- [list, Index, ...]: Return _sanitize_and_check([Index, Index, ...])
    Lists are sorted and converted to Index.
- [Index, Index, ...]: Return ([Index, Index, ...], TYPE)
    TYPE = 'special' if at least one special type, 'array' otherwise.

Parameters
----------
indexes : list of Index or list objects

Returns
-------
sanitized_indexes : list of Index or list objects
type : {'list', 'array', 'special'}"""""""
pandas/core/indexes/api.py,"def all_indexes_same(indexes) -> bool:
    itr = iter(indexes)
    first = next(itr)
    return all((first.equals(index) for index in itr))","""""""Determine if all indexes contain the same elements.

Parameters
----------
indexes : iterable of Index objects

Returns
-------
bool
    True if all indexes contain the same elements, False otherwise."""""""
pandas/core/indexes/base.py,"def _maybe_return_indexers(meth: F) -> F:

    @functools.wraps(meth)
    def join(self, other: Index, *, how: JoinHow='left', level=None, return_indexers: bool=False, sort: bool=False):
        (join_index, lidx, ridx) = meth(self, other, how=how, level=level, sort=sort)
        if not return_indexers:
            return join_index
        if lidx is not None:
            lidx = ensure_platform_int(lidx)
        if ridx is not None:
            ridx = ensure_platform_int(ridx)
        return (join_index, lidx, ridx)
    return cast(F, join)","""""""Decorator to simplify 'return_indexers' checks in Index.join."""""""
pandas/core/indexes/base.py,"def _new_Index(cls, d):
    if issubclass(cls, ABCPeriodIndex):
        from pandas.core.indexes.period import _new_PeriodIndex
        return _new_PeriodIndex(cls, **d)
    if issubclass(cls, ABCMultiIndex):
        if 'labels' in d and 'codes' not in d:
            d['codes'] = d.pop('labels')
        d['verify_integrity'] = False
    elif 'dtype' not in d and 'data' in d:
        d['dtype'] = d['data'].dtype
    return cls.__new__(cls, **d)","""""""This is called upon unpickling, rather than the default which doesn't
have arguments and breaks __new__."""""""
pandas/core/indexes/base.py,"def ensure_index_from_sequences(sequences, names=None) -> Index:
    from pandas.core.indexes.multi import MultiIndex
    if len(sequences) == 1:
        if names is not None:
            names = names[0]
        return Index(sequences[0], name=names)
    else:
        return MultiIndex.from_arrays(sequences, names=names)","""""""Construct an index from sequences of data.

A single sequence returns an Index. Many sequences returns a
MultiIndex.

Parameters
----------
sequences : sequence of sequences
names : sequence of str

Returns
-------
index : Index or MultiIndex

Examples
--------
>>> ensure_index_from_sequences([[1, 2, 3]], names=[""name""])
Index([1, 2, 3], dtype='int64', name='name')

>>> ensure_index_from_sequences([[""a"", ""a""], [""a"", ""b""]], names=[""L1"", ""L2""])
MultiIndex([('a', 'a'),
            ('a', 'b')],
           names=['L1', 'L2'])

See Also
--------
ensure_index"""""""
pandas/core/indexes/base.py,"def ensure_index(index_like: Axes, copy: bool=False) -> Index:
    if isinstance(index_like, Index):
        if copy:
            index_like = index_like.copy()
        return index_like
    if isinstance(index_like, ABCSeries):
        name = index_like.name
        return Index(index_like, name=name, copy=copy)
    if is_iterator(index_like):
        index_like = list(index_like)
    if isinstance(index_like, list):
        if type(index_like) is not list:
            index_like = list(index_like)
        if len(index_like) and lib.is_all_arraylike(index_like):
            from pandas.core.indexes.multi import MultiIndex
            return MultiIndex.from_arrays(index_like)
        else:
            return Index(index_like, copy=copy, tupleize_cols=False)
    else:
        return Index(index_like, copy=copy)","""""""Ensure that we have an index from some index-like object.

Parameters
----------
index_like : sequence
    An Index or other sequence
copy : bool, default False

Returns
-------
index : Index or MultiIndex

See Also
--------
ensure_index_from_sequences

Examples
--------
>>> ensure_index(['a', 'b'])
Index(['a', 'b'], dtype='object')

>>> ensure_index([('a', 'a'),  ('b', 'c')])
Index([('a', 'a'), ('b', 'c')], dtype='object')

>>> ensure_index([['a', 'a'], ['b', 'c']])
MultiIndex([('a', 'b'),
        ('a', 'c')],
       )"""""""
pandas/core/indexes/base.py,"def ensure_has_len(seq):
    try:
        len(seq)
    except TypeError:
        return list(seq)
    else:
        return seq","""""""If seq is an iterator, put its values into a list."""""""
pandas/core/indexes/base.py,"def trim_front(strings: list[str]) -> list[str]:
    if not strings:
        return strings
    while all(strings) and all((x[0] == ' ' for x in strings)):
        strings = [x[1:] for x in strings]
    return strings","""""""Trims zeros and decimal points.

Examples
--------
>>> trim_front(["" a"", "" b""])
['a', 'b']

>>> trim_front(["" a"", "" ""])
['a', '']"""""""
pandas/core/indexes/base.py,"def maybe_extract_name(name, obj, cls) -> Hashable:
    if name is None and isinstance(obj, (Index, ABCSeries)):
        name = obj.name
    if not is_hashable(name):
        raise TypeError(f'{cls.__name__}.name must be a hashable type')
    return name","""""""If no name is passed, then extract it from data, validating hashability."""""""
pandas/core/indexes/base.py,"def get_unanimous_names(*indexes: Index) -> tuple[Hashable, ...]:
    name_tups = [tuple(i.names) for i in indexes]
    name_sets = [{*ns} for ns in zip_longest(*name_tups)]
    names = tuple((ns.pop() if len(ns) == 1 else None for ns in name_sets))
    return names","""""""Return common name if all indices agree, otherwise None (level-by-level).

Parameters
----------
indexes : list of Index objects

Returns
-------
list
    A list representing the unanimous 'names' found."""""""
pandas/core/indexes/base.py,"def _unpack_nested_dtype(other: Index) -> Index:
    dtype = other.dtype
    if isinstance(dtype, CategoricalDtype):
        return dtype.categories
    elif isinstance(dtype, ArrowDtype):
        import pyarrow as pa
        if pa.types.is_dictionary(dtype.pyarrow_dtype):
            other = other.astype(ArrowDtype(dtype.pyarrow_dtype.value_type))
    return other","""""""When checking if our dtype is comparable with another, we need
to unpack CategoricalDtype to look at its categories.dtype.

Parameters
----------
other : Index

Returns
-------
Index"""""""
pandas/core/indexes/datetimes.py,"def _new_DatetimeIndex(cls, d):
    if 'data' in d and (not isinstance(d['data'], DatetimeIndex)):
        data = d.pop('data')
        if not isinstance(data, DatetimeArray):
            tz = d.pop('tz')
            freq = d.pop('freq')
            dta = DatetimeArray._simple_new(data, dtype=tz_to_dtype(tz), freq=freq)
        else:
            dta = data
            for key in ['tz', 'freq']:
                if key in d:
                    assert d[key] == getattr(dta, key)
                    d.pop(key)
        result = cls._simple_new(dta, **d)
    else:
        with warnings.catch_warnings():
            warnings.simplefilter('ignore')
            result = cls.__new__(cls, **d)
    return result","""""""This is called upon unpickling, rather than the default which doesn't
have arguments and breaks __new__"""""""
pandas/core/indexes/datetimes.py,"def date_range(start=None, end=None, periods=None, freq=None, tz=None, normalize: bool=False, name: Hashable | None=None, inclusive: IntervalClosedType='both', *, unit: str | None=None, **kwargs) -> DatetimeIndex:
    if freq is None and com.any_none(periods, start, end):
        freq = 'D'
    dtarr = DatetimeArray._generate_range(start=start, end=end, periods=periods, freq=freq, tz=tz, normalize=normalize, inclusive=inclusive, unit=unit, **kwargs)
    return DatetimeIndex._simple_new(dtarr, name=name)","""""""Return a fixed frequency DatetimeIndex.

Returns the range of equally spaced time points (where the difference between any
two adjacent points is specified by the given frequency) such that they all
satisfy `start <[=] x <[=] end`, where the first one and the last one are, resp.,
the first and last time points in that range that fall on the boundary of ``freq``
(if given as a frequency string) or that are valid for ``freq`` (if given as a
:class:`pandas.tseries.offsets.DateOffset`). (If exactly one of ``start``,
``end``, or ``freq`` is *not* specified, this missing parameter can be computed
given ``periods``, the number of timesteps in the range. See the note below.)

Parameters
----------
start : str or datetime-like, optional
    Left bound for generating dates.
end : str or datetime-like, optional
    Right bound for generating dates.
periods : int, optional
    Number of periods to generate.
freq : str, Timedelta, datetime.timedelta, or DateOffset, default 'D'
    Frequency strings can have multiples, e.g. '5H'. See
    :ref:`here <timeseries.offset_aliases>` for a list of
    frequency aliases.
tz : str or tzinfo, optional
    Time zone name for returning localized DatetimeIndex, for example
    'Asia/Hong_Kong'. By default, the resulting DatetimeIndex is
    timezone-naive unless timezone-aware datetime-likes are passed.
normalize : bool, default False
    Normalize start/end dates to midnight before generating date range.
name : str, default None
    Name of the resulting DatetimeIndex.
inclusive : {""both"", ""neither"", ""left"", ""right""}, default ""both""
    Include boundaries; Whether to set each bound as closed or open.

    .. versionadded:: 1.4.0
unit : str, default None
    Specify the desired resolution of the result.

    .. versionadded:: 2.0.0
**kwargs
    For compatibility. Has no effect on the result.

Returns
-------
DatetimeIndex

See Also
--------
DatetimeIndex : An immutable container for datetimes.
timedelta_range : Return a fixed frequency TimedeltaIndex.
period_range : Return a fixed frequency PeriodIndex.
interval_range : Return a fixed frequency IntervalIndex.

Notes
-----
Of the four parameters ``start``, ``end``, ``periods``, and ``freq``,
exactly three must be specified. If ``freq`` is omitted, the resulting
``DatetimeIndex`` will have ``periods`` linearly spaced elements between
``start`` and ``end`` (closed on both sides).

To learn more about the frequency strings, please see `this link
<https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#offset-aliases>`__.

Examples
--------
**Specifying the values**

The next four examples generate the same `DatetimeIndex`, but vary
the combination of `start`, `end` and `periods`.

Specify `start` and `end`, with the default daily frequency.

>>> pd.date_range(start='1/1/2018', end='1/08/2018')
DatetimeIndex(['2018-01-01', '2018-01-02', '2018-01-03', '2018-01-04',
               '2018-01-05', '2018-01-06', '2018-01-07', '2018-01-08'],
              dtype='datetime64[ns]', freq='D')

Specify timezone-aware `start` and `end`, with the default daily frequency.

>>> pd.date_range(
...     start=pd.to_datetime(""1/1/2018"").tz_localize(""Europe/Berlin""),
...     end=pd.to_datetime(""1/08/2018"").tz_localize(""Europe/Berlin""),
... )
DatetimeIndex(['2018-01-01 00:00:00+01:00', '2018-01-02 00:00:00+01:00',
               '2018-01-03 00:00:00+01:00', '2018-01-04 00:00:00+01:00',
               '2018-01-05 00:00:00+01:00', '2018-01-06 00:00:00+01:00',
               '2018-01-07 00:00:00+01:00', '2018-01-08 00:00:00+01:00'],
              dtype='datetime64[ns, Europe/Berlin]', freq='D')

Specify `start` and `periods`, the number of periods (days).

>>> pd.date_range(start='1/1/2018', periods=8)
DatetimeIndex(['2018-01-01', '2018-01-02', '2018-01-03', '2018-01-04',
               '2018-01-05', '2018-01-06', '2018-01-07', '2018-01-08'],
              dtype='datetime64[ns]', freq='D')

Specify `end` and `periods`, the number of periods (days).

>>> pd.date_range(end='1/1/2018', periods=8)
DatetimeIndex(['2017-12-25', '2017-12-26', '2017-12-27', '2017-12-28',
               '2017-12-29', '2017-12-30', '2017-12-31', '2018-01-01'],
              dtype='datetime64[ns]', freq='D')

Specify `start`, `end`, and `periods`; the frequency is generated
automatically (linearly spaced).

>>> pd.date_range(start='2018-04-24', end='2018-04-27', periods=3)
DatetimeIndex(['2018-04-24 00:00:00', '2018-04-25 12:00:00',
               '2018-04-27 00:00:00'],
              dtype='datetime64[ns]', freq=None)

**Other Parameters**

Changed the `freq` (frequency) to ``'ME'`` (month end frequency).

>>> pd.date_range(start='1/1/2018', periods=5, freq='ME')
DatetimeIndex(['2018-01-31', '2018-02-28', '2018-03-31', '2018-04-30',
               '2018-05-31'],
              dtype='datetime64[ns]', freq='ME')

Multiples are allowed

>>> pd.date_range(start='1/1/2018', periods=5, freq='3ME')
DatetimeIndex(['2018-01-31', '2018-04-30', '2018-07-31', '2018-10-31',
               '2019-01-31'],
              dtype='datetime64[ns]', freq='3ME')

`freq` can also be specified as an Offset object.

>>> pd.date_range(start='1/1/2018', periods=5, freq=pd.offsets.MonthEnd(3))
DatetimeIndex(['2018-01-31', '2018-04-30', '2018-07-31', '2018-10-31',
               '2019-01-31'],
              dtype='datetime64[ns]', freq='3ME')

Specify `tz` to set the timezone.

>>> pd.date_range(start='1/1/2018', periods=5, tz='Asia/Tokyo')
DatetimeIndex(['2018-01-01 00:00:00+09:00', '2018-01-02 00:00:00+09:00',
               '2018-01-03 00:00:00+09:00', '2018-01-04 00:00:00+09:00',
               '2018-01-05 00:00:00+09:00'],
              dtype='datetime64[ns, Asia/Tokyo]', freq='D')

`inclusive` controls whether to include `start` and `end` that are on the
boundary. The default, ""both"", includes boundary points on either end.

>>> pd.date_range(start='2017-01-01', end='2017-01-04', inclusive=""both"")
DatetimeIndex(['2017-01-01', '2017-01-02', '2017-01-03', '2017-01-04'],
              dtype='datetime64[ns]', freq='D')

Use ``inclusive='left'`` to exclude `end` if it falls on the boundary.

>>> pd.date_range(start='2017-01-01', end='2017-01-04', inclusive='left')
DatetimeIndex(['2017-01-01', '2017-01-02', '2017-01-03'],
              dtype='datetime64[ns]', freq='D')

Use ``inclusive='right'`` to exclude `start` if it falls on the boundary, and
similarly ``inclusive='neither'`` will exclude both `start` and `end`.

>>> pd.date_range(start='2017-01-01', end='2017-01-04', inclusive='right')
DatetimeIndex(['2017-01-02', '2017-01-03', '2017-01-04'],
              dtype='datetime64[ns]', freq='D')

**Specify a unit**

>>> pd.date_range(start=""2017-01-01"", periods=10, freq=""100AS"", unit=""s"")
DatetimeIndex(['2017-01-01', '2117-01-01', '2217-01-01', '2317-01-01',
               '2417-01-01', '2517-01-01', '2617-01-01', '2717-01-01',
               '2817-01-01', '2917-01-01'],
              dtype='datetime64[s]', freq='100AS-JAN')"""""""
pandas/core/indexes/datetimes.py,"def bdate_range(start=None, end=None, periods: int | None=None, freq: Frequency | dt.timedelta='B', tz=None, normalize: bool=True, name: Hashable | None=None, weekmask=None, holidays=None, inclusive: IntervalClosedType='both', **kwargs) -> DatetimeIndex:
    if freq is None:
        msg = 'freq must be specified for bdate_range; use date_range instead'
        raise TypeError(msg)
    if isinstance(freq, str) and freq.startswith('C'):
        try:
            weekmask = weekmask or 'Mon Tue Wed Thu Fri'
            freq = prefix_mapping[freq](holidays=holidays, weekmask=weekmask)
        except (KeyError, TypeError) as err:
            msg = f'invalid custom frequency string: {freq}'
            raise ValueError(msg) from err
    elif holidays or weekmask:
        msg = f'a custom frequency string is required when holidays or weekmask are passed, got frequency {freq}'
        raise ValueError(msg)
    return date_range(start=start, end=end, periods=periods, freq=freq, tz=tz, normalize=normalize, name=name, inclusive=inclusive, **kwargs)","""""""Return a fixed frequency DatetimeIndex with business day as the default.

Parameters
----------
start : str or datetime-like, default None
    Left bound for generating dates.
end : str or datetime-like, default None
    Right bound for generating dates.
periods : int, default None
    Number of periods to generate.
freq : str, Timedelta, datetime.timedelta, or DateOffset, default 'B'
    Frequency strings can have multiples, e.g. '5H'. The default is
    business daily ('B').
tz : str or None
    Time zone name for returning localized DatetimeIndex, for example
    Asia/Beijing.
normalize : bool, default False
    Normalize start/end dates to midnight before generating date range.
name : str, default None
    Name of the resulting DatetimeIndex.
weekmask : str or None, default None
    Weekmask of valid business days, passed to ``numpy.busdaycalendar``,
    only used when custom frequency strings are passed.  The default
    value None is equivalent to 'Mon Tue Wed Thu Fri'.
holidays : list-like or None, default None
    Dates to exclude from the set of valid business days, passed to
    ``numpy.busdaycalendar``, only used when custom frequency strings
    are passed.
inclusive : {""both"", ""neither"", ""left"", ""right""}, default ""both""
    Include boundaries; Whether to set each bound as closed or open.

    .. versionadded:: 1.4.0
**kwargs
    For compatibility. Has no effect on the result.

Returns
-------
DatetimeIndex

Notes
-----
Of the four parameters: ``start``, ``end``, ``periods``, and ``freq``,
exactly three must be specified.  Specifying ``freq`` is a requirement
for ``bdate_range``.  Use ``date_range`` if specifying ``freq`` is not
desired.

To learn more about the frequency strings, please see `this link
<https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#offset-aliases>`__.

Examples
--------
Note how the two weekend days are skipped in the result.

>>> pd.bdate_range(start='1/1/2018', end='1/08/2018')
DatetimeIndex(['2018-01-01', '2018-01-02', '2018-01-03', '2018-01-04',
           '2018-01-05', '2018-01-08'],
          dtype='datetime64[ns]', freq='B')"""""""
pandas/core/indexes/extension.py,"def _inherit_from_data(name: str, delegate: type, cache: bool=False, wrap: bool=False):
    attr = getattr(delegate, name)
    if isinstance(attr, property) or type(attr).__name__ == 'getset_descriptor':
        if cache:

            def cached(self):
                return getattr(self._data, name)
            cached.__name__ = name
            cached.__doc__ = attr.__doc__
            method = cache_readonly(cached)
        else:

            def fget(self):
                result = getattr(self._data, name)
                if wrap:
                    if isinstance(result, type(self._data)):
                        return type(self)._simple_new(result, name=self.name)
                    elif isinstance(result, ABCDataFrame):
                        return result.set_index(self)
                    return Index(result, name=self.name)
                return result

            def fset(self, value) -> None:
                setattr(self._data, name, value)
            fget.__name__ = name
            fget.__doc__ = attr.__doc__
            method = property(fget, fset)
    elif not callable(attr):
        method = attr
    else:

        def method(self, *args, **kwargs):
            if 'inplace' in kwargs:
                raise ValueError(f'cannot use inplace with {type(self).__name__}')
            result = attr(self._data, *args, **kwargs)
            if wrap:
                if isinstance(result, type(self._data)):
                    return type(self)._simple_new(result, name=self.name)
                elif isinstance(result, ABCDataFrame):
                    return result.set_index(self)
                return Index(result, name=self.name)
            return result
        method.__name__ = name
        method.__doc__ = attr.__doc__
    return method","""""""Make an alias for a method of the underlying ExtensionArray.

Parameters
----------
name : str
    Name of an attribute the class should inherit from its EA parent.
delegate : class
cache : bool, default False
    Whether to convert wrapped properties into cache_readonly
wrap : bool, default False
    Whether to wrap the inherited result in an Index.

Returns
-------
attribute, method, property, or cache_readonly"""""""
pandas/core/indexes/extension.py,"def inherit_names(names: list[str], delegate: type, cache: bool=False, wrap: bool=False) -> Callable[[type[_ExtensionIndexT]], type[_ExtensionIndexT]]:

    def wrapper(cls: type[_ExtensionIndexT]) -> type[_ExtensionIndexT]:
        for name in names:
            meth = _inherit_from_data(name, delegate, cache=cache, wrap=wrap)
            setattr(cls, name, meth)
        return cls
    return wrapper","""""""Class decorator to pin attributes from an ExtensionArray to a Index subclass.

Parameters
----------
names : List[str]
delegate : class
cache : bool, default False
wrap : bool, default False
    Whether to wrap the inherited result in an Index."""""""
pandas/core/indexes/interval.py,"def _new_IntervalIndex(cls, d):
    return cls.from_arrays(**d)","""""""This is called upon unpickling, rather than the default which doesn't have
arguments and breaks __new__."""""""
pandas/core/indexes/interval.py,"def _is_valid_endpoint(endpoint) -> bool:
    return any([is_number(endpoint), isinstance(endpoint, Timestamp), isinstance(endpoint, Timedelta), endpoint is None])","""""""Helper for interval_range to check if start/end are valid types."""""""
pandas/core/indexes/interval.py,"def _is_type_compatible(a, b) -> bool:
    is_ts_compat = lambda x: isinstance(x, (Timestamp, BaseOffset))
    is_td_compat = lambda x: isinstance(x, (Timedelta, BaseOffset))
    return is_number(a) and is_number(b) or (is_ts_compat(a) and is_ts_compat(b)) or (is_td_compat(a) and is_td_compat(b)) or com.any_none(a, b)","""""""Helper for interval_range to check type compat of start/end/freq."""""""
pandas/core/indexes/interval.py,"def interval_range(start=None, end=None, periods=None, freq=None, name: Hashable | None=None, closed: IntervalClosedType='right') -> IntervalIndex:
    start = maybe_box_datetimelike(start)
    end = maybe_box_datetimelike(end)
    endpoint = start if start is not None else end
    if freq is None and com.any_none(periods, start, end):
        freq = 1 if is_number(endpoint) else 'D'
    if com.count_not_none(start, end, periods, freq) != 3:
        raise ValueError('Of the four parameters: start, end, periods, and freq, exactly three must be specified')
    if not _is_valid_endpoint(start):
        raise ValueError(f'start must be numeric or datetime-like, got {start}')
    if not _is_valid_endpoint(end):
        raise ValueError(f'end must be numeric or datetime-like, got {end}')
    if is_float(periods):
        periods = int(periods)
    elif not is_integer(periods) and periods is not None:
        raise TypeError(f'periods must be a number, got {periods}')
    if freq is not None and (not is_number(freq)):
        try:
            freq = to_offset(freq)
        except ValueError as err:
            raise ValueError(f'freq must be numeric or convertible to DateOffset, got {freq}') from err
    if not all([_is_type_compatible(start, end), _is_type_compatible(start, freq), _is_type_compatible(end, freq)]):
        raise TypeError('start, end, freq need to be type compatible')
    if periods is not None:
        periods += 1
    breaks: np.ndarray | TimedeltaIndex | DatetimeIndex
    if is_number(endpoint):
        if com.all_not_none(start, end, freq):
            breaks = np.arange(start, end + freq * 0.1, freq)
        else:
            if periods is None:
                periods = int((end - start) // freq) + 1
            elif start is None:
                start = end - (periods - 1) * freq
            elif end is None:
                end = start + (periods - 1) * freq
            breaks = np.linspace(start, end, periods)
        if all((is_integer(x) for x in com.not_none(start, end, freq))):
            breaks = maybe_downcast_numeric(breaks, np.dtype('int64'))
    elif isinstance(endpoint, Timestamp):
        breaks = date_range(start=start, end=end, periods=periods, freq=freq)
    else:
        breaks = timedelta_range(start=start, end=end, periods=periods, freq=freq)
    return IntervalIndex.from_breaks(breaks, name=name, closed=closed)","""""""Return a fixed frequency IntervalIndex.

Parameters
----------
start : numeric or datetime-like, default None
    Left bound for generating intervals.
end : numeric or datetime-like, default None
    Right bound for generating intervals.
periods : int, default None
    Number of periods to generate.
freq : numeric, str, Timedelta, datetime.timedelta, or DateOffset, default None
    The length of each interval. Must be consistent with the type of start
    and end, e.g. 2 for numeric, or '5H' for datetime-like.  Default is 1
    for numeric and 'D' for datetime-like.
name : str, default None
    Name of the resulting IntervalIndex.
closed : {'left', 'right', 'both', 'neither'}, default 'right'
    Whether the intervals are closed on the left-side, right-side, both
    or neither.

Returns
-------
IntervalIndex

See Also
--------
IntervalIndex : An Index of intervals that are all closed on the same side.

Notes
-----
Of the four parameters ``start``, ``end``, ``periods``, and ``freq``,
exactly three must be specified. If ``freq`` is omitted, the resulting
``IntervalIndex`` will have ``periods`` linearly spaced elements between
``start`` and ``end``, inclusively.

To learn more about datetime-like frequency strings, please see `this link
<https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#offset-aliases>`__.

Examples
--------
Numeric ``start`` and  ``end`` is supported.

>>> pd.interval_range(start=0, end=5)
IntervalIndex([(0, 1], (1, 2], (2, 3], (3, 4], (4, 5]],
              dtype='interval[int64, right]')

Additionally, datetime-like input is also supported.

>>> pd.interval_range(start=pd.Timestamp('2017-01-01'),
...                   end=pd.Timestamp('2017-01-04'))
IntervalIndex([(2017-01-01 00:00:00, 2017-01-02 00:00:00],
               (2017-01-02 00:00:00, 2017-01-03 00:00:00],
               (2017-01-03 00:00:00, 2017-01-04 00:00:00]],
              dtype='interval[datetime64[ns], right]')

The ``freq`` parameter specifies the frequency between the left and right.
endpoints of the individual intervals within the ``IntervalIndex``.  For
numeric ``start`` and ``end``, the frequency must also be numeric.

>>> pd.interval_range(start=0, periods=4, freq=1.5)
IntervalIndex([(0.0, 1.5], (1.5, 3.0], (3.0, 4.5], (4.5, 6.0]],
              dtype='interval[float64, right]')

Similarly, for datetime-like ``start`` and ``end``, the frequency must be
convertible to a DateOffset.

>>> pd.interval_range(start=pd.Timestamp('2017-01-01'),
...                   periods=3, freq='MS')
IntervalIndex([(2017-01-01 00:00:00, 2017-02-01 00:00:00],
               (2017-02-01 00:00:00, 2017-03-01 00:00:00],
               (2017-03-01 00:00:00, 2017-04-01 00:00:00]],
              dtype='interval[datetime64[ns], right]')

Specify ``start``, ``end``, and ``periods``; the frequency is generated
automatically (linearly spaced).

>>> pd.interval_range(start=0, end=6, periods=4)
IntervalIndex([(0.0, 1.5], (1.5, 3.0], (3.0, 4.5], (4.5, 6.0]],
          dtype='interval[float64, right]')

The ``closed`` parameter specifies which endpoints of the individual
intervals within the ``IntervalIndex`` are closed.

>>> pd.interval_range(end=5, periods=4, closed='both')
IntervalIndex([[1, 2], [2, 3], [3, 4], [4, 5]],
              dtype='interval[int64, both]')"""""""
pandas/core/indexes/multi.py,"def names_compat(meth: F) -> F:

    @wraps(meth)
    def new_meth(self_or_cls, *args, **kwargs):
        if 'name' in kwargs and 'names' in kwargs:
            raise TypeError('Can only provide one of `names` and `name`')
        if 'name' in kwargs:
            kwargs['names'] = kwargs.pop('name')
        return meth(self_or_cls, *args, **kwargs)
    return cast(F, new_meth)","""""""A decorator to allow either `name` or `names` keyword but not both.

This makes it easier to share code with base class."""""""
pandas/core/indexes/multi.py,"def _lexsort_depth(codes: list[np.ndarray], nlevels: int) -> int:
    int64_codes = [ensure_int64(level_codes) for level_codes in codes]
    for k in range(nlevels, 0, -1):
        if libalgos.is_lexsorted(int64_codes[:k]):
            return k
    return 0","""""""Count depth (up to a maximum of `nlevels`) with which codes are lexsorted."""""""
pandas/core/indexes/multi.py,"def maybe_droplevels(index: Index, key) -> Index:
    original_index = index
    if isinstance(key, tuple):
        for _ in key:
            try:
                index = index._drop_level_numbers([0])
            except ValueError:
                return original_index
    else:
        try:
            index = index._drop_level_numbers([0])
        except ValueError:
            pass
    return index","""""""Attempt to drop level or levels from the given index.

Parameters
----------
index: Index
key : scalar or tuple

Returns
-------
Index"""""""
pandas/core/indexes/multi.py,"def _coerce_indexer_frozen(array_like, categories, copy: bool=False) -> np.ndarray:
    array_like = coerce_indexer_dtype(array_like, categories)
    if copy:
        array_like = array_like.copy()
    array_like.flags.writeable = False
    return array_like","""""""Coerce the array-like indexer to the smallest integer dtype that can encode all
of the given categories.

Parameters
----------
array_like : array-like
categories : array-like
copy : bool

Returns
-------
np.ndarray
    Non-writeable."""""""
pandas/core/indexes/multi.py,"def _require_listlike(level, arr, arrname: str):
    if level is not None and (not is_list_like(level)):
        if not is_list_like(arr):
            raise TypeError(f'{arrname} must be list-like')
        if len(arr) > 0 and is_list_like(arr[0]):
            raise TypeError(f'{arrname} must be list-like')
        level = [level]
        arr = [arr]
    elif level is None or is_list_like(level):
        if not is_list_like(arr) or not is_list_like(arr[0]):
            raise TypeError(f'{arrname} must be list of lists-like')
    return (level, arr)","""""""Ensure that level is either None or listlike, and arr is list-of-listlike."""""""
pandas/core/indexes/period.py,"def period_range(start=None, end=None, periods: int | None=None, freq=None, name: Hashable | None=None) -> PeriodIndex:
    if com.count_not_none(start, end, periods) != 2:
        raise ValueError('Of the three parameters: start, end, and periods, exactly two must be specified')
    if freq is None and (not isinstance(start, Period) and (not isinstance(end, Period))):
        freq = 'D'
    (data, freq) = PeriodArray._generate_range(start, end, periods, freq, fields={})
    dtype = PeriodDtype(freq)
    data = PeriodArray(data, dtype=dtype)
    return PeriodIndex(data, name=name)","""""""Return a fixed frequency PeriodIndex.

The day (calendar) is the default frequency.

Parameters
----------
start : str, datetime, date, pandas.Timestamp, or period-like, default None
    Left bound for generating periods.
end : str, datetime, date, pandas.Timestamp, or period-like, default None
    Right bound for generating periods.
periods : int, default None
    Number of periods to generate.
freq : str or DateOffset, optional
    Frequency alias. By default the freq is taken from `start` or `end`
    if those are Period objects. Otherwise, the default is ``""D""`` for
    daily frequency.
name : str, default None
    Name of the resulting PeriodIndex.

Returns
-------
PeriodIndex

Notes
-----
Of the three parameters: ``start``, ``end``, and ``periods``, exactly two
must be specified.

To learn more about the frequency strings, please see `this link
<https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#offset-aliases>`__.

Examples
--------
>>> pd.period_range(start='2017-01-01', end='2018-01-01', freq='M')
PeriodIndex(['2017-01', '2017-02', '2017-03', '2017-04', '2017-05', '2017-06',
         '2017-07', '2017-08', '2017-09', '2017-10', '2017-11', '2017-12',
         '2018-01'],
        dtype='period[M]')

If ``start`` or ``end`` are ``Period`` objects, they will be used as anchor
endpoints for a ``PeriodIndex`` with frequency matching that of the
``period_range`` constructor.

>>> pd.period_range(start=pd.Period('2017Q1', freq='Q'),
...                 end=pd.Period('2017Q2', freq='Q'), freq='M')
PeriodIndex(['2017-03', '2017-04', '2017-05', '2017-06'],
            dtype='period[M]')"""""""
pandas/core/indexes/timedeltas.py,"def timedelta_range(start=None, end=None, periods: int | None=None, freq=None, name=None, closed=None, *, unit: str | None=None) -> TimedeltaIndex:
    if freq is None and com.any_none(periods, start, end):
        freq = 'D'
    (freq, _) = dtl.maybe_infer_freq(freq)
    tdarr = TimedeltaArray._generate_range(start, end, periods, freq, closed=closed, unit=unit)
    return TimedeltaIndex._simple_new(tdarr, name=name)","""""""Return a fixed frequency TimedeltaIndex with day as the default.

Parameters
----------
start : str or timedelta-like, default None
    Left bound for generating timedeltas.
end : str or timedelta-like, default None
    Right bound for generating timedeltas.
periods : int, default None
    Number of periods to generate.
freq : str, Timedelta, datetime.timedelta, or DateOffset, default 'D'
    Frequency strings can have multiples, e.g. '5H'.
name : str, default None
    Name of the resulting TimedeltaIndex.
closed : str, default None
    Make the interval closed with respect to the given frequency to
    the 'left', 'right', or both sides (None).
unit : str, default None
    Specify the desired resolution of the result.

    .. versionadded:: 2.0.0

Returns
-------
TimedeltaIndex

Notes
-----
Of the four parameters ``start``, ``end``, ``periods``, and ``freq``,
exactly three must be specified. If ``freq`` is omitted, the resulting
``TimedeltaIndex`` will have ``periods`` linearly spaced elements between
``start`` and ``end`` (closed on both sides).

To learn more about the frequency strings, please see `this link
<https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#offset-aliases>`__.

Examples
--------
>>> pd.timedelta_range(start='1 day', periods=4)
TimedeltaIndex(['1 days', '2 days', '3 days', '4 days'],
               dtype='timedelta64[ns]', freq='D')

The ``closed`` parameter specifies which endpoint is included.  The default
behavior is to include both endpoints.

>>> pd.timedelta_range(start='1 day', periods=4, closed='right')
TimedeltaIndex(['2 days', '3 days', '4 days'],
               dtype='timedelta64[ns]', freq='D')

The ``freq`` parameter specifies the frequency of the TimedeltaIndex.
Only fixed frequencies can be passed, non-fixed frequencies such as
'M' (month end) will raise.

>>> pd.timedelta_range(start='1 day', end='2 days', freq='6H')
TimedeltaIndex(['1 days 00:00:00', '1 days 06:00:00', '1 days 12:00:00',
                '1 days 18:00:00', '2 days 00:00:00'],
               dtype='timedelta64[ns]', freq='6H')

Specify ``start``, ``end``, and ``periods``; the frequency is generated
automatically (linearly spaced).

>>> pd.timedelta_range(start='1 day', end='5 days', periods=4)
TimedeltaIndex(['1 days 00:00:00', '2 days 08:00:00', '3 days 16:00:00',
                '5 days 00:00:00'],
               dtype='timedelta64[ns]', freq=None)

**Specify a unit**

>>> pd.timedelta_range(""1 Day"", periods=3, freq=""100000D"", unit=""s"")
TimedeltaIndex(['1 days 00:00:00', '100001 days 00:00:00',
                '200001 days 00:00:00'],
               dtype='timedelta64[s]', freq='100000D')"""""""
pandas/core/indexing.py,"def _tuplify(ndim: int, loc: Hashable) -> tuple[Hashable | slice, ...]:
    _tup: list[Hashable | slice]
    _tup = [slice(None, None) for _ in range(ndim)]
    _tup[0] = loc
    return tuple(_tup)","""""""Given an indexer for the first dimension, create an equivalent tuple
for indexing over all dimensions.

Parameters
----------
ndim : int
loc : object

Returns
-------
tuple"""""""
pandas/core/indexing.py,"def _tupleize_axis_indexer(ndim: int, axis: AxisInt, key) -> tuple:
    new_key = [slice(None)] * ndim
    new_key[axis] = key
    return tuple(new_key)","""""""If we have an axis, adapt the given key to be axis-independent."""""""
pandas/core/indexing.py,"def check_bool_indexer(index: Index, key) -> np.ndarray:
    result = key
    if isinstance(key, ABCSeries) and (not key.index.equals(index)):
        indexer = result.index.get_indexer_for(index)
        if -1 in indexer:
            raise IndexingError('Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).')
        result = result.take(indexer)
        if not isinstance(result.dtype, ExtensionDtype):
            return result.astype(bool)._values
    if is_object_dtype(key):
        result = np.asarray(result, dtype=bool)
    elif not is_array_like(result):
        result = pd_array(result, dtype=bool)
    return check_array_indexer(index, result)","""""""Check if key is a valid boolean indexer for an object with such index and
perform reindexing or conversion if needed.

This function assumes that is_bool_indexer(key) == True.

Parameters
----------
index : Index
    Index of the object on which the indexing is done.
key : list-like
    Boolean indexer to check.

Returns
-------
np.array
    Resulting key.

Raises
------
IndexError
    If the key does not have the same length as index.
IndexingError
    If the index of the key is unalignable to index."""""""
pandas/core/indexing.py,"def convert_missing_indexer(indexer):
    if isinstance(indexer, dict):
        indexer = indexer['key']
        if isinstance(indexer, bool):
            raise KeyError('cannot use a single bool to index into setitem')
        return (indexer, True)
    return (indexer, False)","""""""Reverse convert a missing indexer, which is a dict
return the scalar indexer and a boolean indicating if we converted"""""""
pandas/core/indexing.py,"def convert_from_missing_indexer_tuple(indexer, axes):

    def get_indexer(_i, _idx):
        return axes[_i].get_loc(_idx['key']) if isinstance(_idx, dict) else _idx
    return tuple((get_indexer(_i, _idx) for (_i, _idx) in enumerate(indexer)))","""""""Create a filtered indexer that doesn't have any missing indexers."""""""
pandas/core/indexing.py,"def maybe_convert_ix(*args):
    for arg in args:
        if not isinstance(arg, (np.ndarray, list, ABCSeries, Index)):
            return args
    return np.ix_(*args)","""""""We likely want to take the cross-product."""""""
pandas/core/indexing.py,"def is_nested_tuple(tup, labels) -> bool:
    if not isinstance(tup, tuple):
        return False
    for k in tup:
        if is_list_like(k) or isinstance(k, slice):
            return isinstance(labels, MultiIndex)
    return False","""""""Returns
-------
bool"""""""
pandas/core/indexing.py,"def is_label_like(key) -> bool:
    return not isinstance(key, slice) and (not is_list_like_indexer(key)) and (key is not Ellipsis)","""""""Returns
-------
bool"""""""
pandas/core/indexing.py,"def need_slice(obj: slice) -> bool:
    return obj.start is not None or obj.stop is not None or (obj.step is not None and obj.step != 1)","""""""Returns
-------
bool"""""""
pandas/core/indexing.py,"def check_dict_or_set_indexers(key) -> None:
    if isinstance(key, set) or (isinstance(key, tuple) and any((isinstance(x, set) for x in key))):
        raise TypeError('Passing a set as an indexer is not supported. Use a list instead.')
    if isinstance(key, dict) or (isinstance(key, tuple) and any((isinstance(x, dict) for x in key))):
        raise TypeError('Passing a dict as an indexer is not supported. Use a list instead.')","""""""Check if the indexer is or contains a dict or set, which is no longer allowed."""""""
pandas/core/interchange/from_dataframe.py,"def from_dataframe(df, allow_copy: bool=True) -> pd.DataFrame:
    if isinstance(df, pd.DataFrame):
        return df
    if not hasattr(df, '__dataframe__'):
        raise ValueError('`df` does not support __dataframe__')
    return _from_dataframe(df.__dataframe__(allow_copy=allow_copy), allow_copy=allow_copy)","""""""Build a ``pd.DataFrame`` from any DataFrame supporting the interchange protocol.

Parameters
----------
df : DataFrameXchg
    Object supporting the interchange protocol, i.e. `__dataframe__` method.
allow_copy : bool, default: True
    Whether to allow copying the memory to perform the conversion
    (if false then zero-copy approach is requested).

Returns
-------
pd.DataFrame

Examples
--------
>>> df_not_necessarily_pandas = pd.DataFrame({'A': [1, 2], 'B': [3, 4]})
>>> interchange_object = df_not_necessarily_pandas.__dataframe__()
>>> interchange_object.column_names()
Index(['A', 'B'], dtype='object')
>>> df_pandas = (pd.api.interchange.from_dataframe
...              (interchange_object.select_columns_by_name(['A'])))
>>> df_pandas
     A
0    1
1    2

These methods (``column_names``, ``select_columns_by_name``) should work
for any dataframe library which implements the interchange protocol."""""""
pandas/core/interchange/from_dataframe.py,"def _from_dataframe(df: DataFrameXchg, allow_copy: bool=True):
    pandas_dfs = []
    for chunk in df.get_chunks():
        pandas_df = protocol_df_chunk_to_pandas(chunk)
        pandas_dfs.append(pandas_df)
    if not allow_copy and len(pandas_dfs) > 1:
        raise RuntimeError('To join chunks a copy is required which is forbidden by allow_copy=False')
    if not pandas_dfs:
        pandas_df = protocol_df_chunk_to_pandas(df)
    elif len(pandas_dfs) == 1:
        pandas_df = pandas_dfs[0]
    else:
        pandas_df = pd.concat(pandas_dfs, axis=0, ignore_index=True, copy=False)
    index_obj = df.metadata.get('pandas.index', None)
    if index_obj is not None:
        pandas_df.index = index_obj
    return pandas_df","""""""Build a ``pd.DataFrame`` from the DataFrame interchange object.

Parameters
----------
df : DataFrameXchg
    Object supporting the interchange protocol, i.e. `__dataframe__` method.
allow_copy : bool, default: True
    Whether to allow copying the memory to perform the conversion
    (if false then zero-copy approach is requested).

Returns
-------
pd.DataFrame"""""""
pandas/core/interchange/from_dataframe.py,"def protocol_df_chunk_to_pandas(df: DataFrameXchg) -> pd.DataFrame:
    columns: dict[str, Any] = {}
    buffers = []
    for name in df.column_names():
        if not isinstance(name, str):
            raise ValueError(f'Column {name} is not a string')
        if name in columns:
            raise ValueError(f'Column {name} is not unique')
        col = df.get_column_by_name(name)
        dtype = col.dtype[0]
        if dtype in (DtypeKind.INT, DtypeKind.UINT, DtypeKind.FLOAT, DtypeKind.BOOL):
            (columns[name], buf) = primitive_column_to_ndarray(col)
        elif dtype == DtypeKind.CATEGORICAL:
            (columns[name], buf) = categorical_column_to_series(col)
        elif dtype == DtypeKind.STRING:
            (columns[name], buf) = string_column_to_ndarray(col)
        elif dtype == DtypeKind.DATETIME:
            (columns[name], buf) = datetime_column_to_ndarray(col)
        else:
            raise NotImplementedError(f'Data type {dtype} not handled yet')
        buffers.append(buf)
    pandas_df = pd.DataFrame(columns)
    pandas_df.attrs['_INTERCHANGE_PROTOCOL_BUFFERS'] = buffers
    return pandas_df","""""""Convert interchange protocol chunk to ``pd.DataFrame``.

Parameters
----------
df : DataFrameXchg

Returns
-------
pd.DataFrame"""""""
pandas/core/interchange/from_dataframe.py,"def primitive_column_to_ndarray(col: Column) -> tuple[np.ndarray, Any]:
    buffers = col.get_buffers()
    (data_buff, data_dtype) = buffers['data']
    data = buffer_to_ndarray(data_buff, data_dtype, offset=col.offset, length=col.size())
    data = set_nulls(data, col, buffers['validity'])
    return (data, buffers)","""""""Convert a column holding one of the primitive dtypes to a NumPy array.

A primitive type is one of: int, uint, float, bool.

Parameters
----------
col : Column

Returns
-------
tuple
    Tuple of np.ndarray holding the data and the memory owner object
    that keeps the memory alive."""""""
pandas/core/interchange/from_dataframe.py,"def categorical_column_to_series(col: Column) -> tuple[pd.Series, Any]:
    categorical = col.describe_categorical
    if not categorical['is_dictionary']:
        raise NotImplementedError('Non-dictionary categoricals not supported yet')
    cat_column = categorical['categories']
    if hasattr(cat_column, '_col'):
        categories = np.array(cat_column._col)
    else:
        raise NotImplementedError(""Interchanging categorical columns isn't supported yet, and our fallback of using the `col._col` attribute (a ndarray) failed."")
    buffers = col.get_buffers()
    (codes_buff, codes_dtype) = buffers['data']
    codes = buffer_to_ndarray(codes_buff, codes_dtype, offset=col.offset, length=col.size())
    if len(categories) > 0:
        values = categories[codes % len(categories)]
    else:
        values = codes
    cat = pd.Categorical(values, categories=categories, ordered=categorical['is_ordered'])
    data = pd.Series(cat)
    data = set_nulls(data, col, buffers['validity'])
    return (data, buffers)","""""""Convert a column holding categorical data to a pandas Series.

Parameters
----------
col : Column

Returns
-------
tuple
    Tuple of pd.Series holding the data and the memory owner object
    that keeps the memory alive."""""""
pandas/core/interchange/from_dataframe.py,"def string_column_to_ndarray(col: Column) -> tuple[np.ndarray, Any]:
    (null_kind, sentinel_val) = col.describe_null
    if null_kind not in (ColumnNullType.NON_NULLABLE, ColumnNullType.USE_BITMASK, ColumnNullType.USE_BYTEMASK):
        raise NotImplementedError(f'{null_kind} null kind is not yet supported for string columns.')
    buffers = col.get_buffers()
    assert buffers['offsets'], 'String buffers must contain offsets'
    (data_buff, protocol_data_dtype) = buffers['data']
    assert protocol_data_dtype[1] == 8
    assert protocol_data_dtype[2] in (ArrowCTypes.STRING, ArrowCTypes.LARGE_STRING)
    data_dtype = (DtypeKind.UINT, 8, ArrowCTypes.UINT8, Endianness.NATIVE)
    data = buffer_to_ndarray(data_buff, data_dtype, offset=0, length=data_buff.bufsize)
    (offset_buff, offset_dtype) = buffers['offsets']
    offsets = buffer_to_ndarray(offset_buff, offset_dtype, offset=col.offset, length=col.size() + 1)
    null_pos = None
    if null_kind in (ColumnNullType.USE_BITMASK, ColumnNullType.USE_BYTEMASK):
        assert buffers['validity'], 'Validity buffers cannot be empty for masks'
        (valid_buff, valid_dtype) = buffers['validity']
        null_pos = buffer_to_ndarray(valid_buff, valid_dtype, offset=col.offset, length=col.size())
        if sentinel_val == 0:
            null_pos = ~null_pos
    str_list: list[None | float | str] = [None] * col.size()
    for i in range(col.size()):
        if null_pos is not None and null_pos[i]:
            str_list[i] = np.nan
            continue
        units = data[offsets[i]:offsets[i + 1]]
        str_bytes = bytes(units)
        string = str_bytes.decode(encoding='utf-8')
        str_list[i] = string
    return (np.asarray(str_list, dtype='object'), buffers)","""""""Convert a column holding string data to a NumPy array.

Parameters
----------
col : Column

Returns
-------
tuple
    Tuple of np.ndarray holding the data and the memory owner object
    that keeps the memory alive."""""""
pandas/core/interchange/from_dataframe.py,"def parse_datetime_format_str(format_str, data) -> pd.Series | np.ndarray:
    timestamp_meta = re.match('ts([smun]):(.*)', format_str)
    if timestamp_meta:
        (unit, tz) = (timestamp_meta.group(1), timestamp_meta.group(2))
        if unit != 's':
            unit += 's'
        data = data.astype(f'datetime64[{unit}]')
        if tz != '':
            data = pd.Series(data).dt.tz_localize('UTC').dt.tz_convert(tz)
        return data
    date_meta = re.match('td([Dm])', format_str)
    if date_meta:
        unit = date_meta.group(1)
        if unit == 'D':
            data = (data.astype(np.uint64) * (24 * 60 * 60)).astype('datetime64[s]')
        elif unit == 'm':
            data = data.astype('datetime64[ms]')
        else:
            raise NotImplementedError(f'Date unit is not supported: {unit}')
        return data
    raise NotImplementedError(f'DateTime kind is not supported: {format_str}')","""""""Parse datetime `format_str` to interpret the `data`."""""""
pandas/core/interchange/from_dataframe.py,"def datetime_column_to_ndarray(col: Column) -> tuple[np.ndarray | pd.Series, Any]:
    buffers = col.get_buffers()
    (_, _, format_str, _) = col.dtype
    (dbuf, dtype) = buffers['data']
    data = buffer_to_ndarray(dbuf, (DtypeKind.UINT, dtype[1], getattr(ArrowCTypes, f'UINT{dtype[1]}'), Endianness.NATIVE), offset=col.offset, length=col.size())
    data = parse_datetime_format_str(format_str, data)
    data = set_nulls(data, col, buffers['validity'])
    return (data, buffers)","""""""Convert a column holding DateTime data to a NumPy array.

Parameters
----------
col : Column

Returns
-------
tuple
    Tuple of np.ndarray holding the data and the memory owner object
    that keeps the memory alive."""""""
pandas/core/interchange/from_dataframe.py,"def buffer_to_ndarray(buffer: Buffer, dtype: tuple[DtypeKind, int, str, str], *, length: int, offset: int=0) -> np.ndarray:
    (kind, bit_width, _, _) = dtype
    column_dtype = _NP_DTYPES.get(kind, {}).get(bit_width, None)
    if column_dtype is None:
        raise NotImplementedError(f'Conversion for {dtype} is not yet supported.')
    ctypes_type = np.ctypeslib.as_ctypes_type(column_dtype)
    if bit_width == 1:
        assert length is not None, '`length` must be specified for a bit-mask buffer.'
        pa = import_optional_dependency('pyarrow')
        arr = pa.BooleanArray.from_buffers(pa.bool_(), length, [None, pa.foreign_buffer(buffer.ptr, length)], offset=offset)
        return np.asarray(arr)
    else:
        data_pointer = ctypes.cast(buffer.ptr + offset * bit_width // 8, ctypes.POINTER(ctypes_type))
        if length > 0:
            return np.ctypeslib.as_array(data_pointer, shape=(length,))
        return np.array([], dtype=ctypes_type)","""""""Build a NumPy array from the passed buffer.

Parameters
----------
buffer : Buffer
    Buffer to build a NumPy array from.
dtype : tuple
    Data type of the buffer conforming protocol dtypes format.
offset : int, default: 0
    Number of elements to offset from the start of the buffer.
length : int, optional
    If the buffer is a bit-mask, specifies a number of bits to read
    from the buffer. Has no effect otherwise.

Returns
-------
np.ndarray

Notes
-----
The returned array doesn't own the memory. The caller of this function is
responsible for keeping the memory owner object alive as long as
the returned NumPy array is being used."""""""
pandas/core/interchange/from_dataframe.py,"def set_nulls(data: np.ndarray | pd.Series, col: Column, validity: tuple[Buffer, tuple[DtypeKind, int, str, str]] | None, allow_modify_inplace: bool=True):
    (null_kind, sentinel_val) = col.describe_null
    null_pos = None
    if null_kind == ColumnNullType.USE_SENTINEL:
        null_pos = pd.Series(data) == sentinel_val
    elif null_kind in (ColumnNullType.USE_BITMASK, ColumnNullType.USE_BYTEMASK):
        assert validity, 'Expected to have a validity buffer for the mask'
        (valid_buff, valid_dtype) = validity
        null_pos = buffer_to_ndarray(valid_buff, valid_dtype, offset=col.offset, length=col.size())
        if sentinel_val == 0:
            null_pos = ~null_pos
    elif null_kind in (ColumnNullType.NON_NULLABLE, ColumnNullType.USE_NAN):
        pass
    else:
        raise NotImplementedError(f'Null kind {null_kind} is not yet supported.')
    if null_pos is not None and np.any(null_pos):
        if not allow_modify_inplace:
            data = data.copy()
        try:
            data[null_pos] = None
        except TypeError:
            data = data.astype(float)
            data[null_pos] = None
        except SettingWithCopyError:
            data = data.copy()
            data[null_pos] = None
    return data","""""""Set null values for the data according to the column null kind.

Parameters
----------
data : np.ndarray or pd.Series
    Data to set nulls in.
col : Column
    Column object that describes the `data`.
validity : tuple(Buffer, dtype) or None
    The return value of ``col.buffers()``. We do not access the ``col.buffers()``
    here to not take the ownership of the memory of buffer objects.
allow_modify_inplace : bool, default: True
    Whether to modify the `data` inplace when zero-copy is possible (True) or always
    modify a copy of the `data` (False).

Returns
-------
np.ndarray or pd.Series
    Data with the nulls being set."""""""
pandas/core/interchange/utils.py,"def dtype_to_arrow_c_fmt(dtype: DtypeObj) -> str:
    if isinstance(dtype, CategoricalDtype):
        return ArrowCTypes.INT64
    elif dtype == np.dtype('O'):
        return ArrowCTypes.STRING
    elif isinstance(dtype, ArrowDtype):
        import pyarrow as pa
        pa_type = dtype.pyarrow_dtype
        if pa.types.is_decimal(pa_type):
            return f'd:{pa_type.precision},{pa_type.scale}'
        elif pa.types.is_timestamp(pa_type) and pa_type.tz is not None:
            return f'ts{pa_type.unit[0]}:{pa_type.tz}'
        format_str = PYARROW_CTYPES.get(str(pa_type), None)
        if format_str is not None:
            return format_str
    format_str = getattr(ArrowCTypes, dtype.name.upper(), None)
    if format_str is not None:
        return format_str
    if lib.is_np_dtype(dtype, 'M'):
        resolution = np.datetime_data(dtype)[0][0]
        return ArrowCTypes.TIMESTAMP.format(resolution=resolution, tz='')
    elif isinstance(dtype, DatetimeTZDtype):
        return ArrowCTypes.TIMESTAMP.format(resolution=dtype.unit[0], tz=dtype.tz)
    raise NotImplementedError(f'Conversion of {dtype} to Arrow C format string is not implemented.')","""""""Represent pandas `dtype` as a format string in Apache Arrow C notation.

Parameters
----------
dtype : np.dtype
    Datatype of pandas DataFrame to represent.

Returns
-------
str
    Format string in Apache Arrow C notation of the given `dtype`."""""""
pandas/core/internals/api.py,"def make_block(values, placement, klass=None, ndim=None, dtype: Dtype | None=None) -> Block:
    if dtype is not None:
        dtype = pandas_dtype(dtype)
    (values, dtype) = extract_pandas_array(values, dtype, ndim)
    if klass is ExtensionBlock and isinstance(values.dtype, PeriodDtype):
        klass = None
    if klass is None:
        dtype = dtype or values.dtype
        klass = get_block_type(dtype)
    elif klass is DatetimeTZBlock and (not isinstance(values.dtype, DatetimeTZDtype)):
        values = DatetimeArray._simple_new(values, dtype=dtype)
    if not isinstance(placement, BlockPlacement):
        placement = BlockPlacement(placement)
    ndim = maybe_infer_ndim(values, placement, ndim)
    if isinstance(values.dtype, (PeriodDtype, DatetimeTZDtype)):
        values = extract_array(values, extract_numpy=True)
        values = ensure_block_shape(values, ndim)
    check_ndim(values, placement, ndim)
    values = maybe_coerce_values(values)
    return klass(values, ndim=ndim, placement=placement)","""""""This is a pseudo-public analogue to blocks.new_block.

We ask that downstream libraries use this rather than any fully-internal
APIs, including but not limited to:

- core.internals.blocks.make_block
- Block.make_block
- Block.make_block_same_class
- Block.__init__"""""""
pandas/core/internals/api.py,"def maybe_infer_ndim(values, placement: BlockPlacement, ndim: int | None) -> int:
    if ndim is None:
        if not isinstance(values.dtype, np.dtype):
            if len(placement) != 1:
                ndim = 1
            else:
                ndim = 2
        else:
            ndim = values.ndim
    return ndim","""""""If `ndim` is not provided, infer it from placement and values."""""""
pandas/core/internals/array_manager.py,"def concat_arrays(to_concat: list) -> ArrayLike:
    to_concat_no_proxy = [x for x in to_concat if not isinstance(x, NullArrayProxy)]
    dtypes = {x.dtype for x in to_concat_no_proxy}
    single_dtype = len(dtypes) == 1
    if single_dtype:
        target_dtype = to_concat_no_proxy[0].dtype
    elif all((lib.is_np_dtype(x, 'iub') for x in dtypes)):
        target_dtype = np_find_common_type(*dtypes)
    else:
        target_dtype = find_common_type([arr.dtype for arr in to_concat_no_proxy])
    to_concat = [arr.to_array(target_dtype) if isinstance(arr, NullArrayProxy) else astype_array(arr, target_dtype, copy=False) for arr in to_concat]
    if isinstance(to_concat[0], ExtensionArray):
        cls = type(to_concat[0])
        return cls._concat_same_type(to_concat)
    result = np.concatenate(to_concat)
    if len(result) == 0:
        kinds = {obj.dtype.kind for obj in to_concat_no_proxy}
        if len(kinds) != 1:
            if 'b' in kinds:
                result = result.astype(object)
    return result","""""""Alternative for concat_compat but specialized for use in the ArrayManager.

Differences: only deals with 1D arrays (no axis keyword), assumes
ensure_wrapped_if_datetimelike and does not skip empty arrays to determine
the dtype.
In addition ensures that all NullArrayProxies get replaced with actual
arrays.

Parameters
----------
to_concat : list of arrays

Returns
-------
np.ndarray or ExtensionArray"""""""
pandas/core/internals/base.py,"def interleaved_dtype(dtypes: list[DtypeObj]) -> DtypeObj | None:
    if not len(dtypes):
        return None
    return find_common_type(dtypes)","""""""Find the common dtype for `blocks`.

Parameters
----------
blocks : List[DtypeObj]

Returns
-------
dtype : np.dtype, ExtensionDtype, or None
    None is returned when `blocks` is empty."""""""
pandas/core/internals/blocks.py,"def maybe_split(meth: F) -> F:

    @wraps(meth)
    def newfunc(self, *args, **kwargs) -> list[Block]:
        if self.ndim == 1 or self.shape[0] == 1:
            return meth(self, *args, **kwargs)
        else:
            return self.split_and_operate(meth, *args, **kwargs)
    return cast(F, newfunc)","""""""If we have a multi-column block, split and operate block-wise.  Otherwise
use the original method."""""""
pandas/core/internals/blocks.py,"def maybe_coerce_values(values: ArrayLike) -> ArrayLike:
    if isinstance(values, np.ndarray):
        values = ensure_wrapped_if_datetimelike(values)
        if issubclass(values.dtype.type, str):
            values = np.array(values, dtype=object)
    if isinstance(values, (DatetimeArray, TimedeltaArray)) and values.freq is not None:
        values = values._with_freq(None)
    return values","""""""Input validation for values passed to __init__. Ensure that
any datetime64/timedelta64 dtypes are in nanoseconds.  Ensure
that we do not have string dtypes.

Parameters
----------
values : np.ndarray or ExtensionArray

Returns
-------
values : np.ndarray or ExtensionArray"""""""
pandas/core/internals/blocks.py,"def get_block_type(dtype: DtypeObj) -> type[Block]:
    if isinstance(dtype, DatetimeTZDtype):
        return DatetimeTZBlock
    elif isinstance(dtype, PeriodDtype):
        return NDArrayBackedExtensionBlock
    elif isinstance(dtype, ExtensionDtype):
        return ExtensionBlock
    kind = dtype.kind
    if kind in 'Mm':
        return DatetimeLikeBlock
    return NumpyBlock","""""""Find the appropriate Block subclass to use for the given values and dtype.

Parameters
----------
dtype : numpy or pandas dtype

Returns
-------
cls : class, subclass of Block"""""""
pandas/core/internals/blocks.py,"def check_ndim(values, placement: BlockPlacement, ndim: int) -> None:
    if values.ndim > ndim:
        raise ValueError(f'Wrong number of dimensions. values.ndim > ndim [{values.ndim} > {ndim}]')
    if not is_1d_only_ea_dtype(values.dtype):
        if values.ndim != ndim:
            raise ValueError(f'Wrong number of dimensions. values.ndim != ndim [{values.ndim} != {ndim}]')
        if len(placement) != len(values):
            raise ValueError(f'Wrong number of items passed {len(values)}, placement implies {len(placement)}')
    elif ndim == 2 and len(placement) != 1:
        raise ValueError('need to split')","""""""ndim inference and validation.

Validates that values.ndim and ndim are consistent.
Validates that len(values) and len(placement) are consistent.

Parameters
----------
values : array-like
placement : BlockPlacement
ndim : int

Raises
------
ValueError : the number of dimensions do not match"""""""
pandas/core/internals/blocks.py,"def extract_pandas_array(values: ArrayLike, dtype: DtypeObj | None, ndim: int) -> tuple[ArrayLike, DtypeObj | None]:
    if isinstance(values, ABCNumpyExtensionArray):
        values = values.to_numpy()
        if ndim and ndim > 1:
            values = np.atleast_2d(values)
    if isinstance(dtype, NumpyEADtype):
        dtype = dtype.numpy_dtype
    return (values, dtype)","""""""Ensure that we don't allow NumpyExtensionArray / NumpyEADtype in internals."""""""
pandas/core/internals/blocks.py,"def extend_blocks(result, blocks=None) -> list[Block]:
    if blocks is None:
        blocks = []
    if isinstance(result, list):
        for r in result:
            if isinstance(r, list):
                blocks.extend(r)
            else:
                blocks.append(r)
    else:
        assert isinstance(result, Block), type(result)
        blocks.append(result)
    return blocks","""""""return a new extended blocks, given the result"""""""
pandas/core/internals/blocks.py,"def ensure_block_shape(values: ArrayLike, ndim: int=1) -> ArrayLike:
    if values.ndim < ndim:
        if not is_1d_only_ea_dtype(values.dtype):
            values = cast('np.ndarray | DatetimeArray | TimedeltaArray', values)
            values = values.reshape(1, -1)
    return values","""""""Reshape if possible to have values.ndim == ndim."""""""
pandas/core/internals/blocks.py,"def to_native_types(values: ArrayLike, *, na_rep: str='nan', quoting=None, float_format=None, decimal: str='.', **kwargs) -> npt.NDArray[np.object_]:
    if isinstance(values, Categorical) and values.categories.dtype.kind in 'Mm':
        values = algos.take_nd(values.categories._values, ensure_platform_int(values._codes), fill_value=na_rep)
    values = ensure_wrapped_if_datetimelike(values)
    if isinstance(values, (DatetimeArray, TimedeltaArray)):
        if values.ndim == 1:
            result = values._format_native_types(na_rep=na_rep, **kwargs)
            result = result.astype(object, copy=False)
            return result
        results_converted = []
        for i in range(len(values)):
            result = values[i, :]._format_native_types(na_rep=na_rep, **kwargs)
            results_converted.append(result.astype(object, copy=False))
        return np.vstack(results_converted)
    elif values.dtype.kind == 'f' and (not isinstance(values.dtype, SparseDtype)):
        if float_format is None and decimal == '.':
            mask = isna(values)
            if not quoting:
                values = values.astype(str)
            else:
                values = np.array(values, dtype='object')
            values[mask] = na_rep
            values = values.astype(object, copy=False)
            return values
        from pandas.io.formats.format import FloatArrayFormatter
        formatter = FloatArrayFormatter(values, na_rep=na_rep, float_format=float_format, decimal=decimal, quoting=quoting, fixed_width=False)
        res = formatter.get_result_as_array()
        res = res.astype(object, copy=False)
        return res
    elif isinstance(values, ExtensionArray):
        mask = isna(values)
        new_values = np.asarray(values.astype(object))
        new_values[mask] = na_rep
        return new_values
    else:
        mask = isna(values)
        itemsize = writers.word_len(na_rep)
        if values.dtype != _dtype_obj and (not quoting) and itemsize:
            values = values.astype(str)
            if values.dtype.itemsize / np.dtype('U1').itemsize < itemsize:
                values = values.astype(f'<U{itemsize}')
        else:
            values = np.array(values, dtype='object')
        values[mask] = na_rep
        values = values.astype(object, copy=False)
        return values","""""""convert to our native types format"""""""
pandas/core/internals/blocks.py,"def external_values(values: ArrayLike) -> ArrayLike:
    if isinstance(values, (PeriodArray, IntervalArray)):
        return values.astype(object)
    elif isinstance(values, (DatetimeArray, TimedeltaArray)):
        values = values._ndarray
    if isinstance(values, np.ndarray) and using_copy_on_write():
        values = values.view()
        values.flags.writeable = False
    return values","""""""The array that Series.values returns (public attribute).

This has some historical constraints, and is overridden in block
subclasses to return the correct array (e.g. period returns
object ndarray and datetimetz a datetime64[ns] ndarray instead of
proper extension array)."""""""
pandas/core/internals/concat.py,"def _concatenate_array_managers(mgrs: list[ArrayManager], axes: list[Index], concat_axis: AxisInt) -> Manager2D:
    if concat_axis == 1:
        return mgrs[0].concat_vertical(mgrs, axes)
    else:
        assert concat_axis == 0
        return mgrs[0].concat_horizontal(mgrs, axes)","""""""Concatenate array managers into one.

Parameters
----------
mgrs_indexers : list of (ArrayManager, {axis: indexer,...}) tuples
axes : list of Index
concat_axis : int

Returns
-------
ArrayManager"""""""
pandas/core/internals/concat.py,"def concatenate_managers(mgrs_indexers, axes: list[Index], concat_axis: AxisInt, copy: bool) -> Manager2D:
    needs_copy = copy and concat_axis == 0
    if isinstance(mgrs_indexers[0][0], ArrayManager):
        mgrs = _maybe_reindex_columns_na_proxy(axes, mgrs_indexers, needs_copy)
        return _concatenate_array_managers(mgrs, axes, concat_axis)
    if concat_axis == 0:
        mgrs = _maybe_reindex_columns_na_proxy(axes, mgrs_indexers, needs_copy)
        return mgrs[0].concat_horizontal(mgrs, axes)
    if len(mgrs_indexers) > 0 and mgrs_indexers[0][0].nblocks > 0:
        first_dtype = mgrs_indexers[0][0].blocks[0].dtype
        if first_dtype in [np.float64, np.float32]:
            if all((_is_homogeneous_mgr(mgr, first_dtype) for (mgr, _) in mgrs_indexers)) and len(mgrs_indexers) > 1:
                shape = tuple((len(x) for x in axes))
                nb = _concat_homogeneous_fastpath(mgrs_indexers, shape, first_dtype)
                return BlockManager((nb,), axes)
    mgrs = _maybe_reindex_columns_na_proxy(axes, mgrs_indexers, needs_copy)
    if len(mgrs) == 1:
        mgr = mgrs[0]
        out = mgr.copy(deep=False)
        out.axes = axes
        return out
    concat_plan = _get_combined_plan(mgrs)
    blocks = []
    values: ArrayLike
    for (placement, join_units) in concat_plan:
        unit = join_units[0]
        blk = unit.block
        if _is_uniform_join_units(join_units):
            vals = [ju.block.values for ju in join_units]
            if not blk.is_extension:
                values = np.concatenate(vals, axis=1)
            elif is_1d_only_ea_dtype(blk.dtype):
                values = concat_compat(vals, axis=0, ea_compat_axis=True)
                values = ensure_block_shape(values, ndim=2)
            else:
                values = concat_compat(vals, axis=1)
            values = ensure_wrapped_if_datetimelike(values)
            fastpath = blk.values.dtype == values.dtype
        else:
            values = _concatenate_join_units(join_units, copy=copy)
            fastpath = False
        if fastpath:
            b = blk.make_block_same_class(values, placement=placement)
        else:
            b = new_block_2d(values, placement=placement)
        blocks.append(b)
    return BlockManager(tuple(blocks), axes)","""""""Concatenate block managers into one.

Parameters
----------
mgrs_indexers : list of (BlockManager, {axis: indexer,...}) tuples
axes : list of Index
concat_axis : int
copy : bool

Returns
-------
BlockManager"""""""
pandas/core/internals/concat.py,"def _maybe_reindex_columns_na_proxy(axes: list[Index], mgrs_indexers: list[tuple[BlockManager, dict[int, np.ndarray]]], needs_copy: bool) -> list[BlockManager]:
    new_mgrs = []
    for (mgr, indexers) in mgrs_indexers:
        for (i, indexer) in indexers.items():
            mgr = mgr.reindex_indexer(axes[i], indexers[i], axis=i, copy=False, only_slice=True, allow_dups=True, use_na_proxy=True)
        if needs_copy and (not indexers):
            mgr = mgr.copy()
        new_mgrs.append(mgr)
    return new_mgrs","""""""Reindex along columns so that all of the BlockManagers being concatenated
have matching columns.

Columns added in this reindexing have dtype=np.void, indicating they
should be ignored when choosing a column's final dtype."""""""
pandas/core/internals/concat.py,"def _is_homogeneous_mgr(mgr: BlockManager, first_dtype: DtypeObj) -> bool:
    if mgr.nblocks != 1:
        return False
    blk = mgr.blocks[0]
    if not (blk.mgr_locs.is_slice_like and blk.mgr_locs.as_slice.step == 1):
        return False
    return blk.dtype == first_dtype","""""""Check if this Manager can be treated as a single ndarray."""""""
pandas/core/internals/concat.py,"def _concat_homogeneous_fastpath(mgrs_indexers, shape: Shape, first_dtype: np.dtype) -> Block:
    if all((not indexers for (_, indexers) in mgrs_indexers)):
        arrs = [mgr.blocks[0].values.T for (mgr, _) in mgrs_indexers]
        arr = np.concatenate(arrs).T
        bp = libinternals.BlockPlacement(slice(shape[0]))
        nb = new_block_2d(arr, bp)
        return nb
    arr = np.empty(shape, dtype=first_dtype)
    if first_dtype == np.float64:
        take_func = libalgos.take_2d_axis0_float64_float64
    else:
        take_func = libalgos.take_2d_axis0_float32_float32
    start = 0
    for (mgr, indexers) in mgrs_indexers:
        mgr_len = mgr.shape[1]
        end = start + mgr_len
        if 0 in indexers:
            take_func(mgr.blocks[0].values, indexers[0], arr[:, start:end])
        else:
            arr[:, start:end] = mgr.blocks[0].values
        start += mgr_len
    bp = libinternals.BlockPlacement(slice(shape[0]))
    nb = new_block_2d(arr, bp)
    return nb","""""""With single-Block managers with homogeneous dtypes (that can already hold nan),
we avoid [...]"""""""
pandas/core/internals/concat.py,"def _concatenate_join_units(join_units: list[JoinUnit], copy: bool) -> ArrayLike:
    (empty_dtype, empty_dtype_future) = _get_empty_dtype(join_units)
    has_none_blocks = any((unit.block.dtype.kind == 'V' for unit in join_units))
    upcasted_na = _dtype_to_na_value(empty_dtype, has_none_blocks)
    to_concat = [ju.get_reindexed_values(empty_dtype=empty_dtype, upcasted_na=upcasted_na) for ju in join_units]
    if any((is_1d_only_ea_dtype(t.dtype) for t in to_concat)):
        to_concat = [t if is_1d_only_ea_dtype(t.dtype) else t[0, :] for t in to_concat]
        concat_values = concat_compat(to_concat, axis=0, ea_compat_axis=True)
        concat_values = ensure_block_shape(concat_values, 2)
    else:
        concat_values = concat_compat(to_concat, axis=1)
    if empty_dtype != empty_dtype_future:
        if empty_dtype == concat_values.dtype:
            warnings.warn('The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.', FutureWarning, stacklevel=find_stack_level())
    return concat_values","""""""Concatenate values from several join units along axis=1."""""""
pandas/core/internals/concat.py,"def _dtype_to_na_value(dtype: DtypeObj, has_none_blocks: bool):
    if isinstance(dtype, ExtensionDtype):
        return dtype.na_value
    elif dtype.kind in 'mM':
        return dtype.type('NaT')
    elif dtype.kind in 'fc':
        return dtype.type('NaN')
    elif dtype.kind == 'b':
        return None
    elif dtype.kind in 'iu':
        if not has_none_blocks:
            return None
        return np.nan
    elif dtype.kind == 'O':
        return np.nan
    raise NotImplementedError","""""""Find the NA value to go with this dtype."""""""
pandas/core/internals/concat.py,"def _get_empty_dtype(join_units: Sequence[JoinUnit]) -> tuple[DtypeObj, DtypeObj]:
    if lib.dtypes_all_equal([ju.block.dtype for ju in join_units]):
        empty_dtype = join_units[0].block.dtype
        return (empty_dtype, empty_dtype)
    has_none_blocks = any((unit.block.dtype.kind == 'V' for unit in join_units))
    dtypes = [unit.block.dtype for unit in join_units if not unit.is_na]
    if not len(dtypes):
        dtypes = [unit.block.dtype for unit in join_units if unit.block.dtype.kind != 'V']
    dtype = find_common_type(dtypes)
    if has_none_blocks:
        dtype = ensure_dtype_can_hold_na(dtype)
    dtype_future = dtype
    if len(dtypes) != len(join_units):
        dtypes_future = [unit.block.dtype for unit in join_units if not unit.is_na_after_size_and_isna_all_deprecation]
        if not len(dtypes_future):
            dtypes_future = [unit.block.dtype for unit in join_units if unit.block.dtype.kind != 'V']
        if len(dtypes) != len(dtypes_future):
            dtype_future = find_common_type(dtypes_future)
            if has_none_blocks:
                dtype_future = ensure_dtype_can_hold_na(dtype_future)
    return (dtype, dtype_future)","""""""Return dtype and N/A values to use when concatenating specified units.

Returned N/A value may be None which means there was no casting involved.

Returns
-------
dtype"""""""
pandas/core/internals/concat.py,"def _is_uniform_join_units(join_units: list[JoinUnit]) -> bool:
    first = join_units[0].block
    if first.dtype.kind == 'V':
        return False
    return all((type(ju.block) is type(first) for ju in join_units)) and all((ju.block.dtype == first.dtype or ju.block.dtype.kind in 'iub' for ju in join_units)) and all((not ju.is_na or ju.block.is_extension for ju in join_units))","""""""Check if the join units consist of blocks of uniform type that can
be concatenated using Block.concat_same_type instead of the generic
_concatenate_join_units (which uses `concat_compat`)."""""""
pandas/core/internals/construction.py,"def arrays_to_mgr(arrays, columns: Index, index, *, dtype: DtypeObj | None=None, verify_integrity: bool=True, typ: str | None=None, consolidate: bool=True) -> Manager:
    if verify_integrity:
        if index is None:
            index = _extract_index(arrays)
        else:
            index = ensure_index(index)
        (arrays, refs) = _homogenize(arrays, index, dtype)
    else:
        index = ensure_index(index)
        arrays = [extract_array(x, extract_numpy=True) for x in arrays]
        refs = [None] * len(arrays)
        for arr in arrays:
            if not isinstance(arr, (np.ndarray, ExtensionArray)) or arr.ndim != 1 or len(arr) != len(index):
                raise ValueError('Arrays must be 1-dimensional np.ndarray or ExtensionArray with length matching len(index)')
    columns = ensure_index(columns)
    if len(columns) != len(arrays):
        raise ValueError('len(arrays) must match len(columns)')
    axes = [columns, index]
    if typ == 'block':
        return create_block_manager_from_column_arrays(arrays, axes, consolidate=consolidate, refs=refs)
    elif typ == 'array':
        return ArrayManager(arrays, [index, columns])
    else:
        raise ValueError(f""'typ' needs to be one of {{'block', 'array'}}, got '{typ}'"")","""""""Segregate Series based on type and coerce into matrices.

Needs to handle a lot of exceptional cases."""""""
pandas/core/internals/construction.py,"def rec_array_to_mgr(data: np.rec.recarray | np.ndarray, index, columns, dtype: DtypeObj | None, copy: bool, typ: str) -> Manager:
    fdata = ma.getdata(data)
    if index is None:
        index = default_index(len(fdata))
    else:
        index = ensure_index(index)
    if columns is not None:
        columns = ensure_index(columns)
    (arrays, arr_columns) = to_arrays(fdata, columns)
    (arrays, arr_columns) = reorder_arrays(arrays, arr_columns, columns, len(index))
    if columns is None:
        columns = arr_columns
    mgr = arrays_to_mgr(arrays, columns, index, dtype=dtype, typ=typ)
    if copy:
        mgr = mgr.copy()
    return mgr","""""""Extract from a masked rec array and create the manager."""""""
pandas/core/internals/construction.py,"def mgr_to_mgr(mgr, typ: str, copy: bool=True) -> Manager:
    new_mgr: Manager
    if typ == 'block':
        if isinstance(mgr, BlockManager):
            new_mgr = mgr
        elif mgr.ndim == 2:
            new_mgr = arrays_to_mgr(mgr.arrays, mgr.axes[0], mgr.axes[1], typ='block')
        else:
            new_mgr = SingleBlockManager.from_array(mgr.arrays[0], mgr.index)
    elif typ == 'array':
        if isinstance(mgr, ArrayManager):
            new_mgr = mgr
        elif mgr.ndim == 2:
            arrays = [mgr.iget_values(i) for i in range(len(mgr.axes[0]))]
            if copy:
                arrays = [arr.copy() for arr in arrays]
            new_mgr = ArrayManager(arrays, [mgr.axes[1], mgr.axes[0]])
        else:
            array = mgr.internal_values()
            if copy:
                array = array.copy()
            new_mgr = SingleArrayManager([array], [mgr.index])
    else:
        raise ValueError(f""'typ' needs to be one of {{'block', 'array'}}, got '{typ}'"")
    return new_mgr","""""""Convert to specific type of Manager. Does not copy if the type is already
correct. Does not guarantee a copy otherwise. `copy` keyword only controls
whether conversion from Block->ArrayManager copies the 1D arrays."""""""
pandas/core/internals/construction.py,"def _check_values_indices_shape_match(values: np.ndarray, index: Index, columns: Index) -> None:
    if values.shape[1] != len(columns) or values.shape[0] != len(index):
        if values.shape[0] == 0 < len(index):
            raise ValueError('Empty data passed with indices specified.')
        passed = values.shape
        implied = (len(index), len(columns))
        raise ValueError(f'Shape of passed values is {passed}, indices imply {implied}')","""""""Check that the shape implied by our axes matches the actual shape of the
data."""""""
pandas/core/internals/construction.py,"def dict_to_mgr(data: dict, index, columns, *, dtype: DtypeObj | None=None, typ: str='block', copy: bool=True) -> Manager:
    arrays: Sequence[Any] | Series
    if columns is not None:
        from pandas.core.series import Series
        arrays = Series(data, index=columns, dtype=object)
        missing = arrays.isna()
        if index is None:
            index = _extract_index(arrays[~missing])
        else:
            index = ensure_index(index)
        if missing.any() and (not is_integer_dtype(dtype)):
            nan_dtype: DtypeObj
            if dtype is not None:
                midxs = missing.values.nonzero()[0]
                for i in midxs:
                    arr = sanitize_array(arrays.iat[i], index, dtype=dtype)
                    arrays.iat[i] = arr
            else:
                nan_dtype = np.dtype('object')
                val = construct_1d_arraylike_from_scalar(np.nan, len(index), nan_dtype)
                nmissing = missing.sum()
                if copy:
                    rhs = [val] * nmissing
                else:
                    rhs = [val.copy() for _ in range(nmissing)]
                arrays.loc[missing] = rhs
        arrays = list(arrays)
        columns = ensure_index(columns)
    else:
        keys = list(data.keys())
        columns = Index(keys) if keys else default_index(0)
        arrays = [com.maybe_iterable_to_list(data[k]) for k in keys]
    if copy:
        if typ == 'block':
            arrays = [x.copy() if isinstance(x, ExtensionArray) else x.copy(deep=True) if isinstance(x, Index) or (isinstance(x, ABCSeries) and is_1d_only_ea_dtype(x.dtype)) else x for x in arrays]
        else:
            arrays = [x.copy() if hasattr(x, 'dtype') else x for x in arrays]
    return arrays_to_mgr(arrays, columns, index, dtype=dtype, typ=typ, consolidate=copy)","""""""Segregate Series based on type and coerce into matrices.
Needs to handle a lot of exceptional cases.

Used in DataFrame.__init__"""""""
pandas/core/internals/construction.py,"def nested_data_to_arrays(data: Sequence, columns: Index | None, index: Index | None, dtype: DtypeObj | None) -> tuple[list[ArrayLike], Index, Index]:
    if is_named_tuple(data[0]) and columns is None:
        columns = ensure_index(data[0]._fields)
    (arrays, columns) = to_arrays(data, columns, dtype=dtype)
    columns = ensure_index(columns)
    if index is None:
        if isinstance(data[0], ABCSeries):
            index = _get_names_from_index(data)
        else:
            index = default_index(len(data))
    return (arrays, columns, index)","""""""Convert a single sequence of arrays to multiple arrays."""""""
pandas/core/internals/construction.py,"def treat_as_nested(data) -> bool:
    return len(data) > 0 and is_list_like(data[0]) and (getattr(data[0], 'ndim', 1) == 1) and (not (isinstance(data, ExtensionArray) and data.ndim == 2))","""""""Check if we should use nested_data_to_arrays."""""""
pandas/core/internals/construction.py,"def _ensure_2d(values: np.ndarray) -> np.ndarray:
    if values.ndim == 1:
        values = values.reshape((values.shape[0], 1))
    elif values.ndim != 2:
        raise ValueError(f'Must pass 2-d input. shape={values.shape}')
    return values","""""""Reshape 1D values, raise on anything else other than 2D."""""""
pandas/core/internals/construction.py,"def _extract_index(data) -> Index:
    index: Index
    if len(data) == 0:
        return default_index(0)
    raw_lengths = []
    indexes: list[list[Hashable] | Index] = []
    have_raw_arrays = False
    have_series = False
    have_dicts = False
    for val in data:
        if isinstance(val, ABCSeries):
            have_series = True
            indexes.append(val.index)
        elif isinstance(val, dict):
            have_dicts = True
            indexes.append(list(val.keys()))
        elif is_list_like(val) and getattr(val, 'ndim', 1) == 1:
            have_raw_arrays = True
            raw_lengths.append(len(val))
        elif isinstance(val, np.ndarray) and val.ndim > 1:
            raise ValueError('Per-column arrays must each be 1-dimensional')
    if not indexes and (not raw_lengths):
        raise ValueError('If using all scalar values, you must pass an index')
    if have_series:
        index = union_indexes(indexes)
    elif have_dicts:
        index = union_indexes(indexes, sort=False)
    if have_raw_arrays:
        lengths = list(set(raw_lengths))
        if len(lengths) > 1:
            raise ValueError('All arrays must be of the same length')
        if have_dicts:
            raise ValueError('Mixing dicts with non-Series may lead to ambiguous ordering.')
        if have_series:
            if lengths[0] != len(index):
                msg = f'array length {lengths[0]} does not match index length {len(index)}'
                raise ValueError(msg)
        else:
            index = default_index(lengths[0])
    return ensure_index(index)","""""""Try to infer an Index from the passed data, raise ValueError on failure."""""""
pandas/core/internals/construction.py,"def reorder_arrays(arrays: list[ArrayLike], arr_columns: Index, columns: Index | None, length: int) -> tuple[list[ArrayLike], Index]:
    if columns is not None:
        if not columns.equals(arr_columns):
            new_arrays: list[ArrayLike] = []
            indexer = arr_columns.get_indexer(columns)
            for (i, k) in enumerate(indexer):
                if k == -1:
                    arr = np.empty(length, dtype=object)
                    arr.fill(np.nan)
                else:
                    arr = arrays[k]
                new_arrays.append(arr)
            arrays = new_arrays
            arr_columns = columns
    return (arrays, arr_columns)","""""""Pre-emptively (cheaply) reindex arrays with new columns."""""""
pandas/core/internals/construction.py,"def dataclasses_to_dicts(data):
    from dataclasses import asdict
    return list(map(asdict, data))","""""""Converts a list of dataclass instances to a list of dictionaries.

Parameters
----------
data : List[Type[dataclass]]

Returns
--------
list_dict : List[dict]

Examples
--------
>>> from dataclasses import dataclass
>>> @dataclass
... class Point:
...     x: int
...     y: int

>>> dataclasses_to_dicts([Point(1, 2), Point(2, 3)])
[{'x': 1, 'y': 2}, {'x': 2, 'y': 3}]"""""""
pandas/core/internals/construction.py,"def to_arrays(data, columns: Index | None, dtype: DtypeObj | None=None) -> tuple[list[ArrayLike], Index]:
    if not len(data):
        if isinstance(data, np.ndarray):
            if data.dtype.names is not None:
                columns = ensure_index(data.dtype.names)
                arrays = [data[name] for name in columns]
                if len(data) == 0:
                    for (i, arr) in enumerate(arrays):
                        if arr.ndim == 2:
                            arrays[i] = arr[:, 0]
                return (arrays, columns)
        return ([], ensure_index([]))
    elif isinstance(data, np.ndarray) and data.dtype.names is not None:
        columns = Index(list(data.dtype.names))
        arrays = [data[k] for k in columns]
        return (arrays, columns)
    if isinstance(data[0], (list, tuple)):
        arr = _list_to_arrays(data)
    elif isinstance(data[0], abc.Mapping):
        (arr, columns) = _list_of_dict_to_arrays(data, columns)
    elif isinstance(data[0], ABCSeries):
        (arr, columns) = _list_of_series_to_arrays(data, columns)
    else:
        data = [tuple(x) for x in data]
        arr = _list_to_arrays(data)
    (content, columns) = _finalize_columns_and_data(arr, columns, dtype)
    return (content, columns)","""""""Return list of arrays, columns.

Returns
-------
list[ArrayLike]
    These will become columns in a DataFrame.
Index
    This will become frame.columns.

Notes
-----
Ensures that len(result_arrays) == len(result_index)."""""""
pandas/core/internals/construction.py,"def _list_of_dict_to_arrays(data: list[dict], columns: Index | None) -> tuple[np.ndarray, Index]:
    if columns is None:
        gen = (list(x.keys()) for x in data)
        sort = not any((isinstance(d, dict) for d in data))
        pre_cols = lib.fast_unique_multiple_list_gen(gen, sort=sort)
        columns = ensure_index(pre_cols)
    data = [d if type(d) is dict else dict(d) for d in data]
    content = lib.dicts_to_array(data, list(columns))
    return (content, columns)","""""""Convert list of dicts to numpy arrays

if `columns` is not passed, column names are inferred from the records
- for OrderedDict and dicts, the column names match
  the key insertion-order from the first record to the last.
- For other kinds of dict-likes, the keys are lexically sorted.

Parameters
----------
data : iterable
    collection of records (OrderedDict, dict)
columns: iterables or None

Returns
-------
content : np.ndarray[object, ndim=2]
columns : Index"""""""
pandas/core/internals/construction.py,"def _finalize_columns_and_data(content: np.ndarray, columns: Index | None, dtype: DtypeObj | None) -> tuple[list[ArrayLike], Index]:
    contents = list(content.T)
    try:
        columns = _validate_or_indexify_columns(contents, columns)
    except AssertionError as err:
        raise ValueError(err) from err
    if len(contents) and contents[0].dtype == np.object_:
        contents = convert_object_array(contents, dtype=dtype)
    return (contents, columns)","""""""Ensure we have valid columns, cast object dtypes if possible."""""""
pandas/core/internals/construction.py,"def _validate_or_indexify_columns(content: list[np.ndarray], columns: Index | None) -> Index:
    if columns is None:
        columns = default_index(len(content))
    else:
        is_mi_list = isinstance(columns, list) and all((isinstance(col, list) for col in columns))
        if not is_mi_list and len(columns) != len(content):
            raise AssertionError(f'{len(columns)} columns passed, passed data had {len(content)} columns')
        if is_mi_list:
            if len({len(col) for col in columns}) > 1:
                raise ValueError('Length of columns passed for MultiIndex columns is different')
            if columns and len(columns[0]) != len(content):
                raise ValueError(f'{len(columns[0])} columns passed, passed data had {len(content)} columns')
    return columns","""""""If columns is None, make numbers as column names; Otherwise, validate that
columns have valid length.

Parameters
----------
content : list of np.ndarrays
columns : Index or None

Returns
-------
Index
    If columns is None, assign positional column index value as columns.

Raises
------
1. AssertionError when content is not composed of list of lists, and if
    length of columns is not equal to length of content.
2. ValueError when content is list of lists, but length of each sub-list
    is not equal
3. ValueError when content is list of lists, but length of sub-list is
    not equal to length of content"""""""
pandas/core/internals/construction.py,"def convert_object_array(content: list[npt.NDArray[np.object_]], dtype: DtypeObj | None, dtype_backend: str='numpy', coerce_float: bool=False) -> list[ArrayLike]:

    def convert(arr):
        if dtype != np.dtype('O'):
            arr = lib.maybe_convert_objects(arr, try_float=coerce_float, convert_to_nullable_dtype=dtype_backend != 'numpy')
            if dtype is None:
                if arr.dtype == np.dtype('O'):
                    arr = maybe_infer_to_datetimelike(arr)
                    if dtype_backend != 'numpy' and arr.dtype == np.dtype('O'):
                        arr = StringDtype().construct_array_type()._from_sequence(arr)
                elif dtype_backend != 'numpy' and isinstance(arr, np.ndarray):
                    if arr.dtype.kind in 'iufb':
                        arr = pd_array(arr, copy=False)
            elif isinstance(dtype, ExtensionDtype):
                cls = dtype.construct_array_type()
                arr = cls._from_sequence(arr, dtype=dtype, copy=False)
            elif dtype.kind in 'mM':
                arr = maybe_cast_to_datetime(arr, dtype)
        return arr
    arrays = [convert(arr) for arr in content]
    return arrays","""""""Internal function to convert object array.

Parameters
----------
content: List[np.ndarray]
dtype: np.dtype or ExtensionDtype
dtype_backend: Controls if nullable/pyarrow dtypes are returned.
coerce_float: Cast floats that are integers to int.

Returns
-------
List[ArrayLike]"""""""
pandas/core/internals/managers.py,"def raise_construction_error(tot_items: int, block_shape: Shape, axes: list[Index], e: ValueError | None=None):
    passed = tuple(map(int, [tot_items] + list(block_shape)))
    if len(passed) <= 2:
        passed = passed[::-1]
    implied = tuple((len(ax) for ax in axes))
    if len(implied) <= 2:
        implied = implied[::-1]
    if passed == implied and e is not None:
        raise e
    if block_shape[0] == 0:
        raise ValueError('Empty data passed with indices specified.')
    raise ValueError(f'Shape of passed values is {passed}, indices imply {implied}')","""""""raise a helpful message about our construction"""""""
pandas/core/internals/managers.py,"def _consolidate(blocks: tuple[Block, ...]) -> tuple[Block, ...]:
    gkey = lambda x: x._consolidate_key
    grouper = itertools.groupby(sorted(blocks, key=gkey), gkey)
    new_blocks: list[Block] = []
    for ((_can_consolidate, dtype), group_blocks) in grouper:
        (merged_blocks, _) = _merge_blocks(list(group_blocks), dtype=dtype, can_consolidate=_can_consolidate)
        new_blocks = extend_blocks(merged_blocks, new_blocks)
    return tuple(new_blocks)","""""""Merge blocks having same dtype, exclude non-consolidating blocks"""""""
pandas/core/internals/managers.py,"def _fast_count_smallints(arr: npt.NDArray[np.intp]):
    counts = np.bincount(arr)
    nz = counts.nonzero()[0]
    return zip(nz, counts[nz])","""""""Faster version of set(arr) for sequences of small numbers."""""""
pandas/core/internals/ops.py,"def _reset_block_mgr_locs(nbs: list[Block], locs) -> None:
    for nb in nbs:
        nblocs = locs[nb.mgr_locs.indexer]
        nb.mgr_locs = nblocs","""""""Reset mgr_locs to correspond to our original DataFrame."""""""
pandas/core/internals/ops.py,"def _get_same_shape_values(lblk: Block, rblk: Block, left_ea: bool, right_ea: bool) -> tuple[ArrayLike, ArrayLike]:
    lvals = lblk.values
    rvals = rblk.values
    assert rblk.mgr_locs.is_slice_like, rblk.mgr_locs
    if not (left_ea or right_ea):
        lvals = lvals[rblk.mgr_locs.indexer, :]
        assert lvals.shape == rvals.shape, (lvals.shape, rvals.shape)
    elif left_ea and right_ea:
        assert lvals.shape == rvals.shape, (lvals.shape, rvals.shape)
    elif right_ea:
        lvals = lvals[rblk.mgr_locs.indexer, :]
        assert lvals.shape[0] == 1, lvals.shape
        lvals = lvals[0, :]
    else:
        assert rvals.shape[0] == 1, rvals.shape
        rvals = rvals[0, :]
    return (lvals, rvals)","""""""Slice lblk.values to align with rblk.  Squeeze if we have EAs."""""""
pandas/core/internals/ops.py,"def blockwise_all(left: BlockManager, right: BlockManager, op) -> bool:
    for info in _iter_block_pairs(left, right):
        res = op(info.lvals, info.rvals)
        if not res:
            return False
    return True","""""""Blockwise `all` reduction."""""""
pandas/core/methods/describe.py,"def describe_ndframe(*, obj: NDFrameT, include: str | Sequence[str] | None, exclude: str | Sequence[str] | None, percentiles: Sequence[float] | np.ndarray | None) -> NDFrameT:
    percentiles = _refine_percentiles(percentiles)
    describer: NDFrameDescriberAbstract
    if obj.ndim == 1:
        describer = SeriesDescriber(obj=cast('Series', obj))
    else:
        describer = DataFrameDescriber(obj=cast('DataFrame', obj), include=include, exclude=exclude)
    result = describer.describe(percentiles=percentiles)
    return cast(NDFrameT, result)","""""""Describe series or dataframe.

Called from pandas.core.generic.NDFrame.describe()

Parameters
----------
obj: DataFrame or Series
    Either dataframe or series to be described.
include : 'all', list-like of dtypes or None (default), optional
    A white list of data types to include in the result. Ignored for ``Series``.
exclude : list-like of dtypes or None (default), optional,
    A black list of data types to omit from the result. Ignored for ``Series``.
percentiles : list-like of numbers, optional
    The percentiles to include in the output. All should fall between 0 and 1.
    The default is ``[.25, .5, .75]``, which returns the 25th, 50th, and
    75th percentiles.

Returns
-------
Dataframe or series description."""""""
pandas/core/methods/describe.py,"def reorder_columns(ldesc: Sequence[Series]) -> list[Hashable]:
    names: list[Hashable] = []
    seen_names: set[Hashable] = set()
    ldesc_indexes = sorted((x.index for x in ldesc), key=len)
    for idxnames in ldesc_indexes:
        for name in idxnames:
            if name not in seen_names:
                seen_names.add(name)
                names.append(name)
    return names","""""""Set a convenient order for rows for display."""""""
pandas/core/methods/describe.py,"def describe_numeric_1d(series: Series, percentiles: Sequence[float]) -> Series:
    from pandas import Series
    formatted_percentiles = format_percentiles(percentiles)
    stat_index = ['count', 'mean', 'std', 'min'] + formatted_percentiles + ['max']
    d = [series.count(), series.mean(), series.std(), series.min()] + series.quantile(percentiles).tolist() + [series.max()]
    dtype: DtypeObj | None
    if isinstance(series.dtype, ExtensionDtype):
        if isinstance(series.dtype, ArrowDtype):
            if series.dtype.kind == 'm':
                dtype = None
            else:
                import pyarrow as pa
                dtype = ArrowDtype(pa.float64())
        else:
            dtype = Float64Dtype()
    elif series.dtype.kind in 'iufb':
        dtype = np.dtype('float')
    else:
        dtype = None
    return Series(d, index=stat_index, name=series.name, dtype=dtype)","""""""Describe series containing numerical data.

Parameters
----------
series : Series
    Series to be described.
percentiles : list-like of numbers
    The percentiles to include in the output."""""""
pandas/core/methods/describe.py,"def describe_categorical_1d(data: Series, percentiles_ignored: Sequence[float]) -> Series:
    names = ['count', 'unique', 'top', 'freq']
    objcounts = data.value_counts()
    count_unique = len(objcounts[objcounts != 0])
    if count_unique > 0:
        (top, freq) = (objcounts.index[0], objcounts.iloc[0])
        dtype = None
    else:
        (top, freq) = (np.nan, np.nan)
        dtype = 'object'
    result = [data.count(), count_unique, top, freq]
    from pandas import Series
    return Series(result, index=names, name=data.name, dtype=dtype)","""""""Describe series containing categorical data.

Parameters
----------
data : Series
    Series to be described.
percentiles_ignored : list-like of numbers
    Ignored, but in place to unify interface."""""""
pandas/core/methods/describe.py,"def describe_timestamp_as_categorical_1d(data: Series, percentiles_ignored: Sequence[float]) -> Series:
    names = ['count', 'unique']
    objcounts = data.value_counts()
    count_unique = len(objcounts[objcounts != 0])
    result: list[float | Timestamp] = [data.count(), count_unique]
    dtype = None
    if count_unique > 0:
        (top, freq) = (objcounts.index[0], objcounts.iloc[0])
        tz = data.dt.tz
        asint = data.dropna().values.view('i8')
        top = Timestamp(top)
        if top.tzinfo is not None and tz is not None:
            top = top.tz_convert(tz)
        else:
            top = top.tz_localize(tz)
        names += ['top', 'freq', 'first', 'last']
        result += [top, freq, Timestamp(asint.min(), tz=tz), Timestamp(asint.max(), tz=tz)]
    else:
        names += ['top', 'freq']
        result += [np.nan, np.nan]
        dtype = 'object'
    from pandas import Series
    return Series(result, index=names, name=data.name, dtype=dtype)","""""""Describe series containing timestamp data treated as categorical.

Parameters
----------
data : Series
    Series to be described.
percentiles_ignored : list-like of numbers
    Ignored, but in place to unify interface."""""""
pandas/core/methods/describe.py,"def describe_timestamp_1d(data: Series, percentiles: Sequence[float]) -> Series:
    from pandas import Series
    formatted_percentiles = format_percentiles(percentiles)
    stat_index = ['count', 'mean', 'min'] + formatted_percentiles + ['max']
    d = [data.count(), data.mean(), data.min()] + data.quantile(percentiles).tolist() + [data.max()]
    return Series(d, index=stat_index, name=data.name)","""""""Describe series containing datetime64 dtype.

Parameters
----------
data : Series
    Series to be described.
percentiles : list-like of numbers
    The percentiles to include in the output."""""""
pandas/core/methods/describe.py,"def select_describe_func(data: Series) -> Callable:
    if is_bool_dtype(data.dtype):
        return describe_categorical_1d
    elif is_numeric_dtype(data):
        return describe_numeric_1d
    elif data.dtype.kind == 'M' or isinstance(data.dtype, DatetimeTZDtype):
        return describe_timestamp_1d
    elif data.dtype.kind == 'm':
        return describe_numeric_1d
    else:
        return describe_categorical_1d","""""""Select proper function for describing series based on data type.

Parameters
----------
data : Series
    Series to be described."""""""
pandas/core/methods/describe.py,"def _refine_percentiles(percentiles: Sequence[float] | np.ndarray | None) -> npt.NDArray[np.float64]:
    if percentiles is None:
        return np.array([0.25, 0.5, 0.75])
    percentiles = list(percentiles)
    validate_percentile(percentiles)
    if 0.5 not in percentiles:
        percentiles.append(0.5)
    percentiles = np.asarray(percentiles)
    unique_pcts = np.unique(percentiles)
    assert percentiles is not None
    if len(unique_pcts) < len(percentiles):
        raise ValueError('percentiles cannot contain duplicates')
    return unique_pcts","""""""Ensure that percentiles are unique and sorted.

Parameters
----------
percentiles : list-like of numbers, optional
    The percentiles to include in the output."""""""
pandas/core/methods/to_dict.py,"def to_dict(df: DataFrame, orient: Literal['dict', 'list', 'series', 'split', 'tight', 'records', 'index']='dict', *, into: type[MutableMappingT] | MutableMappingT=dict, index: bool=True) -> MutableMappingT | list[MutableMappingT]:
    if not df.columns.is_unique:
        warnings.warn('DataFrame columns are not unique, some columns will be omitted.', UserWarning, stacklevel=find_stack_level())
    into_c = com.standardize_mapping(into)
    orient = orient.lower()
    if not index and orient not in ['split', 'tight']:
        raise ValueError(""'index=False' is only valid when 'orient' is 'split' or 'tight'"")
    if orient == 'series':
        return into_c(((k, v) for (k, v) in df.items()))
    box_native_indices = [i for (i, col_dtype) in enumerate(df.dtypes.values) if col_dtype == np.dtype(object) or isinstance(col_dtype, ExtensionDtype)]
    are_all_object_dtype_cols = len(box_native_indices) == len(df.dtypes)
    if orient == 'dict':
        return into_c(((k, v.to_dict(into=into)) for (k, v) in df.items()))
    elif orient == 'list':
        object_dtype_indices_as_set: set[int] = set(box_native_indices)
        return into_c(((k, list(map(maybe_box_native, v.to_numpy().tolist())) if i in object_dtype_indices_as_set else v.to_numpy().tolist()) for (i, (k, v)) in enumerate(df.items())))
    elif orient == 'split':
        data = df._create_data_for_split_and_tight_to_dict(are_all_object_dtype_cols, box_native_indices)
        return into_c(((('index', df.index.tolist()),) if index else ()) + (('columns', df.columns.tolist()), ('data', data)))
    elif orient == 'tight':
        data = df._create_data_for_split_and_tight_to_dict(are_all_object_dtype_cols, box_native_indices)
        return into_c(((('index', df.index.tolist()),) if index else ()) + (('columns', df.columns.tolist()), ('data', [list(map(maybe_box_native, t)) for t in df.itertuples(index=False, name=None)])) + ((('index_names', list(df.index.names)),) if index else ()) + (('column_names', list(df.columns.names)),))
    elif orient == 'records':
        columns = df.columns.tolist()
        if are_all_object_dtype_cols:
            rows = (dict(zip(columns, row)) for row in df.itertuples(index=False, name=None))
            return [into_c(((k, maybe_box_native(v)) for (k, v) in row.items())) for row in rows]
        else:
            data = [into_c(zip(columns, t)) for t in df.itertuples(index=False, name=None)]
            if box_native_indices:
                object_dtype_indices_as_set = set(box_native_indices)
                object_dtype_cols = {col for (i, col) in enumerate(df.columns) if i in object_dtype_indices_as_set}
                for row in data:
                    for col in object_dtype_cols:
                        row[col] = maybe_box_native(row[col])
            return data
    elif orient == 'index':
        if not df.index.is_unique:
            raise ValueError(""DataFrame index must be unique for orient='index'."")
        columns = df.columns.tolist()
        if are_all_object_dtype_cols:
            return into_c(((t[0], dict(zip(df.columns, map(maybe_box_native, t[1:])))) for t in df.itertuples(name=None)))
        elif box_native_indices:
            object_dtype_indices_as_set = set(box_native_indices)
            is_object_dtype_by_index = [i in object_dtype_indices_as_set for i in range(len(df.columns))]
            return into_c(((t[0], {columns[i]: maybe_box_native(v) if is_object_dtype_by_index[i] else v for (i, v) in enumerate(t[1:])}) for t in df.itertuples(name=None)))
        else:
            return into_c(((t[0], dict(zip(df.columns, t[1:]))) for t in df.itertuples(name=None)))
    else:
        raise ValueError(f""orient '{orient}' not understood"")","""""""Convert the DataFrame to a dictionary.

The type of the key-value pairs can be customized with the parameters
(see below).

Parameters
----------
orient : str {'dict', 'list', 'series', 'split', 'tight', 'records', 'index'}
    Determines the type of the values of the dictionary.

    - 'dict' (default) : dict like {column -> {index -> value}}
    - 'list' : dict like {column -> [values]}
    - 'series' : dict like {column -> Series(values)}
    - 'split' : dict like
      {'index' -> [index], 'columns' -> [columns], 'data' -> [values]}
    - 'tight' : dict like
      {'index' -> [index], 'columns' -> [columns], 'data' -> [values],
      'index_names' -> [index.names], 'column_names' -> [column.names]}
    - 'records' : list like
      [{column -> value}, ... , {column -> value}]
    - 'index' : dict like {index -> {column -> value}}

    .. versionadded:: 1.4.0
        'tight' as an allowed value for the ``orient`` argument

into : class, default dict
    The collections.abc.MutableMapping subclass used for all Mappings
    in the return value.  Can be the actual class or an empty
    instance of the mapping type you want.  If you want a
    collections.defaultdict, you must pass it initialized.

index : bool, default True
    Whether to include the index item (and index_names item if `orient`
    is 'tight') in the returned dictionary. Can only be ``False``
    when `orient` is 'split' or 'tight'.

    .. versionadded:: 2.0.0

Returns
-------
dict, list or collections.abc.Mapping
    Return a collections.abc.MutableMapping object representing the
    DataFrame. The resulting transformation depends on the `orient` parameter."""""""
pandas/core/missing.py,"def check_value_size(value, mask: npt.NDArray[np.bool_], length: int):
    if is_array_like(value):
        if len(value) != length:
            raise ValueError(f""Length of 'value' does not match. Got ({len(value)})  expected {length}"")
        value = value[mask]
    return value","""""""Validate the size of the values passed to ExtensionArray.fillna."""""""
pandas/core/missing.py,"def mask_missing(arr: ArrayLike, values_to_mask) -> npt.NDArray[np.bool_]:
    (dtype, values_to_mask) = infer_dtype_from(values_to_mask)
    if isinstance(dtype, np.dtype):
        values_to_mask = np.array(values_to_mask, dtype=dtype)
    else:
        cls = dtype.construct_array_type()
        if not lib.is_list_like(values_to_mask):
            values_to_mask = [values_to_mask]
        values_to_mask = cls._from_sequence(values_to_mask, dtype=dtype, copy=False)
    potential_na = False
    if is_object_dtype(arr.dtype):
        potential_na = True
        arr_mask = ~isna(arr)
    na_mask = isna(values_to_mask)
    nonna = values_to_mask[~na_mask]
    mask = np.zeros(arr.shape, dtype=bool)
    for x in nonna:
        if is_numeric_v_string_like(arr, x):
            pass
        else:
            if potential_na:
                new_mask = np.zeros(arr.shape, dtype=np.bool_)
                new_mask[arr_mask] = arr[arr_mask] == x
            else:
                new_mask = arr == x
                if not isinstance(new_mask, np.ndarray):
                    new_mask = new_mask.to_numpy(dtype=bool, na_value=False)
            mask |= new_mask
    if na_mask.any():
        mask |= isna(arr)
    return mask","""""""Return a masking array of same size/shape as arr
with entries equaling any member of values_to_mask set to True

Parameters
----------
arr : ArrayLike
values_to_mask: list, tuple, or scalar

Returns
-------
np.ndarray[bool]"""""""
pandas/core/missing.py,"def find_valid_index(how: str, is_valid: npt.NDArray[np.bool_]) -> int | None:
    assert how in ['first', 'last']
    if len(is_valid) == 0:
        return None
    if is_valid.ndim == 2:
        is_valid = is_valid.any(axis=1)
    if how == 'first':
        idxpos = is_valid[:].argmax()
    elif how == 'last':
        idxpos = len(is_valid) - 1 - is_valid[::-1].argmax()
    chk_notna = is_valid[idxpos]
    if not chk_notna:
        return None
    return idxpos","""""""Retrieves the positional index of the first valid value.

Parameters
----------
how : {'first', 'last'}
    Use this parameter to change between the first or last valid index.
is_valid: np.ndarray
    Mask to find na_values.

Returns
-------
int or None"""""""
pandas/core/missing.py,"def interpolate_2d_inplace(data: np.ndarray, index: Index, axis: AxisInt, method: str='linear', limit: int | None=None, limit_direction: str='forward', limit_area: str | None=None, fill_value: Any | None=None, **kwargs) -> None:
    clean_interp_method(method, index, **kwargs)
    if is_valid_na_for_dtype(fill_value, data.dtype):
        fill_value = na_value_for_dtype(data.dtype, compat=False)
    if method == 'time':
        if not needs_i8_conversion(index.dtype):
            raise ValueError('time-weighted interpolation only works on Series or DataFrames with a DatetimeIndex')
        method = 'values'
    limit_direction = validate_limit_direction(limit_direction)
    limit_area_validated = validate_limit_area(limit_area)
    limit = algos.validate_limit(nobs=None, limit=limit)
    indices = _index_to_interp_indices(index, method)

    def func(yvalues: np.ndarray) -> None:
        _interpolate_1d(indices=indices, yvalues=yvalues, method=method, limit=limit, limit_direction=limit_direction, limit_area=limit_area_validated, fill_value=fill_value, bounds_error=False, **kwargs)
    np.apply_along_axis(func, axis, data)","""""""Column-wise application of _interpolate_1d.

Notes
-----
Alters 'data' in-place.

The signature does differ from _interpolate_1d because it only
includes what is needed for Block.interpolate."""""""
pandas/core/missing.py,"def _index_to_interp_indices(index: Index, method: str) -> np.ndarray:
    xarr = index._values
    if needs_i8_conversion(xarr.dtype):
        xarr = xarr.view('i8')
    if method == 'linear':
        inds = xarr
        inds = cast(np.ndarray, inds)
    else:
        inds = np.asarray(xarr)
        if method in ('values', 'index'):
            if inds.dtype == np.object_:
                inds = lib.maybe_convert_objects(inds)
    return inds","""""""Convert Index to ndarray of indices to pass to NumPy/SciPy."""""""
pandas/core/missing.py,"def _interpolate_1d(indices: np.ndarray, yvalues: np.ndarray, method: str='linear', limit: int | None=None, limit_direction: str='forward', limit_area: Literal['inside', 'outside'] | None=None, fill_value: Any | None=None, bounds_error: bool=False, order: int | None=None, **kwargs) -> None:
    invalid = isna(yvalues)
    valid = ~invalid
    if not valid.any():
        return
    if valid.all():
        return
    all_nans = set(np.flatnonzero(invalid))
    first_valid_index = find_valid_index(how='first', is_valid=valid)
    if first_valid_index is None:
        first_valid_index = 0
    start_nans = set(range(first_valid_index))
    last_valid_index = find_valid_index(how='last', is_valid=valid)
    if last_valid_index is None:
        last_valid_index = len(yvalues)
    end_nans = set(range(1 + last_valid_index, len(valid)))
    preserve_nans: list | set
    if limit_direction == 'forward':
        preserve_nans = start_nans | set(_interp_limit(invalid, limit, 0))
    elif limit_direction == 'backward':
        preserve_nans = end_nans | set(_interp_limit(invalid, 0, limit))
    else:
        preserve_nans = set(_interp_limit(invalid, limit, limit))
    if limit_area == 'inside':
        preserve_nans |= start_nans | end_nans
    elif limit_area == 'outside':
        mid_nans = all_nans - start_nans - end_nans
        preserve_nans |= mid_nans
    preserve_nans = sorted(preserve_nans)
    is_datetimelike = yvalues.dtype.kind in 'mM'
    if is_datetimelike:
        yvalues = yvalues.view('i8')
    if method in NP_METHODS:
        indexer = np.argsort(indices[valid])
        yvalues[invalid] = np.interp(indices[invalid], indices[valid][indexer], yvalues[valid][indexer])
    else:
        yvalues[invalid] = _interpolate_scipy_wrapper(indices[valid], yvalues[valid], indices[invalid], method=method, fill_value=fill_value, bounds_error=bounds_error, order=order, **kwargs)
    if is_datetimelike:
        yvalues[preserve_nans] = NaT.value
    else:
        yvalues[preserve_nans] = np.nan
    return","""""""Logic for the 1-d interpolation.  The input
indices and yvalues will each be 1-d arrays of the same length.

Bounds_error is currently hardcoded to False since non-scipy ones don't
take it as an argument.

Notes
-----
Fills 'yvalues' in-place."""""""
pandas/core/missing.py,"def _interpolate_scipy_wrapper(x: np.ndarray, y: np.ndarray, new_x: np.ndarray, method: str, fill_value=None, bounds_error: bool=False, order=None, **kwargs):
    extra = f'{method} interpolation requires SciPy.'
    import_optional_dependency('scipy', extra=extra)
    from scipy import interpolate
    new_x = np.asarray(new_x)
    alt_methods = {'barycentric': interpolate.barycentric_interpolate, 'krogh': interpolate.krogh_interpolate, 'from_derivatives': _from_derivatives, 'piecewise_polynomial': _from_derivatives, 'cubicspline': _cubicspline_interpolate, 'akima': _akima_interpolate, 'pchip': interpolate.pchip_interpolate}
    interp1d_methods = ['nearest', 'zero', 'slinear', 'quadratic', 'cubic', 'polynomial']
    if method in interp1d_methods:
        if method == 'polynomial':
            kind = order
        else:
            kind = method
        terp = interpolate.interp1d(x, y, kind=kind, fill_value=fill_value, bounds_error=bounds_error)
        new_y = terp(new_x)
    elif method == 'spline':
        if isna(order) or order <= 0:
            raise ValueError(f'order needs to be specified and greater than 0; got order: {order}')
        terp = interpolate.UnivariateSpline(x, y, k=order, **kwargs)
        new_y = terp(new_x)
    else:
        if not x.flags.writeable:
            x = x.copy()
        if not y.flags.writeable:
            y = y.copy()
        if not new_x.flags.writeable:
            new_x = new_x.copy()
        terp = alt_methods[method]
        new_y = terp(x, y, new_x, **kwargs)
    return new_y","""""""Passed off to scipy.interpolate.interp1d. method is scipy's kind.
Returns an array interpolated at new_x.  Add any new methods to
the list in _clean_interp_method."""""""
pandas/core/missing.py,"def _from_derivatives(xi: np.ndarray, yi: np.ndarray, x: np.ndarray, order=None, der: int | list[int] | None=0, extrapolate: bool=False):
    from scipy import interpolate
    method = interpolate.BPoly.from_derivatives
    m = method(xi, yi.reshape(-1, 1), orders=order, extrapolate=extrapolate)
    return m(x)","""""""Convenience function for interpolate.BPoly.from_derivatives.

Construct a piecewise polynomial in the Bernstein basis, compatible
with the specified values and derivatives at breakpoints.

Parameters
----------
xi : array-like
    sorted 1D array of x-coordinates
yi : array-like or list of array-likes
    yi[i][j] is the j-th derivative known at xi[i]
order: None or int or array-like of ints. Default: None.
    Specifies the degree of local polynomials. If not None, some
    derivatives are ignored.
der : int or list
    How many derivatives to extract; None for all potentially nonzero
    derivatives (that is a number equal to the number of points), or a
    list of derivatives to extract. This number includes the function
    value as 0th derivative.
 extrapolate : bool, optional
    Whether to extrapolate to ouf-of-bounds points based on first and last
    intervals, or to return NaNs. Default: True.

See Also
--------
scipy.interpolate.BPoly.from_derivatives

Returns
-------
y : scalar or array-like
    The result, of length R or length M or M by R."""""""
pandas/core/missing.py,"def _akima_interpolate(xi: np.ndarray, yi: np.ndarray, x: np.ndarray, der: int | list[int] | None=0, axis: AxisInt=0):
    from scipy import interpolate
    P = interpolate.Akima1DInterpolator(xi, yi, axis=axis)
    return P(x, nu=der)","""""""Convenience function for akima interpolation.
xi and yi are arrays of values used to approximate some function f,
with ``yi = f(xi)``.

See `Akima1DInterpolator` for details.

Parameters
----------
xi : np.ndarray
    A sorted list of x-coordinates, of length N.
yi : np.ndarray
    A 1-D array of real values.  `yi`'s length along the interpolation
    axis must be equal to the length of `xi`. If N-D array, use axis
    parameter to select correct axis.
x : np.ndarray
    Of length M.
der : int, optional
    How many derivatives to extract; None for all potentially
    nonzero derivatives (that is a number equal to the number
    of points), or a list of derivatives to extract. This number
    includes the function value as 0th derivative.
axis : int, optional
    Axis in the yi array corresponding to the x-coordinate values.

See Also
--------
scipy.interpolate.Akima1DInterpolator

Returns
-------
y : scalar or array-like
    The result, of length R or length M or M by R,"""""""
pandas/core/missing.py,"def _cubicspline_interpolate(xi: np.ndarray, yi: np.ndarray, x: np.ndarray, axis: AxisInt=0, bc_type: str | tuple[Any, Any]='not-a-knot', extrapolate=None):
    from scipy import interpolate
    P = interpolate.CubicSpline(xi, yi, axis=axis, bc_type=bc_type, extrapolate=extrapolate)
    return P(x)","""""""Convenience function for cubic spline data interpolator.

See `scipy.interpolate.CubicSpline` for details.

Parameters
----------
xi : np.ndarray, shape (n,)
    1-d array containing values of the independent variable.
    Values must be real, finite and in strictly increasing order.
yi : np.ndarray
    Array containing values of the dependent variable. It can have
    arbitrary number of dimensions, but the length along ``axis``
    (see below) must match the length of ``x``. Values must be finite.
x : np.ndarray, shape (m,)
axis : int, optional
    Axis along which `y` is assumed to be varying. Meaning that for
    ``x[i]`` the corresponding values are ``np.take(y, i, axis=axis)``.
    Default is 0.
bc_type : string or 2-tuple, optional
    Boundary condition type. Two additional equations, given by the
    boundary conditions, are required to determine all coefficients of
    polynomials on each segment [2]_.
    If `bc_type` is a string, then the specified condition will be applied
    at both ends of a spline. Available conditions are:
    * 'not-a-knot' (default): The first and second segment at a curve end
      are the same polynomial. It is a good default when there is no
      information on boundary conditions.
    * 'periodic': The interpolated functions is assumed to be periodic
      of period ``x[-1] - x[0]``. The first and last value of `y` must be
      identical: ``y[0] == y[-1]``. This boundary condition will result in
      ``y'[0] == y'[-1]`` and ``y''[0] == y''[-1]``.
    * 'clamped': The first derivative at curves ends are zero. Assuming
      a 1D `y`, ``bc_type=((1, 0.0), (1, 0.0))`` is the same condition.
    * 'natural': The second derivative at curve ends are zero. Assuming
      a 1D `y`, ``bc_type=((2, 0.0), (2, 0.0))`` is the same condition.
    If `bc_type` is a 2-tuple, the first and the second value will be
    applied at the curve start and end respectively. The tuple values can
    be one of the previously mentioned strings (except 'periodic') or a
    tuple `(order, deriv_values)` allowing to specify arbitrary
    derivatives at curve ends:
    * `order`: the derivative order, 1 or 2.
    * `deriv_value`: array-like containing derivative values, shape must
      be the same as `y`, excluding ``axis`` dimension. For example, if
      `y` is 1D, then `deriv_value` must be a scalar. If `y` is 3D with
      the shape (n0, n1, n2) and axis=2, then `deriv_value` must be 2D
      and have the shape (n0, n1).
extrapolate : {bool, 'periodic', None}, optional
    If bool, determines whether to extrapolate to out-of-bounds points
    based on first and last intervals, or to return NaNs. If 'periodic',
    periodic extrapolation is used. If None (default), ``extrapolate`` is
    set to 'periodic' for ``bc_type='periodic'`` and to True otherwise.

See Also
--------
scipy.interpolate.CubicHermiteSpline

Returns
-------
y : scalar or array-like
    The result, of shape (m,)

References
----------
.. [1] `Cubic Spline Interpolation
        <https://en.wikiversity.org/wiki/Cubic_Spline_Interpolation>`_
        on Wikiversity.
.. [2] Carl de Boor, ""A Practical Guide to Splines"", Springer-Verlag, 1978."""""""
pandas/core/missing.py,"def _interpolate_with_limit_area(values: np.ndarray, method: Literal['pad', 'backfill'], limit: int | None, limit_area: Literal['inside', 'outside']) -> None:
    invalid = isna(values)
    is_valid = ~invalid
    if not invalid.all():
        first = find_valid_index(how='first', is_valid=is_valid)
        if first is None:
            first = 0
        last = find_valid_index(how='last', is_valid=is_valid)
        if last is None:
            last = len(values)
        pad_or_backfill_inplace(values, method=method, limit=limit)
        if limit_area == 'inside':
            invalid[first:last + 1] = False
        elif limit_area == 'outside':
            invalid[:first] = invalid[last + 1:] = False
        else:
            raise ValueError(""limit_area should be 'inside' or 'outside'"")
        values[invalid] = np.nan","""""""Apply interpolation and limit_area logic to values along a to-be-specified axis.

Parameters
----------
values: np.ndarray
    Input array.
method: str
    Interpolation method. Could be ""bfill"" or ""pad""
limit: int, optional
    Index limit on interpolation.
limit_area: {'inside', 'outside'}
    Limit area for interpolation.

Notes
-----
Modifies values in-place."""""""
pandas/core/missing.py,"def pad_or_backfill_inplace(values: np.ndarray, method: Literal['pad', 'backfill']='pad', axis: AxisInt=0, limit: int | None=None, limit_area: Literal['inside', 'outside'] | None=None) -> None:
    if limit_area is not None:
        np.apply_along_axis(partial(_interpolate_with_limit_area, method=method, limit=limit, limit_area=limit_area), axis, values)
        return
    transf = (lambda x: x) if axis == 0 else lambda x: x.T
    if values.ndim == 1:
        if axis != 0:
            raise AssertionError('cannot interpolate on a ndim == 1 with axis != 0')
        values = values.reshape(tuple((1,) + values.shape))
    method = clean_fill_method(method)
    tvalues = transf(values)
    func = get_fill_func(method, ndim=2)
    func(tvalues, limit=limit)
    return","""""""Perform an actual interpolation of values, values will be make 2-d if
needed fills inplace, returns the result.

Parameters
----------
values: np.ndarray
    Input array.
method: str, default ""pad""
    Interpolation method. Could be ""bfill"" or ""pad""
axis: 0 or 1
    Interpolation axis
limit: int, optional
    Index limit on interpolation.
limit_area: str, optional
    Limit area for interpolation. Can be ""inside"" or ""outside""

Notes
-----
Modifies values in-place."""""""
pandas/core/missing.py,"def _datetimelike_compat(func: F) -> F:

    @wraps(func)
    def new_func(values, limit: int | None=None, mask=None):
        if needs_i8_conversion(values.dtype):
            if mask is None:
                mask = isna(values)
            (result, mask) = func(values.view('i8'), limit=limit, mask=mask)
            return (result.view(values.dtype), mask)
        return func(values, limit=limit, mask=mask)
    return cast(F, new_func)","""""""Wrapper to handle datetime64 and timedelta64 dtypes."""""""
pandas/core/missing.py,"def _interp_limit(invalid: npt.NDArray[np.bool_], fw_limit: int | None, bw_limit: int | None):
    N = len(invalid)
    f_idx = set()
    b_idx = set()

    def inner(invalid, limit: int):
        limit = min(limit, N)
        windowed = _rolling_window(invalid, limit + 1).all(1)
        idx = set(np.where(windowed)[0] + limit) | set(np.where((~invalid[:limit + 1]).cumsum() == 0)[0])
        return idx
    if fw_limit is not None:
        if fw_limit == 0:
            f_idx = set(np.where(invalid)[0])
        else:
            f_idx = inner(invalid, fw_limit)
    if bw_limit is not None:
        if bw_limit == 0:
            return f_idx
        else:
            b_idx_inv = list(inner(invalid[::-1], bw_limit))
            b_idx = set(N - 1 - np.asarray(b_idx_inv))
            if fw_limit == 0:
                return b_idx
    return f_idx & b_idx","""""""Get indexers of values that won't be filled
because they exceed the limits.

Parameters
----------
invalid : np.ndarray[bool]
fw_limit : int or None
    forward limit to index
bw_limit : int or None
    backward limit to index

Returns
-------
set of indexers

Notes
-----
This is equivalent to the more readable, but slower

.. code-block:: python

    def _interp_limit(invalid, fw_limit, bw_limit):
        for x in np.where(invalid)[0]:
            if invalid[max(0, x - fw_limit):x + bw_limit + 1].all():
                yield x"""""""
pandas/core/missing.py,"def _rolling_window(a: npt.NDArray[np.bool_], window: int) -> npt.NDArray[np.bool_]:
    shape = a.shape[:-1] + (a.shape[-1] - window + 1, window)
    strides = a.strides + (a.strides[-1],)
    return np.lib.stride_tricks.as_strided(a, shape=shape, strides=strides)","""""""[True, True, False, True, False], 2 ->

[
    [True,  True],
    [True, False],
    [False, True],
    [True, False],
]"""""""
pandas/core/nanops.py,"def _get_fill_value(dtype: DtypeObj, fill_value: Scalar | None=None, fill_value_typ=None):
    if fill_value is not None:
        return fill_value
    if _na_ok_dtype(dtype):
        if fill_value_typ is None:
            return np.nan
        elif fill_value_typ == '+inf':
            return np.inf
        else:
            return -np.inf
    elif fill_value_typ == '+inf':
        return lib.i8max
    else:
        return iNaT","""""""return the correct fill value for the dtype of the values"""""""
pandas/core/nanops.py,"def _maybe_get_mask(values: np.ndarray, skipna: bool, mask: npt.NDArray[np.bool_] | None) -> npt.NDArray[np.bool_] | None:
    if mask is None:
        if values.dtype.kind in 'biu':
            return None
        if skipna or values.dtype.kind in 'mM':
            mask = isna(values)
    return mask","""""""Compute a mask if and only if necessary.

This function will compute a mask iff it is necessary. Otherwise,
return the provided mask (potentially None) when a mask does not need to be
computed.

A mask is never necessary if the values array is of boolean or integer
dtypes, as these are incapable of storing NaNs. If passing a NaN-capable
dtype that is interpretable as either boolean or integer data (eg,
timedelta64), a mask must be provided.

If the skipna parameter is False, a new mask will not be computed.

The mask is computed using isna() by default. Setting invert=True selects
notna() as the masking function.

Parameters
----------
values : ndarray
    input array to potentially compute mask for
skipna : bool
    boolean for whether NaNs should be skipped
mask : Optional[ndarray]
    nan-mask if known

Returns
-------
Optional[np.ndarray[bool]]"""""""
pandas/core/nanops.py,"def _get_values(values: np.ndarray, skipna: bool, fill_value: Any=None, fill_value_typ: str | None=None, mask: npt.NDArray[np.bool_] | None=None) -> tuple[np.ndarray, npt.NDArray[np.bool_] | None]:
    mask = _maybe_get_mask(values, skipna, mask)
    dtype = values.dtype
    datetimelike = False
    if values.dtype.kind in 'mM':
        values = np.asarray(values.view('i8'))
        datetimelike = True
    if skipna and mask is not None:
        fill_value = _get_fill_value(dtype, fill_value=fill_value, fill_value_typ=fill_value_typ)
        if fill_value is not None:
            if mask.any():
                if datetimelike or _na_ok_dtype(dtype):
                    values = values.copy()
                    np.putmask(values, mask, fill_value)
                else:
                    values = np.where(~mask, values, fill_value)
    return (values, mask)","""""""Utility to get the values view, mask, dtype, dtype_max, and fill_value.

If both mask and fill_value/fill_value_typ are not None and skipna is True,
the values array will be copied.

For input arrays of boolean or integer dtypes, copies will only occur if a
precomputed mask, a fill_value/fill_value_typ, and skipna=True are
provided.

Parameters
----------
values : ndarray
    input array to potentially compute mask for
skipna : bool
    boolean for whether NaNs should be skipped
fill_value : Any
    value to fill NaNs with
fill_value_typ : str
    Set to '+inf' or '-inf' to handle dtype-specific infinities
mask : Optional[np.ndarray[bool]]
    nan-mask if known

Returns
-------
values : ndarray
    Potential copy of input value array
mask : Optional[ndarray[bool]]
    Mask for values, if deemed necessary to compute"""""""
pandas/core/nanops.py,"def _wrap_results(result, dtype: np.dtype, fill_value=None):
    if result is NaT:
        pass
    elif dtype.kind == 'M':
        if fill_value is None:
            fill_value = iNaT
        if not isinstance(result, np.ndarray):
            assert not isna(fill_value), 'Expected non-null fill_value'
            if result == fill_value:
                result = np.nan
            if isna(result):
                result = np.datetime64('NaT', 'ns').astype(dtype)
            else:
                result = np.int64(result).view(dtype)
            result = result.astype(dtype, copy=False)
        else:
            result = result.astype(dtype)
    elif dtype.kind == 'm':
        if not isinstance(result, np.ndarray):
            if result == fill_value or np.isnan(result):
                result = np.timedelta64('NaT').astype(dtype)
            elif np.fabs(result) > lib.i8max:
                raise ValueError('overflow in timedelta operation')
            else:
                result = np.int64(result).astype(dtype, copy=False)
        else:
            result = result.astype('m8[ns]').view(dtype)
    return result","""""""wrap our results if needed"""""""
pandas/core/nanops.py,"def _datetimelike_compat(func: F) -> F:

    @functools.wraps(func)
    def new_func(values: np.ndarray, *, axis: AxisInt | None=None, skipna: bool=True, mask: npt.NDArray[np.bool_] | None=None, **kwargs):
        orig_values = values
        datetimelike = values.dtype.kind in 'mM'
        if datetimelike and mask is None:
            mask = isna(values)
        result = func(values, axis=axis, skipna=skipna, mask=mask, **kwargs)
        if datetimelike:
            result = _wrap_results(result, orig_values.dtype, fill_value=iNaT)
            if not skipna:
                assert mask is not None
                result = _mask_datetimelike_result(result, axis, mask, orig_values)
        return result
    return cast(F, new_func)","""""""If we have datetime64 or timedelta64 values, ensure we have a correct
mask before calling the wrapped function, then cast back afterwards."""""""
pandas/core/nanops.py,"def _na_for_min_count(values: np.ndarray, axis: AxisInt | None) -> Scalar | np.ndarray:
    if values.dtype.kind in 'iufcb':
        values = values.astype('float64')
    fill_value = na_value_for_dtype(values.dtype)
    if values.ndim == 1:
        return fill_value
    elif axis is None:
        return fill_value
    else:
        result_shape = values.shape[:axis] + values.shape[axis + 1:]
        return np.full(result_shape, fill_value, dtype=values.dtype)","""""""Return the missing value for `values`.

Parameters
----------
values : ndarray
axis : int or None
    axis for the reduction, required if values.ndim > 1.

Returns
-------
result : scalar or ndarray
    For 1-D values, returns a scalar of the correct missing type.
    For 2-D values, returns a 1-D array where each element is missing."""""""
pandas/core/nanops.py,"def maybe_operate_rowwise(func: F) -> F:

    @functools.wraps(func)
    def newfunc(values: np.ndarray, *, axis: AxisInt | None=None, **kwargs):
        if axis == 1 and values.ndim == 2 and values.flags['C_CONTIGUOUS'] and (values.shape[1] / 1000 > values.shape[0]) and (values.dtype != object) and (values.dtype != bool):
            arrs = list(values)
            if kwargs.get('mask') is not None:
                mask = kwargs.pop('mask')
                results = [func(arrs[i], mask=mask[i], **kwargs) for i in range(len(arrs))]
            else:
                results = [func(x, **kwargs) for x in arrs]
            return np.array(results)
        return func(values, axis=axis, **kwargs)
    return cast(F, newfunc)","""""""NumPy operations on C-contiguous ndarrays with axis=1 can be
very slow if axis 1 >> axis 0.
Operate row-by-row and concatenate the results."""""""
pandas/core/nanops.py,"def nanany(values: np.ndarray, *, axis: AxisInt | None=None, skipna: bool=True, mask: npt.NDArray[np.bool_] | None=None) -> bool:
    if values.dtype.kind in 'iub' and mask is None:
        return values.any(axis)
    if values.dtype.kind == 'M':
        warnings.warn(""'any' with datetime64 dtypes is deprecated and will raise in a future version. Use (obj != pd.Timestamp(0)).any() instead."", FutureWarning, stacklevel=find_stack_level())
    (values, _) = _get_values(values, skipna, fill_value=False, mask=mask)
    if values.dtype == object:
        values = values.astype(bool)
    return values.any(axis)","""""""Check if any elements along an axis evaluate to True.

Parameters
----------
values : ndarray
axis : int, optional
skipna : bool, default True
mask : ndarray[bool], optional
    nan-mask if known

Returns
-------
result : bool

Examples
--------
>>> from pandas.core import nanops
>>> s = pd.Series([1, 2])
>>> nanops.nanany(s.values)
True

>>> from pandas.core import nanops
>>> s = pd.Series([np.nan])
>>> nanops.nanany(s.values)
False"""""""
pandas/core/nanops.py,"def nanall(values: np.ndarray, *, axis: AxisInt | None=None, skipna: bool=True, mask: npt.NDArray[np.bool_] | None=None) -> bool:
    if values.dtype.kind in 'iub' and mask is None:
        return values.all(axis)
    if values.dtype.kind == 'M':
        warnings.warn(""'all' with datetime64 dtypes is deprecated and will raise in a future version. Use (obj != pd.Timestamp(0)).all() instead."", FutureWarning, stacklevel=find_stack_level())
    (values, _) = _get_values(values, skipna, fill_value=True, mask=mask)
    if values.dtype == object:
        values = values.astype(bool)
    return values.all(axis)","""""""Check if all elements along an axis evaluate to True.

Parameters
----------
values : ndarray
axis : int, optional
skipna : bool, default True
mask : ndarray[bool], optional
    nan-mask if known

Returns
-------
result : bool

Examples
--------
>>> from pandas.core import nanops
>>> s = pd.Series([1, 2, np.nan])
>>> nanops.nanall(s.values)
True

>>> from pandas.core import nanops
>>> s = pd.Series([1, 0])
>>> nanops.nanall(s.values)
False"""""""
pandas/core/nanops.py,"@disallow('M8')
@_datetimelike_compat
@maybe_operate_rowwise
def nansum(values: np.ndarray, *, axis: AxisInt | None=None, skipna: bool=True, min_count: int=0, mask: npt.NDArray[np.bool_] | None=None) -> float:
    dtype = values.dtype
    (values, mask) = _get_values(values, skipna, fill_value=0, mask=mask)
    dtype_sum = _get_dtype_max(dtype)
    if dtype.kind == 'f':
        dtype_sum = dtype
    elif dtype.kind == 'm':
        dtype_sum = np.dtype(np.float64)
    the_sum = values.sum(axis, dtype=dtype_sum)
    the_sum = _maybe_null_out(the_sum, axis, mask, values.shape, min_count=min_count)
    return the_sum","""""""Sum the elements along an axis ignoring NaNs

Parameters
----------
values : ndarray[dtype]
axis : int, optional
skipna : bool, default True
min_count: int, default 0
mask : ndarray[bool], optional
    nan-mask if known

Returns
-------
result : dtype

Examples
--------
>>> from pandas.core import nanops
>>> s = pd.Series([1, 2, np.nan])
>>> nanops.nansum(s.values)
3.0"""""""
pandas/core/nanops.py,"@bottleneck_switch()
@_datetimelike_compat
def nanmean(values: np.ndarray, *, axis: AxisInt | None=None, skipna: bool=True, mask: npt.NDArray[np.bool_] | None=None) -> float:
    dtype = values.dtype
    (values, mask) = _get_values(values, skipna, fill_value=0, mask=mask)
    dtype_sum = _get_dtype_max(dtype)
    dtype_count = np.dtype(np.float64)
    if dtype.kind in 'mM':
        dtype_sum = np.dtype(np.float64)
    elif dtype.kind in 'iu':
        dtype_sum = np.dtype(np.float64)
    elif dtype.kind == 'f':
        dtype_sum = dtype
        dtype_count = dtype
    count = _get_counts(values.shape, mask, axis, dtype=dtype_count)
    the_sum = values.sum(axis, dtype=dtype_sum)
    the_sum = _ensure_numeric(the_sum)
    if axis is not None and getattr(the_sum, 'ndim', False):
        count = cast(np.ndarray, count)
        with np.errstate(all='ignore'):
            the_mean = the_sum / count
        ct_mask = count == 0
        if ct_mask.any():
            the_mean[ct_mask] = np.nan
    else:
        the_mean = the_sum / count if count > 0 else np.nan
    return the_mean","""""""Compute the mean of the element along an axis ignoring NaNs

Parameters
----------
values : ndarray
axis : int, optional
skipna : bool, default True
mask : ndarray[bool], optional
    nan-mask if known

Returns
-------
float
    Unless input is a float array, in which case use the same
    precision as the input array.

Examples
--------
>>> from pandas.core import nanops
>>> s = pd.Series([1, 2, np.nan])
>>> nanops.nanmean(s.values)
1.5"""""""
pandas/core/nanops.py,"@bottleneck_switch()
def nanmedian(values, *, axis: AxisInt | None=None, skipna: bool=True, mask=None):

    def get_median(x, _mask=None):
        if _mask is None:
            _mask = notna(x)
        else:
            _mask = ~_mask
        if not skipna and (not _mask.all()):
            return np.nan
        with warnings.catch_warnings():
            warnings.filterwarnings('ignore', 'All-NaN slice encountered', RuntimeWarning)
            res = np.nanmedian(x[_mask])
        return res
    dtype = values.dtype
    (values, mask) = _get_values(values, skipna, mask=mask, fill_value=0)
    if values.dtype.kind != 'f':
        if values.dtype == object:
            inferred = lib.infer_dtype(values)
            if inferred in ['string', 'mixed']:
                raise TypeError(f'Cannot convert {values} to numeric')
        try:
            values = values.astype('f8')
        except ValueError as err:
            raise TypeError(str(err)) from err
    if mask is not None:
        values[mask] = np.nan
    notempty = values.size
    if values.ndim > 1 and axis is not None:
        if notempty:
            if not skipna:
                res = np.apply_along_axis(get_median, axis, values)
            else:
                with warnings.catch_warnings():
                    warnings.filterwarnings('ignore', 'All-NaN slice encountered', RuntimeWarning)
                    if values.shape[1] == 1 and axis == 0 or (values.shape[0] == 1 and axis == 1):
                        res = np.nanmedian(np.squeeze(values), keepdims=True)
                    else:
                        res = np.nanmedian(values, axis=axis)
        else:
            res = _get_empty_reduction_result(values.shape, axis)
    else:
        res = get_median(values, mask) if notempty else np.nan
    return _wrap_results(res, dtype)","""""""Parameters
----------
values : ndarray
axis : int, optional
skipna : bool, default True
mask : ndarray[bool], optional
    nan-mask if known

Returns
-------
result : float
    Unless input is a float array, in which case use the same
    precision as the input array.

Examples
--------
>>> from pandas.core import nanops
>>> s = pd.Series([1, np.nan, 2, 2])
>>> nanops.nanmedian(s.values)
2.0"""""""
pandas/core/nanops.py,"def _get_empty_reduction_result(shape: Shape, axis: AxisInt) -> np.ndarray:
    shp = np.array(shape)
    dims = np.arange(len(shape))
    ret = np.empty(shp[dims != axis], dtype=np.float64)
    ret.fill(np.nan)
    return ret","""""""The result from a reduction on an empty ndarray.

Parameters
----------
shape : Tuple[int, ...]
axis : int

Returns
-------
np.ndarray"""""""
pandas/core/nanops.py,"def _get_counts_nanvar(values_shape: Shape, mask: npt.NDArray[np.bool_] | None, axis: AxisInt | None, ddof: int, dtype: np.dtype=np.dtype(np.float64)) -> tuple[float | np.ndarray, float | np.ndarray]:
    count = _get_counts(values_shape, mask, axis, dtype=dtype)
    d = count - dtype.type(ddof)
    if is_float(count):
        if count <= ddof:
            count = np.nan
            d = np.nan
    else:
        count = cast(np.ndarray, count)
        mask = count <= ddof
        if mask.any():
            np.putmask(d, mask, np.nan)
            np.putmask(count, mask, np.nan)
    return (count, d)","""""""Get the count of non-null values along an axis, accounting
for degrees of freedom.

Parameters
----------
values_shape : Tuple[int, ...]
    shape tuple from values ndarray, used if mask is None
mask : Optional[ndarray[bool]]
    locations in values that should be considered missing
axis : Optional[int]
    axis to count along
ddof : int
    degrees of freedom
dtype : type, optional
    type to use for count

Returns
-------
count : int, np.nan or np.ndarray
d : int, np.nan or np.ndarray"""""""
pandas/core/nanops.py,"@bottleneck_switch(ddof=1)
def nanstd(values, *, axis: AxisInt | None=None, skipna: bool=True, ddof: int=1, mask=None):
    if values.dtype == 'M8[ns]':
        values = values.view('m8[ns]')
    orig_dtype = values.dtype
    (values, mask) = _get_values(values, skipna, mask=mask)
    result = np.sqrt(nanvar(values, axis=axis, skipna=skipna, ddof=ddof, mask=mask))
    return _wrap_results(result, orig_dtype)","""""""Compute the standard deviation along given axis while ignoring NaNs

Parameters
----------
values : ndarray
axis : int, optional
skipna : bool, default True
ddof : int, default 1
    Delta Degrees of Freedom. The divisor used in calculations is N - ddof,
    where N represents the number of elements.
mask : ndarray[bool], optional
    nan-mask if known

Returns
-------
result : float
    Unless input is a float array, in which case use the same
    precision as the input array.

Examples
--------
>>> from pandas.core import nanops
>>> s = pd.Series([1, np.nan, 2, 3])
>>> nanops.nanstd(s.values)
1.0"""""""
pandas/core/nanops.py,"@disallow('M8', 'm8')
@bottleneck_switch(ddof=1)
def nanvar(values: np.ndarray, *, axis: AxisInt | None=None, skipna: bool=True, ddof: int=1, mask=None):
    dtype = values.dtype
    mask = _maybe_get_mask(values, skipna, mask)
    if dtype.kind in 'iu':
        values = values.astype('f8')
        if mask is not None:
            values[mask] = np.nan
    if values.dtype.kind == 'f':
        (count, d) = _get_counts_nanvar(values.shape, mask, axis, ddof, values.dtype)
    else:
        (count, d) = _get_counts_nanvar(values.shape, mask, axis, ddof)
    if skipna and mask is not None:
        values = values.copy()
        np.putmask(values, mask, 0)
    avg = _ensure_numeric(values.sum(axis=axis, dtype=np.float64)) / count
    if axis is not None:
        avg = np.expand_dims(avg, axis)
    sqr = _ensure_numeric((avg - values) ** 2)
    if mask is not None:
        np.putmask(sqr, mask, 0)
    result = sqr.sum(axis=axis, dtype=np.float64) / d
    if dtype.kind == 'f':
        result = result.astype(dtype, copy=False)
    return result","""""""Compute the variance along given axis while ignoring NaNs

Parameters
----------
values : ndarray
axis : int, optional
skipna : bool, default True
ddof : int, default 1
    Delta Degrees of Freedom. The divisor used in calculations is N - ddof,
    where N represents the number of elements.
mask : ndarray[bool], optional
    nan-mask if known

Returns
-------
result : float
    Unless input is a float array, in which case use the same
    precision as the input array.

Examples
--------
>>> from pandas.core import nanops
>>> s = pd.Series([1, np.nan, 2, 3])
>>> nanops.nanvar(s.values)
1.0"""""""
pandas/core/nanops.py,"@disallow('M8', 'm8')
def nansem(values: np.ndarray, *, axis: AxisInt | None=None, skipna: bool=True, ddof: int=1, mask: npt.NDArray[np.bool_] | None=None) -> float:
    nanvar(values, axis=axis, skipna=skipna, ddof=ddof, mask=mask)
    mask = _maybe_get_mask(values, skipna, mask)
    if values.dtype.kind != 'f':
        values = values.astype('f8')
    if not skipna and mask is not None and mask.any():
        return np.nan
    (count, _) = _get_counts_nanvar(values.shape, mask, axis, ddof, values.dtype)
    var = nanvar(values, axis=axis, skipna=skipna, ddof=ddof, mask=mask)
    return np.sqrt(var) / np.sqrt(count)","""""""Compute the standard error in the mean along given axis while ignoring NaNs

Parameters
----------
values : ndarray
axis : int, optional
skipna : bool, default True
ddof : int, default 1
    Delta Degrees of Freedom. The divisor used in calculations is N - ddof,
    where N represents the number of elements.
mask : ndarray[bool], optional
    nan-mask if known

Returns
-------
result : float64
    Unless input is a float array, in which case use the same
    precision as the input array.

Examples
--------
>>> from pandas.core import nanops
>>> s = pd.Series([1, np.nan, 2, 3])
>>> nanops.nansem(s.values)
 0.5773502691896258"""""""
pandas/core/nanops.py,"def nanargmax(values: np.ndarray, *, axis: AxisInt | None=None, skipna: bool=True, mask: npt.NDArray[np.bool_] | None=None) -> int | np.ndarray:
    (values, mask) = _get_values(values, True, fill_value_typ='-inf', mask=mask)
    result = values.argmax(axis)
    result = _maybe_arg_null_out(result, axis, mask, skipna)
    return result","""""""Parameters
----------
values : ndarray
axis : int, optional
skipna : bool, default True
mask : ndarray[bool], optional
    nan-mask if known

Returns
-------
result : int or ndarray[int]
    The index/indices  of max value in specified axis or -1 in the NA case

Examples
--------
>>> from pandas.core import nanops
>>> arr = np.array([1, 2, 3, np.nan, 4])
>>> nanops.nanargmax(arr)
4

>>> arr = np.array(range(12), dtype=np.float64).reshape(4, 3)
>>> arr[2:, 2] = np.nan
>>> arr
array([[ 0.,  1.,  2.],
       [ 3.,  4.,  5.],
       [ 6.,  7., nan],
       [ 9., 10., nan]])
>>> nanops.nanargmax(arr, axis=1)
array([2, 2, 1, 1])"""""""
pandas/core/nanops.py,"def nanargmin(values: np.ndarray, *, axis: AxisInt | None=None, skipna: bool=True, mask: npt.NDArray[np.bool_] | None=None) -> int | np.ndarray:
    (values, mask) = _get_values(values, True, fill_value_typ='+inf', mask=mask)
    result = values.argmin(axis)
    result = _maybe_arg_null_out(result, axis, mask, skipna)
    return result","""""""Parameters
----------
values : ndarray
axis : int, optional
skipna : bool, default True
mask : ndarray[bool], optional
    nan-mask if known

Returns
-------
result : int or ndarray[int]
    The index/indices of min value in specified axis or -1 in the NA case

Examples
--------
>>> from pandas.core import nanops
>>> arr = np.array([1, 2, 3, np.nan, 4])
>>> nanops.nanargmin(arr)
0

>>> arr = np.array(range(12), dtype=np.float64).reshape(4, 3)
>>> arr[2:, 0] = np.nan
>>> arr
array([[ 0.,  1.,  2.],
       [ 3.,  4.,  5.],
       [nan,  7.,  8.],
       [nan, 10., 11.]])
>>> nanops.nanargmin(arr, axis=1)
array([0, 0, 1, 1])"""""""
pandas/core/nanops.py,"@disallow('M8', 'm8')
@maybe_operate_rowwise
def nanskew(values: np.ndarray, *, axis: AxisInt | None=None, skipna: bool=True, mask: npt.NDArray[np.bool_] | None=None) -> float:
    mask = _maybe_get_mask(values, skipna, mask)
    if values.dtype.kind != 'f':
        values = values.astype('f8')
        count = _get_counts(values.shape, mask, axis)
    else:
        count = _get_counts(values.shape, mask, axis, dtype=values.dtype)
    if skipna and mask is not None:
        values = values.copy()
        np.putmask(values, mask, 0)
    elif not skipna and mask is not None and mask.any():
        return np.nan
    with np.errstate(invalid='ignore', divide='ignore'):
        mean = values.sum(axis, dtype=np.float64) / count
    if axis is not None:
        mean = np.expand_dims(mean, axis)
    adjusted = values - mean
    if skipna and mask is not None:
        np.putmask(adjusted, mask, 0)
    adjusted2 = adjusted ** 2
    adjusted3 = adjusted2 * adjusted
    m2 = adjusted2.sum(axis, dtype=np.float64)
    m3 = adjusted3.sum(axis, dtype=np.float64)
    m2 = _zero_out_fperr(m2)
    m3 = _zero_out_fperr(m3)
    with np.errstate(invalid='ignore', divide='ignore'):
        result = count * (count - 1) ** 0.5 / (count - 2) * (m3 / m2 ** 1.5)
    dtype = values.dtype
    if dtype.kind == 'f':
        result = result.astype(dtype, copy=False)
    if isinstance(result, np.ndarray):
        result = np.where(m2 == 0, 0, result)
        result[count < 3] = np.nan
    else:
        result = dtype.type(0) if m2 == 0 else result
        if count < 3:
            return np.nan
    return result","""""""Compute the sample skewness.

The statistic computed here is the adjusted Fisher-Pearson standardized
moment coefficient G1. The algorithm computes this coefficient directly
from the second and third central moment.

Parameters
----------
values : ndarray
axis : int, optional
skipna : bool, default True
mask : ndarray[bool], optional
    nan-mask if known

Returns
-------
result : float64
    Unless input is a float array, in which case use the same
    precision as the input array.

Examples
--------
>>> from pandas.core import nanops
>>> s = pd.Series([1, np.nan, 1, 2])
>>> nanops.nanskew(s.values)
1.7320508075688787"""""""
pandas/core/nanops.py,"@disallow('M8', 'm8')
@maybe_operate_rowwise
def nankurt(values: np.ndarray, *, axis: AxisInt | None=None, skipna: bool=True, mask: npt.NDArray[np.bool_] | None=None) -> float:
    mask = _maybe_get_mask(values, skipna, mask)
    if values.dtype.kind != 'f':
        values = values.astype('f8')
        count = _get_counts(values.shape, mask, axis)
    else:
        count = _get_counts(values.shape, mask, axis, dtype=values.dtype)
    if skipna and mask is not None:
        values = values.copy()
        np.putmask(values, mask, 0)
    elif not skipna and mask is not None and mask.any():
        return np.nan
    with np.errstate(invalid='ignore', divide='ignore'):
        mean = values.sum(axis, dtype=np.float64) / count
    if axis is not None:
        mean = np.expand_dims(mean, axis)
    adjusted = values - mean
    if skipna and mask is not None:
        np.putmask(adjusted, mask, 0)
    adjusted2 = adjusted ** 2
    adjusted4 = adjusted2 ** 2
    m2 = adjusted2.sum(axis, dtype=np.float64)
    m4 = adjusted4.sum(axis, dtype=np.float64)
    with np.errstate(invalid='ignore', divide='ignore'):
        adj = 3 * (count - 1) ** 2 / ((count - 2) * (count - 3))
        numerator = count * (count + 1) * (count - 1) * m4
        denominator = (count - 2) * (count - 3) * m2 ** 2
    numerator = _zero_out_fperr(numerator)
    denominator = _zero_out_fperr(denominator)
    if not isinstance(denominator, np.ndarray):
        if count < 4:
            return np.nan
        if denominator == 0:
            return values.dtype.type(0)
    with np.errstate(invalid='ignore', divide='ignore'):
        result = numerator / denominator - adj
    dtype = values.dtype
    if dtype.kind == 'f':
        result = result.astype(dtype, copy=False)
    if isinstance(result, np.ndarray):
        result = np.where(denominator == 0, 0, result)
        result[count < 4] = np.nan
    return result","""""""Compute the sample excess kurtosis

The statistic computed here is the adjusted Fisher-Pearson standardized
moment coefficient G2, computed directly from the second and fourth
central moment.

Parameters
----------
values : ndarray
axis : int, optional
skipna : bool, default True
mask : ndarray[bool], optional
    nan-mask if known

Returns
-------
result : float64
    Unless input is a float array, in which case use the same
    precision as the input array.

Examples
--------
>>> from pandas.core import nanops
>>> s = pd.Series([1, np.nan, 1, 3, 2])
>>> nanops.nankurt(s.values)
-1.2892561983471076"""""""
pandas/core/nanops.py,"@disallow('M8', 'm8')
@maybe_operate_rowwise
def nanprod(values: np.ndarray, *, axis: AxisInt | None=None, skipna: bool=True, min_count: int=0, mask: npt.NDArray[np.bool_] | None=None) -> float:
    mask = _maybe_get_mask(values, skipna, mask)
    if skipna and mask is not None:
        values = values.copy()
        values[mask] = 1
    result = values.prod(axis)
    return _maybe_null_out(result, axis, mask, values.shape, min_count=min_count)","""""""Parameters
----------
values : ndarray[dtype]
axis : int, optional
skipna : bool, default True
min_count: int, default 0
mask : ndarray[bool], optional
    nan-mask if known

Returns
-------
Dtype
    The product of all elements on a given axis. ( NaNs are treated as 1)

Examples
--------
>>> from pandas.core import nanops
>>> s = pd.Series([1, 2, 3, np.nan])
>>> nanops.nanprod(s.values)
6.0"""""""
pandas/core/nanops.py,"def _get_counts(values_shape: Shape, mask: npt.NDArray[np.bool_] | None, axis: AxisInt | None, dtype: np.dtype[np.floating]=np.dtype(np.float64)) -> np.floating | npt.NDArray[np.floating]:
    if axis is None:
        if mask is not None:
            n = mask.size - mask.sum()
        else:
            n = np.prod(values_shape)
        return dtype.type(n)
    if mask is not None:
        count = mask.shape[axis] - mask.sum(axis)
    else:
        count = values_shape[axis]
    if is_integer(count):
        return dtype.type(count)
    return count.astype(dtype, copy=False)","""""""Get the count of non-null values along an axis

Parameters
----------
values_shape : tuple of int
    shape tuple from values ndarray, used if mask is None
mask : Optional[ndarray[bool]]
    locations in values that should be considered missing
axis : Optional[int]
    axis to count along
dtype : type, optional
    type to use for count

Returns
-------
count : scalar or array"""""""
pandas/core/nanops.py,"def _maybe_null_out(result: np.ndarray | float | NaTType, axis: AxisInt | None, mask: npt.NDArray[np.bool_] | None, shape: tuple[int, ...], min_count: int=1) -> np.ndarray | float | NaTType:
    if mask is None and min_count == 0:
        return result
    if axis is not None and isinstance(result, np.ndarray):
        if mask is not None:
            null_mask = mask.shape[axis] - mask.sum(axis) - min_count < 0
        else:
            below_count = shape[axis] - min_count < 0
            new_shape = shape[:axis] + shape[axis + 1:]
            null_mask = np.broadcast_to(below_count, new_shape)
        if np.any(null_mask):
            if is_numeric_dtype(result):
                if np.iscomplexobj(result):
                    result = result.astype('c16')
                elif not is_float_dtype(result):
                    result = result.astype('f8', copy=False)
                result[null_mask] = np.nan
            else:
                result[null_mask] = None
    elif result is not NaT:
        if check_below_min_count(shape, mask, min_count):
            result_dtype = getattr(result, 'dtype', None)
            if is_float_dtype(result_dtype):
                result = result_dtype.type('nan')
            else:
                result = np.nan
    return result","""""""Returns
-------
Dtype
    The product of all elements on a given axis. ( NaNs are treated as 1)"""""""
pandas/core/nanops.py,"def check_below_min_count(shape: tuple[int, ...], mask: npt.NDArray[np.bool_] | None, min_count: int) -> bool:
    if min_count > 0:
        if mask is None:
            non_nulls = np.prod(shape)
        else:
            non_nulls = mask.size - mask.sum()
        if non_nulls < min_count:
            return True
    return False","""""""Check for the `min_count` keyword. Returns True if below `min_count` (when
missing value should be returned from the reduction).

Parameters
----------
shape : tuple
    The shape of the values (`values.shape`).
mask : ndarray[bool] or None
    Boolean numpy array (typically of same shape as `shape`) or None.
min_count : int
    Keyword passed through from sum/prod call.

Returns
-------
bool"""""""
pandas/core/nanops.py,"@disallow('M8', 'm8')
def nancorr(a: np.ndarray, b: np.ndarray, *, method: CorrelationMethod='pearson', min_periods: int | None=None) -> float:
    if len(a) != len(b):
        raise AssertionError('Operands to nancorr must have same size')
    if min_periods is None:
        min_periods = 1
    valid = notna(a) & notna(b)
    if not valid.all():
        a = a[valid]
        b = b[valid]
    if len(a) < min_periods:
        return np.nan
    a = _ensure_numeric(a)
    b = _ensure_numeric(b)
    f = get_corr_func(method)
    return f(a, b)","""""""a, b: ndarrays"""""""
pandas/core/nanops.py,"def na_accum_func(values: ArrayLike, accum_func, *, skipna: bool) -> ArrayLike:
    (mask_a, mask_b) = {np.cumprod: (1.0, np.nan), np.maximum.accumulate: (-np.inf, np.nan), np.cumsum: (0.0, np.nan), np.minimum.accumulate: (np.inf, np.nan)}[accum_func]
    assert values.dtype.kind not in 'mM'
    if skipna and (not issubclass(values.dtype.type, (np.integer, np.bool_))):
        vals = values.copy()
        mask = isna(vals)
        vals[mask] = mask_a
        result = accum_func(vals, axis=0)
        result[mask] = mask_b
    else:
        result = accum_func(values, axis=0)
    return result","""""""Cumulative function with skipna support.

Parameters
----------
values : np.ndarray or ExtensionArray
accum_func : {np.cumprod, np.maximum.accumulate, np.cumsum, np.minimum.accumulate}
skipna : bool

Returns
-------
np.ndarray or ExtensionArray"""""""
pandas/core/ops/array_ops.py,"def fill_binop(left, right, fill_value):
    if fill_value is not None:
        left_mask = isna(left)
        right_mask = isna(right)
        mask = left_mask ^ right_mask
        if left_mask.any():
            left = left.copy()
            left[left_mask & mask] = fill_value
        if right_mask.any():
            right = right.copy()
            right[right_mask & mask] = fill_value
    return (left, right)","""""""If a non-None fill_value is given, replace null entries in left and right
with this value, but only in positions where _one_ of left/right is null,
not both.

Parameters
----------
left : array-like
right : array-like
fill_value : object

Returns
-------
left : array-like
right : array-like

Notes
-----
Makes copies if fill_value is not None and NAs are present."""""""
pandas/core/ops/array_ops.py,"def _masked_arith_op(x: np.ndarray, y, op):
    xrav = x.ravel()
    if isinstance(y, np.ndarray):
        dtype = find_common_type([x.dtype, y.dtype])
        result = np.empty(x.size, dtype=dtype)
        if len(x) != len(y):
            raise ValueError(x.shape, y.shape)
        ymask = notna(y)
        yrav = y.ravel()
        mask = notna(xrav) & ymask.ravel()
        if mask.any():
            result[mask] = op(xrav[mask], yrav[mask])
    else:
        if not is_scalar(y):
            raise TypeError(f'Cannot broadcast np.ndarray with operand of type {type(y)}')
        result = np.empty(x.size, dtype=x.dtype)
        mask = notna(xrav)
        if op is pow:
            mask = np.where(x == 1, False, mask)
        elif op is roperator.rpow:
            mask = np.where(y == 1, False, mask)
        if mask.any():
            result[mask] = op(xrav[mask], y)
    np.putmask(result, ~mask, np.nan)
    result = result.reshape(x.shape)
    return result","""""""If the given arithmetic operation fails, attempt it again on
only the non-null elements of the input array(s).

Parameters
----------
x : np.ndarray
y : np.ndarray, Series, Index
op : binary operator"""""""
pandas/core/ops/array_ops.py,"def _na_arithmetic_op(left: np.ndarray, right, op, is_cmp: bool=False):
    if isinstance(right, str):
        func = op
    else:
        func = partial(expressions.evaluate, op)
    try:
        result = func(left, right)
    except TypeError:
        if not is_cmp and (left.dtype == object or getattr(right, 'dtype', None) == object):
            result = _masked_arith_op(left, right, op)
        else:
            raise
    if is_cmp and (is_scalar(result) or result is NotImplemented):
        return invalid_comparison(left, right, op)
    return missing.dispatch_fill_zeros(op, left, right, result)","""""""Return the result of evaluating op on the passed in values.

If native types are not compatible, try coercion to object dtype.

Parameters
----------
left : np.ndarray
right : np.ndarray or scalar
    Excludes DataFrame, Series, Index, ExtensionArray.
is_cmp : bool, default False
    If this a comparison operation.

Returns
-------
array-like

Raises
------
TypeError : invalid operation"""""""
pandas/core/ops/array_ops.py,"def arithmetic_op(left: ArrayLike, right: Any, op):
    if should_extension_dispatch(left, right) or isinstance(right, (Timedelta, BaseOffset, Timestamp)) or right is NaT:
        res_values = op(left, right)
    else:
        _bool_arith_check(op, left, right)
        res_values = _na_arithmetic_op(left, right, op)
    return res_values","""""""Evaluate an arithmetic operation `+`, `-`, `*`, `/`, `//`, `%`, `**`, ...

Note: the caller is responsible for ensuring that numpy warnings are
suppressed (with np.errstate(all=""ignore"")) if needed.

Parameters
----------
left : np.ndarray or ExtensionArray
right : object
    Cannot be a DataFrame or Index.  Series is *not* excluded.
op : {operator.add, operator.sub, ...}
    Or one of the reversed variants from roperator.

Returns
-------
ndarray or ExtensionArray
    Or a 2-tuple of these in the case of divmod or rdivmod."""""""
pandas/core/ops/array_ops.py,"def comparison_op(left: ArrayLike, right: Any, op) -> ArrayLike:
    lvalues = ensure_wrapped_if_datetimelike(left)
    rvalues = ensure_wrapped_if_datetimelike(right)
    rvalues = lib.item_from_zerodim(rvalues)
    if isinstance(rvalues, list):
        rvalues = np.asarray(rvalues)
    if isinstance(rvalues, (np.ndarray, ABCExtensionArray)):
        if len(lvalues) != len(rvalues):
            raise ValueError('Lengths must match to compare', lvalues.shape, rvalues.shape)
    if should_extension_dispatch(lvalues, rvalues) or ((isinstance(rvalues, (Timedelta, BaseOffset, Timestamp)) or right is NaT) and lvalues.dtype != object):
        res_values = op(lvalues, rvalues)
    elif is_scalar(rvalues) and isna(rvalues):
        if op is operator.ne:
            res_values = np.ones(lvalues.shape, dtype=bool)
        else:
            res_values = np.zeros(lvalues.shape, dtype=bool)
    elif is_numeric_v_string_like(lvalues, rvalues):
        return invalid_comparison(lvalues, rvalues, op)
    elif lvalues.dtype == object or isinstance(rvalues, str):
        res_values = comp_method_OBJECT_ARRAY(op, lvalues, rvalues)
    else:
        res_values = _na_arithmetic_op(lvalues, rvalues, op, is_cmp=True)
    return res_values","""""""Evaluate a comparison operation `=`, `!=`, `>=`, `>`, `<=`, or `<`.

Note: the caller is responsible for ensuring that numpy warnings are
suppressed (with np.errstate(all=""ignore"")) if needed.

Parameters
----------
left : np.ndarray or ExtensionArray
right : object
    Cannot be a DataFrame, Series, or Index.
op : {operator.eq, operator.ne, operator.gt, operator.ge, operator.lt, operator.le}

Returns
-------
ndarray or ExtensionArray"""""""
pandas/core/ops/array_ops.py,"def logical_op(left: ArrayLike, right: Any, op) -> ArrayLike:

    def fill_bool(x, left=None):
        if x.dtype.kind in 'cfO':
            mask = isna(x)
            if mask.any():
                x = x.astype(object)
                x[mask] = False
        if left is None or left.dtype.kind == 'b':
            x = x.astype(bool)
        return x
    right = lib.item_from_zerodim(right)
    if is_list_like(right) and (not hasattr(right, 'dtype')):
        warnings.warn('Logical ops (and, or, xor) between Pandas objects and dtype-less sequences (e.g. list, tuple) are deprecated and will raise in a future version. Wrap the object in a Series, Index, or np.array before operating instead.', FutureWarning, stacklevel=find_stack_level())
        right = construct_1d_object_array_from_listlike(right)
    lvalues = ensure_wrapped_if_datetimelike(left)
    rvalues = right
    if should_extension_dispatch(lvalues, rvalues):
        res_values = op(lvalues, rvalues)
    else:
        if isinstance(rvalues, np.ndarray):
            is_other_int_dtype = rvalues.dtype.kind in 'iu'
            if not is_other_int_dtype:
                rvalues = fill_bool(rvalues, lvalues)
        else:
            is_other_int_dtype = lib.is_integer(rvalues)
        res_values = na_logical_op(lvalues, rvalues, op)
        if not (left.dtype.kind in 'iu' and is_other_int_dtype):
            res_values = fill_bool(res_values)
    return res_values","""""""Evaluate a logical operation `|`, `&`, or `^`.

Parameters
----------
left : np.ndarray or ExtensionArray
right : object
    Cannot be a DataFrame, Series, or Index.
op : {operator.and_, operator.or_, operator.xor}
    Or one of the reversed variants from roperator.

Returns
-------
ndarray or ExtensionArray"""""""
pandas/core/ops/array_ops.py,"def get_array_op(op):
    if isinstance(op, partial):
        return op
    op_name = op.__name__.strip('_').lstrip('r')
    if op_name == 'arith_op':
        return op
    if op_name in {'eq', 'ne', 'lt', 'le', 'gt', 'ge'}:
        return partial(comparison_op, op=op)
    elif op_name in {'and', 'or', 'xor', 'rand', 'ror', 'rxor'}:
        return partial(logical_op, op=op)
    elif op_name in {'add', 'sub', 'mul', 'truediv', 'floordiv', 'mod', 'divmod', 'pow'}:
        return partial(arithmetic_op, op=op)
    else:
        raise NotImplementedError(op_name)","""""""Return a binary array operation corresponding to the given operator op.

Parameters
----------
op : function
    Binary operator from operator or roperator module.

Returns
-------
functools.partial"""""""
pandas/core/ops/array_ops.py,"def maybe_prepare_scalar_for_op(obj, shape: Shape):
    if type(obj) is datetime.timedelta:
        return Timedelta(obj)
    elif type(obj) is datetime.datetime:
        return Timestamp(obj)
    elif isinstance(obj, np.datetime64):
        if isna(obj):
            from pandas.core.arrays import DatetimeArray
            if is_unitless(obj.dtype):
                obj = obj.astype('datetime64[ns]')
            elif not is_supported_unit(get_unit_from_dtype(obj.dtype)):
                unit = get_unit_from_dtype(obj.dtype)
                closest_unit = npy_unit_to_abbrev(get_supported_reso(unit))
                obj = obj.astype(f'datetime64[{closest_unit}]')
            right = np.broadcast_to(obj, shape)
            return DatetimeArray(right)
        return Timestamp(obj)
    elif isinstance(obj, np.timedelta64):
        if isna(obj):
            from pandas.core.arrays import TimedeltaArray
            if is_unitless(obj.dtype):
                obj = obj.astype('timedelta64[ns]')
            elif not is_supported_unit(get_unit_from_dtype(obj.dtype)):
                unit = get_unit_from_dtype(obj.dtype)
                closest_unit = npy_unit_to_abbrev(get_supported_reso(unit))
                obj = obj.astype(f'timedelta64[{closest_unit}]')
            right = np.broadcast_to(obj, shape)
            return TimedeltaArray(right)
        return Timedelta(obj)
    return obj","""""""Cast non-pandas objects to pandas types to unify behavior of arithmetic
and comparison operations.

Parameters
----------
obj: object
shape : tuple[int]

Returns
-------
out : object

Notes
-----
Be careful to call this *after* determining the `name` attribute to be
attached to the result of the arithmetic operation."""""""
pandas/core/ops/array_ops.py,"def _bool_arith_check(op, a: np.ndarray, b):
    if op in _BOOL_OP_NOT_ALLOWED:
        if a.dtype.kind == 'b' and (is_bool_dtype(b) or lib.is_bool(b)):
            op_name = op.__name__.strip('_').lstrip('r')
            raise NotImplementedError(f""operator '{op_name}' not implemented for bool dtypes"")","""""""In contrast to numpy, pandas raises an error for certain operations
with booleans."""""""
pandas/core/ops/common.py,"def unpack_zerodim_and_defer(name: str) -> Callable[[F], F]:

    def wrapper(method: F) -> F:
        return _unpack_zerodim_and_defer(method, name)
    return wrapper","""""""Boilerplate for pandas conventions in arithmetic and comparison methods.

Parameters
----------
name : str

Returns
-------
decorator"""""""
pandas/core/ops/common.py,"def _unpack_zerodim_and_defer(method, name: str):
    stripped_name = name.removeprefix('__').removesuffix('__')
    is_cmp = stripped_name in {'eq', 'ne', 'lt', 'le', 'gt', 'ge'}

    @wraps(method)
    def new_method(self, other):
        if is_cmp and isinstance(self, ABCIndex) and isinstance(other, ABCSeries):
            pass
        else:
            prio = getattr(other, '__pandas_priority__', None)
            if prio is not None:
                if prio > self.__pandas_priority__:
                    return NotImplemented
        other = item_from_zerodim(other)
        return method(self, other)
    return new_method","""""""Boilerplate for pandas conventions in arithmetic and comparison methods.

Ensure method returns NotImplemented when operating against ""senior""
classes.  Ensure zero-dimensional ndarrays are always unpacked.

Parameters
----------
method : binary method
name : str

Returns
-------
method"""""""
pandas/core/ops/common.py,"def get_op_result_name(left, right):
    if isinstance(right, (ABCSeries, ABCIndex)):
        name = _maybe_match_name(left, right)
    else:
        name = left.name
    return name","""""""Find the appropriate name to pin to an operation result.  This result
should always be either an Index or a Series.

Parameters
----------
left : {Series, Index}
right : object

Returns
-------
name : object
    Usually a string"""""""
pandas/core/ops/common.py,"def _maybe_match_name(a, b):
    a_has = hasattr(a, 'name')
    b_has = hasattr(b, 'name')
    if a_has and b_has:
        try:
            if a.name == b.name:
                return a.name
            elif is_matching_na(a.name, b.name):
                return a.name
            else:
                return None
        except TypeError:
            if is_matching_na(a.name, b.name):
                return a.name
            return None
        except ValueError:
            return None
    elif a_has:
        return a.name
    elif b_has:
        return b.name
    return None","""""""Try to find a name to attach to the result of an operation between
a and b.  If only one of these has a `name` attribute, return that
name.  Otherwise return a consensus name if they match or None if
they have different names.

Parameters
----------
a : object
b : object

Returns
-------
name : str or None

See Also
--------
pandas.core.common.consensus_name_attr"""""""
pandas/core/ops/dispatch.py,"def should_extension_dispatch(left: ArrayLike, right: Any) -> bool:
    return isinstance(left, ABCExtensionArray) or isinstance(right, ABCExtensionArray)","""""""Identify cases where Series operation should dispatch to ExtensionArray method.

Parameters
----------
left : np.ndarray or ExtensionArray
right : object

Returns
-------
bool"""""""
pandas/core/ops/docstrings.py,"def make_flex_doc(op_name: str, typ: str) -> str:
    op_name = op_name.replace('__', '')
    op_desc = _op_descriptions[op_name]
    op_desc_op = op_desc['op']
    assert op_desc_op is not None
    if op_name.startswith('r'):
        equiv = f'other {op_desc_op} {typ}'
    elif op_name == 'divmod':
        equiv = f'{op_name}({typ}, other)'
    else:
        equiv = f'{typ} {op_desc_op} other'
    if typ == 'series':
        base_doc = _flex_doc_SERIES
        if op_desc['reverse']:
            base_doc += _see_also_reverse_SERIES.format(reverse=op_desc['reverse'], see_also_desc=op_desc['see_also_desc'])
        doc_no_examples = base_doc.format(desc=op_desc['desc'], op_name=op_name, equiv=equiv, series_returns=op_desc['series_returns'])
        ser_example = op_desc['series_examples']
        if ser_example:
            doc = doc_no_examples + ser_example
        else:
            doc = doc_no_examples
    elif typ == 'dataframe':
        if op_name in ['eq', 'ne', 'le', 'lt', 'ge', 'gt']:
            base_doc = _flex_comp_doc_FRAME
            doc = _flex_comp_doc_FRAME.format(op_name=op_name, desc=op_desc['desc'])
        else:
            base_doc = _flex_doc_FRAME
            doc = base_doc.format(desc=op_desc['desc'], op_name=op_name, equiv=equiv, reverse=op_desc['reverse'])
    else:
        raise AssertionError('Invalid typ argument.')
    return doc","""""""Make the appropriate substitutions for the given operation and class-typ
into either _flex_doc_SERIES or _flex_doc_FRAME to return the docstring
to attach to a generated method.

Parameters
----------
op_name : str {'__add__', '__sub__', ... '__eq__', '__ne__', ...}
typ : str {series, 'dataframe']}

Returns
-------
doc : str"""""""
pandas/core/ops/invalid.py,"def invalid_comparison(left, right, op) -> npt.NDArray[np.bool_]:
    if op is operator.eq:
        res_values = np.zeros(left.shape, dtype=bool)
    elif op is operator.ne:
        res_values = np.ones(left.shape, dtype=bool)
    else:
        typ = type(right).__name__
        raise TypeError(f'Invalid comparison between dtype={left.dtype} and {typ}')
    return res_values","""""""If a comparison has mismatched types and is not necessarily meaningful,
follow python3 conventions by:

    - returning all-False for equality
    - returning all-True for inequality
    - raising TypeError otherwise

Parameters
----------
left : array-like
right : scalar, array-like
op : operator.{eq, ne, lt, le, gt}

Raises
------
TypeError : on inequality comparisons"""""""
pandas/core/ops/invalid.py,"def make_invalid_op(name: str):

    def invalid_op(self, other=None):
        typ = type(self).__name__
        raise TypeError(f'cannot perform {name} with this index type: {typ}')
    invalid_op.__name__ = name
    return invalid_op","""""""Return a binary method that always raises a TypeError.

Parameters
----------
name : str

Returns
-------
invalid_op : function"""""""
pandas/core/ops/mask_ops.py,"def kleene_or(left: bool | np.ndarray | libmissing.NAType, right: bool | np.ndarray | libmissing.NAType, left_mask: np.ndarray | None, right_mask: np.ndarray | None):
    if left_mask is None:
        return kleene_or(right, left, right_mask, left_mask)
    if not isinstance(left, np.ndarray):
        raise TypeError('Either `left` or `right` need to be a np.ndarray.')
    raise_for_nan(right, method='or')
    if right is libmissing.NA:
        result = left.copy()
    else:
        result = left | right
    if right_mask is not None:
        left_false = ~(left | left_mask)
        right_false = ~(right | right_mask)
        mask = left_false & right_mask | right_false & left_mask | left_mask & right_mask
    elif right is True:
        mask = np.zeros_like(left_mask)
    elif right is libmissing.NA:
        mask = ~left & ~left_mask | left_mask
    else:
        mask = left_mask.copy()
    return (result, mask)","""""""Boolean ``or`` using Kleene logic.

Values are NA where we have ``NA | NA`` or ``NA | False``.
``NA | True`` is considered True.

Parameters
----------
left, right : ndarray, NA, or bool
    The values of the array.
left_mask, right_mask : ndarray, optional
    The masks. Only one of these may be None, which implies that
    the associated `left` or `right` value is a scalar.

Returns
-------
result, mask: ndarray[bool]
    The result of the logical or, and the new mask."""""""
pandas/core/ops/mask_ops.py,"def kleene_xor(left: bool | np.ndarray | libmissing.NAType, right: bool | np.ndarray | libmissing.NAType, left_mask: np.ndarray | None, right_mask: np.ndarray | None):
    if left_mask is None:
        return kleene_xor(right, left, right_mask, left_mask)
    if not isinstance(left, np.ndarray):
        raise TypeError('Either `left` or `right` need to be a np.ndarray.')
    raise_for_nan(right, method='xor')
    if right is libmissing.NA:
        result = np.zeros_like(left)
    else:
        result = left ^ right
    if right_mask is None:
        if right is libmissing.NA:
            mask = np.ones_like(left_mask)
        else:
            mask = left_mask.copy()
    else:
        mask = left_mask | right_mask
    return (result, mask)","""""""Boolean ``xor`` using Kleene logic.

This is the same as ``or``, with the following adjustments

* True, True -> False
* True, NA   -> NA

Parameters
----------
left, right : ndarray, NA, or bool
    The values of the array.
left_mask, right_mask : ndarray, optional
    The masks. Only one of these may be None, which implies that
    the associated `left` or `right` value is a scalar.

Returns
-------
result, mask: ndarray[bool]
    The result of the logical xor, and the new mask."""""""
pandas/core/ops/mask_ops.py,"def kleene_and(left: bool | libmissing.NAType | np.ndarray, right: bool | libmissing.NAType | np.ndarray, left_mask: np.ndarray | None, right_mask: np.ndarray | None):
    if left_mask is None:
        return kleene_and(right, left, right_mask, left_mask)
    if not isinstance(left, np.ndarray):
        raise TypeError('Either `left` or `right` need to be a np.ndarray.')
    raise_for_nan(right, method='and')
    if right is libmissing.NA:
        result = np.zeros_like(left)
    else:
        result = left & right
    if right_mask is None:
        if right is libmissing.NA:
            mask = left & ~left_mask | left_mask
        else:
            mask = left_mask.copy()
            if right is False:
                mask[:] = False
    else:
        left_false = ~(left | left_mask)
        right_false = ~(right | right_mask)
        mask = left_mask & ~right_false | right_mask & ~left_false
    return (result, mask)","""""""Boolean ``and`` using Kleene logic.

Values are ``NA`` for ``NA & NA`` or ``True & NA``.

Parameters
----------
left, right : ndarray, NA, or bool
    The values of the array.
left_mask, right_mask : ndarray, optional
    The masks. Only one of these may be None, which implies that
    the associated `left` or `right` value is a scalar.

Returns
-------
result, mask: ndarray[bool]
    The result of the logical xor, and the new mask."""""""
pandas/core/ops/missing.py,"def _fill_zeros(result: np.ndarray, x, y):
    if result.dtype.kind == 'f':
        return result
    is_variable_type = hasattr(y, 'dtype')
    is_scalar_type = not isinstance(y, np.ndarray)
    if not is_variable_type and (not is_scalar_type):
        return result
    if is_scalar_type:
        y = np.array(y)
    if y.dtype.kind in 'iu':
        ymask = y == 0
        if ymask.any():
            mask = ymask & ~np.isnan(result)
            result = result.astype('float64', copy=False)
            np.putmask(result, mask, np.nan)
    return result","""""""If this is a reversed op, then flip x,y

If we have an integer value (or array in y)
and we have 0's, fill them with np.nan,
return the result.

Mask the nan's from x."""""""
pandas/core/ops/missing.py,"def mask_zero_div_zero(x, y, result: np.ndarray) -> np.ndarray:
    if not hasattr(y, 'dtype'):
        y = np.array(y)
    if not hasattr(x, 'dtype'):
        x = np.array(x)
    zmask = y == 0
    if zmask.any():
        zneg_mask = zmask & np.signbit(y)
        zpos_mask = zmask & ~zneg_mask
        x_lt0 = x < 0
        x_gt0 = x > 0
        nan_mask = zmask & (x == 0)
        neginf_mask = zpos_mask & x_lt0 | zneg_mask & x_gt0
        posinf_mask = zpos_mask & x_gt0 | zneg_mask & x_lt0
        if nan_mask.any() or neginf_mask.any() or posinf_mask.any():
            result = result.astype('float64', copy=False)
            result[nan_mask] = np.nan
            result[posinf_mask] = np.inf
            result[neginf_mask] = -np.inf
    return result","""""""Set results of  0 // 0 to np.nan, regardless of the dtypes
of the numerator or the denominator.

Parameters
----------
x : ndarray
y : ndarray
result : ndarray

Returns
-------
ndarray
    The filled result.

Examples
--------
>>> x = np.array([1, 0, -1], dtype=np.int64)
>>> x
array([ 1,  0, -1])
>>> y = 0       # int 0; numpy behavior is different with float
>>> result = x // y
>>> result      # raw numpy result does not fill division by zero
array([0, 0, 0])
>>> mask_zero_div_zero(x, y, result)
array([ inf,  nan, -inf])"""""""
pandas/core/ops/missing.py,"def dispatch_fill_zeros(op, left, right, result):
    if op is divmod:
        result = (mask_zero_div_zero(left, right, result[0]), _fill_zeros(result[1], left, right))
    elif op is roperator.rdivmod:
        result = (mask_zero_div_zero(right, left, result[0]), _fill_zeros(result[1], right, left))
    elif op is operator.floordiv:
        result = mask_zero_div_zero(left, right, result)
    elif op is roperator.rfloordiv:
        result = mask_zero_div_zero(right, left, result)
    elif op is operator.mod:
        result = _fill_zeros(result, left, right)
    elif op is roperator.rmod:
        result = _fill_zeros(result, right, left)
    return result","""""""Call _fill_zeros with the appropriate fill value depending on the operation,
with special logic for divmod and rdivmod.

Parameters
----------
op : function (operator.add, operator.div, ...)
left : object (np.ndarray for non-reversed ops)
    We have excluded ExtensionArrays here
right : object (np.ndarray for reversed ops)
    We have excluded ExtensionArrays here
result : ndarray

Returns
-------
result : np.ndarray

Notes
-----
For divmod and rdivmod, the `result` parameter and returned `result`
is a 2-tuple of ndarray objects."""""""
pandas/core/resample.py,"def get_resampler(obj: Series | DataFrame, kind=None, **kwds) -> Resampler:
    tg = TimeGrouper(obj, **kwds)
    return tg._get_resampler(obj, kind=kind)","""""""Create a TimeGrouper and return our resampler."""""""
pandas/core/resample.py,"def get_resampler_for_grouping(groupby: GroupBy, rule, how=None, fill_method=None, limit: int | None=None, kind=None, on=None, include_groups: bool=True, **kwargs) -> Resampler:
    tg = TimeGrouper(freq=rule, key=on, **kwargs)
    resampler = tg._get_resampler(groupby.obj, kind=kind)
    return resampler._get_resampler_for_grouping(groupby=groupby, include_groups=include_groups, key=tg.key)","""""""Return our appropriate resampler when grouping as well."""""""
pandas/core/resample.py,"def _get_timestamp_range_edges(first: Timestamp, last: Timestamp, freq: BaseOffset, unit: str, closed: Literal['right', 'left']='left', origin: TimeGrouperOrigin='start_day', offset: Timedelta | None=None) -> tuple[Timestamp, Timestamp]:
    if isinstance(freq, Tick):
        index_tz = first.tz
        if isinstance(origin, Timestamp) and origin.tz != index_tz:
            raise ValueError('The origin must have the same timezone as the index.')
        elif isinstance(origin, Timestamp):
            if origin <= first:
                first = origin
            elif origin >= last:
                last = origin
        if origin == 'epoch':
            origin = Timestamp('1970-01-01', tz=index_tz)
        if isinstance(freq, Day):
            first = first.tz_localize(None)
            last = last.tz_localize(None)
            if isinstance(origin, Timestamp):
                origin = origin.tz_localize(None)
        (first, last) = _adjust_dates_anchored(first, last, freq, closed=closed, origin=origin, offset=offset, unit=unit)
        if isinstance(freq, Day):
            first = first.tz_localize(index_tz)
            last = last.tz_localize(index_tz)
    else:
        if isinstance(origin, Timestamp):
            first = origin
        first = first.normalize()
        last = last.normalize()
        if closed == 'left':
            first = Timestamp(freq.rollback(first))
        else:
            first = Timestamp(first - freq)
        last = Timestamp(last + freq)
    return (first, last)","""""""Adjust the `first` Timestamp to the preceding Timestamp that resides on
the provided offset. Adjust the `last` Timestamp to the following
Timestamp that resides on the provided offset. Input Timestamps that
already reside on the offset will be adjusted depending on the type of
offset and the `closed` parameter.

Parameters
----------
first : pd.Timestamp
    The beginning Timestamp of the range to be adjusted.
last : pd.Timestamp
    The ending Timestamp of the range to be adjusted.
freq : pd.DateOffset
    The dateoffset to which the Timestamps will be adjusted.
closed : {'right', 'left'}, default ""left""
    Which side of bin interval is closed.
origin : {'epoch', 'start', 'start_day'} or Timestamp, default 'start_day'
    The timestamp on which to adjust the grouping. The timezone of origin must
    match the timezone of the index.
    If a timestamp is not used, these values are also supported:

    - 'epoch': `origin` is 1970-01-01
    - 'start': `origin` is the first value of the timeseries
    - 'start_day': `origin` is the first day at midnight of the timeseries
offset : pd.Timedelta, default is None
    An offset timedelta added to the origin.

Returns
-------
A tuple of length 2, containing the adjusted pd.Timestamp objects."""""""
pandas/core/resample.py,"def _get_period_range_edges(first: Period, last: Period, freq: BaseOffset, closed: Literal['right', 'left']='left', origin: TimeGrouperOrigin='start_day', offset: Timedelta | None=None) -> tuple[Period, Period]:
    if not all((isinstance(obj, Period) for obj in [first, last])):
        raise TypeError(""'first' and 'last' must be instances of type Period"")
    first_ts = first.to_timestamp()
    last_ts = last.to_timestamp()
    adjust_first = not freq.is_on_offset(first_ts)
    adjust_last = freq.is_on_offset(last_ts)
    (first_ts, last_ts) = _get_timestamp_range_edges(first_ts, last_ts, freq, unit='ns', closed=closed, origin=origin, offset=offset)
    first = (first_ts + int(adjust_first) * freq).to_period(freq)
    last = (last_ts - int(adjust_last) * freq).to_period(freq)
    return (first, last)","""""""Adjust the provided `first` and `last` Periods to the respective Period of
the given offset that encompasses them.

Parameters
----------
first : pd.Period
    The beginning Period of the range to be adjusted.
last : pd.Period
    The ending Period of the range to be adjusted.
freq : pd.DateOffset
    The freq to which the Periods will be adjusted.
closed : {'right', 'left'}, default ""left""
    Which side of bin interval is closed.
origin : {'epoch', 'start', 'start_day'}, Timestamp, default 'start_day'
    The timestamp on which to adjust the grouping. The timezone of origin must
    match the timezone of the index.

    If a timestamp is not used, these values are also supported:

    - 'epoch': `origin` is 1970-01-01
    - 'start': `origin` is the first value of the timeseries
    - 'start_day': `origin` is the first day at midnight of the timeseries
offset : pd.Timedelta, default is None
    An offset timedelta added to the origin.

Returns
-------
A tuple of length 2, containing the adjusted pd.Period objects."""""""
pandas/core/resample.py,"def asfreq(obj: NDFrameT, freq, method=None, how=None, normalize: bool=False, fill_value=None) -> NDFrameT:
    if isinstance(obj.index, PeriodIndex):
        if method is not None:
            raise NotImplementedError(""'method' argument is not supported"")
        if how is None:
            how = 'E'
        if isinstance(freq, BaseOffset):
            freq = freq_to_period_freqstr(freq.n, freq.name)
        new_obj = obj.copy()
        new_obj.index = obj.index.asfreq(freq, how=how)
    elif len(obj.index) == 0:
        new_obj = obj.copy()
        new_obj.index = _asfreq_compat(obj.index, freq)
    else:
        dti = date_range(obj.index.min(), obj.index.max(), freq=freq)
        dti.name = obj.index.name
        new_obj = obj.reindex(dti, method=method, fill_value=fill_value)
        if normalize:
            new_obj.index = new_obj.index.normalize()
    return new_obj","""""""Utility frequency conversion method for Series/DataFrame.

See :meth:`pandas.NDFrame.asfreq` for full documentation."""""""
pandas/core/resample.py,"def _asfreq_compat(index: DatetimeIndex | PeriodIndex | TimedeltaIndex, freq):
    if len(index) != 0:
        raise ValueError('Can only set arbitrary freq for empty DatetimeIndex or TimedeltaIndex')
    new_index: Index
    if isinstance(index, PeriodIndex):
        new_index = index.asfreq(freq=freq)
    elif isinstance(index, DatetimeIndex):
        new_index = DatetimeIndex([], dtype=index.dtype, freq=freq, name=index.name)
    elif isinstance(index, TimedeltaIndex):
        new_index = TimedeltaIndex([], dtype=index.dtype, freq=freq, name=index.name)
    else:
        raise TypeError(type(index))
    return new_index","""""""Helper to mimic asfreq on (empty) DatetimeIndex and TimedeltaIndex.

Parameters
----------
index : PeriodIndex, DatetimeIndex, or TimedeltaIndex
freq : DateOffset

Returns
-------
same type as index"""""""
pandas/core/resample.py,"def maybe_warn_args_and_kwargs(cls, kernel: str, args, kwargs) -> None:
    warn_args = args is not None and len(args) > 0
    warn_kwargs = kwargs is not None and len(kwargs) > 0
    if warn_args and warn_kwargs:
        msg = 'args and kwargs'
    elif warn_args:
        msg = 'args'
    elif warn_kwargs:
        msg = 'kwargs'
    else:
        return
    warnings.warn(f'Passing additional {msg} to {cls.__name__}.{kernel} has no impact on the result and is deprecated. This will raise a TypeError in a future version of pandas.', category=FutureWarning, stacklevel=find_stack_level())","""""""Warn for deprecation of args and kwargs in resample functions.

Parameters
----------
cls : type
    Class to warn about.
kernel : str
    Operation name.
args : tuple or None
    args passed by user. Will be None if and only if kernel does not have args.
kwargs : dict or None
    kwargs passed by user. Will be None if and only if kernel does not have kwargs."""""""
pandas/core/reshape/concat.py,"def concat(objs: Iterable[Series | DataFrame] | Mapping[HashableT, Series | DataFrame], *, axis: Axis=0, join: str='outer', ignore_index: bool=False, keys: Iterable[Hashable] | None=None, levels=None, names: list[HashableT] | None=None, verify_integrity: bool=False, sort: bool=False, copy: bool | None=None) -> DataFrame | Series:
    if copy is None:
        if using_copy_on_write():
            copy = False
        else:
            copy = True
    elif copy and using_copy_on_write():
        copy = False
    op = _Concatenator(objs, axis=axis, ignore_index=ignore_index, join=join, keys=keys, levels=levels, names=names, verify_integrity=verify_integrity, copy=copy, sort=sort)
    return op.get_result()","""""""Concatenate pandas objects along a particular axis.

Allows optional set logic along the other axes.

Can also add a layer of hierarchical indexing on the concatenation axis,
which may be useful if the labels are the same (or overlapping) on
the passed axis number.

Parameters
----------
objs : a sequence or mapping of Series or DataFrame objects
    If a mapping is passed, the sorted keys will be used as the `keys`
    argument, unless it is passed, in which case the values will be
    selected (see below). Any None objects will be dropped silently unless
    they are all None in which case a ValueError will be raised.
axis : {0/'index', 1/'columns'}, default 0
    The axis to concatenate along.
join : {'inner', 'outer'}, default 'outer'
    How to handle indexes on other axis (or axes).
ignore_index : bool, default False
    If True, do not use the index values along the concatenation axis. The
    resulting axis will be labeled 0, ..., n - 1. This is useful if you are
    concatenating objects where the concatenation axis does not have
    meaningful indexing information. Note the index values on the other
    axes are still respected in the join.
keys : sequence, default None
    If multiple levels passed, should contain tuples. Construct
    hierarchical index using the passed keys as the outermost level.
levels : list of sequences, default None
    Specific levels (unique values) to use for constructing a
    MultiIndex. Otherwise they will be inferred from the keys.
names : list, default None
    Names for the levels in the resulting hierarchical index.
verify_integrity : bool, default False
    Check whether the new concatenated axis contains duplicates. This can
    be very expensive relative to the actual data concatenation.
sort : bool, default False
    Sort non-concatenation axis if it is not already aligned.

copy : bool, default True
    If False, do not copy data unnecessarily.

Returns
-------
object, type of objs
    When concatenating all ``Series`` along the index (axis=0), a
    ``Series`` is returned. When ``objs`` contains at least one
    ``DataFrame``, a ``DataFrame`` is returned. When concatenating along
    the columns (axis=1), a ``DataFrame`` is returned.

See Also
--------
DataFrame.join : Join DataFrames using indexes.
DataFrame.merge : Merge DataFrames by indexes or columns.

Notes
-----
The keys, levels, and names arguments are all optional.

A walkthrough of how this method fits in with other tools for combining
pandas objects can be found `here
<https://pandas.pydata.org/pandas-docs/stable/user_guide/merging.html>`__.

It is not recommended to build DataFrames by adding single rows in a
for loop. Build a list of rows and make a DataFrame in a single concat.

Examples
--------
Combine two ``Series``.

>>> s1 = pd.Series(['a', 'b'])
>>> s2 = pd.Series(['c', 'd'])
>>> pd.concat([s1, s2])
0    a
1    b
0    c
1    d
dtype: object

Clear the existing index and reset it in the result
by setting the ``ignore_index`` option to ``True``.

>>> pd.concat([s1, s2], ignore_index=True)
0    a
1    b
2    c
3    d
dtype: object

Add a hierarchical index at the outermost level of
the data with the ``keys`` option.

>>> pd.concat([s1, s2], keys=['s1', 's2'])
s1  0    a
    1    b
s2  0    c
    1    d
dtype: object

Label the index keys you create with the ``names`` option.

>>> pd.concat([s1, s2], keys=['s1', 's2'],
...           names=['Series name', 'Row ID'])
Series name  Row ID
s1           0         a
             1         b
s2           0         c
             1         d
dtype: object

Combine two ``DataFrame`` objects with identical columns.

>>> df1 = pd.DataFrame([['a', 1], ['b', 2]],
...                    columns=['letter', 'number'])
>>> df1
  letter  number
0      a       1
1      b       2
>>> df2 = pd.DataFrame([['c', 3], ['d', 4]],
...                    columns=['letter', 'number'])
>>> df2
  letter  number
0      c       3
1      d       4
>>> pd.concat([df1, df2])
  letter  number
0      a       1
1      b       2
0      c       3
1      d       4

Combine ``DataFrame`` objects with overlapping columns
and return everything. Columns outside the intersection will
be filled with ``NaN`` values.

>>> df3 = pd.DataFrame([['c', 3, 'cat'], ['d', 4, 'dog']],
...                    columns=['letter', 'number', 'animal'])
>>> df3
  letter  number animal
0      c       3    cat
1      d       4    dog
>>> pd.concat([df1, df3], sort=False)
  letter  number animal
0      a       1    NaN
1      b       2    NaN
0      c       3    cat
1      d       4    dog

Combine ``DataFrame`` objects with overlapping columns
and return only those that are shared by passing ``inner`` to
the ``join`` keyword argument.

>>> pd.concat([df1, df3], join=""inner"")
  letter  number
0      a       1
1      b       2
0      c       3
1      d       4

Combine ``DataFrame`` objects horizontally along the x axis by
passing in ``axis=1``.

>>> df4 = pd.DataFrame([['bird', 'polly'], ['monkey', 'george']],
...                    columns=['animal', 'name'])
>>> pd.concat([df1, df4], axis=1)
  letter  number  animal    name
0      a       1    bird   polly
1      b       2  monkey  george

Prevent the result from including duplicate index values with the
``verify_integrity`` option.

>>> df5 = pd.DataFrame([1], index=['a'])
>>> df5
   0
a  1
>>> df6 = pd.DataFrame([2], index=['a'])
>>> df6
   0
a  2
>>> pd.concat([df5, df6], verify_integrity=True)
Traceback (most recent call last):
    ...
ValueError: Indexes have overlapping values: ['a']

Append a single row to the end of a ``DataFrame`` object.

>>> df7 = pd.DataFrame({'a': 1, 'b': 2}, index=[0])
>>> df7
    a   b
0   1   2
>>> new_row = pd.Series({'a': 3, 'b': 4})
>>> new_row
a    3
b    4
dtype: int64
>>> pd.concat([df7, new_row.to_frame().T], ignore_index=True)
    a   b
0   1   2
1   3   4"""""""
pandas/core/reshape/encoding.py,"def get_dummies(data, prefix=None, prefix_sep: str | Iterable[str] | dict[str, str]='_', dummy_na: bool=False, columns=None, sparse: bool=False, drop_first: bool=False, dtype: NpDtype | None=None) -> DataFrame:
    from pandas.core.reshape.concat import concat
    dtypes_to_encode = ['object', 'string', 'category']
    if isinstance(data, DataFrame):
        if columns is None:
            data_to_encode = data.select_dtypes(include=dtypes_to_encode)
        elif not is_list_like(columns):
            raise TypeError('Input must be a list-like for parameter `columns`')
        else:
            data_to_encode = data[columns]

        def check_len(item, name: str):
            if is_list_like(item):
                if not len(item) == data_to_encode.shape[1]:
                    len_msg = f""Length of '{name}' ({len(item)}) did not match the length of the columns being encoded ({data_to_encode.shape[1]}).""
                    raise ValueError(len_msg)
        check_len(prefix, 'prefix')
        check_len(prefix_sep, 'prefix_sep')
        if isinstance(prefix, str):
            prefix = itertools.cycle([prefix])
        if isinstance(prefix, dict):
            prefix = [prefix[col] for col in data_to_encode.columns]
        if prefix is None:
            prefix = data_to_encode.columns
        if isinstance(prefix_sep, str):
            prefix_sep = itertools.cycle([prefix_sep])
        elif isinstance(prefix_sep, dict):
            prefix_sep = [prefix_sep[col] for col in data_to_encode.columns]
        with_dummies: list[DataFrame]
        if data_to_encode.shape == data.shape:
            with_dummies = []
        elif columns is not None:
            with_dummies = [data.drop(columns, axis=1)]
        else:
            with_dummies = [data.select_dtypes(exclude=dtypes_to_encode)]
        for (col, pre, sep) in zip(data_to_encode.items(), prefix, prefix_sep):
            dummy = _get_dummies_1d(col[1], prefix=pre, prefix_sep=sep, dummy_na=dummy_na, sparse=sparse, drop_first=drop_first, dtype=dtype)
            with_dummies.append(dummy)
        result = concat(with_dummies, axis=1)
    else:
        result = _get_dummies_1d(data, prefix, prefix_sep, dummy_na, sparse=sparse, drop_first=drop_first, dtype=dtype)
    return result","""""""Convert categorical variable into dummy/indicator variables.

Each variable is converted in as many 0/1 variables as there are different
values. Columns in the output are each named after a value; if the input is
a DataFrame, the name of the original variable is prepended to the value.

Parameters
----------
data : array-like, Series, or DataFrame
    Data of which to get dummy indicators.
prefix : str, list of str, or dict of str, default None
    String to append DataFrame column names.
    Pass a list with length equal to the number of columns
    when calling get_dummies on a DataFrame. Alternatively, `prefix`
    can be a dictionary mapping column names to prefixes.
prefix_sep : str, default '_'
    If appending prefix, separator/delimiter to use. Or pass a
    list or dictionary as with `prefix`.
dummy_na : bool, default False
    Add a column to indicate NaNs, if False NaNs are ignored.
columns : list-like, default None
    Column names in the DataFrame to be encoded.
    If `columns` is None then all the columns with
    `object`, `string`, or `category` dtype will be converted.
sparse : bool, default False
    Whether the dummy-encoded columns should be backed by
    a :class:`SparseArray` (True) or a regular NumPy array (False).
drop_first : bool, default False
    Whether to get k-1 dummies out of k categorical levels by removing the
    first level.
dtype : dtype, default bool
    Data type for new columns. Only a single dtype is allowed.

Returns
-------
DataFrame
    Dummy-coded data. If `data` contains other columns than the
    dummy-coded one(s), these will be prepended, unaltered, to the result.

See Also
--------
Series.str.get_dummies : Convert Series of strings to dummy codes.
:func:`~pandas.from_dummies` : Convert dummy codes to categorical ``DataFrame``.

Notes
-----
Reference :ref:`the user guide <reshaping.dummies>` for more examples.

Examples
--------
>>> s = pd.Series(list('abca'))

>>> pd.get_dummies(s)
       a      b      c
0   True  False  False
1  False   True  False
2  False  False   True
3   True  False  False

>>> s1 = ['a', 'b', np.nan]

>>> pd.get_dummies(s1)
       a      b
0   True  False
1  False   True
2  False  False

>>> pd.get_dummies(s1, dummy_na=True)
       a      b    NaN
0   True  False  False
1  False   True  False
2  False  False   True

>>> df = pd.DataFrame({'A': ['a', 'b', 'a'], 'B': ['b', 'a', 'c'],
...                    'C': [1, 2, 3]})

>>> pd.get_dummies(df, prefix=['col1', 'col2'])
   C  col1_a  col1_b  col2_a  col2_b  col2_c
0  1    True   False   False    True   False
1  2   False    True    True   False   False
2  3    True   False   False   False    True

>>> pd.get_dummies(pd.Series(list('abcaa')))
       a      b      c
0   True  False  False
1  False   True  False
2  False  False   True
3   True  False  False
4   True  False  False

>>> pd.get_dummies(pd.Series(list('abcaa')), drop_first=True)
       b      c
0  False  False
1   True  False
2  False   True
3  False  False
4  False  False

>>> pd.get_dummies(pd.Series(list('abc')), dtype=float)
     a    b    c
0  1.0  0.0  0.0
1  0.0  1.0  0.0
2  0.0  0.0  1.0"""""""
pandas/core/reshape/encoding.py,"def from_dummies(data: DataFrame, sep: None | str=None, default_category: None | Hashable | dict[str, Hashable]=None) -> DataFrame:
    from pandas.core.reshape.concat import concat
    if not isinstance(data, DataFrame):
        raise TypeError(f""Expected 'data' to be a 'DataFrame'; Received 'data' of type: {type(data).__name__}"")
    col_isna_mask = cast(Series, data.isna().any())
    if col_isna_mask.any():
        raise ValueError(f""Dummy DataFrame contains NA value in column: '{col_isna_mask.idxmax()}'"")
    try:
        data_to_decode = data.astype('boolean', copy=False)
    except TypeError:
        raise TypeError('Passed DataFrame contains non-dummy data')
    variables_slice = defaultdict(list)
    if sep is None:
        variables_slice[''] = list(data.columns)
    elif isinstance(sep, str):
        for col in data_to_decode.columns:
            prefix = col.split(sep)[0]
            if len(prefix) == len(col):
                raise ValueError(f'Separator not specified for column: {col}')
            variables_slice[prefix].append(col)
    else:
        raise TypeError(f""Expected 'sep' to be of type 'str' or 'None'; Received 'sep' of type: {type(sep).__name__}"")
    if default_category is not None:
        if isinstance(default_category, dict):
            if not len(default_category) == len(variables_slice):
                len_msg = f""Length of 'default_category' ({len(default_category)}) did not match the length of the columns being encoded ({len(variables_slice)})""
                raise ValueError(len_msg)
        elif isinstance(default_category, Hashable):
            default_category = dict(zip(variables_slice, [default_category] * len(variables_slice)))
        else:
            raise TypeError(f""Expected 'default_category' to be of type 'None', 'Hashable', or 'dict'; Received 'default_category' of type: {type(default_category).__name__}"")
    cat_data = {}
    for (prefix, prefix_slice) in variables_slice.items():
        if sep is None:
            cats = prefix_slice.copy()
        else:
            cats = [col[len(prefix + sep):] for col in prefix_slice]
        assigned = data_to_decode.loc[:, prefix_slice].sum(axis=1)
        if any(assigned > 1):
            raise ValueError(f'Dummy DataFrame contains multi-assignment(s); First instance in row: {assigned.idxmax()}')
        if any(assigned == 0):
            if isinstance(default_category, dict):
                cats.append(default_category[prefix])
            else:
                raise ValueError(f'Dummy DataFrame contains unassigned value(s); First instance in row: {assigned.idxmin()}')
            data_slice = concat((data_to_decode.loc[:, prefix_slice], assigned == 0), axis=1)
        else:
            data_slice = data_to_decode.loc[:, prefix_slice]
        cats_array = data._constructor_sliced(cats, dtype=data.columns.dtype)
        true_values = data_slice.idxmax(axis=1)
        indexer = data_slice.columns.get_indexer_for(true_values)
        cat_data[prefix] = cats_array.take(indexer).set_axis(data.index)
    result = DataFrame(cat_data)
    if sep is not None:
        result.columns = result.columns.astype(data.columns.dtype)
    return result","""""""Create a categorical ``DataFrame`` from a ``DataFrame`` of dummy variables.

Inverts the operation performed by :func:`~pandas.get_dummies`.

.. versionadded:: 1.5.0

Parameters
----------
data : DataFrame
    Data which contains dummy-coded variables in form of integer columns of
    1's and 0's.
sep : str, default None
    Separator used in the column names of the dummy categories they are
    character indicating the separation of the categorical names from the prefixes.
    For example, if your column names are 'prefix_A' and 'prefix_B',
    you can strip the underscore by specifying sep='_'.
default_category : None, Hashable or dict of Hashables, default None
    The default category is the implied category when a value has none of the
    listed categories specified with a one, i.e. if all dummies in a row are
    zero. Can be a single value for all variables or a dict directly mapping
    the default categories to a prefix of a variable.

Returns
-------
DataFrame
    Categorical data decoded from the dummy input-data.

Raises
------
ValueError
    * When the input ``DataFrame`` ``data`` contains NA values.
    * When the input ``DataFrame`` ``data`` contains column names with separators
      that do not match the separator specified with ``sep``.
    * When a ``dict`` passed to ``default_category`` does not include an implied
      category for each prefix.
    * When a value in ``data`` has more than one category assigned to it.
    * When ``default_category=None`` and a value in ``data`` has no category
      assigned to it.
TypeError
    * When the input ``data`` is not of type ``DataFrame``.
    * When the input ``DataFrame`` ``data`` contains non-dummy data.
    * When the passed ``sep`` is of a wrong data type.
    * When the passed ``default_category`` is of a wrong data type.

See Also
--------
:func:`~pandas.get_dummies` : Convert ``Series`` or ``DataFrame`` to dummy codes.
:class:`~pandas.Categorical` : Represent a categorical variable in classic.

Notes
-----
The columns of the passed dummy data should only include 1's and 0's,
or boolean values.

Examples
--------
>>> df = pd.DataFrame({""a"": [1, 0, 0, 1], ""b"": [0, 1, 0, 0],
...                    ""c"": [0, 0, 1, 0]})

>>> df
   a  b  c
0  1  0  0
1  0  1  0
2  0  0  1
3  1  0  0

>>> pd.from_dummies(df)
0     a
1     b
2     c
3     a

>>> df = pd.DataFrame({""col1_a"": [1, 0, 1], ""col1_b"": [0, 1, 0],
...                    ""col2_a"": [0, 1, 0], ""col2_b"": [1, 0, 0],
...                    ""col2_c"": [0, 0, 1]})

>>> df
      col1_a  col1_b  col2_a  col2_b  col2_c
0       1       0       0       1       0
1       0       1       1       0       0
2       1       0       0       0       1

>>> pd.from_dummies(df, sep=""_"")
    col1    col2
0    a       b
1    b       a
2    a       c

>>> df = pd.DataFrame({""col1_a"": [1, 0, 0], ""col1_b"": [0, 1, 0],
...                    ""col2_a"": [0, 1, 0], ""col2_b"": [1, 0, 0],
...                    ""col2_c"": [0, 0, 0]})

>>> df
      col1_a  col1_b  col2_a  col2_b  col2_c
0       1       0       0       1       0
1       0       1       1       0       0
2       0       0       0       0       0

>>> pd.from_dummies(df, sep=""_"", default_category={""col1"": ""d"", ""col2"": ""e""})
    col1    col2
0    a       b
1    b       a
2    d       e"""""""
pandas/core/reshape/melt.py,"def lreshape(data: DataFrame, groups, dropna: bool=True) -> DataFrame:
    if isinstance(groups, dict):
        keys = list(groups.keys())
        values = list(groups.values())
    else:
        (keys, values) = zip(*groups)
    all_cols = list(set.union(*(set(x) for x in values)))
    id_cols = list(data.columns.difference(all_cols))
    K = len(values[0])
    for seq in values:
        if len(seq) != K:
            raise ValueError('All column lists must be same length')
    mdata = {}
    pivot_cols = []
    for (target, names) in zip(keys, values):
        to_concat = [data[col]._values for col in names]
        mdata[target] = concat_compat(to_concat)
        pivot_cols.append(target)
    for col in id_cols:
        mdata[col] = np.tile(data[col]._values, K)
    if dropna:
        mask = np.ones(len(mdata[pivot_cols[0]]), dtype=bool)
        for c in pivot_cols:
            mask &= notna(mdata[c])
        if not mask.all():
            mdata = {k: v[mask] for (k, v) in mdata.items()}
    return data._constructor(mdata, columns=id_cols + pivot_cols)","""""""Reshape wide-format data to long. Generalized inverse of DataFrame.pivot.

Accepts a dictionary, ``groups``, in which each key is a new column name
and each value is a list of old column names that will be ""melted"" under
the new column name as part of the reshape.

Parameters
----------
data : DataFrame
    The wide-format DataFrame.
groups : dict
    {new_name : list_of_columns}.
dropna : bool, default True
    Do not include columns whose entries are all NaN.

Returns
-------
DataFrame
    Reshaped DataFrame.

See Also
--------
melt : Unpivot a DataFrame from wide to long format, optionally leaving
    identifiers set.
pivot : Create a spreadsheet-style pivot table as a DataFrame.
DataFrame.pivot : Pivot without aggregation that can handle
    non-numeric data.
DataFrame.pivot_table : Generalization of pivot that can handle
    duplicate values for one index/column pair.
DataFrame.unstack : Pivot based on the index values instead of a
    column.
wide_to_long : Wide panel to long format. Less flexible but more
    user-friendly than melt.

Examples
--------
>>> data = pd.DataFrame({'hr1': [514, 573], 'hr2': [545, 526],
...                      'team': ['Red Sox', 'Yankees'],
...                      'year1': [2007, 2007], 'year2': [2008, 2008]})
>>> data
   hr1  hr2     team  year1  year2
0  514  545  Red Sox   2007   2008
1  573  526  Yankees   2007   2008

>>> pd.lreshape(data, {'year': ['year1', 'year2'], 'hr': ['hr1', 'hr2']})
      team  year   hr
0  Red Sox  2007  514
1  Yankees  2007  573
2  Red Sox  2008  545
3  Yankees  2008  526"""""""
pandas/core/reshape/melt.py,"def wide_to_long(df: DataFrame, stubnames, i, j, sep: str='', suffix: str='\\d+') -> DataFrame:

    def get_var_names(df, stub: str, sep: str, suffix: str) -> list[str]:
        regex = f'^{re.escape(stub)}{re.escape(sep)}{suffix}$'
        pattern = re.compile(regex)
        return [col for col in df.columns if pattern.match(col)]

    def melt_stub(df, stub: str, i, j, value_vars, sep: str):
        newdf = melt(df, id_vars=i, value_vars=value_vars, value_name=stub.rstrip(sep), var_name=j)
        newdf[j] = Categorical(newdf[j])
        newdf[j] = newdf[j].str.replace(re.escape(stub + sep), '', regex=True)
        newdf[j] = to_numeric(newdf[j], errors='ignore')
        return newdf.set_index(i + [j])
    if not is_list_like(stubnames):
        stubnames = [stubnames]
    else:
        stubnames = list(stubnames)
    if any((col in stubnames for col in df.columns)):
        raise ValueError(""stubname can't be identical to a column name"")
    if not is_list_like(i):
        i = [i]
    else:
        i = list(i)
    if df[i].duplicated().any():
        raise ValueError('the id variables need to uniquely identify each row')
    value_vars = [get_var_names(df, stub, sep, suffix) for stub in stubnames]
    value_vars_flattened = [e for sublist in value_vars for e in sublist]
    id_vars = list(set(df.columns.tolist()).difference(value_vars_flattened))
    _melted = [melt_stub(df, s, i, j, v, sep) for (s, v) in zip(stubnames, value_vars)]
    melted = _melted[0].join(_melted[1:], how='outer')
    if len(i) == 1:
        new = df[id_vars].set_index(i).join(melted)
        return new
    new = df[id_vars].merge(melted.reset_index(), on=i).set_index(i + [j])
    return new","""""""Unpivot a DataFrame from wide to long format.

Less flexible but more user-friendly than melt.

With stubnames ['A', 'B'], this function expects to find one or more
group of columns with format
A-suffix1, A-suffix2,..., B-suffix1, B-suffix2,...
You specify what you want to call this suffix in the resulting long format
with `j` (for example `j='year'`)

Each row of these wide variables are assumed to be uniquely identified by
`i` (can be a single column name or a list of column names)

All remaining variables in the data frame are left intact.

Parameters
----------
df : DataFrame
    The wide-format DataFrame.
stubnames : str or list-like
    The stub name(s). The wide format variables are assumed to
    start with the stub names.
i : str or list-like
    Column(s) to use as id variable(s).
j : str
    The name of the sub-observation variable. What you wish to name your
    suffix in the long format.
sep : str, default """"
    A character indicating the separation of the variable names
    in the wide format, to be stripped from the names in the long format.
    For example, if your column names are A-suffix1, A-suffix2, you
    can strip the hyphen by specifying `sep='-'`.
suffix : str, default '\\d+'
    A regular expression capturing the wanted suffixes. '\\d+' captures
    numeric suffixes. Suffixes with no numbers could be specified with the
    negated character class '\\D+'. You can also further disambiguate
    suffixes, for example, if your wide variables are of the form A-one,
    B-two,.., and you have an unrelated column A-rating, you can ignore the
    last one by specifying `suffix='(!?one|two)'`. When all suffixes are
    numeric, they are cast to int64/float64.

Returns
-------
DataFrame
    A DataFrame that contains each stub name as a variable, with new index
    (i, j).

See Also
--------
melt : Unpivot a DataFrame from wide to long format, optionally leaving
    identifiers set.
pivot : Create a spreadsheet-style pivot table as a DataFrame.
DataFrame.pivot : Pivot without aggregation that can handle
    non-numeric data.
DataFrame.pivot_table : Generalization of pivot that can handle
    duplicate values for one index/column pair.
DataFrame.unstack : Pivot based on the index values instead of a
    column.

Notes
-----
All extra variables are left untouched. This simply uses
`pandas.melt` under the hood, but is hard-coded to ""do the right thing""
in a typical case.

Examples
--------
>>> np.random.seed(123)
>>> df = pd.DataFrame({""A1970"" : {0 : ""a"", 1 : ""b"", 2 : ""c""},
...                    ""A1980"" : {0 : ""d"", 1 : ""e"", 2 : ""f""},
...                    ""B1970"" : {0 : 2.5, 1 : 1.2, 2 : .7},
...                    ""B1980"" : {0 : 3.2, 1 : 1.3, 2 : .1},
...                    ""X""     : dict(zip(range(3), np.random.randn(3)))
...                   })
>>> df[""id""] = df.index
>>> df
  A1970 A1980  B1970  B1980         X  id
0     a     d    2.5    3.2 -1.085631   0
1     b     e    1.2    1.3  0.997345   1
2     c     f    0.7    0.1  0.282978   2
>>> pd.wide_to_long(df, [""A"", ""B""], i=""id"", j=""year"")
... # doctest: +NORMALIZE_WHITESPACE
                X  A    B
id year
0  1970 -1.085631  a  2.5
1  1970  0.997345  b  1.2
2  1970  0.282978  c  0.7
0  1980 -1.085631  d  3.2
1  1980  0.997345  e  1.3
2  1980  0.282978  f  0.1

With multiple id columns

>>> df = pd.DataFrame({
...     'famid': [1, 1, 1, 2, 2, 2, 3, 3, 3],
...     'birth': [1, 2, 3, 1, 2, 3, 1, 2, 3],
...     'ht1': [2.8, 2.9, 2.2, 2, 1.8, 1.9, 2.2, 2.3, 2.1],
...     'ht2': [3.4, 3.8, 2.9, 3.2, 2.8, 2.4, 3.3, 3.4, 2.9]
... })
>>> df
   famid  birth  ht1  ht2
0      1      1  2.8  3.4
1      1      2  2.9  3.8
2      1      3  2.2  2.9
3      2      1  2.0  3.2
4      2      2  1.8  2.8
5      2      3  1.9  2.4
6      3      1  2.2  3.3
7      3      2  2.3  3.4
8      3      3  2.1  2.9
>>> l = pd.wide_to_long(df, stubnames='ht', i=['famid', 'birth'], j='age')
>>> l
... # doctest: +NORMALIZE_WHITESPACE
                  ht
famid birth age
1     1     1    2.8
            2    3.4
      2     1    2.9
            2    3.8
      3     1    2.2
            2    2.9
2     1     1    2.0
            2    3.2
      2     1    1.8
            2    2.8
      3     1    1.9
            2    2.4
3     1     1    2.2
            2    3.3
      2     1    2.3
            2    3.4
      3     1    2.1
            2    2.9

Going from long back to wide just takes some creative use of `unstack`

>>> w = l.unstack()
>>> w.columns = w.columns.map('{0[0]}{0[1]}'.format)
>>> w.reset_index()
   famid  birth  ht1  ht2
0      1      1  2.8  3.4
1      1      2  2.9  3.8
2      1      3  2.2  2.9
3      2      1  2.0  3.2
4      2      2  1.8  2.8
5      2      3  1.9  2.4
6      3      1  2.2  3.3
7      3      2  2.3  3.4
8      3      3  2.1  2.9

Less wieldy column names are also handled

>>> np.random.seed(0)
>>> df = pd.DataFrame({'A(weekly)-2010': np.random.rand(3),
...                    'A(weekly)-2011': np.random.rand(3),
...                    'B(weekly)-2010': np.random.rand(3),
...                    'B(weekly)-2011': np.random.rand(3),
...                    'X' : np.random.randint(3, size=3)})
>>> df['id'] = df.index
>>> df # doctest: +NORMALIZE_WHITESPACE, +ELLIPSIS
   A(weekly)-2010  A(weekly)-2011  B(weekly)-2010  B(weekly)-2011  X  id
0        0.548814        0.544883        0.437587        0.383442  0   0
1        0.715189        0.423655        0.891773        0.791725  1   1
2        0.602763        0.645894        0.963663        0.528895  1   2

>>> pd.wide_to_long(df, ['A(weekly)', 'B(weekly)'], i='id',
...                 j='year', sep='-')
... # doctest: +NORMALIZE_WHITESPACE
         X  A(weekly)  B(weekly)
id year
0  2010  0   0.548814   0.437587
1  2010  1   0.715189   0.891773
2  2010  1   0.602763   0.963663
0  2011  0   0.544883   0.383442
1  2011  1   0.423655   0.791725
2  2011  1   0.645894   0.528895

If we have many columns, we could also use a regex to find our
stubnames and pass that list on to wide_to_long

>>> stubnames = sorted(
...     set([match[0] for match in df.columns.str.findall(
...         r'[A-B]\(.*\)').values if match != []])
... )
>>> list(stubnames)
['A(weekly)', 'B(weekly)']

All of the above examples have integers as suffixes. It is possible to
have non-integers as suffixes.

>>> df = pd.DataFrame({
...     'famid': [1, 1, 1, 2, 2, 2, 3, 3, 3],
...     'birth': [1, 2, 3, 1, 2, 3, 1, 2, 3],
...     'ht_one': [2.8, 2.9, 2.2, 2, 1.8, 1.9, 2.2, 2.3, 2.1],
...     'ht_two': [3.4, 3.8, 2.9, 3.2, 2.8, 2.4, 3.3, 3.4, 2.9]
... })
>>> df
   famid  birth  ht_one  ht_two
0      1      1     2.8     3.4
1      1      2     2.9     3.8
2      1      3     2.2     2.9
3      2      1     2.0     3.2
4      2      2     1.8     2.8
5      2      3     1.9     2.4
6      3      1     2.2     3.3
7      3      2     2.3     3.4
8      3      3     2.1     2.9

>>> l = pd.wide_to_long(df, stubnames='ht', i=['famid', 'birth'], j='age',
...                     sep='_', suffix=r'\w+')
>>> l
... # doctest: +NORMALIZE_WHITESPACE
                  ht
famid birth age
1     1     one  2.8
            two  3.4
      2     one  2.9
            two  3.8
      3     one  2.2
            two  2.9
2     1     one  2.0
            two  3.2
      2     one  1.8
            two  2.8
      3     one  1.9
            two  2.4
3     1     one  2.2
            two  3.3
      2     one  2.3
            two  3.4
      3     one  2.1
            two  2.9"""""""
pandas/core/reshape/merge.py,"def _cross_merge(left: DataFrame, right: DataFrame, on: IndexLabel | AnyArrayLike | None=None, left_on: IndexLabel | AnyArrayLike | None=None, right_on: IndexLabel | AnyArrayLike | None=None, left_index: bool=False, right_index: bool=False, sort: bool=False, suffixes: Suffixes=('_x', '_y'), copy: bool | None=None, indicator: str | bool=False, validate: str | None=None) -> DataFrame:
    if left_index or right_index or right_on is not None or (left_on is not None) or (on is not None):
        raise MergeError('Can not pass on, right_on, left_on or set right_index=True or left_index=True')
    cross_col = f'_cross_{uuid.uuid4()}'
    left = left.assign(**{cross_col: 1})
    right = right.assign(**{cross_col: 1})
    left_on = right_on = [cross_col]
    res = merge(left, right, how='inner', on=on, left_on=left_on, right_on=right_on, left_index=left_index, right_index=right_index, sort=sort, suffixes=suffixes, indicator=indicator, validate=validate, copy=copy)
    del res[cross_col]
    return res","""""""See merge.__doc__ with how='cross'"""""""
pandas/core/reshape/merge.py,"def _groupby_and_merge(by, left: DataFrame | Series, right: DataFrame | Series, merge_pieces):
    pieces = []
    if not isinstance(by, (list, tuple)):
        by = [by]
    lby = left.groupby(by, sort=False)
    rby: groupby.DataFrameGroupBy | groupby.SeriesGroupBy | None = None
    if all((item in right.columns for item in by)):
        rby = right.groupby(by, sort=False)
    for (key, lhs) in lby.grouper.get_iterator(lby._selected_obj, axis=lby.axis):
        if rby is None:
            rhs = right
        else:
            try:
                rhs = right.take(rby.indices[key])
            except KeyError:
                lcols = lhs.columns.tolist()
                cols = lcols + [r for r in right.columns if r not in set(lcols)]
                merged = lhs.reindex(columns=cols)
                merged.index = range(len(merged))
                pieces.append(merged)
                continue
        merged = merge_pieces(lhs, rhs)
        merged[by] = key
        pieces.append(merged)
    from pandas.core.reshape.concat import concat
    result = concat(pieces, ignore_index=True)
    result = result.reindex(columns=pieces[0].columns, copy=False)
    return (result, lby)","""""""groupby & merge; we are always performing a left-by type operation

Parameters
----------
by: field to group
left: DataFrame
right: DataFrame
merge_pieces: function for merging"""""""
pandas/core/reshape/merge.py,"def merge_ordered(left: DataFrame | Series, right: DataFrame | Series, on: IndexLabel | None=None, left_on: IndexLabel | None=None, right_on: IndexLabel | None=None, left_by=None, right_by=None, fill_method: str | None=None, suffixes: Suffixes=('_x', '_y'), how: JoinHow='outer') -> DataFrame:

    def _merger(x, y) -> DataFrame:
        op = _OrderedMerge(x, y, on=on, left_on=left_on, right_on=right_on, suffixes=suffixes, fill_method=fill_method, how=how)
        return op.get_result()
    if left_by is not None and right_by is not None:
        raise ValueError('Can only group either left or right frames')
    if left_by is not None:
        if isinstance(left_by, str):
            left_by = [left_by]
        check = set(left_by).difference(left.columns)
        if len(check) != 0:
            raise KeyError(f'{check} not found in left columns')
        (result, _) = _groupby_and_merge(left_by, left, right, lambda x, y: _merger(x, y))
    elif right_by is not None:
        if isinstance(right_by, str):
            right_by = [right_by]
        check = set(right_by).difference(right.columns)
        if len(check) != 0:
            raise KeyError(f'{check} not found in right columns')
        (result, _) = _groupby_and_merge(right_by, right, left, lambda x, y: _merger(y, x))
    else:
        result = _merger(left, right)
    return result","""""""Perform a merge for ordered data with optional filling/interpolation.

Designed for ordered data like time series data. Optionally
perform group-wise merge (see examples).

Parameters
----------
left : DataFrame or named Series
right : DataFrame or named Series
on : label or list
    Field names to join on. Must be found in both DataFrames.
left_on : label or list, or array-like
    Field names to join on in left DataFrame. Can be a vector or list of
    vectors of the length of the DataFrame to use a particular vector as
    the join key instead of columns.
right_on : label or list, or array-like
    Field names to join on in right DataFrame or vector/list of vectors per
    left_on docs.
left_by : column name or list of column names
    Group left DataFrame by group columns and merge piece by piece with
    right DataFrame. Must be None if either left or right are a Series.
right_by : column name or list of column names
    Group right DataFrame by group columns and merge piece by piece with
    left DataFrame. Must be None if either left or right are a Series.
fill_method : {'ffill', None}, default None
    Interpolation method for data.
suffixes : list-like, default is (""_x"", ""_y"")
    A length-2 sequence where each element is optionally a string
    indicating the suffix to add to overlapping column names in
    `left` and `right` respectively. Pass a value of `None` instead
    of a string to indicate that the column name from `left` or
    `right` should be left as-is, with no suffix. At least one of the
    values must not be None.

how : {'left', 'right', 'outer', 'inner'}, default 'outer'
    * left: use only keys from left frame (SQL: left outer join)
    * right: use only keys from right frame (SQL: right outer join)
    * outer: use union of keys from both frames (SQL: full outer join)
    * inner: use intersection of keys from both frames (SQL: inner join).

Returns
-------
DataFrame
    The merged DataFrame output type will be the same as
    'left', if it is a subclass of DataFrame.

See Also
--------
merge : Merge with a database-style join.
merge_asof : Merge on nearest keys.

Examples
--------
>>> from pandas import merge_ordered
>>> df1 = pd.DataFrame(
...     {
...         ""key"": [""a"", ""c"", ""e"", ""a"", ""c"", ""e""],
...         ""lvalue"": [1, 2, 3, 1, 2, 3],
...         ""group"": [""a"", ""a"", ""a"", ""b"", ""b"", ""b""]
...     }
... )
>>> df1
  key  lvalue group
0   a       1     a
1   c       2     a
2   e       3     a
3   a       1     b
4   c       2     b
5   e       3     b

>>> df2 = pd.DataFrame({""key"": [""b"", ""c"", ""d""], ""rvalue"": [1, 2, 3]})
>>> df2
  key  rvalue
0   b       1
1   c       2
2   d       3

>>> merge_ordered(df1, df2, fill_method=""ffill"", left_by=""group"")
  key  lvalue group  rvalue
0   a       1     a     NaN
1   b       1     a     1.0
2   c       2     a     2.0
3   d       2     a     3.0
4   e       3     a     3.0
5   a       1     b     NaN
6   b       1     b     1.0
7   c       2     b     2.0
8   d       2     b     3.0
9   e       3     b     3.0"""""""
pandas/core/reshape/merge.py,"def merge_asof(left: DataFrame | Series, right: DataFrame | Series, on: IndexLabel | None=None, left_on: IndexLabel | None=None, right_on: IndexLabel | None=None, left_index: bool=False, right_index: bool=False, by=None, left_by=None, right_by=None, suffixes: Suffixes=('_x', '_y'), tolerance: int | Timedelta | None=None, allow_exact_matches: bool=True, direction: str='backward') -> DataFrame:
    op = _AsOfMerge(left, right, on=on, left_on=left_on, right_on=right_on, left_index=left_index, right_index=right_index, by=by, left_by=left_by, right_by=right_by, suffixes=suffixes, how='asof', tolerance=tolerance, allow_exact_matches=allow_exact_matches, direction=direction)
    return op.get_result()","""""""Perform a merge by key distance.

This is similar to a left-join except that we match on nearest
key rather than equal keys. Both DataFrames must be sorted by the key.

For each row in the left DataFrame:

  - A ""backward"" search selects the last row in the right DataFrame whose
    'on' key is less than or equal to the left's key.

  - A ""forward"" search selects the first row in the right DataFrame whose
    'on' key is greater than or equal to the left's key.

  - A ""nearest"" search selects the row in the right DataFrame whose 'on'
    key is closest in absolute distance to the left's key.

Optionally match on equivalent keys with 'by' before searching with 'on'.

Parameters
----------
left : DataFrame or named Series
right : DataFrame or named Series
on : label
    Field name to join on. Must be found in both DataFrames.
    The data MUST be ordered. Furthermore this must be a numeric column,
    such as datetimelike, integer, or float. On or left_on/right_on
    must be given.
left_on : label
    Field name to join on in left DataFrame.
right_on : label
    Field name to join on in right DataFrame.
left_index : bool
    Use the index of the left DataFrame as the join key.
right_index : bool
    Use the index of the right DataFrame as the join key.
by : column name or list of column names
    Match on these columns before performing merge operation.
left_by : column name
    Field names to match on in the left DataFrame.
right_by : column name
    Field names to match on in the right DataFrame.
suffixes : 2-length sequence (tuple, list, ...)
    Suffix to apply to overlapping column names in the left and right
    side, respectively.
tolerance : int or Timedelta, optional, default None
    Select asof tolerance within this range; must be compatible
    with the merge index.
allow_exact_matches : bool, default True

    - If True, allow matching with the same 'on' value
      (i.e. less-than-or-equal-to / greater-than-or-equal-to)
    - If False, don't match the same 'on' value
      (i.e., strictly less-than / strictly greater-than).

direction : 'backward' (default), 'forward', or 'nearest'
    Whether to search for prior, subsequent, or closest matches.

Returns
-------
DataFrame

See Also
--------
merge : Merge with a database-style join.
merge_ordered : Merge with optional filling/interpolation.

Examples
--------
>>> left = pd.DataFrame({""a"": [1, 5, 10], ""left_val"": [""a"", ""b"", ""c""]})
>>> left
    a left_val
0   1        a
1   5        b
2  10        c

>>> right = pd.DataFrame({""a"": [1, 2, 3, 6, 7], ""right_val"": [1, 2, 3, 6, 7]})
>>> right
   a  right_val
0  1          1
1  2          2
2  3          3
3  6          6
4  7          7

>>> pd.merge_asof(left, right, on=""a"")
    a left_val  right_val
0   1        a          1
1   5        b          3
2  10        c          7

>>> pd.merge_asof(left, right, on=""a"", allow_exact_matches=False)
    a left_val  right_val
0   1        a        NaN
1   5        b        3.0
2  10        c        7.0

>>> pd.merge_asof(left, right, on=""a"", direction=""forward"")
    a left_val  right_val
0   1        a        1.0
1   5        b        6.0
2  10        c        NaN

>>> pd.merge_asof(left, right, on=""a"", direction=""nearest"")
    a left_val  right_val
0   1        a          1
1   5        b          6
2  10        c          7

We can use indexed DataFrames as well.

>>> left = pd.DataFrame({""left_val"": [""a"", ""b"", ""c""]}, index=[1, 5, 10])
>>> left
   left_val
1         a
5         b
10        c

>>> right = pd.DataFrame({""right_val"": [1, 2, 3, 6, 7]}, index=[1, 2, 3, 6, 7])
>>> right
   right_val
1          1
2          2
3          3
6          6
7          7

>>> pd.merge_asof(left, right, left_index=True, right_index=True)
   left_val  right_val
1         a          1
5         b          3
10        c          7

Here is a real-world times-series example

>>> quotes = pd.DataFrame(
...     {
...         ""time"": [
...             pd.Timestamp(""2016-05-25 13:30:00.023""),
...             pd.Timestamp(""2016-05-25 13:30:00.023""),
...             pd.Timestamp(""2016-05-25 13:30:00.030""),
...             pd.Timestamp(""2016-05-25 13:30:00.041""),
...             pd.Timestamp(""2016-05-25 13:30:00.048""),
...             pd.Timestamp(""2016-05-25 13:30:00.049""),
...             pd.Timestamp(""2016-05-25 13:30:00.072""),
...             pd.Timestamp(""2016-05-25 13:30:00.075"")
...         ],
...         ""ticker"": [
...                ""GOOG"",
...                ""MSFT"",
...                ""MSFT"",
...                ""MSFT"",
...                ""GOOG"",
...                ""AAPL"",
...                ""GOOG"",
...                ""MSFT""
...            ],
...            ""bid"": [720.50, 51.95, 51.97, 51.99, 720.50, 97.99, 720.50, 52.01],
...            ""ask"": [720.93, 51.96, 51.98, 52.00, 720.93, 98.01, 720.88, 52.03]
...     }
... )
>>> quotes
                     time ticker     bid     ask
0 2016-05-25 13:30:00.023   GOOG  720.50  720.93
1 2016-05-25 13:30:00.023   MSFT   51.95   51.96
2 2016-05-25 13:30:00.030   MSFT   51.97   51.98
3 2016-05-25 13:30:00.041   MSFT   51.99   52.00
4 2016-05-25 13:30:00.048   GOOG  720.50  720.93
5 2016-05-25 13:30:00.049   AAPL   97.99   98.01
6 2016-05-25 13:30:00.072   GOOG  720.50  720.88
7 2016-05-25 13:30:00.075   MSFT   52.01   52.03

>>> trades = pd.DataFrame(
...        {
...            ""time"": [
...                pd.Timestamp(""2016-05-25 13:30:00.023""),
...                pd.Timestamp(""2016-05-25 13:30:00.038""),
...                pd.Timestamp(""2016-05-25 13:30:00.048""),
...                pd.Timestamp(""2016-05-25 13:30:00.048""),
...                pd.Timestamp(""2016-05-25 13:30:00.048"")
...            ],
...            ""ticker"": [""MSFT"", ""MSFT"", ""GOOG"", ""GOOG"", ""AAPL""],
...            ""price"": [51.95, 51.95, 720.77, 720.92, 98.0],
...            ""quantity"": [75, 155, 100, 100, 100]
...        }
...    )
>>> trades
                     time ticker   price  quantity
0 2016-05-25 13:30:00.023   MSFT   51.95        75
1 2016-05-25 13:30:00.038   MSFT   51.95       155
2 2016-05-25 13:30:00.048   GOOG  720.77       100
3 2016-05-25 13:30:00.048   GOOG  720.92       100
4 2016-05-25 13:30:00.048   AAPL   98.00       100

By default we are taking the asof of the quotes

>>> pd.merge_asof(trades, quotes, on=""time"", by=""ticker"")
                     time ticker   price  quantity     bid     ask
0 2016-05-25 13:30:00.023   MSFT   51.95        75   51.95   51.96
1 2016-05-25 13:30:00.038   MSFT   51.95       155   51.97   51.98
2 2016-05-25 13:30:00.048   GOOG  720.77       100  720.50  720.93
3 2016-05-25 13:30:00.048   GOOG  720.92       100  720.50  720.93
4 2016-05-25 13:30:00.048   AAPL   98.00       100     NaN     NaN

We only asof within 2ms between the quote time and the trade time

>>> pd.merge_asof(
...     trades, quotes, on=""time"", by=""ticker"", tolerance=pd.Timedelta(""2ms"")
... )
                     time ticker   price  quantity     bid     ask
0 2016-05-25 13:30:00.023   MSFT   51.95        75   51.95   51.96
1 2016-05-25 13:30:00.038   MSFT   51.95       155     NaN     NaN
2 2016-05-25 13:30:00.048   GOOG  720.77       100  720.50  720.93
3 2016-05-25 13:30:00.048   GOOG  720.92       100  720.50  720.93
4 2016-05-25 13:30:00.048   AAPL   98.00       100     NaN     NaN

We only asof within 10ms between the quote time and the trade time
and we exclude exact matches on time. However *prior* data will
propagate forward

>>> pd.merge_asof(
...     trades,
...     quotes,
...     on=""time"",
...     by=""ticker"",
...     tolerance=pd.Timedelta(""10ms""),
...     allow_exact_matches=False
... )
                     time ticker   price  quantity     bid     ask
0 2016-05-25 13:30:00.023   MSFT   51.95        75     NaN     NaN
1 2016-05-25 13:30:00.038   MSFT   51.95       155   51.97   51.98
2 2016-05-25 13:30:00.048   GOOG  720.77       100     NaN     NaN
3 2016-05-25 13:30:00.048   GOOG  720.92       100     NaN     NaN
4 2016-05-25 13:30:00.048   AAPL   98.00       100     NaN     NaN"""""""
pandas/core/reshape/merge.py,"def get_join_indexers(left_keys: list[ArrayLike], right_keys: list[ArrayLike], sort: bool=False, how: MergeHow | Literal['asof']='inner') -> tuple[npt.NDArray[np.intp], npt.NDArray[np.intp]]:
    assert len(left_keys) == len(right_keys), 'left_keys and right_keys must be the same length'
    left_n = len(left_keys[0])
    right_n = len(right_keys[0])
    if left_n == 0:
        if how in ['left', 'inner', 'cross']:
            return _get_empty_indexer()
        elif not sort and how in ['right', 'outer']:
            return _get_no_sort_one_missing_indexer(right_n, True)
    elif right_n == 0:
        if how in ['right', 'inner', 'cross']:
            return _get_empty_indexer()
        elif not sort and how in ['left', 'outer']:
            return _get_no_sort_one_missing_indexer(left_n, False)
    if not sort and how == 'outer':
        sort = True
    mapped = (_factorize_keys(left_keys[n], right_keys[n], sort=sort, how=how) for n in range(len(left_keys)))
    zipped = zip(*mapped)
    (llab, rlab, shape) = (list(x) for x in zipped)
    (lkey, rkey) = _get_join_keys(llab, rlab, tuple(shape), sort)
    (lkey, rkey, count) = _factorize_keys(lkey, rkey, sort=sort, how=how)
    kwargs = {}
    if how in ('inner', 'left', 'right'):
        kwargs['sort'] = sort
    join_func = {'inner': libjoin.inner_join, 'left': libjoin.left_outer_join, 'right': lambda x, y, count, **kwargs: libjoin.left_outer_join(y, x, count, **kwargs)[::-1], 'outer': libjoin.full_outer_join}[how]
    return join_func(lkey, rkey, count, **kwargs)","""""""Parameters
----------
left_keys : list[ndarray, ExtensionArray, Index, Series]
right_keys : list[ndarray, ExtensionArray, Index, Series]
sort : bool, default False
how : {'inner', 'outer', 'left', 'right'}, default 'inner'

Returns
-------
np.ndarray[np.intp]
    Indexer into the left_keys.
np.ndarray[np.intp]
    Indexer into the right_keys."""""""
pandas/core/reshape/merge.py,"def restore_dropped_levels_multijoin(left: MultiIndex, right: MultiIndex, dropped_level_names, join_index: Index, lindexer: npt.NDArray[np.intp], rindexer: npt.NDArray[np.intp]) -> tuple[FrozenList, FrozenList, FrozenList]:

    def _convert_to_multiindex(index: Index) -> MultiIndex:
        if isinstance(index, MultiIndex):
            return index
        else:
            return MultiIndex.from_arrays([index._values], names=[index.name])
    join_index = _convert_to_multiindex(join_index)
    join_levels = join_index.levels
    join_codes = join_index.codes
    join_names = join_index.names
    for dropped_level_name in dropped_level_names:
        if dropped_level_name in left.names:
            idx = left
            indexer = lindexer
        else:
            idx = right
            indexer = rindexer
        name_idx = idx.names.index(dropped_level_name)
        restore_levels = idx.levels[name_idx]
        codes = idx.codes[name_idx]
        if indexer is None:
            restore_codes = codes
        else:
            restore_codes = algos.take_nd(codes, indexer, fill_value=-1)
        join_levels = join_levels + [restore_levels]
        join_codes = join_codes + [restore_codes]
        join_names = join_names + [dropped_level_name]
    return (join_levels, join_codes, join_names)","""""""*this is an internal non-public method*

Returns the levels, labels and names of a multi-index to multi-index join.
Depending on the type of join, this method restores the appropriate
dropped levels of the joined multi-index.
The method relies on lindexer, rindexer which hold the index positions of
left and right, where a join was feasible

Parameters
----------
left : MultiIndex
    left index
right : MultiIndex
    right index
dropped_level_names : str array
    list of non-common level names
join_index : Index
    the index of the join between the
    common levels of left and right
lindexer : np.ndarray[np.intp]
    left indexer
rindexer : np.ndarray[np.intp]
    right indexer

Returns
-------
levels : list of Index
    levels of combined multiindexes
labels : np.ndarray[np.intp]
    labels of combined multiindexes
names : List[Hashable]
    names of combined multiindex levels"""""""
pandas/core/reshape/merge.py,"def _get_empty_indexer() -> tuple[npt.NDArray[np.intp], npt.NDArray[np.intp]]:
    return (np.array([], dtype=np.intp), np.array([], dtype=np.intp))","""""""Return empty join indexers."""""""
pandas/core/reshape/merge.py,"def _get_no_sort_one_missing_indexer(n: int, left_missing: bool) -> tuple[npt.NDArray[np.intp], npt.NDArray[np.intp]]:
    idx = np.arange(n, dtype=np.intp)
    idx_missing = np.full(shape=n, fill_value=-1, dtype=np.intp)
    if left_missing:
        return (idx_missing, idx)
    return (idx, idx_missing)","""""""Return join indexers where all of one side is selected without sorting
and none of the other side is selected.

Parameters
----------
n : int
    Length of indexers to create.
left_missing : bool
    If True, the left indexer will contain only -1's.
    If False, the right indexer will contain only -1's.

Returns
-------
np.ndarray[np.intp]
    Left indexer
np.ndarray[np.intp]
    Right indexer"""""""
pandas/core/reshape/merge.py,"def _factorize_keys(lk: ArrayLike, rk: ArrayLike, sort: bool=True, how: MergeHow | Literal['asof']='inner') -> tuple[npt.NDArray[np.intp], npt.NDArray[np.intp], int]:
    if isinstance(lk.dtype, DatetimeTZDtype) and isinstance(rk.dtype, DatetimeTZDtype) or (lib.is_np_dtype(lk.dtype, 'M') and lib.is_np_dtype(rk.dtype, 'M')):
        (lk, rk) = cast('DatetimeArray', lk)._ensure_matching_resos(rk)
        lk = cast('DatetimeArray', lk)._ndarray
        rk = cast('DatetimeArray', rk)._ndarray
    elif isinstance(lk.dtype, CategoricalDtype) and isinstance(rk.dtype, CategoricalDtype) and (lk.dtype == rk.dtype):
        assert isinstance(lk, Categorical)
        assert isinstance(rk, Categorical)
        rk = lk._encode_with_my_categories(rk)
        lk = ensure_int64(lk.codes)
        rk = ensure_int64(rk.codes)
    elif isinstance(lk, ExtensionArray) and lk.dtype == rk.dtype:
        if isinstance(lk.dtype, ArrowDtype) and is_string_dtype(lk.dtype) or (isinstance(lk.dtype, StringDtype) and lk.dtype.storage in ['pyarrow', 'pyarrow_numpy']):
            import pyarrow as pa
            import pyarrow.compute as pc
            len_lk = len(lk)
            lk = lk._pa_array
            rk = rk._pa_array
            dc = pa.chunked_array(lk.chunks + rk.chunks).combine_chunks().dictionary_encode()
            length = len(dc.dictionary)
            (llab, rlab, count) = (pc.fill_null(dc.indices[slice(len_lk)], length).to_numpy().astype(np.intp, copy=False), pc.fill_null(dc.indices[slice(len_lk, None)], length).to_numpy().astype(np.intp, copy=False), len(dc.dictionary))
            if dc.null_count > 0:
                count += 1
            if how == 'right':
                return (rlab, llab, count)
            return (llab, rlab, count)
        if not isinstance(lk, BaseMaskedArray) and (not (isinstance(lk.dtype, ArrowDtype) and (is_numeric_dtype(lk.dtype.numpy_dtype) or (is_string_dtype(lk.dtype) and (not sort))))):
            (lk, _) = lk._values_for_factorize()
            (rk, _) = rk._values_for_factorize()
    if needs_i8_conversion(lk.dtype) and lk.dtype == rk.dtype:
        lk = np.asarray(lk, dtype=np.int64)
        rk = np.asarray(rk, dtype=np.int64)
    (klass, lk, rk) = _convert_arrays_and_get_rizer_klass(lk, rk)
    rizer = klass(max(len(lk), len(rk)))
    if isinstance(lk, BaseMaskedArray):
        assert isinstance(rk, BaseMaskedArray)
        llab = rizer.factorize(lk._data, mask=lk._mask)
        rlab = rizer.factorize(rk._data, mask=rk._mask)
    elif isinstance(lk, ArrowExtensionArray):
        assert isinstance(rk, ArrowExtensionArray)
        llab = rizer.factorize(lk.to_numpy(na_value=1, dtype=lk.dtype.numpy_dtype), mask=lk.isna())
        rlab = rizer.factorize(rk.to_numpy(na_value=1, dtype=lk.dtype.numpy_dtype), mask=rk.isna())
    else:
        llab = rizer.factorize(lk)
        rlab = rizer.factorize(rk)
    assert llab.dtype == np.dtype(np.intp), llab.dtype
    assert rlab.dtype == np.dtype(np.intp), rlab.dtype
    count = rizer.get_count()
    if sort:
        uniques = rizer.uniques.to_array()
        (llab, rlab) = _sort_labels(uniques, llab, rlab)
    lmask = llab == -1
    lany = lmask.any()
    rmask = rlab == -1
    rany = rmask.any()
    if lany or rany:
        if lany:
            np.putmask(llab, lmask, count)
        if rany:
            np.putmask(rlab, rmask, count)
        count += 1
    if how == 'right':
        return (rlab, llab, count)
    return (llab, rlab, count)","""""""Encode left and right keys as enumerated types.

This is used to get the join indexers to be used when merging DataFrames.

Parameters
----------
lk : ndarray, ExtensionArray
    Left key.
rk : ndarray, ExtensionArray
    Right key.
sort : bool, defaults to True
    If True, the encoding is done such that the unique elements in the
    keys are sorted.
how : {'left', 'right', 'outer', 'inner'}, default 'inner'
    Type of merge.

Returns
-------
np.ndarray[np.intp]
    Left (resp. right if called with `key='right'`) labels, as enumerated type.
np.ndarray[np.intp]
    Right (resp. left if called with `key='right'`) labels, as enumerated type.
int
    Number of unique elements in union of left and right labels.

See Also
--------
merge : Merge DataFrame or named Series objects
    with a database-style join.
algorithms.factorize : Encode the object as an enumerated type
    or categorical variable.

Examples
--------
>>> lk = np.array([""a"", ""c"", ""b""])
>>> rk = np.array([""a"", ""c""])

Here, the unique values are `'a', 'b', 'c'`. With the default
`sort=True`, the encoding will be `{0: 'a', 1: 'b', 2: 'c'}`:

>>> pd.core.reshape.merge._factorize_keys(lk, rk)
(array([0, 2, 1]), array([0, 2]), 3)

With the `sort=False`, the encoding will correspond to the order
in which the unique elements first appear: `{0: 'a', 1: 'c', 2: 'b'}`:

>>> pd.core.reshape.merge._factorize_keys(lk, rk, sort=False)
(array([0, 1, 2]), array([0, 1]), 3)"""""""
pandas/core/reshape/merge.py,"def _items_overlap_with_suffix(left: Index, right: Index, suffixes: Suffixes) -> tuple[Index, Index]:
    if not is_list_like(suffixes, allow_sets=False) or isinstance(suffixes, dict):
        raise TypeError(f""Passing 'suffixes' as a {type(suffixes)}, is not supported. Provide 'suffixes' as a tuple instead."")
    to_rename = left.intersection(right)
    if len(to_rename) == 0:
        return (left, right)
    (lsuffix, rsuffix) = suffixes
    if not lsuffix and (not rsuffix):
        raise ValueError(f'columns overlap but no suffix specified: {to_rename}')

    def renamer(x, suffix: str | None):
        """"""
        Rename the left and right indices.

        If there is overlap, and suffix is not None, add
        suffix, otherwise, leave it as-is.

        Parameters
        ----------
        x : original column name
        suffix : str or None

        Returns
        -------
        x : renamed column name
        """"""
        if x in to_rename and suffix is not None:
            return f'{x}{suffix}'
        return x
    lrenamer = partial(renamer, suffix=lsuffix)
    rrenamer = partial(renamer, suffix=rsuffix)
    llabels = left._transform_index(lrenamer)
    rlabels = right._transform_index(rrenamer)
    dups = []
    if not llabels.is_unique:
        dups = llabels[llabels.duplicated() & ~left.duplicated()].tolist()
    if not rlabels.is_unique:
        dups.extend(rlabels[rlabels.duplicated() & ~right.duplicated()].tolist())
    if dups:
        raise MergeError(f""Passing 'suffixes' which cause duplicate columns {set(dups)} is not allowed."")
    return (llabels, rlabels)","""""""Suffixes type validation.

If two indices overlap, add suffixes to overlapping entries.

If corresponding suffix is empty, the entry is simply converted to string."""""""
pandas/core/reshape/pivot.py,"def __internal_pivot_table(data: DataFrame, values, index, columns, aggfunc: AggFuncTypeBase | AggFuncTypeDict, fill_value, margins: bool, dropna: bool, margins_name: Hashable, observed: bool, sort: bool) -> DataFrame:
    keys = index + columns
    values_passed = values is not None
    if values_passed:
        if is_list_like(values):
            values_multi = True
            values = list(values)
        else:
            values_multi = False
            values = [values]
        for i in values:
            if i not in data:
                raise KeyError(i)
        to_filter = []
        for x in keys + values:
            if isinstance(x, Grouper):
                x = x.key
            try:
                if x in data:
                    to_filter.append(x)
            except TypeError:
                pass
        if len(to_filter) < len(data.columns):
            data = data[to_filter]
    else:
        values = data.columns
        for key in keys:
            try:
                values = values.drop(key)
            except (TypeError, ValueError, KeyError):
                pass
        values = list(values)
    grouped = data.groupby(keys, observed=observed, sort=sort, dropna=dropna)
    agged = grouped.agg(aggfunc)
    if dropna and isinstance(agged, ABCDataFrame) and len(agged.columns):
        agged = agged.dropna(how='all')
    table = agged
    if table.index.nlevels > 1 and index:
        index_names = agged.index.names[:len(index)]
        to_unstack = []
        for i in range(len(index), len(keys)):
            name = agged.index.names[i]
            if name is None or name in index_names:
                to_unstack.append(i)
            else:
                to_unstack.append(name)
        table = agged.unstack(to_unstack, fill_value=fill_value)
    if not dropna:
        if isinstance(table.index, MultiIndex):
            m = MultiIndex.from_arrays(cartesian_product(table.index.levels), names=table.index.names)
            table = table.reindex(m, axis=0, fill_value=fill_value)
        if isinstance(table.columns, MultiIndex):
            m = MultiIndex.from_arrays(cartesian_product(table.columns.levels), names=table.columns.names)
            table = table.reindex(m, axis=1, fill_value=fill_value)
    if sort is True and isinstance(table, ABCDataFrame):
        table = table.sort_index(axis=1)
    if fill_value is not None:
        table = table.fillna(fill_value)
        if aggfunc is len and (not observed) and lib.is_integer(fill_value):
            table = table.astype(np.int64)
    if margins:
        if dropna:
            data = data[data.notna().all(axis=1)]
        table = _add_margins(table, data, values, rows=index, cols=columns, aggfunc=aggfunc, observed=dropna, margins_name=margins_name, fill_value=fill_value)
    if values_passed and (not values_multi) and (table.columns.nlevels > 1):
        table.columns = table.columns.droplevel(0)
    if len(index) == 0 and len(columns) > 0:
        table = table.T
    if isinstance(table, ABCDataFrame) and dropna:
        table = table.dropna(how='all', axis=1)
    return table","""""""Helper of :func:`pandas.pivot_table` for any non-list ``aggfunc``."""""""
pandas/core/reshape/pivot.py,"def crosstab(index, columns, values=None, rownames=None, colnames=None, aggfunc=None, margins: bool=False, margins_name: Hashable='All', dropna: bool=True, normalize: bool | Literal[0, 1, 'all', 'index', 'columns']=False) -> DataFrame:
    if values is None and aggfunc is not None:
        raise ValueError('aggfunc cannot be used without values.')
    if values is not None and aggfunc is None:
        raise ValueError('values cannot be used without an aggfunc.')
    if not is_nested_list_like(index):
        index = [index]
    if not is_nested_list_like(columns):
        columns = [columns]
    common_idx = None
    pass_objs = [x for x in index + columns if isinstance(x, (ABCSeries, ABCDataFrame))]
    if pass_objs:
        common_idx = get_objs_combined_axis(pass_objs, intersect=True, sort=False)
    rownames = _get_names(index, rownames, prefix='row')
    colnames = _get_names(columns, colnames, prefix='col')
    (rownames_mapper, unique_rownames, colnames_mapper, unique_colnames) = _build_names_mapper(rownames, colnames)
    from pandas import DataFrame
    data = {**dict(zip(unique_rownames, index)), **dict(zip(unique_colnames, columns))}
    df = DataFrame(data, index=common_idx)
    if values is None:
        df['__dummy__'] = 0
        kwargs = {'aggfunc': len, 'fill_value': 0}
    else:
        df['__dummy__'] = values
        kwargs = {'aggfunc': aggfunc}
    table = df.pivot_table('__dummy__', index=unique_rownames, columns=unique_colnames, margins=margins, margins_name=margins_name, dropna=dropna, **kwargs)
    if normalize is not False:
        table = _normalize(table, normalize=normalize, margins=margins, margins_name=margins_name)
    table = table.rename_axis(index=rownames_mapper, axis=0)
    table = table.rename_axis(columns=colnames_mapper, axis=1)
    return table","""""""Compute a simple cross tabulation of two (or more) factors.

By default, computes a frequency table of the factors unless an
array of values and an aggregation function are passed.

Parameters
----------
index : array-like, Series, or list of arrays/Series
    Values to group by in the rows.
columns : array-like, Series, or list of arrays/Series
    Values to group by in the columns.
values : array-like, optional
    Array of values to aggregate according to the factors.
    Requires `aggfunc` be specified.
rownames : sequence, default None
    If passed, must match number of row arrays passed.
colnames : sequence, default None
    If passed, must match number of column arrays passed.
aggfunc : function, optional
    If specified, requires `values` be specified as well.
margins : bool, default False
    Add row/column margins (subtotals).
margins_name : str, default 'All'
    Name of the row/column that will contain the totals
    when margins is True.
dropna : bool, default True
    Do not include columns whose entries are all NaN.
normalize : bool, {'all', 'index', 'columns'}, or {0,1}, default False
    Normalize by dividing all values by the sum of values.

    - If passed 'all' or `True`, will normalize over all values.
    - If passed 'index' will normalize over each row.
    - If passed 'columns' will normalize over each column.
    - If margins is `True`, will also normalize margin values.

Returns
-------
DataFrame
    Cross tabulation of the data.

See Also
--------
DataFrame.pivot : Reshape data based on column values.
pivot_table : Create a pivot table as a DataFrame.

Notes
-----
Any Series passed will have their name attributes used unless row or column
names for the cross-tabulation are specified.

Any input passed containing Categorical data will have **all** of its
categories included in the cross-tabulation, even if the actual data does
not contain any instances of a particular category.

In the event that there aren't overlapping indexes an empty DataFrame will
be returned.

Reference :ref:`the user guide <reshaping.crosstabulations>` for more examples.

Examples
--------
>>> a = np.array([""foo"", ""foo"", ""foo"", ""foo"", ""bar"", ""bar"",
...               ""bar"", ""bar"", ""foo"", ""foo"", ""foo""], dtype=object)
>>> b = np.array([""one"", ""one"", ""one"", ""two"", ""one"", ""one"",
...               ""one"", ""two"", ""two"", ""two"", ""one""], dtype=object)
>>> c = np.array([""dull"", ""dull"", ""shiny"", ""dull"", ""dull"", ""shiny"",
...               ""shiny"", ""dull"", ""shiny"", ""shiny"", ""shiny""],
...              dtype=object)
>>> pd.crosstab(a, [b, c], rownames=['a'], colnames=['b', 'c'])
b   one        two
c   dull shiny dull shiny
a
bar    1     2    1     0
foo    2     2    1     2

Here 'c' and 'f' are not represented in the data and will not be
shown in the output because dropna is True by default. Set
dropna=False to preserve categories with no data.

>>> foo = pd.Categorical(['a', 'b'], categories=['a', 'b', 'c'])
>>> bar = pd.Categorical(['d', 'e'], categories=['d', 'e', 'f'])
>>> pd.crosstab(foo, bar)
col_0  d  e
row_0
a      1  0
b      0  1
>>> pd.crosstab(foo, bar, dropna=False)
col_0  d  e  f
row_0
a      1  0  0
b      0  1  0
c      0  0  0"""""""
pandas/core/reshape/pivot.py,"def _build_names_mapper(rownames: list[str], colnames: list[str]) -> tuple[dict[str, str], list[str], dict[str, str], list[str]]:

    def get_duplicates(names):
        seen: set = set()
        return {name for name in names if name not in seen}
    shared_names = set(rownames).intersection(set(colnames))
    dup_names = get_duplicates(rownames) | get_duplicates(colnames) | shared_names
    rownames_mapper = {f'row_{i}': name for (i, name) in enumerate(rownames) if name in dup_names}
    unique_rownames = [f'row_{i}' if name in dup_names else name for (i, name) in enumerate(rownames)]
    colnames_mapper = {f'col_{i}': name for (i, name) in enumerate(colnames) if name in dup_names}
    unique_colnames = [f'col_{i}' if name in dup_names else name for (i, name) in enumerate(colnames)]
    return (rownames_mapper, unique_rownames, colnames_mapper, unique_colnames)","""""""Given the names of a DataFrame's rows and columns, returns a set of unique row
and column names and mappers that convert to original names.

A row or column name is replaced if it is duplicate among the rows of the inputs,
among the columns of the inputs or between the rows and the columns.

Parameters
----------
rownames: list[str]
colnames: list[str]

Returns
-------
Tuple(Dict[str, str], List[str], Dict[str, str], List[str])

rownames_mapper: dict[str, str]
    a dictionary with new row names as keys and original rownames as values
unique_rownames: list[str]
    a list of rownames with duplicate names replaced by dummy names
colnames_mapper: dict[str, str]
    a dictionary with new column names as keys and original column names as values
unique_colnames: list[str]
    a list of column names with duplicate names replaced by dummy names"""""""
pandas/core/reshape/reshape.py,"def _unstack_extension_series(series: Series, level, fill_value, sort: bool) -> DataFrame:
    df = series.to_frame()
    result = df.unstack(level=level, fill_value=fill_value, sort=sort)
    result.columns = result.columns.droplevel(0)
    return result","""""""Unstack an ExtensionArray-backed Series.

The ExtensionDtype is preserved.

Parameters
----------
series : Series
    A Series with an ExtensionArray for values
level : Any
    The level name or number.
fill_value : Any
    The user-level (not physical storage) fill value to use for
    missing values introduced by the reshape. Passed to
    ``series.values.take``.
sort : bool
    Whether to sort the resulting MuliIndex levels

Returns
-------
DataFrame
    Each column of the DataFrame will have the same dtype as
    the input Series."""""""
pandas/core/reshape/reshape.py,"def stack(frame: DataFrame, level=-1, dropna: bool=True, sort: bool=True):

    def stack_factorize(index):
        if index.is_unique:
            return (index, np.arange(len(index)))
        (codes, categories) = factorize_from_iterable(index)
        return (categories, codes)
    (N, K) = frame.shape
    level_num = frame.columns._get_level_number(level)
    if isinstance(frame.columns, MultiIndex):
        return _stack_multi_columns(frame, level_num=level_num, dropna=dropna, sort=sort)
    elif isinstance(frame.index, MultiIndex):
        new_levels = list(frame.index.levels)
        new_codes = [lab.repeat(K) for lab in frame.index.codes]
        (clev, clab) = stack_factorize(frame.columns)
        new_levels.append(clev)
        new_codes.append(np.tile(clab, N).ravel())
        new_names = list(frame.index.names)
        new_names.append(frame.columns.name)
        new_index = MultiIndex(levels=new_levels, codes=new_codes, names=new_names, verify_integrity=False)
    else:
        (levels, (ilab, clab)) = zip(*map(stack_factorize, (frame.index, frame.columns)))
        codes = (ilab.repeat(K), np.tile(clab, N).ravel())
        new_index = MultiIndex(levels=levels, codes=codes, names=[frame.index.name, frame.columns.name], verify_integrity=False)
    new_values: ArrayLike
    if not frame.empty and frame._is_homogeneous_type:
        dtypes = list(frame.dtypes._values)
        dtype = dtypes[0]
        if isinstance(dtype, ExtensionDtype):
            arr = dtype.construct_array_type()
            new_values = arr._concat_same_type([col._values for (_, col) in frame.items()])
            new_values = _reorder_for_extension_array_stack(new_values, N, K)
        else:
            new_values = frame._values.ravel()
    else:
        new_values = frame._values.ravel()
    if dropna:
        mask = notna(new_values)
        new_values = new_values[mask]
        new_index = new_index[mask]
    return frame._constructor_sliced(new_values, index=new_index)","""""""Convert DataFrame to Series with multi-level Index. Columns become the
second level of the resulting hierarchical index

Returns
-------
stacked : Series or DataFrame"""""""
pandas/core/reshape/reshape.py,"def _stack_multi_column_index(columns: MultiIndex) -> MultiIndex:
    if len(columns.levels) <= 2:
        return columns.levels[0]._rename(name=columns.names[0])
    levs = [[lev[c] if c >= 0 else None for c in codes] for (lev, codes) in zip(columns.levels[:-1], columns.codes[:-1])]
    tuples = zip(*levs)
    unique_tuples = (key for (key, _) in itertools.groupby(tuples))
    new_levs = zip(*unique_tuples)
    return MultiIndex.from_arrays([Index(new_lev, dtype=lev.dtype) if None not in new_lev else new_lev for (new_lev, lev) in zip(new_levs, columns.levels)], names=columns.names[:-1])","""""""Creates a MultiIndex from the first N-1 levels of this MultiIndex."""""""
pandas/core/reshape/reshape.py,"def _reorder_for_extension_array_stack(arr: ExtensionArray, n_rows: int, n_columns: int) -> ExtensionArray:
    idx = np.arange(n_rows * n_columns).reshape(n_columns, n_rows).T.ravel()
    return arr.take(idx)","""""""Re-orders the values when stacking multiple extension-arrays.

The indirect stacking method used for EAs requires a followup
take to get the order correct.

Parameters
----------
arr : ExtensionArray
n_rows, n_columns : int
    The number of rows and columns in the original DataFrame.

Returns
-------
taken : ExtensionArray
    The original `arr` with elements re-ordered appropriately

Examples
--------
>>> arr = np.array(['a', 'b', 'c', 'd', 'e', 'f'])
>>> _reorder_for_extension_array_stack(arr, 2, 3)
array(['a', 'c', 'e', 'b', 'd', 'f'], dtype='<U1')

>>> _reorder_for_extension_array_stack(arr, 3, 2)
array(['a', 'd', 'b', 'e', 'c', 'f'], dtype='<U1')"""""""
pandas/core/reshape/tile.py,"def cut(x, bins, right: bool=True, labels=None, retbins: bool=False, precision: int=3, include_lowest: bool=False, duplicates: str='raise', ordered: bool=True):
    original = x
    x_idx = _preprocess_for_cut(x)
    (x_idx, _) = _coerce_to_type(x_idx)
    if not np.iterable(bins):
        bins = _nbins_to_bins(x_idx, bins, right)
    elif isinstance(bins, IntervalIndex):
        if bins.is_overlapping:
            raise ValueError('Overlapping IntervalIndex is not accepted.')
    else:
        bins = Index(bins)
        if not bins.is_monotonic_increasing:
            raise ValueError('bins must increase monotonically.')
    (fac, bins) = _bins_to_cuts(x_idx, bins, right=right, labels=labels, precision=precision, include_lowest=include_lowest, duplicates=duplicates, ordered=ordered)
    return _postprocess_for_cut(fac, bins, retbins, original)","""""""Bin values into discrete intervals.

Use `cut` when you need to segment and sort data values into bins. This
function is also useful for going from a continuous variable to a
categorical variable. For example, `cut` could convert ages to groups of
age ranges. Supports binning into an equal number of bins, or a
pre-specified array of bins.

Parameters
----------
x : array-like
    The input array to be binned. Must be 1-dimensional.
bins : int, sequence of scalars, or IntervalIndex
    The criteria to bin by.

    * int : Defines the number of equal-width bins in the range of `x`. The
      range of `x` is extended by .1% on each side to include the minimum
      and maximum values of `x`.
    * sequence of scalars : Defines the bin edges allowing for non-uniform
      width. No extension of the range of `x` is done.
    * IntervalIndex : Defines the exact bins to be used. Note that
      IntervalIndex for `bins` must be non-overlapping.

right : bool, default True
    Indicates whether `bins` includes the rightmost edge or not. If
    ``right == True`` (the default), then the `bins` ``[1, 2, 3, 4]``
    indicate (1,2], (2,3], (3,4]. This argument is ignored when
    `bins` is an IntervalIndex.
labels : array or False, default None
    Specifies the labels for the returned bins. Must be the same length as
    the resulting bins. If False, returns only integer indicators of the
    bins. This affects the type of the output container (see below).
    This argument is ignored when `bins` is an IntervalIndex. If True,
    raises an error. When `ordered=False`, labels must be provided.
retbins : bool, default False
    Whether to return the bins or not. Useful when bins is provided
    as a scalar.
precision : int, default 3
    The precision at which to store and display the bins labels.
include_lowest : bool, default False
    Whether the first interval should be left-inclusive or not.
duplicates : {default 'raise', 'drop'}, optional
    If bin edges are not unique, raise ValueError or drop non-uniques.
ordered : bool, default True
    Whether the labels are ordered or not. Applies to returned types
    Categorical and Series (with Categorical dtype). If True,
    the resulting categorical will be ordered. If False, the resulting
    categorical will be unordered (labels must be provided).

Returns
-------
out : Categorical, Series, or ndarray
    An array-like object representing the respective bin for each value
    of `x`. The type depends on the value of `labels`.

    * None (default) : returns a Series for Series `x` or a
      Categorical for all other inputs. The values stored within
      are Interval dtype.

    * sequence of scalars : returns a Series for Series `x` or a
      Categorical for all other inputs. The values stored within
      are whatever the type in the sequence is.

    * False : returns an ndarray of integers.

bins : numpy.ndarray or IntervalIndex.
    The computed or specified bins. Only returned when `retbins=True`.
    For scalar or sequence `bins`, this is an ndarray with the computed
    bins. If set `duplicates=drop`, `bins` will drop non-unique bin. For
    an IntervalIndex `bins`, this is equal to `bins`.

See Also
--------
qcut : Discretize variable into equal-sized buckets based on rank
    or based on sample quantiles.
Categorical : Array type for storing data that come from a
    fixed set of values.
Series : One-dimensional array with axis labels (including time series).
IntervalIndex : Immutable Index implementing an ordered, sliceable set.

Notes
-----
Any NA values will be NA in the result. Out of bounds values will be NA in
the resulting Series or Categorical object.

Reference :ref:`the user guide <reshaping.tile.cut>` for more examples.

Examples
--------
Discretize into three equal-sized bins.

>>> pd.cut(np.array([1, 7, 5, 4, 6, 3]), 3)
... # doctest: +ELLIPSIS
[(0.994, 3.0], (5.0, 7.0], (3.0, 5.0], (3.0, 5.0], (5.0, 7.0], ...
Categories (3, interval[float64, right]): [(0.994, 3.0] < (3.0, 5.0] ...

>>> pd.cut(np.array([1, 7, 5, 4, 6, 3]), 3, retbins=True)
... # doctest: +ELLIPSIS
([(0.994, 3.0], (5.0, 7.0], (3.0, 5.0], (3.0, 5.0], (5.0, 7.0], ...
Categories (3, interval[float64, right]): [(0.994, 3.0] < (3.0, 5.0] ...
array([0.994, 3.   , 5.   , 7.   ]))

Discovers the same bins, but assign them specific labels. Notice that
the returned Categorical's categories are `labels` and is ordered.

>>> pd.cut(np.array([1, 7, 5, 4, 6, 3]),
...        3, labels=[""bad"", ""medium"", ""good""])
['bad', 'good', 'medium', 'medium', 'good', 'bad']
Categories (3, object): ['bad' < 'medium' < 'good']

``ordered=False`` will result in unordered categories when labels are passed.
This parameter can be used to allow non-unique labels:

>>> pd.cut(np.array([1, 7, 5, 4, 6, 3]), 3,
...        labels=[""B"", ""A"", ""B""], ordered=False)
['B', 'B', 'A', 'A', 'B', 'B']
Categories (2, object): ['A', 'B']

``labels=False`` implies you just want the bins back.

>>> pd.cut([0, 1, 1, 2], bins=4, labels=False)
array([0, 1, 1, 3])

Passing a Series as an input returns a Series with categorical dtype:

>>> s = pd.Series(np.array([2, 4, 6, 8, 10]),
...               index=['a', 'b', 'c', 'd', 'e'])
>>> pd.cut(s, 3)
... # doctest: +ELLIPSIS
a    (1.992, 4.667]
b    (1.992, 4.667]
c    (4.667, 7.333]
d     (7.333, 10.0]
e     (7.333, 10.0]
dtype: category
Categories (3, interval[float64, right]): [(1.992, 4.667] < (4.667, ...

Passing a Series as an input returns a Series with mapping value.
It is used to map numerically to intervals based on bins.

>>> s = pd.Series(np.array([2, 4, 6, 8, 10]),
...               index=['a', 'b', 'c', 'd', 'e'])
>>> pd.cut(s, [0, 2, 4, 6, 8, 10], labels=False, retbins=True, right=False)
... # doctest: +ELLIPSIS
(a    1.0
 b    2.0
 c    3.0
 d    4.0
 e    NaN
 dtype: float64,
 array([ 0,  2,  4,  6,  8, 10]))

Use `drop` optional when bins is not unique

>>> pd.cut(s, [0, 2, 4, 6, 10, 10], labels=False, retbins=True,
...        right=False, duplicates='drop')
... # doctest: +ELLIPSIS
(a    1.0
 b    2.0
 c    3.0
 d    3.0
 e    NaN
 dtype: float64,
 array([ 0,  2,  4,  6, 10]))

Passing an IntervalIndex for `bins` results in those categories exactly.
Notice that values not covered by the IntervalIndex are set to NaN. 0
is to the left of the first bin (which is closed on the right), and 1.5
falls between two bins.

>>> bins = pd.IntervalIndex.from_tuples([(0, 1), (2, 3), (4, 5)])
>>> pd.cut([0, 0.5, 1.5, 2.5, 4.5], bins)
[NaN, (0.0, 1.0], NaN, (2.0, 3.0], (4.0, 5.0]]
Categories (3, interval[int64, right]): [(0, 1] < (2, 3] < (4, 5]]"""""""
pandas/core/reshape/tile.py,"def qcut(x, q, labels=None, retbins: bool=False, precision: int=3, duplicates: str='raise'):
    original = x
    x_idx = _preprocess_for_cut(x)
    (x_idx, _) = _coerce_to_type(x_idx)
    quantiles = np.linspace(0, 1, q + 1) if is_integer(q) else q
    bins = x_idx.to_series().dropna().quantile(quantiles)
    (fac, bins) = _bins_to_cuts(x_idx, Index(bins), labels=labels, precision=precision, include_lowest=True, duplicates=duplicates)
    return _postprocess_for_cut(fac, bins, retbins, original)","""""""Quantile-based discretization function.

Discretize variable into equal-sized buckets based on rank or based
on sample quantiles. For example 1000 values for 10 quantiles would
produce a Categorical object indicating quantile membership for each data point.

Parameters
----------
x : 1d ndarray or Series
q : int or list-like of float
    Number of quantiles. 10 for deciles, 4 for quartiles, etc. Alternately
    array of quantiles, e.g. [0, .25, .5, .75, 1.] for quartiles.
labels : array or False, default None
    Used as labels for the resulting bins. Must be of the same length as
    the resulting bins. If False, return only integer indicators of the
    bins. If True, raises an error.
retbins : bool, optional
    Whether to return the (bins, labels) or not. Can be useful if bins
    is given as a scalar.
precision : int, optional
    The precision at which to store and display the bins labels.
duplicates : {default 'raise', 'drop'}, optional
    If bin edges are not unique, raise ValueError or drop non-uniques.

Returns
-------
out : Categorical or Series or array of integers if labels is False
    The return type (Categorical or Series) depends on the input: a Series
    of type category if input is a Series else Categorical. Bins are
    represented as categories when categorical data is returned.
bins : ndarray of floats
    Returned only if `retbins` is True.

Notes
-----
Out of bounds values will be NA in the resulting Categorical object

Examples
--------
>>> pd.qcut(range(5), 4)
... # doctest: +ELLIPSIS
[(-0.001, 1.0], (-0.001, 1.0], (1.0, 2.0], (2.0, 3.0], (3.0, 4.0]]
Categories (4, interval[float64, right]): [(-0.001, 1.0] < (1.0, 2.0] ...

>>> pd.qcut(range(5), 3, labels=[""good"", ""medium"", ""bad""])
... # doctest: +SKIP
[good, good, medium, bad, bad]
Categories (3, object): [good < medium < bad]

>>> pd.qcut(range(5), 4, labels=False)
array([0, 0, 1, 2, 3])"""""""
pandas/core/reshape/tile.py,"def _nbins_to_bins(x_idx: Index, nbins: int, right: bool) -> Index:
    if is_scalar(nbins) and nbins < 1:
        raise ValueError('`bins` should be a positive integer.')
    if x_idx.size == 0:
        raise ValueError('Cannot cut empty array')
    rng = (x_idx.min(), x_idx.max())
    (mn, mx) = rng
    is_dt_or_td = lib.is_np_dtype(x_idx.dtype, 'mM') or isinstance(x_idx.dtype, DatetimeTZDtype)
    if is_numeric_dtype(x_idx.dtype) and (np.isinf(mn) or np.isinf(mx)):
        raise ValueError('cannot specify integer `bins` when input data contains infinity')
    if mn == mx:
        if is_dt_or_td:
            td = Timedelta(seconds=1)
            bins = x_idx._values._generate_range(start=mn - td, end=mx + td, periods=nbins + 1, freq=None)
        else:
            mn -= 0.001 * abs(mn) if mn != 0 else 0.001
            mx += 0.001 * abs(mx) if mx != 0 else 0.001
            bins = np.linspace(mn, mx, nbins + 1, endpoint=True)
    else:
        if is_dt_or_td:
            bins = x_idx._values._generate_range(start=mn, end=mx, periods=nbins + 1, freq=None)
        else:
            bins = np.linspace(mn, mx, nbins + 1, endpoint=True)
        adj = (mx - mn) * 0.001
        if right:
            bins[0] -= adj
        else:
            bins[-1] += adj
    return Index(bins)","""""""If a user passed an integer N for bins, convert this to a sequence of N
equal(ish)-sized bins."""""""
pandas/core/reshape/tile.py,"def _coerce_to_type(x: Index) -> tuple[Index, DtypeObj | None]:
    dtype: DtypeObj | None = None
    if isinstance(x.dtype, DatetimeTZDtype):
        dtype = x.dtype
    elif lib.is_np_dtype(x.dtype, 'M'):
        x = to_datetime(x).astype('datetime64[ns]', copy=False)
        dtype = np.dtype('datetime64[ns]')
    elif lib.is_np_dtype(x.dtype, 'm'):
        x = to_timedelta(x)
        dtype = np.dtype('timedelta64[ns]')
    elif is_bool_dtype(x.dtype):
        x = x.astype(np.int64)
    elif isinstance(x.dtype, ExtensionDtype) and is_numeric_dtype(x.dtype):
        x_arr = x.to_numpy(dtype=np.float64, na_value=np.nan)
        x = Index(x_arr)
    return (Index(x), dtype)","""""""if the passed data is of datetime/timedelta, bool or nullable int type,
this method converts it to numeric so that cut or qcut method can
handle it"""""""
pandas/core/reshape/tile.py,"def _format_labels(bins: Index, precision: int, right: bool=True, include_lowest: bool=False):
    closed: IntervalLeftRight = 'right' if right else 'left'
    formatter: Callable[[Any], Timestamp] | Callable[[Any], Timedelta]
    if isinstance(bins.dtype, DatetimeTZDtype):
        formatter = lambda x: x
        adjust = lambda x: x - Timedelta('1ns')
    elif lib.is_np_dtype(bins.dtype, 'M'):
        formatter = lambda x: x
        adjust = lambda x: x - Timedelta('1ns')
    elif lib.is_np_dtype(bins.dtype, 'm'):
        formatter = lambda x: x
        adjust = lambda x: x - Timedelta('1ns')
    else:
        precision = _infer_precision(precision, bins)
        formatter = lambda x: _round_frac(x, precision)
        adjust = lambda x: x - 10 ** (-precision)
    breaks = [formatter(b) for b in bins]
    if right and include_lowest:
        breaks[0] = adjust(breaks[0])
    return IntervalIndex.from_breaks(breaks, closed=closed)","""""""based on the dtype, return our labels"""""""
pandas/core/reshape/tile.py,"def _preprocess_for_cut(x) -> Index:
    ndim = getattr(x, 'ndim', None)
    if ndim is None:
        x = np.asarray(x)
    if x.ndim != 1:
        raise ValueError('Input array must be 1 dimensional')
    return Index(x)","""""""handles preprocessing for cut where we convert passed
input to array, strip the index information and store it
separately"""""""
pandas/core/reshape/tile.py,"def _postprocess_for_cut(fac, bins, retbins: bool, original):
    if isinstance(original, ABCSeries):
        fac = original._constructor(fac, index=original.index, name=original.name)
    if not retbins:
        return fac
    if isinstance(bins, Index) and is_numeric_dtype(bins.dtype):
        bins = bins._values
    return (fac, bins)","""""""handles post processing for the cut method where
we combine the index information if the originally passed
datatype was a series"""""""
pandas/core/reshape/tile.py,"def _round_frac(x, precision: int):
    if not np.isfinite(x) or x == 0:
        return x
    else:
        (frac, whole) = np.modf(x)
        if whole == 0:
            digits = -int(np.floor(np.log10(abs(frac)))) - 1 + precision
        else:
            digits = precision
        return np.around(x, digits)","""""""Round the fractional part of the given number"""""""
pandas/core/reshape/tile.py,"def _infer_precision(base_precision: int, bins: Index) -> int:
    for precision in range(base_precision, 20):
        levels = np.asarray([_round_frac(b, precision) for b in bins])
        if algos.unique(levels).size == bins.size:
            return precision
    return base_precision","""""""Infer an appropriate precision for _round_frac"""""""
pandas/core/reshape/util.py,"def cartesian_product(X) -> list[np.ndarray]:
    msg = 'Input must be a list-like of list-likes'
    if not is_list_like(X):
        raise TypeError(msg)
    for x in X:
        if not is_list_like(x):
            raise TypeError(msg)
    if len(X) == 0:
        return []
    lenX = np.fromiter((len(x) for x in X), dtype=np.intp)
    cumprodX = np.cumprod(lenX)
    if np.any(cumprodX < 0):
        raise ValueError('Product space too large to allocate arrays!')
    a = np.roll(cumprodX, 1)
    a[0] = 1
    if cumprodX[-1] != 0:
        b = cumprodX[-1] / cumprodX
    else:
        b = np.zeros_like(cumprodX)
    return [tile_compat(np.repeat(x, b[i]), np.prod(a[i])) for (i, x) in enumerate(X)]","""""""Numpy version of itertools.product.
Sometimes faster (for large inputs)...

Parameters
----------
X : list-like of list-likes

Returns
-------
product : list of ndarrays

Examples
--------
>>> cartesian_product([list('ABC'), [1, 2]])
[array(['A', 'A', 'B', 'B', 'C', 'C'], dtype='<U1'), array([1, 2, 1, 2, 1, 2])]

See Also
--------
itertools.product : Cartesian product of input iterables.  Equivalent to
    nested for-loops."""""""
pandas/core/reshape/util.py,"def tile_compat(arr: NumpyIndexT, num: int) -> NumpyIndexT:
    if isinstance(arr, np.ndarray):
        return np.tile(arr, num)
    taker = np.tile(np.arange(len(arr)), num)
    return arr.take(taker)","""""""Index compat for np.tile.

Notes
-----
Does not support multi-dimensional `num`."""""""
pandas/core/sample.py,"def preprocess_weights(obj: NDFrame, weights, axis: AxisInt) -> np.ndarray:
    if isinstance(weights, ABCSeries):
        weights = weights.reindex(obj.axes[axis])
    if isinstance(weights, str):
        if isinstance(obj, ABCDataFrame):
            if axis == 0:
                try:
                    weights = obj[weights]
                except KeyError as err:
                    raise KeyError('String passed to weights not a valid column') from err
            else:
                raise ValueError('Strings can only be passed to weights when sampling from rows on a DataFrame')
        else:
            raise ValueError('Strings cannot be passed as weights when sampling from a Series.')
    if isinstance(obj, ABCSeries):
        func = obj._constructor
    else:
        func = obj._constructor_sliced
    weights = func(weights, dtype='float64')._values
    if len(weights) != obj.shape[axis]:
        raise ValueError('Weights and axis to be sampled must be of same length')
    if lib.has_infs(weights):
        raise ValueError('weight vector may not include `inf` values')
    if (weights < 0).any():
        raise ValueError('weight vector many not include negative values')
    missing = np.isnan(weights)
    if missing.any():
        weights = weights.copy()
        weights[missing] = 0
    return weights","""""""Process and validate the `weights` argument to `NDFrame.sample` and
`.GroupBy.sample`.

Returns `weights` as an ndarray[np.float64], validated except for normalizing
weights (because that must be done groupwise in groupby sampling)."""""""
pandas/core/sample.py,"def process_sampling_size(n: int | None, frac: float | None, replace: bool) -> int | None:
    if n is None and frac is None:
        n = 1
    elif n is not None and frac is not None:
        raise ValueError('Please enter a value for `frac` OR `n`, not both')
    elif n is not None:
        if n < 0:
            raise ValueError('A negative number of rows requested. Please provide `n` >= 0.')
        if n % 1 != 0:
            raise ValueError('Only integers accepted as `n` values')
    else:
        assert frac is not None
        if frac > 1 and (not replace):
            raise ValueError('Replace has to be set to `True` when upsampling the population `frac` > 1.')
        if frac < 0:
            raise ValueError('A negative number of rows requested. Please provide `frac` >= 0.')
    return n","""""""Process and validate the `n` and `frac` arguments to `NDFrame.sample` and
`.GroupBy.sample`.

Returns None if `frac` should be used (variable sampling sizes), otherwise returns
the constant sampling size."""""""
pandas/core/sample.py,"def sample(obj_len: int, size: int, replace: bool, weights: np.ndarray | None, random_state: np.random.RandomState | np.random.Generator) -> np.ndarray:
    if weights is not None:
        weight_sum = weights.sum()
        if weight_sum != 0:
            weights = weights / weight_sum
        else:
            raise ValueError('Invalid weights: weights sum to zero')
    return random_state.choice(obj_len, size=size, replace=replace, p=weights).astype(np.intp, copy=False)","""""""Randomly sample `size` indices in `np.arange(obj_len)`

Parameters
----------
obj_len : int
    The length of the indices being considered
size : int
    The number of values to choose
replace : bool
    Allow or disallow sampling of the same row more than once.
weights : np.ndarray[np.float64] or None
    If None, equal probability weighting, otherwise weights according
    to the vector normalized
random_state: np.random.RandomState or np.random.Generator
    State used for the random sampling

Returns
-------
np.ndarray[np.intp]"""""""
pandas/core/series.py,"def _coerce_method(converter):

    def wrapper(self):
        if len(self) == 1:
            warnings.warn(f'Calling {converter.__name__} on a single element Series is deprecated and will raise a TypeError in the future. Use {converter.__name__}(ser.iloc[0]) instead', FutureWarning, stacklevel=find_stack_level())
            return converter(self.iloc[0])
        raise TypeError(f'cannot convert the series to {converter}')
    wrapper.__name__ = f'__{converter.__name__}__'
    return wrapper","""""""Install the scalar coercion methods."""""""
pandas/core/sorting.py,"def get_indexer_indexer(target: Index, level: Level | list[Level] | None, ascending: list[bool] | bool, kind: SortKind, na_position: NaPosition, sort_remaining: bool, key: IndexKeyFunc) -> npt.NDArray[np.intp] | None:
    target = ensure_key_mapped(target, key, levels=level)
    target = target._sort_levels_monotonic()
    if level is not None:
        (_, indexer) = target.sortlevel(level, ascending=ascending, sort_remaining=sort_remaining, na_position=na_position)
    elif ascending and target.is_monotonic_increasing or (not ascending and target.is_monotonic_decreasing):
        return None
    elif isinstance(target, ABCMultiIndex):
        codes = [lev.codes for lev in target._get_codes_for_sorting()]
        indexer = lexsort_indexer(codes, orders=ascending, na_position=na_position, codes_given=True)
    else:
        indexer = nargsort(target, kind=kind, ascending=cast(bool, ascending), na_position=na_position)
    return indexer","""""""Helper method that return the indexer according to input parameters for
the sort_index method of DataFrame and Series.

Parameters
----------
target : Index
level : int or level name or list of ints or list of level names
ascending : bool or list of bools, default True
kind : {'quicksort', 'mergesort', 'heapsort', 'stable'}
na_position : {'first', 'last'}
sort_remaining : bool
key : callable, optional

Returns
-------
Optional[ndarray[intp]]
    The indexer for the new index."""""""
pandas/core/sorting.py,"def get_group_index(labels, shape: Shape, sort: bool, xnull: bool) -> npt.NDArray[np.int64]:

    def _int64_cut_off(shape) -> int:
        acc = 1
        for (i, mul) in enumerate(shape):
            acc *= int(mul)
            if not acc < lib.i8max:
                return i
        return len(shape)

    def maybe_lift(lab, size: int) -> tuple[np.ndarray, int]:
        return (lab + 1, size + 1) if (lab == -1).any() else (lab, size)
    labels = [ensure_int64(x) for x in labels]
    lshape = list(shape)
    if not xnull:
        for (i, (lab, size)) in enumerate(zip(labels, shape)):
            (labels[i], lshape[i]) = maybe_lift(lab, size)
    labels = list(labels)
    while True:
        nlev = _int64_cut_off(lshape)
        stride = np.prod(lshape[1:nlev], dtype='i8')
        out = stride * labels[0].astype('i8', subok=False, copy=False)
        for i in range(1, nlev):
            if lshape[i] == 0:
                stride = np.int64(0)
            else:
                stride //= lshape[i]
            out += labels[i] * stride
        if xnull:
            mask = labels[0] == -1
            for lab in labels[1:nlev]:
                mask |= lab == -1
            out[mask] = -1
        if nlev == len(lshape):
            break
        (comp_ids, obs_ids) = compress_group_index(out, sort=sort)
        labels = [comp_ids] + labels[nlev:]
        lshape = [len(obs_ids)] + lshape[nlev:]
    return out","""""""For the particular label_list, gets the offsets into the hypothetical list
representing the totally ordered cartesian product of all possible label
combinations, *as long as* this space fits within int64 bounds;
otherwise, though group indices identify unique combinations of
labels, they cannot be deconstructed.
- If `sort`, rank of returned ids preserve lexical ranks of labels.
  i.e. returned id's can be used to do lexical sort on labels;
- If `xnull` nulls (-1 labels) are passed through.

Parameters
----------
labels : sequence of arrays
    Integers identifying levels at each location
shape : tuple[int, ...]
    Number of unique levels at each location
sort : bool
    If the ranks of returned ids should match lexical ranks of labels
xnull : bool
    If true nulls are excluded. i.e. -1 values in the labels are
    passed through.

Returns
-------
An array of type int64 where two elements are equal if their corresponding
labels are equal at all location.

Notes
-----
The length of `labels` and `shape` must be identical."""""""
pandas/core/sorting.py,"def get_compressed_ids(labels, sizes: Shape) -> tuple[npt.NDArray[np.intp], npt.NDArray[np.int64]]:
    ids = get_group_index(labels, sizes, sort=True, xnull=False)
    return compress_group_index(ids, sort=True)","""""""Group_index is offsets into cartesian product of all possible labels. This
space can be huge, so this function compresses it, by computing offsets
(comp_ids) into the list of unique labels (obs_group_ids).

Parameters
----------
labels : list of label arrays
sizes : tuple[int] of size of the levels

Returns
-------
np.ndarray[np.intp]
    comp_ids
np.ndarray[np.int64]
    obs_group_ids"""""""
pandas/core/sorting.py,"def decons_obs_group_ids(comp_ids: npt.NDArray[np.intp], obs_ids: npt.NDArray[np.intp], shape: Shape, labels: Sequence[npt.NDArray[np.signedinteger]], xnull: bool) -> list[npt.NDArray[np.intp]]:
    if not xnull:
        lift = np.fromiter(((a == -1).any() for a in labels), dtype=np.intp)
        arr_shape = np.asarray(shape, dtype=np.intp) + lift
        shape = tuple(arr_shape)
    if not is_int64_overflow_possible(shape):
        out = _decons_group_index(obs_ids, shape)
        return out if xnull or not lift.any() else [x - y for (x, y) in zip(out, lift)]
    indexer = unique_label_indices(comp_ids)
    return [lab[indexer].astype(np.intp, subok=False, copy=True) for lab in labels]","""""""Reconstruct labels from observed group ids.

Parameters
----------
comp_ids : np.ndarray[np.intp]
obs_ids: np.ndarray[np.intp]
shape : tuple[int]
labels : Sequence[np.ndarray[np.signedinteger]]
xnull : bool
    If nulls are excluded; i.e. -1 labels are passed through."""""""
pandas/core/sorting.py,"def lexsort_indexer(keys: Sequence[ArrayLike | Index | Series], orders=None, na_position: str='last', key: Callable | None=None, codes_given: bool=False) -> npt.NDArray[np.intp]:
    from pandas.core.arrays import Categorical
    if na_position not in ['last', 'first']:
        raise ValueError(f'invalid na_position: {na_position}')
    if isinstance(orders, bool):
        orders = [orders] * len(keys)
    elif orders is None:
        orders = [True] * len(keys)
    labels = []
    for (k, order) in zip(keys, orders):
        k = ensure_key_mapped(k, key)
        if codes_given:
            codes = cast(np.ndarray, k)
            n = codes.max() + 1 if len(codes) else 0
        else:
            cat = Categorical(k, ordered=True)
            codes = cat.codes
            n = len(cat.categories)
        mask = codes == -1
        if na_position == 'last' and mask.any():
            codes = np.where(mask, n, codes)
        if not order:
            codes = np.where(mask, codes, n - codes - 1)
        labels.append(codes)
    return np.lexsort(labels[::-1])","""""""Performs lexical sorting on a set of keys

Parameters
----------
keys : Sequence[ArrayLike | Index | Series]
    Sequence of arrays to be sorted by the indexer
    Sequence[Series] is only if key is not None.
orders : bool or list of booleans, optional
    Determines the sorting order for each element in keys. If a list,
    it must be the same length as keys. This determines whether the
    corresponding element in keys should be sorted in ascending
    (True) or descending (False) order. if bool, applied to all
    elements as above. if None, defaults to True.
na_position : {'first', 'last'}, default 'last'
    Determines placement of NA elements in the sorted list (""last"" or ""first"")
key : Callable, optional
    Callable key function applied to every element in keys before sorting
codes_given: bool, False
    Avoid categorical materialization if codes are already provided.

Returns
-------
np.ndarray[np.intp]"""""""
pandas/core/sorting.py,"def nargsort(items: ArrayLike | Index | Series, kind: SortKind='quicksort', ascending: bool=True, na_position: str='last', key: Callable | None=None, mask: npt.NDArray[np.bool_] | None=None) -> npt.NDArray[np.intp]:
    if key is not None:
        items = ensure_key_mapped(items, key)
        return nargsort(items, kind=kind, ascending=ascending, na_position=na_position, key=None, mask=mask)
    if isinstance(items, ABCRangeIndex):
        return items.argsort(ascending=ascending)
    elif not isinstance(items, ABCMultiIndex):
        items = extract_array(items)
    else:
        raise TypeError('nargsort does not support MultiIndex. Use index.sort_values instead.')
    if mask is None:
        mask = np.asarray(isna(items))
    if not isinstance(items, np.ndarray):
        return items.argsort(ascending=ascending, kind=kind, na_position=na_position)
    idx = np.arange(len(items))
    non_nans = items[~mask]
    non_nan_idx = idx[~mask]
    nan_idx = np.nonzero(mask)[0]
    if not ascending:
        non_nans = non_nans[::-1]
        non_nan_idx = non_nan_idx[::-1]
    indexer = non_nan_idx[non_nans.argsort(kind=kind)]
    if not ascending:
        indexer = indexer[::-1]
    if na_position == 'last':
        indexer = np.concatenate([indexer, nan_idx])
    elif na_position == 'first':
        indexer = np.concatenate([nan_idx, indexer])
    else:
        raise ValueError(f'invalid na_position: {na_position}')
    return ensure_platform_int(indexer)","""""""Intended to be a drop-in replacement for np.argsort which handles NaNs.

Adds ascending, na_position, and key parameters.

(GH #6399, #5231, #27237)

Parameters
----------
items : np.ndarray, ExtensionArray, Index, or Series
kind : {'quicksort', 'mergesort', 'heapsort', 'stable'}, default 'quicksort'
ascending : bool, default True
na_position : {'first', 'last'}, default 'last'
key : Optional[Callable], default None
mask : Optional[np.ndarray[bool]], default None
    Passed when called by ExtensionArray.argsort.

Returns
-------
np.ndarray[np.intp]"""""""
pandas/core/sorting.py,"def nargminmax(values: ExtensionArray, method: str, axis: AxisInt=0):
    assert method in {'argmax', 'argmin'}
    func = np.argmax if method == 'argmax' else np.argmin
    mask = np.asarray(isna(values))
    arr_values = values._values_for_argsort()
    if arr_values.ndim > 1:
        if mask.any():
            if axis == 1:
                zipped = zip(arr_values, mask)
            else:
                zipped = zip(arr_values.T, mask.T)
            return np.array([_nanargminmax(v, m, func) for (v, m) in zipped])
        return func(arr_values, axis=axis)
    return _nanargminmax(arr_values, mask, func)","""""""Implementation of np.argmin/argmax but for ExtensionArray and which
handles missing values.

Parameters
----------
values : ExtensionArray
method : {""argmax"", ""argmin""}
axis : int, default 0

Returns
-------
int"""""""
pandas/core/sorting.py,"def _nanargminmax(values: np.ndarray, mask: npt.NDArray[np.bool_], func) -> int:
    idx = np.arange(values.shape[0])
    non_nans = values[~mask]
    non_nan_idx = idx[~mask]
    return non_nan_idx[func(non_nans)]","""""""See nanargminmax.__doc__."""""""
pandas/core/sorting.py,"def _ensure_key_mapped_multiindex(index: MultiIndex, key: Callable, level=None) -> MultiIndex:
    if level is not None:
        if isinstance(level, (str, int)):
            sort_levels = [level]
        else:
            sort_levels = level
        sort_levels = [index._get_level_number(lev) for lev in sort_levels]
    else:
        sort_levels = list(range(index.nlevels))
    mapped = [ensure_key_mapped(index._get_level_values(level), key) if level in sort_levels else index._get_level_values(level) for level in range(index.nlevels)]
    return type(index).from_arrays(mapped)","""""""Returns a new MultiIndex in which key has been applied
to all levels specified in level (or all levels if level
is None). Used for key sorting for MultiIndex.

Parameters
----------
index : MultiIndex
    Index to which to apply the key function on the
    specified levels.
key : Callable
    Function that takes an Index and returns an Index of
    the same shape. This key is applied to each level
    separately. The name of the level can be used to
    distinguish different levels for application.
level : list-like, int or str, default None
    Level or list of levels to apply the key function to.
    If None, key function is applied to all levels. Other
    levels are left unchanged.

Returns
-------
labels : MultiIndex
    Resulting MultiIndex with modified levels."""""""
pandas/core/sorting.py,"def ensure_key_mapped(values: ArrayLike | Index | Series, key: Callable | None, levels=None) -> ArrayLike | Index | Series:
    from pandas.core.indexes.api import Index
    if not key:
        return values
    if isinstance(values, ABCMultiIndex):
        return _ensure_key_mapped_multiindex(values, key, level=levels)
    result = key(values.copy())
    if len(result) != len(values):
        raise ValueError('User-provided `key` function must not change the shape of the array.')
    try:
        if isinstance(values, Index):
            result = Index(result)
        else:
            type_of_values = type(values)
            result = type_of_values(result)
    except TypeError:
        raise TypeError(f'User-provided `key` function returned an invalid type {type(result)}             which could not be converted to {type(values)}.')
    return result","""""""Applies a callable key function to the values function and checks
that the resulting value has the same shape. Can be called on Index
subclasses, Series, DataFrames, or ndarrays.

Parameters
----------
values : Series, DataFrame, Index subclass, or ndarray
key : Optional[Callable], key to be called on the values array
levels : Optional[List], if values is a MultiIndex, list of levels to
apply the key to."""""""
pandas/core/sorting.py,"def get_flattened_list(comp_ids: npt.NDArray[np.intp], ngroups: int, levels: Iterable[Index], labels: Iterable[np.ndarray]) -> list[tuple]:
    comp_ids = comp_ids.astype(np.int64, copy=False)
    arrays: DefaultDict[int, list[int]] = defaultdict(list)
    for (labs, level) in zip(labels, levels):
        table = hashtable.Int64HashTable(ngroups)
        table.map_keys_to_values(comp_ids, labs.astype(np.int64, copy=False))
        for i in range(ngroups):
            arrays[i].append(level[table.get_item(i)])
    return [tuple(array) for array in arrays.values()]","""""""Map compressed group id -> key tuple."""""""
pandas/core/sorting.py,"def get_indexer_dict(label_list: list[np.ndarray], keys: list[Index]) -> dict[Hashable, npt.NDArray[np.intp]]:
    shape = tuple((len(x) for x in keys))
    group_index = get_group_index(label_list, shape, sort=True, xnull=True)
    if np.all(group_index == -1):
        return {}
    ngroups = (group_index.size and group_index.max()) + 1 if is_int64_overflow_possible(shape) else np.prod(shape, dtype='i8')
    sorter = get_group_index_sorter(group_index, ngroups)
    sorted_labels = [lab.take(sorter) for lab in label_list]
    group_index = group_index.take(sorter)
    return lib.indices_fast(sorter, group_index, keys, sorted_labels)","""""""Returns
-------
dict:
    Labels mapped to indexers."""""""
pandas/core/sorting.py,"def get_group_index_sorter(group_index: npt.NDArray[np.intp], ngroups: int | None=None) -> npt.NDArray[np.intp]:
    if ngroups is None:
        ngroups = 1 + group_index.max()
    count = len(group_index)
    alpha = 0.0
    beta = 1.0
    do_groupsort = count > 0 and alpha + beta * ngroups < count * np.log(count)
    if do_groupsort:
        (sorter, _) = algos.groupsort_indexer(ensure_platform_int(group_index), ngroups)
    else:
        sorter = group_index.argsort(kind='mergesort')
    return ensure_platform_int(sorter)","""""""algos.groupsort_indexer implements `counting sort` and it is at least
O(ngroups), where
    ngroups = prod(shape)
    shape = map(len, keys)
that is, linear in the number of combinations (cartesian product) of unique
values of groupby keys. This can be huge when doing multi-key groupby.
np.argsort(kind='mergesort') is O(count x log(count)) where count is the
length of the data-frame;
Both algorithms are `stable` sort and that is necessary for correctness of
groupby operations. e.g. consider:
    df.groupby(key)[col].transform('first')

Parameters
----------
group_index : np.ndarray[np.intp]
    signed integer dtype
ngroups : int or None, default None

Returns
-------
np.ndarray[np.intp]"""""""
pandas/core/sorting.py,"def compress_group_index(group_index: npt.NDArray[np.int64], sort: bool=True) -> tuple[npt.NDArray[np.int64], npt.NDArray[np.int64]]:
    if len(group_index) and np.all(group_index[1:] >= group_index[:-1]):
        unique_mask = np.concatenate([group_index[:1] > -1, group_index[1:] != group_index[:-1]])
        comp_ids = unique_mask.cumsum()
        comp_ids -= 1
        obs_group_ids = group_index[unique_mask]
    else:
        size_hint = len(group_index)
        table = hashtable.Int64HashTable(size_hint)
        group_index = ensure_int64(group_index)
        (comp_ids, obs_group_ids) = table.get_labels_groupby(group_index)
        if sort and len(obs_group_ids) > 0:
            (obs_group_ids, comp_ids) = _reorder_by_uniques(obs_group_ids, comp_ids)
    return (ensure_int64(comp_ids), ensure_int64(obs_group_ids))","""""""Group_index is offsets into cartesian product of all possible labels. This
space can be huge, so this function compresses it, by computing offsets
(comp_ids) into the list of unique labels (obs_group_ids)."""""""
pandas/core/sorting.py,"def _reorder_by_uniques(uniques: npt.NDArray[np.int64], labels: npt.NDArray[np.intp]) -> tuple[npt.NDArray[np.int64], npt.NDArray[np.intp]]:
    sorter = uniques.argsort()
    reverse_indexer = np.empty(len(sorter), dtype=np.intp)
    reverse_indexer.put(sorter, np.arange(len(sorter)))
    mask = labels < 0
    labels = reverse_indexer.take(labels)
    np.putmask(labels, mask, -1)
    uniques = uniques.take(sorter)
    return (uniques, labels)","""""""Parameters
----------
uniques : np.ndarray[np.int64]
labels : np.ndarray[np.intp]

Returns
-------
np.ndarray[np.int64]
np.ndarray[np.intp]"""""""
pandas/core/strings/accessor.py,"def forbid_nonstring_types(forbidden: list[str] | None, name: str | None=None) -> Callable[[F], F]:
    forbidden = [] if forbidden is None else forbidden
    allowed_types = {'string', 'empty', 'bytes', 'mixed', 'mixed-integer'} - set(forbidden)

    def _forbid_nonstring_types(func: F) -> F:
        func_name = func.__name__ if name is None else name

        @wraps(func)
        def wrapper(self, *args, **kwargs):
            if self._inferred_dtype not in allowed_types:
                msg = f""Cannot use .str.{func_name} with values of inferred dtype '{self._inferred_dtype}'.""
                raise TypeError(msg)
            return func(self, *args, **kwargs)
        wrapper.__name__ = func_name
        return cast(F, wrapper)
    return _forbid_nonstring_types","""""""Decorator to forbid specific types for a method of StringMethods.

For calling `.str.{method}` on a Series or Index, it is necessary to first
initialize the :class:`StringMethods` object, and then call the method.
However, different methods allow different input types, and so this can not
be checked during :meth:`StringMethods.__init__`, but must be done on a
per-method basis. This decorator exists to facilitate this process, and
make it explicit which (inferred) types are disallowed by the method.

:meth:`StringMethods.__init__` allows the *union* of types its different
methods allow (after skipping NaNs; see :meth:`StringMethods._validate`),
namely: ['string', 'empty', 'bytes', 'mixed', 'mixed-integer'].

The default string types ['string', 'empty'] are allowed for all methods.
For the additional types ['bytes', 'mixed', 'mixed-integer'], each method
then needs to forbid the types it is not intended for.

Parameters
----------
forbidden : list-of-str or None
    List of forbidden non-string types, may be one or more of
    `['bytes', 'mixed', 'mixed-integer']`.
name : str, default None
    Name of the method to use in the error message. By default, this is
    None, in which case the name from the method being wrapped will be
    copied. However, for working with further wrappers (like _pat_wrapper
    and _noarg_wrapper), it is necessary to specify the name.

Returns
-------
func : wrapper
    The method to which the decorator is applied, with an added check that
    enforces the inferred type to not be in the list of forbidden types.

Raises
------
TypeError
    If the inferred type of the underlying data is in `forbidden`."""""""
pandas/core/strings/accessor.py,"def cat_safe(list_of_columns: list[npt.NDArray[np.object_]], sep: str):
    try:
        result = cat_core(list_of_columns, sep)
    except TypeError:
        for column in list_of_columns:
            dtype = lib.infer_dtype(column, skipna=True)
            if dtype not in ['string', 'empty']:
                raise TypeError(f'Concatenation requires list-likes containing only strings (or missing values). Offending values found in column {dtype}') from None
    return result","""""""Auxiliary function for :meth:`str.cat`.

Same signature as cat_core, but handles TypeErrors in concatenation, which
happen if the arrays in list_of columns have the wrong dtypes or content.

Parameters
----------
list_of_columns : list of numpy arrays
    List of arrays to be concatenated with sep;
    these arrays may not contain NaNs!
sep : string
    The separator string for concatenating the columns.

Returns
-------
nd.array
    The concatenation of list_of_columns with sep."""""""
pandas/core/strings/accessor.py,"def cat_core(list_of_columns: list, sep: str):
    if sep == '':
        arr_of_cols = np.asarray(list_of_columns, dtype=object)
        return np.sum(arr_of_cols, axis=0)
    list_with_sep = [sep] * (2 * len(list_of_columns) - 1)
    list_with_sep[::2] = list_of_columns
    arr_with_sep = np.asarray(list_with_sep, dtype=object)
    return np.sum(arr_with_sep, axis=0)","""""""Auxiliary function for :meth:`str.cat`

Parameters
----------
list_of_columns : list of numpy arrays
    List of arrays to be concatenated with sep;
    these arrays may not contain NaNs!
sep : string
    The separator string for concatenating the columns.

Returns
-------
nd.array
    The concatenation of list_of_columns with sep."""""""
pandas/core/strings/accessor.py,"def _get_group_names(regex: re.Pattern) -> list[Hashable]:
    names = {v: k for (k, v) in regex.groupindex.items()}
    return [names.get(1 + i, i) for i in range(regex.groups)]","""""""Get named groups from compiled regex.

Unnamed groups are numbered.

Parameters
----------
regex : compiled regex

Returns
-------
list of column labels"""""""
pandas/core/tools/datetimes.py,"def should_cache(arg: ArrayConvertible, unique_share: float=0.7, check_count: int | None=None) -> bool:
    do_caching = True
    if check_count is None:
        if len(arg) <= start_caching_at:
            return False
        if len(arg) <= 5000:
            check_count = len(arg) // 10
        else:
            check_count = 500
    else:
        assert 0 <= check_count <= len(arg), 'check_count must be in next bounds: [0; len(arg)]'
        if check_count == 0:
            return False
    assert 0 < unique_share < 1, 'unique_share must be in next bounds: (0; 1)'
    try:
        unique_elements = set(islice(arg, check_count))
    except TypeError:
        return False
    if len(unique_elements) > check_count * unique_share:
        do_caching = False
    return do_caching","""""""Decides whether to do caching.

If the percent of unique elements among `check_count` elements less
than `unique_share * 100` then we can do caching.

Parameters
----------
arg: listlike, tuple, 1-d array, Series
unique_share: float, default=0.7, optional
    0 < unique_share < 1
check_count: int, optional
    0 <= check_count <= len(arg)

Returns
-------
do_caching: bool

Notes
-----
By default for a sequence of less than 50 items in size, we don't do
caching; for the number of elements less than 5000, we take ten percent of
all elements to check for a uniqueness share; if the sequence size is more
than 5000, then we check only the first 500 elements.
All constants were chosen empirically by."""""""
pandas/core/tools/datetimes.py,"def _maybe_cache(arg: ArrayConvertible, format: str | None, cache: bool, convert_listlike: Callable) -> Series:
    from pandas import Series
    cache_array = Series(dtype=object)
    if cache:
        if not should_cache(arg):
            return cache_array
        if not isinstance(arg, (np.ndarray, ExtensionArray, Index, ABCSeries)):
            arg = np.array(arg)
        unique_dates = unique(arg)
        if len(unique_dates) < len(arg):
            cache_dates = convert_listlike(unique_dates, format)
            try:
                cache_array = Series(cache_dates, index=unique_dates, copy=False)
            except OutOfBoundsDatetime:
                return cache_array
            if not cache_array.index.is_unique:
                cache_array = cache_array[~cache_array.index.duplicated()]
    return cache_array","""""""Create a cache of unique dates from an array of dates

Parameters
----------
arg : listlike, tuple, 1-d array, Series
format : string
    Strftime format to parse time
cache : bool
    True attempts to create a cache of converted values
convert_listlike : function
    Conversion function to apply on dates

Returns
-------
cache_array : Series
    Cache of converted, unique dates. Can be empty"""""""
pandas/core/tools/datetimes.py,"def _box_as_indexlike(dt_array: ArrayLike, utc: bool=False, name: Hashable | None=None) -> Index:
    if lib.is_np_dtype(dt_array.dtype, 'M'):
        tz = 'utc' if utc else None
        return DatetimeIndex(dt_array, tz=tz, name=name)
    return Index(dt_array, name=name, dtype=dt_array.dtype)","""""""Properly boxes the ndarray of datetimes to DatetimeIndex
if it is possible or to generic Index instead

Parameters
----------
dt_array: 1-d array
    Array of datetimes to be wrapped in an Index.
utc : bool
    Whether to convert/localize timestamps to UTC.
name : string, default None
    Name for a resulting index

Returns
-------
result : datetime of converted dates
    - DatetimeIndex if convertible to sole datetime64 type
    - general Index otherwise"""""""
pandas/core/tools/datetimes.py,"def _convert_and_box_cache(arg: DatetimeScalarOrArrayConvertible, cache_array: Series, name: Hashable | None=None) -> Index:
    from pandas import Series
    result = Series(arg, dtype=cache_array.index.dtype).map(cache_array)
    return _box_as_indexlike(result._values, utc=False, name=name)","""""""Convert array of dates with a cache and wrap the result in an Index.

Parameters
----------
arg : integer, float, string, datetime, list, tuple, 1-d array, Series
cache_array : Series
    Cache of converted, unique dates
name : string, default None
    Name for a DatetimeIndex

Returns
-------
result : Index-like of converted dates"""""""
pandas/core/tools/datetimes.py,"def _return_parsed_timezone_results(result: np.ndarray, timezones, utc: bool, name: str) -> Index:
    tz_results = np.empty(len(result), dtype=object)
    non_na_timezones = set()
    for zone in unique(timezones):
        mask = timezones == zone
        dta = DatetimeArray(result[mask]).tz_localize(zone)
        if utc:
            if dta.tzinfo is None:
                dta = dta.tz_localize('utc')
            else:
                dta = dta.tz_convert('utc')
        elif not dta.isna().all():
            non_na_timezones.add(zone)
        tz_results[mask] = dta
    if len(non_na_timezones) > 1:
        warnings.warn('In a future version of pandas, parsing datetimes with mixed time zones will raise a warning unless `utc=True`. Please specify `utc=True` to opt in to the new behaviour and silence this warning. To create a `Series` with mixed offsets and `object` dtype, please use `apply` and `datetime.datetime.strptime`', FutureWarning, stacklevel=find_stack_level())
    return Index(tz_results, name=name)","""""""Return results from array_strptime if a %z or %Z directive was passed.

Parameters
----------
result : ndarray[int64]
    int64 date representations of the dates
timezones : ndarray
    pytz timezone objects
utc : bool
    Whether to convert/localize timestamps to UTC.
name : string, default None
    Name for a DatetimeIndex

Returns
-------
tz_result : Index-like of parsed dates with timezone"""""""
pandas/core/tools/datetimes.py,"def _convert_listlike_datetimes(arg, format: str | None, name: Hashable | None=None, utc: bool=False, unit: str | None=None, errors: DateTimeErrorChoices='raise', dayfirst: bool | None=None, yearfirst: bool | None=None, exact: bool=True):
    if isinstance(arg, (list, tuple)):
        arg = np.array(arg, dtype='O')
    elif isinstance(arg, NumpyExtensionArray):
        arg = np.array(arg)
    arg_dtype = getattr(arg, 'dtype', None)
    tz = 'utc' if utc else None
    if isinstance(arg_dtype, DatetimeTZDtype):
        if not isinstance(arg, (DatetimeArray, DatetimeIndex)):
            return DatetimeIndex(arg, tz=tz, name=name)
        if utc:
            arg = arg.tz_convert(None).tz_localize('utc')
        return arg
    elif isinstance(arg_dtype, ArrowDtype) and arg_dtype.type is Timestamp:
        if utc:
            if isinstance(arg, Index):
                arg_array = cast(ArrowExtensionArray, arg.array)
                if arg_dtype.pyarrow_dtype.tz is not None:
                    arg_array = arg_array._dt_tz_convert('UTC')
                else:
                    arg_array = arg_array._dt_tz_localize('UTC')
                arg = Index(arg_array)
            elif arg_dtype.pyarrow_dtype.tz is not None:
                arg = arg._dt_tz_convert('UTC')
            else:
                arg = arg._dt_tz_localize('UTC')
        return arg
    elif lib.is_np_dtype(arg_dtype, 'M'):
        if not is_supported_unit(get_unit_from_dtype(arg_dtype)):
            arg = astype_overflowsafe(np.asarray(arg), np.dtype('M8[s]'), is_coerce=errors == 'coerce')
        if not isinstance(arg, (DatetimeArray, DatetimeIndex)):
            return DatetimeIndex(arg, tz=tz, name=name)
        elif utc:
            return arg.tz_localize('utc')
        return arg
    elif unit is not None:
        if format is not None:
            raise ValueError('cannot specify both format and unit')
        return _to_datetime_with_unit(arg, unit, name, utc, errors)
    elif getattr(arg, 'ndim', 1) > 1:
        raise TypeError('arg must be a string, datetime, list, tuple, 1-d array, or Series')
    try:
        (arg, _) = maybe_convert_dtype(arg, copy=False, tz=libtimezones.maybe_get_tz(tz))
    except TypeError:
        if errors == 'coerce':
            npvalues = np.array(['NaT'], dtype='datetime64[ns]').repeat(len(arg))
            return DatetimeIndex(npvalues, name=name)
        elif errors == 'ignore':
            idx = Index(arg, name=name)
            return idx
        raise
    arg = ensure_object(arg)
    if format is None:
        format = _guess_datetime_format_for_array(arg, dayfirst=dayfirst)
    if format is not None and format != 'mixed':
        return _array_strptime_with_fallback(arg, name, utc, format, exact, errors)
    (result, tz_parsed) = objects_to_datetime64ns(arg, dayfirst=dayfirst, yearfirst=yearfirst, utc=utc, errors=errors, allow_object=True)
    if tz_parsed is not None:
        dta = DatetimeArray(result, dtype=tz_to_dtype(tz_parsed))
        return DatetimeIndex._simple_new(dta, name=name)
    return _box_as_indexlike(result, utc=utc, name=name)","""""""Helper function for to_datetime. Performs the conversions of 1D listlike
of dates

Parameters
----------
arg : list, tuple, ndarray, Series, Index
    date to be parsed
name : object
    None or string for the Index name
utc : bool
    Whether to convert/localize timestamps to UTC.
unit : str
    None or string of the frequency of the passed data
errors : str
    error handing behaviors from to_datetime, 'raise', 'coerce', 'ignore'
dayfirst : bool
    dayfirst parsing behavior from to_datetime
yearfirst : bool
    yearfirst parsing behavior from to_datetime
exact : bool, default True
    exact format matching behavior from to_datetime

Returns
-------
Index-like of parsed dates"""""""
pandas/core/tools/datetimes.py,"def _array_strptime_with_fallback(arg, name, utc: bool, fmt: str, exact: bool, errors: str) -> Index:
    (result, timezones) = array_strptime(arg, fmt, exact=exact, errors=errors, utc=utc)
    if any((tz is not None for tz in timezones)):
        return _return_parsed_timezone_results(result, timezones, utc, name)
    return _box_as_indexlike(result, utc=utc, name=name)","""""""Call array_strptime, with fallback behavior depending on 'errors'."""""""
pandas/core/tools/datetimes.py,"def _to_datetime_with_unit(arg, unit, name, utc: bool, errors: str) -> Index:
    arg = extract_array(arg, extract_numpy=True)
    if isinstance(arg, IntegerArray):
        arr = arg.astype(f'datetime64[{unit}]')
        tz_parsed = None
    else:
        arg = np.asarray(arg)
        if arg.dtype.kind in 'iu':
            arr = arg.astype(f'datetime64[{unit}]', copy=False)
            try:
                arr = astype_overflowsafe(arr, np.dtype('M8[ns]'), copy=False)
            except OutOfBoundsDatetime:
                if errors == 'raise':
                    raise
                arg = arg.astype(object)
                return _to_datetime_with_unit(arg, unit, name, utc, errors)
            tz_parsed = None
        elif arg.dtype.kind == 'f':
            (mult, _) = precision_from_unit(unit)
            mask = np.isnan(arg) | (arg == iNaT)
            fvalues = (arg * mult).astype('f8', copy=False)
            fvalues[mask] = 0
            if (fvalues < Timestamp.min._value).any() or (fvalues > Timestamp.max._value).any():
                if errors != 'raise':
                    arg = arg.astype(object)
                    return _to_datetime_with_unit(arg, unit, name, utc, errors)
                raise OutOfBoundsDatetime(f""cannot convert input with unit '{unit}'"")
            arr = fvalues.astype('M8[ns]', copy=False)
            arr[mask] = np.datetime64('NaT', 'ns')
            tz_parsed = None
        else:
            arg = arg.astype(object, copy=False)
            (arr, tz_parsed) = tslib.array_with_unit_to_datetime(arg, unit, errors=errors)
    if errors == 'ignore':
        result = Index._with_infer(arr, name=name)
    else:
        result = DatetimeIndex(arr, name=name)
    if not isinstance(result, DatetimeIndex):
        return result
    result = result.tz_localize('UTC').tz_convert(tz_parsed)
    if utc:
        if result.tz is None:
            result = result.tz_localize('utc')
        else:
            result = result.tz_convert('utc')
    return result","""""""to_datetime specalized to the case where a 'unit' is passed."""""""
pandas/core/tools/datetimes.py,"def _adjust_to_origin(arg, origin, unit):
    if origin == 'julian':
        original = arg
        j0 = Timestamp(0).to_julian_date()
        if unit != 'D':
            raise ValueError(""unit must be 'D' for origin='julian'"")
        try:
            arg = arg - j0
        except TypeError as err:
            raise ValueError(""incompatible 'arg' type for given 'origin'='julian'"") from err
        j_max = Timestamp.max.to_julian_date() - j0
        j_min = Timestamp.min.to_julian_date() - j0
        if np.any(arg > j_max) or np.any(arg < j_min):
            raise OutOfBoundsDatetime(f""{original} is Out of Bounds for origin='julian'"")
    else:
        if not ((is_integer(arg) or is_float(arg)) or is_numeric_dtype(np.asarray(arg))):
            raise ValueError(f""'{arg}' is not compatible with origin='{origin}'; it must be numeric with a unit specified"")
        try:
            offset = Timestamp(origin, unit=unit)
        except OutOfBoundsDatetime as err:
            raise OutOfBoundsDatetime(f'origin {origin} is Out of Bounds') from err
        except ValueError as err:
            raise ValueError(f'origin {origin} cannot be converted to a Timestamp') from err
        if offset.tz is not None:
            raise ValueError(f'origin offset {offset} must be tz-naive')
        td_offset = offset - Timestamp(0)
        ioffset = td_offset // Timedelta(1, unit=unit)
        if is_list_like(arg) and (not isinstance(arg, (ABCSeries, Index, np.ndarray))):
            arg = np.asarray(arg)
        arg = arg + ioffset
    return arg","""""""Helper function for to_datetime.
Adjust input argument to the specified origin

Parameters
----------
arg : list, tuple, ndarray, Series, Index
    date to be adjusted
origin : 'julian' or Timestamp
    origin offset for the arg
unit : str
    passed unit from to_datetime, must be 'D'

Returns
-------
ndarray or scalar of adjusted date(s)"""""""
pandas/core/tools/datetimes.py,"def to_datetime(arg: DatetimeScalarOrArrayConvertible | DictConvertible, errors: DateTimeErrorChoices='raise', dayfirst: bool=False, yearfirst: bool=False, utc: bool=False, format: str | None=None, exact: bool | lib.NoDefault=lib.no_default, unit: str | None=None, infer_datetime_format: lib.NoDefault | bool=lib.no_default, origin: str='unix', cache: bool=True) -> DatetimeIndex | Series | DatetimeScalar | NaTType | None:
    if exact is not lib.no_default and format in {'mixed', 'ISO8601'}:
        raise ValueError(""Cannot use 'exact' when 'format' is 'mixed' or 'ISO8601'"")
    if infer_datetime_format is not lib.no_default:
        warnings.warn(""The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument."", stacklevel=find_stack_level())
    if arg is None:
        return None
    if origin != 'unix':
        arg = _adjust_to_origin(arg, origin, unit)
    convert_listlike = partial(_convert_listlike_datetimes, utc=utc, unit=unit, dayfirst=dayfirst, yearfirst=yearfirst, errors=errors, exact=exact)
    result: Timestamp | NaTType | Series | Index
    if isinstance(arg, Timestamp):
        result = arg
        if utc:
            if arg.tz is not None:
                result = arg.tz_convert('utc')
            else:
                result = arg.tz_localize('utc')
    elif isinstance(arg, ABCSeries):
        cache_array = _maybe_cache(arg, format, cache, convert_listlike)
        if not cache_array.empty:
            result = arg.map(cache_array)
        else:
            values = convert_listlike(arg._values, format)
            result = arg._constructor(values, index=arg.index, name=arg.name)
    elif isinstance(arg, (ABCDataFrame, abc.MutableMapping)):
        result = _assemble_from_unit_mappings(arg, errors, utc)
    elif isinstance(arg, Index):
        cache_array = _maybe_cache(arg, format, cache, convert_listlike)
        if not cache_array.empty:
            result = _convert_and_box_cache(arg, cache_array, name=arg.name)
        else:
            result = convert_listlike(arg, format, name=arg.name)
    elif is_list_like(arg):
        try:
            argc = cast(Union[list, tuple, ExtensionArray, np.ndarray, 'Series', Index], arg)
            cache_array = _maybe_cache(argc, format, cache, convert_listlike)
        except OutOfBoundsDatetime:
            if errors == 'raise':
                raise
            from pandas import Series
            cache_array = Series([], dtype=object)
        if not cache_array.empty:
            result = _convert_and_box_cache(argc, cache_array)
        else:
            result = convert_listlike(argc, format)
    else:
        result = convert_listlike(np.array([arg]), format)[0]
        if isinstance(arg, bool) and isinstance(result, np.bool_):
            result = bool(result)
    return result","""""""Convert argument to datetime.

This function converts a scalar, array-like, :class:`Series` or
:class:`DataFrame`/dict-like to a pandas datetime object.

Parameters
----------
arg : int, float, str, datetime, list, tuple, 1-d array, Series, DataFrame/dict-like
    The object to convert to a datetime. If a :class:`DataFrame` is provided, the
    method expects minimally the following columns: :const:`""year""`,
    :const:`""month""`, :const:`""day""`. The column ""year""
    must be specified in 4-digit format.
errors : {'ignore', 'raise', 'coerce'}, default 'raise'
    - If :const:`'raise'`, then invalid parsing will raise an exception.
    - If :const:`'coerce'`, then invalid parsing will be set as :const:`NaT`.
    - If :const:`'ignore'`, then invalid parsing will return the input.
dayfirst : bool, default False
    Specify a date parse order if `arg` is str or is list-like.
    If :const:`True`, parses dates with the day first, e.g. :const:`""10/11/12""`
    is parsed as :const:`2012-11-10`.

    .. warning::

        ``dayfirst=True`` is not strict, but will prefer to parse
        with day first.

yearfirst : bool, default False
    Specify a date parse order if `arg` is str or is list-like.

    - If :const:`True` parses dates with the year first, e.g.
      :const:`""10/11/12""` is parsed as :const:`2010-11-12`.
    - If both `dayfirst` and `yearfirst` are :const:`True`, `yearfirst` is
      preceded (same as :mod:`dateutil`).

    .. warning::

        ``yearfirst=True`` is not strict, but will prefer to parse
        with year first.

utc : bool, default False
    Control timezone-related parsing, localization and conversion.

    - If :const:`True`, the function *always* returns a timezone-aware
      UTC-localized :class:`Timestamp`, :class:`Series` or
      :class:`DatetimeIndex`. To do this, timezone-naive inputs are
      *localized* as UTC, while timezone-aware inputs are *converted* to UTC.

    - If :const:`False` (default), inputs will not be coerced to UTC.
      Timezone-naive inputs will remain naive, while timezone-aware ones
      will keep their time offsets. Limitations exist for mixed
      offsets (typically, daylight savings), see :ref:`Examples
      <to_datetime_tz_examples>` section for details.

    .. warning::

        In a future version of pandas, parsing datetimes with mixed time
        zones will raise a warning unless `utc=True`.
        Please specify `utc=True` to opt in to the new behaviour
        and silence this warning. To create a `Series` with mixed offsets and
        `object` dtype, please use `apply` and `datetime.datetime.strptime`.

    See also: pandas general documentation about `timezone conversion and
    localization
    <https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html
    #time-zone-handling>`_.

format : str, default None
    The strftime to parse time, e.g. :const:`""%d/%m/%Y""`. See
    `strftime documentation
    <https://docs.python.org/3/library/datetime.html
    #strftime-and-strptime-behavior>`_ for more information on choices, though
    note that :const:`""%f""` will parse all the way up to nanoseconds.
    You can also pass:

    - ""ISO8601"", to parse any `ISO8601 <https://en.wikipedia.org/wiki/ISO_8601>`_
      time string (not necessarily in exactly the same format);
    - ""mixed"", to infer the format for each element individually. This is risky,
      and you should probably use it along with `dayfirst`.

    .. note::

        If a :class:`DataFrame` is passed, then `format` has no effect.

exact : bool, default True
    Control how `format` is used:

    - If :const:`True`, require an exact `format` match.
    - If :const:`False`, allow the `format` to match anywhere in the target
      string.

    Cannot be used alongside ``format='ISO8601'`` or ``format='mixed'``.
unit : str, default 'ns'
    The unit of the arg (D,s,ms,us,ns) denote the unit, which is an
    integer or float number. This will be based off the origin.
    Example, with ``unit='ms'`` and ``origin='unix'``, this would calculate
    the number of milliseconds to the unix epoch start.
infer_datetime_format : bool, default False
    If :const:`True` and no `format` is given, attempt to infer the format
    of the datetime strings based on the first non-NaN element,
    and if it can be inferred, switch to a faster method of parsing them.
    In some cases this can increase the parsing speed by ~5-10x.

    .. deprecated:: 2.0.0
        A strict version of this argument is now the default, passing it has
        no effect.

origin : scalar, default 'unix'
    Define the reference date. The numeric values would be parsed as number
    of units (defined by `unit`) since this reference date.

    - If :const:`'unix'` (or POSIX) time; origin is set to 1970-01-01.
    - If :const:`'julian'`, unit must be :const:`'D'`, and origin is set to
      beginning of Julian Calendar. Julian day number :const:`0` is assigned
      to the day starting at noon on January 1, 4713 BC.
    - If Timestamp convertible (Timestamp, dt.datetime, np.datetimt64 or date
      string), origin is set to Timestamp identified by origin.
    - If a float or integer, origin is the millisecond difference
      relative to 1970-01-01.
cache : bool, default True
    If :const:`True`, use a cache of unique, converted dates to apply the
    datetime conversion. May produce significant speed-up when parsing
    duplicate date strings, especially ones with timezone offsets. The cache
    is only used when there are at least 50 values. The presence of
    out-of-bounds values will render the cache unusable and may slow down
    parsing.

Returns
-------
datetime
    If parsing succeeded.
    Return type depends on input (types in parenthesis correspond to
    fallback in case of unsuccessful timezone or out-of-range timestamp
    parsing):

    - scalar: :class:`Timestamp` (or :class:`datetime.datetime`)
    - array-like: :class:`DatetimeIndex` (or :class:`Series` with
      :class:`object` dtype containing :class:`datetime.datetime`)
    - Series: :class:`Series` of :class:`datetime64` dtype (or
      :class:`Series` of :class:`object` dtype containing
      :class:`datetime.datetime`)
    - DataFrame: :class:`Series` of :class:`datetime64` dtype (or
      :class:`Series` of :class:`object` dtype containing
      :class:`datetime.datetime`)

Raises
------
ParserError
    When parsing a date from string fails.
ValueError
    When another datetime conversion error happens. For example when one
    of 'year', 'month', day' columns is missing in a :class:`DataFrame`, or
    when a Timezone-aware :class:`datetime.datetime` is found in an array-like
    of mixed time offsets, and ``utc=False``.

See Also
--------
DataFrame.astype : Cast argument to a specified dtype.
to_timedelta : Convert argument to timedelta.
convert_dtypes : Convert dtypes.

Notes
-----

Many input types are supported, and lead to different output types:

- **scalars** can be int, float, str, datetime object (from stdlib :mod:`datetime`
  module or :mod:`numpy`). They are converted to :class:`Timestamp` when
  possible, otherwise they are converted to :class:`datetime.datetime`.
  None/NaN/null scalars are converted to :const:`NaT`.

- **array-like** can contain int, float, str, datetime objects. They are
  converted to :class:`DatetimeIndex` when possible, otherwise they are
  converted to :class:`Index` with :class:`object` dtype, containing
  :class:`datetime.datetime`. None/NaN/null entries are converted to
  :const:`NaT` in both cases.

- **Series** are converted to :class:`Series` with :class:`datetime64`
  dtype when possible, otherwise they are converted to :class:`Series` with
  :class:`object` dtype, containing :class:`datetime.datetime`. None/NaN/null
  entries are converted to :const:`NaT` in both cases.

- **DataFrame/dict-like** are converted to :class:`Series` with
  :class:`datetime64` dtype. For each row a datetime is created from assembling
  the various dataframe columns. Column keys can be common abbreviations
  like ['year', 'month', 'day', 'minute', 'second', 'ms', 'us', 'ns']) or
  plurals of the same.

The following causes are responsible for :class:`datetime.datetime` objects
being returned (possibly inside an :class:`Index` or a :class:`Series` with
:class:`object` dtype) instead of a proper pandas designated type
(:class:`Timestamp`, :class:`DatetimeIndex` or :class:`Series`
with :class:`datetime64` dtype):

- when any input element is before :const:`Timestamp.min` or after
  :const:`Timestamp.max`, see `timestamp limitations
  <https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html
  #timeseries-timestamp-limits>`_.

- when ``utc=False`` (default) and the input is an array-like or
  :class:`Series` containing mixed naive/aware datetime, or aware with mixed
  time offsets. Note that this happens in the (quite frequent) situation when
  the timezone has a daylight savings policy. In that case you may wish to
  use ``utc=True``.

Examples
--------

**Handling various input formats**

Assembling a datetime from multiple columns of a :class:`DataFrame`. The keys
can be common abbreviations like ['year', 'month', 'day', 'minute', 'second',
'ms', 'us', 'ns']) or plurals of the same

>>> df = pd.DataFrame({'year': [2015, 2016],
...                    'month': [2, 3],
...                    'day': [4, 5]})
>>> pd.to_datetime(df)
0   2015-02-04
1   2016-03-05
dtype: datetime64[ns]

Using a unix epoch time

>>> pd.to_datetime(1490195805, unit='s')
Timestamp('2017-03-22 15:16:45')
>>> pd.to_datetime(1490195805433502912, unit='ns')
Timestamp('2017-03-22 15:16:45.433502912')

.. warning:: For float arg, precision rounding might happen. To prevent
    unexpected behavior use a fixed-width exact type.

Using a non-unix epoch origin

>>> pd.to_datetime([1, 2, 3], unit='D',
...                origin=pd.Timestamp('1960-01-01'))
DatetimeIndex(['1960-01-02', '1960-01-03', '1960-01-04'],
              dtype='datetime64[ns]', freq=None)

**Differences with strptime behavior**

:const:`""%f""` will parse all the way up to nanoseconds.

>>> pd.to_datetime('2018-10-26 12:00:00.0000000011',
...                format='%Y-%m-%d %H:%M:%S.%f')
Timestamp('2018-10-26 12:00:00.000000001')

**Non-convertible date/times**

If a date does not meet the `timestamp limitations
<https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html
#timeseries-timestamp-limits>`_, passing ``errors='ignore'``
will return the original input instead of raising any exception.

Passing ``errors='coerce'`` will force an out-of-bounds date to :const:`NaT`,
in addition to forcing non-dates (or non-parseable dates) to :const:`NaT`.

>>> pd.to_datetime('13000101', format='%Y%m%d', errors='ignore')
'13000101'
>>> pd.to_datetime('13000101', format='%Y%m%d', errors='coerce')
NaT

.. _to_datetime_tz_examples:

**Timezones and time offsets**

The default behaviour (``utc=False``) is as follows:

- Timezone-naive inputs are converted to timezone-naive :class:`DatetimeIndex`:

>>> pd.to_datetime(['2018-10-26 12:00:00', '2018-10-26 13:00:15'])
DatetimeIndex(['2018-10-26 12:00:00', '2018-10-26 13:00:15'],
              dtype='datetime64[ns]', freq=None)

- Timezone-aware inputs *with constant time offset* are converted to
  timezone-aware :class:`DatetimeIndex`:

>>> pd.to_datetime(['2018-10-26 12:00 -0500', '2018-10-26 13:00 -0500'])
DatetimeIndex(['2018-10-26 12:00:00-05:00', '2018-10-26 13:00:00-05:00'],
              dtype='datetime64[ns, UTC-05:00]', freq=None)

- However, timezone-aware inputs *with mixed time offsets* (for example
  issued from a timezone with daylight savings, such as Europe/Paris)
  are **not successfully converted** to a :class:`DatetimeIndex`.
  Parsing datetimes with mixed time zones will show a warning unless
  `utc=True`. If you specify `utc=False` the warning below will be shown
  and a simple :class:`Index` containing :class:`datetime.datetime`
  objects will be returned:

>>> pd.to_datetime(['2020-10-25 02:00 +0200',
...                 '2020-10-25 04:00 +0100'])  # doctest: +SKIP
FutureWarning: In a future version of pandas, parsing datetimes with mixed
time zones will raise a warning unless `utc=True`. Please specify `utc=True`
to opt in to the new behaviour and silence this warning. To create a `Series`
with mixed offsets and `object` dtype, please use `apply` and
`datetime.datetime.strptime`.
Index([2020-10-25 02:00:00+02:00, 2020-10-25 04:00:00+01:00],
      dtype='object')

- A mix of timezone-aware and timezone-naive inputs is also converted to
  a simple :class:`Index` containing :class:`datetime.datetime` objects:

>>> from datetime import datetime
>>> pd.to_datetime([""2020-01-01 01:00:00-01:00"",
...                 datetime(2020, 1, 1, 3, 0)])  # doctest: +SKIP
FutureWarning: In a future version of pandas, parsing datetimes with mixed
time zones will raise a warning unless `utc=True`. Please specify `utc=True`
to opt in to the new behaviour and silence this warning. To create a `Series`
with mixed offsets and `object` dtype, please use `apply` and
`datetime.datetime.strptime`.
Index([2020-01-01 01:00:00-01:00, 2020-01-01 03:00:00], dtype='object')

|

Setting ``utc=True`` solves most of the above issues:

- Timezone-naive inputs are *localized* as UTC

>>> pd.to_datetime(['2018-10-26 12:00', '2018-10-26 13:00'], utc=True)
DatetimeIndex(['2018-10-26 12:00:00+00:00', '2018-10-26 13:00:00+00:00'],
              dtype='datetime64[ns, UTC]', freq=None)

- Timezone-aware inputs are *converted* to UTC (the output represents the
  exact same datetime, but viewed from the UTC time offset `+00:00`).

>>> pd.to_datetime(['2018-10-26 12:00 -0530', '2018-10-26 12:00 -0500'],
...                utc=True)
DatetimeIndex(['2018-10-26 17:30:00+00:00', '2018-10-26 17:00:00+00:00'],
              dtype='datetime64[ns, UTC]', freq=None)

- Inputs can contain both string or datetime, the above
  rules still apply

>>> pd.to_datetime(['2018-10-26 12:00', datetime(2020, 1, 1, 18)], utc=True)
DatetimeIndex(['2018-10-26 12:00:00+00:00', '2020-01-01 18:00:00+00:00'],
              dtype='datetime64[ns, UTC]', freq=None)"""""""
pandas/core/tools/datetimes.py,"def _assemble_from_unit_mappings(arg, errors: DateTimeErrorChoices, utc: bool):
    from pandas import DataFrame, to_numeric, to_timedelta
    arg = DataFrame(arg)
    if not arg.columns.is_unique:
        raise ValueError('cannot assemble with duplicate keys')

    def f(value):
        if value in _unit_map:
            return _unit_map[value]
        if value.lower() in _unit_map:
            return _unit_map[value.lower()]
        return value
    unit = {k: f(k) for k in arg.keys()}
    unit_rev = {v: k for (k, v) in unit.items()}
    required = ['year', 'month', 'day']
    req = sorted(set(required) - set(unit_rev.keys()))
    if len(req):
        _required = ','.join(req)
        raise ValueError(f'to assemble mappings requires at least that [year, month, day] be specified: [{_required}] is missing')
    excess = sorted(set(unit_rev.keys()) - set(_unit_map.values()))
    if len(excess):
        _excess = ','.join(excess)
        raise ValueError(f'extra keys have been passed to the datetime assemblage: [{_excess}]')

    def coerce(values):
        values = to_numeric(values, errors=errors)
        if is_integer_dtype(values):
            values = values.astype('int64', copy=False)
        return values
    values = coerce(arg[unit_rev['year']]) * 10000 + coerce(arg[unit_rev['month']]) * 100 + coerce(arg[unit_rev['day']])
    try:
        values = to_datetime(values, format='%Y%m%d', errors=errors, utc=utc)
    except (TypeError, ValueError) as err:
        raise ValueError(f'cannot assemble the datetimes: {err}') from err
    units: list[UnitChoices] = ['h', 'm', 's', 'ms', 'us', 'ns']
    for u in units:
        value = unit_rev.get(u)
        if value is not None and value in arg:
            try:
                values += to_timedelta(coerce(arg[value]), unit=u, errors=errors)
            except (TypeError, ValueError) as err:
                raise ValueError(f'cannot assemble the datetimes [{value}]: {err}') from err
    return values","""""""assemble the unit specified fields from the arg (DataFrame)
Return a Series for actual parsing

Parameters
----------
arg : DataFrame
errors : {'ignore', 'raise', 'coerce'}, default 'raise'

    - If :const:`'raise'`, then invalid parsing will raise an exception
    - If :const:`'coerce'`, then invalid parsing will be set as :const:`NaT`
    - If :const:`'ignore'`, then invalid parsing will return the input
utc : bool
    Whether to convert/localize timestamps to UTC.

Returns
-------
Series"""""""
pandas/core/tools/datetimes.py,"def _attempt_YYYYMMDD(arg: npt.NDArray[np.object_], errors: str) -> np.ndarray | None:

    def calc(carg):
        carg = carg.astype(object, copy=False)
        parsed = parsing.try_parse_year_month_day(carg / 10000, carg / 100 % 100, carg % 100)
        return tslib.array_to_datetime(parsed, errors=errors)[0]

    def calc_with_mask(carg, mask):
        result = np.empty(carg.shape, dtype='M8[ns]')
        iresult = result.view('i8')
        iresult[~mask] = iNaT
        masked_result = calc(carg[mask].astype(np.float64).astype(np.int64))
        result[mask] = masked_result.astype('M8[ns]')
        return result
    try:
        return calc(arg.astype(np.int64))
    except (ValueError, OverflowError, TypeError):
        pass
    try:
        carg = arg.astype(np.float64)
        return calc_with_mask(carg, notna(carg))
    except (ValueError, OverflowError, TypeError):
        pass
    try:
        mask = ~algorithms.isin(arg, list(nat_strings))
        return calc_with_mask(arg, mask)
    except (ValueError, OverflowError, TypeError):
        pass
    return None","""""""try to parse the YYYYMMDD/%Y%m%d format, try to deal with NaT-like,
arg is a passed in as an object dtype, but could really be ints/strings
with nan-like/or floats (e.g. with nan)

Parameters
----------
arg : np.ndarray[object]
errors : {'raise','ignore','coerce'}"""""""
pandas/core/tools/numeric.py,"def to_numeric(arg, errors: DateTimeErrorChoices='raise', downcast: Literal['integer', 'signed', 'unsigned', 'float'] | None=None, dtype_backend: DtypeBackend | lib.NoDefault=lib.no_default):
    if downcast not in (None, 'integer', 'signed', 'unsigned', 'float'):
        raise ValueError('invalid downcasting method provided')
    if errors not in ('ignore', 'raise', 'coerce'):
        raise ValueError('invalid error value specified')
    check_dtype_backend(dtype_backend)
    is_series = False
    is_index = False
    is_scalars = False
    if isinstance(arg, ABCSeries):
        is_series = True
        values = arg.values
    elif isinstance(arg, ABCIndex):
        is_index = True
        if needs_i8_conversion(arg.dtype):
            values = arg.view('i8')
        else:
            values = arg.values
    elif isinstance(arg, (list, tuple)):
        values = np.array(arg, dtype='O')
    elif is_scalar(arg):
        if is_decimal(arg):
            return float(arg)
        if is_number(arg):
            return arg
        is_scalars = True
        values = np.array([arg], dtype='O')
    elif getattr(arg, 'ndim', 1) > 1:
        raise TypeError('arg must be a list, tuple, 1-d array, or Series')
    else:
        values = arg
    orig_values = values
    mask: npt.NDArray[np.bool_] | None = None
    if isinstance(values, BaseMaskedArray):
        mask = values._mask
        values = values._data[~mask]
    values_dtype = getattr(values, 'dtype', None)
    if isinstance(values_dtype, ArrowDtype):
        mask = values.isna()
        values = values.dropna().to_numpy()
    new_mask: np.ndarray | None = None
    if is_numeric_dtype(values_dtype):
        pass
    elif lib.is_np_dtype(values_dtype, 'mM'):
        values = values.view(np.int64)
    else:
        values = ensure_object(values)
        coerce_numeric = errors not in ('ignore', 'raise')
        try:
            (values, new_mask) = lib.maybe_convert_numeric(values, set(), coerce_numeric=coerce_numeric, convert_to_masked_nullable=dtype_backend is not lib.no_default or isinstance(values_dtype, StringDtype))
        except (ValueError, TypeError):
            if errors == 'raise':
                raise
            values = orig_values
    if new_mask is not None:
        values = values[~new_mask]
    elif dtype_backend is not lib.no_default and new_mask is None or isinstance(values_dtype, StringDtype):
        new_mask = np.zeros(values.shape, dtype=np.bool_)
    if downcast is not None and is_numeric_dtype(values.dtype):
        typecodes: str | None = None
        if downcast in ('integer', 'signed'):
            typecodes = np.typecodes['Integer']
        elif downcast == 'unsigned' and (not len(values) or np.min(values) >= 0):
            typecodes = np.typecodes['UnsignedInteger']
        elif downcast == 'float':
            typecodes = np.typecodes['Float']
            float_32_char = np.dtype(np.float32).char
            float_32_ind = typecodes.index(float_32_char)
            typecodes = typecodes[float_32_ind:]
        if typecodes is not None:
            for typecode in typecodes:
                dtype = np.dtype(typecode)
                if dtype.itemsize <= values.dtype.itemsize:
                    values = maybe_downcast_numeric(values, dtype)
                    if values.dtype == dtype:
                        break
    if (mask is not None or new_mask is not None) and (not is_string_dtype(values.dtype)):
        if mask is None or (new_mask is not None and new_mask.shape == mask.shape):
            mask = new_mask
        else:
            mask = mask.copy()
        assert isinstance(mask, np.ndarray)
        data = np.zeros(mask.shape, dtype=values.dtype)
        data[~mask] = values
        from pandas.core.arrays import ArrowExtensionArray, BooleanArray, FloatingArray, IntegerArray
        klass: type[IntegerArray | BooleanArray | FloatingArray]
        if is_integer_dtype(data.dtype):
            klass = IntegerArray
        elif is_bool_dtype(data.dtype):
            klass = BooleanArray
        else:
            klass = FloatingArray
        values = klass(data, mask)
        if dtype_backend == 'pyarrow' or isinstance(values_dtype, ArrowDtype):
            values = ArrowExtensionArray(values.__arrow_array__())
    if is_series:
        return arg._constructor(values, index=arg.index, name=arg.name)
    elif is_index:
        from pandas import Index
        return Index(values, name=arg.name)
    elif is_scalars:
        return values[0]
    else:
        return values","""""""Convert argument to a numeric type.

The default return dtype is `float64` or `int64`
depending on the data supplied. Use the `downcast` parameter
to obtain other dtypes.

Please note that precision loss may occur if really large numbers
are passed in. Due to the internal limitations of `ndarray`, if
numbers smaller than `-9223372036854775808` (np.iinfo(np.int64).min)
or larger than `18446744073709551615` (np.iinfo(np.uint64).max) are
passed in, it is very likely they will be converted to float so that
they can be stored in an `ndarray`. These warnings apply similarly to
`Series` since it internally leverages `ndarray`.

Parameters
----------
arg : scalar, list, tuple, 1-d array, or Series
    Argument to be converted.
errors : {'ignore', 'raise', 'coerce'}, default 'raise'
    - If 'raise', then invalid parsing will raise an exception.
    - If 'coerce', then invalid parsing will be set as NaN.
    - If 'ignore', then invalid parsing will return the input.
downcast : str, default None
    Can be 'integer', 'signed', 'unsigned', or 'float'.
    If not None, and if the data has been successfully cast to a
    numerical dtype (or if the data was numeric to begin with),
    downcast that resulting data to the smallest numerical dtype
    possible according to the following rules:

    - 'integer' or 'signed': smallest signed int dtype (min.: np.int8)
    - 'unsigned': smallest unsigned int dtype (min.: np.uint8)
    - 'float': smallest float dtype (min.: np.float32)

    As this behaviour is separate from the core conversion to
    numeric values, any errors raised during the downcasting
    will be surfaced regardless of the value of the 'errors' input.

    In addition, downcasting will only occur if the size
    of the resulting data's dtype is strictly larger than
    the dtype it is to be cast to, so if none of the dtypes
    checked satisfy that specification, no downcasting will be
    performed on the data.
dtype_backend : {'numpy_nullable', 'pyarrow'}, default 'numpy_nullable'
    Back-end data type applied to the resultant :class:`DataFrame`
    (still experimental). Behaviour is as follows:

    * ``""numpy_nullable""``: returns nullable-dtype-backed :class:`DataFrame`
      (default).
    * ``""pyarrow""``: returns pyarrow-backed nullable :class:`ArrowDtype`
      DataFrame.

    .. versionadded:: 2.0

Returns
-------
ret
    Numeric if parsing succeeded.
    Return type depends on input.  Series if Series, otherwise ndarray.

See Also
--------
DataFrame.astype : Cast argument to a specified dtype.
to_datetime : Convert argument to datetime.
to_timedelta : Convert argument to timedelta.
numpy.ndarray.astype : Cast a numpy array to a specified type.
DataFrame.convert_dtypes : Convert dtypes.

Examples
--------
Take separate series and convert to numeric, coercing when told to

>>> s = pd.Series(['1.0', '2', -3])
>>> pd.to_numeric(s)
0    1.0
1    2.0
2   -3.0
dtype: float64
>>> pd.to_numeric(s, downcast='float')
0    1.0
1    2.0
2   -3.0
dtype: float32
>>> pd.to_numeric(s, downcast='signed')
0    1
1    2
2   -3
dtype: int8
>>> s = pd.Series(['apple', '1.0', '2', -3])
>>> pd.to_numeric(s, errors='ignore')
0    apple
1      1.0
2        2
3       -3
dtype: object
>>> pd.to_numeric(s, errors='coerce')
0    NaN
1    1.0
2    2.0
3   -3.0
dtype: float64

Downcasting of nullable integer and floating dtypes is supported:

>>> s = pd.Series([1, 2, 3], dtype=""Int64"")
>>> pd.to_numeric(s, downcast=""integer"")
0    1
1    2
2    3
dtype: Int8
>>> s = pd.Series([1.0, 2.1, 3.0], dtype=""Float64"")
>>> pd.to_numeric(s, downcast=""float"")
0    1.0
1    2.1
2    3.0
dtype: Float32"""""""
pandas/core/tools/timedeltas.py,"def to_timedelta(arg: str | int | float | timedelta | list | tuple | range | ArrayLike | Index | Series, unit: UnitChoices | None=None, errors: DateTimeErrorChoices='raise') -> Timedelta | TimedeltaIndex | Series:
    if unit is not None:
        unit = parse_timedelta_unit(unit)
    if errors not in ('ignore', 'raise', 'coerce'):
        raise ValueError(""errors must be one of 'ignore', 'raise', or 'coerce'."")
    if unit in {'Y', 'y', 'M'}:
        raise ValueError(""Units 'M', 'Y', and 'y' are no longer supported, as they do not represent unambiguous timedelta values durations."")
    if arg is None:
        return arg
    elif isinstance(arg, ABCSeries):
        values = _convert_listlike(arg._values, unit=unit, errors=errors)
        return arg._constructor(values, index=arg.index, name=arg.name)
    elif isinstance(arg, ABCIndex):
        return _convert_listlike(arg, unit=unit, errors=errors, name=arg.name)
    elif isinstance(arg, np.ndarray) and arg.ndim == 0:
        arg = lib.item_from_zerodim(arg)
    elif is_list_like(arg) and getattr(arg, 'ndim', 1) == 1:
        return _convert_listlike(arg, unit=unit, errors=errors)
    elif getattr(arg, 'ndim', 1) > 1:
        raise TypeError('arg must be a string, timedelta, list, tuple, 1-d array, or Series')
    if isinstance(arg, str) and unit is not None:
        raise ValueError('unit must not be specified if the input is/contains a str')
    return _coerce_scalar_to_timedelta_type(arg, unit=unit, errors=errors)","""""""Convert argument to timedelta.

Timedeltas are absolute differences in times, expressed in difference
units (e.g. days, hours, minutes, seconds). This method converts
an argument from a recognized timedelta format / value into
a Timedelta type.

Parameters
----------
arg : str, timedelta, list-like or Series
    The data to be converted to timedelta.

    .. versionchanged:: 2.0
        Strings with units 'M', 'Y' and 'y' do not represent
        unambiguous timedelta values and will raise an exception.

unit : str, optional
    Denotes the unit of the arg for numeric `arg`. Defaults to ``""ns""``.

    Possible values:

    * 'W'
    * 'D' / 'days' / 'day'
    * 'hours' / 'hour' / 'hr' / 'h'
    * 'm' / 'minute' / 'min' / 'minutes' / 'T'
    * 's' / 'seconds' / 'sec' / 'second' / 'S'
    * 'ms' / 'milliseconds' / 'millisecond' / 'milli' / 'millis' / 'L'
    * 'us' / 'microseconds' / 'microsecond' / 'micro' / 'micros' / 'U'
    * 'ns' / 'nanoseconds' / 'nano' / 'nanos' / 'nanosecond' / 'N'

    Must not be specified when `arg` context strings and ``errors=""raise""``.

    .. deprecated:: 2.2.0
        Units 'T', 'S', 'L', 'U' and 'N' are deprecated and will be removed
        in a future version. Please use 'min', 's', 'ms', 'us', and 'ns' instead of
        'T', 'S', 'L', 'U' and 'N'.

errors : {'ignore', 'raise', 'coerce'}, default 'raise'
    - If 'raise', then invalid parsing will raise an exception.
    - If 'coerce', then invalid parsing will be set as NaT.
    - If 'ignore', then invalid parsing will return the input.

Returns
-------
timedelta
    If parsing succeeded.
    Return type depends on input:

    - list-like: TimedeltaIndex of timedelta64 dtype
    - Series: Series of timedelta64 dtype
    - scalar: Timedelta

See Also
--------
DataFrame.astype : Cast argument to a specified dtype.
to_datetime : Convert argument to datetime.
convert_dtypes : Convert dtypes.

Notes
-----
If the precision is higher than nanoseconds, the precision of the duration is
truncated to nanoseconds for string inputs.

Examples
--------
Parsing a single string to a Timedelta:

>>> pd.to_timedelta('1 days 06:05:01.00003')
Timedelta('1 days 06:05:01.000030')
>>> pd.to_timedelta('15.5us')
Timedelta('0 days 00:00:00.000015500')

Parsing a list or array of strings:

>>> pd.to_timedelta(['1 days 06:05:01.00003', '15.5us', 'nan'])
TimedeltaIndex(['1 days 06:05:01.000030', '0 days 00:00:00.000015500', NaT],
               dtype='timedelta64[ns]', freq=None)

Converting numbers by specifying the `unit` keyword argument:

>>> pd.to_timedelta(np.arange(5), unit='s')
TimedeltaIndex(['0 days 00:00:00', '0 days 00:00:01', '0 days 00:00:02',
                '0 days 00:00:03', '0 days 00:00:04'],
               dtype='timedelta64[ns]', freq=None)
>>> pd.to_timedelta(np.arange(5), unit='d')
TimedeltaIndex(['0 days', '1 days', '2 days', '3 days', '4 days'],
               dtype='timedelta64[ns]', freq=None)"""""""
pandas/core/tools/timedeltas.py,"def _coerce_scalar_to_timedelta_type(r, unit: UnitChoices | None='ns', errors: DateTimeErrorChoices='raise'):
    result: Timedelta | NaTType
    try:
        result = Timedelta(r, unit)
    except ValueError:
        if errors == 'raise':
            raise
        if errors == 'ignore':
            return r
        result = NaT
    return result","""""""Convert string 'r' to a timedelta object."""""""
pandas/core/tools/timedeltas.py,"def _convert_listlike(arg, unit: UnitChoices | None=None, errors: DateTimeErrorChoices='raise', name: Hashable | None=None):
    arg_dtype = getattr(arg, 'dtype', None)
    if isinstance(arg, (list, tuple)) or arg_dtype is None:
        if not hasattr(arg, '__array__'):
            arg = list(arg)
        arg = np.array(arg, dtype=object)
    elif isinstance(arg_dtype, ArrowDtype) and arg_dtype.kind == 'm':
        return arg
    try:
        td64arr = sequence_to_td64ns(arg, unit=unit, errors=errors, copy=False)[0]
    except ValueError:
        if errors == 'ignore':
            return arg
        else:
            raise
    from pandas import TimedeltaIndex
    value = TimedeltaIndex(td64arr, unit='ns', name=name)
    return value","""""""Convert a list of objects to a timedelta index object."""""""
pandas/core/tools/times.py,"def to_time(arg, format: str | None=None, infer_time_format: bool=False, errors: DateTimeErrorChoices='raise'):

    def _convert_listlike(arg, format):
        if isinstance(arg, (list, tuple)):
            arg = np.array(arg, dtype='O')
        elif getattr(arg, 'ndim', 1) > 1:
            raise TypeError('arg must be a string, datetime, list, tuple, 1-d array, or Series')
        arg = np.asarray(arg, dtype='O')
        if infer_time_format and format is None:
            format = _guess_time_format_for_array(arg)
        times: list[time | None] = []
        if format is not None:
            for element in arg:
                try:
                    times.append(datetime.strptime(element, format).time())
                except (ValueError, TypeError) as err:
                    if errors == 'raise':
                        msg = f'Cannot convert {element} to a time with given format {format}'
                        raise ValueError(msg) from err
                    if errors == 'ignore':
                        return arg
                    else:
                        times.append(None)
        else:
            formats = _time_formats[:]
            format_found = False
            for element in arg:
                time_object = None
                try:
                    time_object = time.fromisoformat(element)
                except (ValueError, TypeError):
                    for time_format in formats:
                        try:
                            time_object = datetime.strptime(element, time_format).time()
                            if not format_found:
                                fmt = formats.pop(formats.index(time_format))
                                formats.insert(0, fmt)
                                format_found = True
                            break
                        except (ValueError, TypeError):
                            continue
                if time_object is not None:
                    times.append(time_object)
                elif errors == 'raise':
                    raise ValueError(f'Cannot convert arg {arg} to a time')
                elif errors == 'ignore':
                    return arg
                else:
                    times.append(None)
        return times
    if arg is None:
        return arg
    elif isinstance(arg, time):
        return arg
    elif isinstance(arg, ABCSeries):
        values = _convert_listlike(arg._values, format)
        return arg._constructor(values, index=arg.index, name=arg.name)
    elif isinstance(arg, ABCIndex):
        return _convert_listlike(arg, format)
    elif is_list_like(arg):
        return _convert_listlike(arg, format)
    return _convert_listlike(np.array([arg]), format)[0]","""""""Parse time strings to time objects using fixed strptime formats (""%H:%M"",
""%H%M"", ""%I:%M%p"", ""%I%M%p"", ""%H:%M:%S"", ""%H%M%S"", ""%I:%M:%S%p"",
""%I%M%S%p"")

Use infer_time_format if all the strings are in the same format to speed
up conversion.

Parameters
----------
arg : string in time format, datetime.time, list, tuple, 1-d array,  Series
format : str, default None
    Format used to convert arg into a time object.  If None, fixed formats
    are used.
infer_time_format: bool, default False
    Infer the time format based on the first non-NaN element.  If all
    strings are in the same format, this will speed up conversion.
errors : {'ignore', 'raise', 'coerce'}, default 'raise'
    - If 'raise', then invalid parsing will raise an exception
    - If 'coerce', then invalid parsing will be set as None
    - If 'ignore', then invalid parsing will return the input

Returns
-------
datetime.time"""""""
pandas/core/util/hashing.py,"def combine_hash_arrays(arrays: Iterator[np.ndarray], num_items: int) -> npt.NDArray[np.uint64]:
    try:
        first = next(arrays)
    except StopIteration:
        return np.array([], dtype=np.uint64)
    arrays = itertools.chain([first], arrays)
    mult = np.uint64(1000003)
    out = np.zeros_like(first) + np.uint64(3430008)
    last_i = 0
    for (i, a) in enumerate(arrays):
        inverse_i = num_items - i
        out ^= a
        out *= mult
        mult += np.uint64(82520 + inverse_i + inverse_i)
        last_i = i
    assert last_i + 1 == num_items, 'Fed in wrong num_items'
    out += np.uint64(97531)
    return out","""""""Parameters
----------
arrays : Iterator[np.ndarray]
num_items : int

Returns
-------
np.ndarray[uint64]

Should be the same as CPython's tupleobject.c"""""""
pandas/core/util/hashing.py,"def hash_pandas_object(obj: Index | DataFrame | Series, index: bool=True, encoding: str='utf8', hash_key: str | None=_default_hash_key, categorize: bool=True) -> Series:
    from pandas import Series
    if hash_key is None:
        hash_key = _default_hash_key
    if isinstance(obj, ABCMultiIndex):
        return Series(hash_tuples(obj, encoding, hash_key), dtype='uint64', copy=False)
    elif isinstance(obj, ABCIndex):
        h = hash_array(obj._values, encoding, hash_key, categorize).astype('uint64', copy=False)
        ser = Series(h, index=obj, dtype='uint64', copy=False)
    elif isinstance(obj, ABCSeries):
        h = hash_array(obj._values, encoding, hash_key, categorize).astype('uint64', copy=False)
        if index:
            index_iter = (hash_pandas_object(obj.index, index=False, encoding=encoding, hash_key=hash_key, categorize=categorize)._values for _ in [None])
            arrays = itertools.chain([h], index_iter)
            h = combine_hash_arrays(arrays, 2)
        ser = Series(h, index=obj.index, dtype='uint64', copy=False)
    elif isinstance(obj, ABCDataFrame):
        hashes = (hash_array(series._values, encoding, hash_key, categorize) for (_, series) in obj.items())
        num_items = len(obj.columns)
        if index:
            index_hash_generator = (hash_pandas_object(obj.index, index=False, encoding=encoding, hash_key=hash_key, categorize=categorize)._values for _ in [None])
            num_items += 1
            _hashes = itertools.chain(hashes, index_hash_generator)
            hashes = (x for x in _hashes)
        h = combine_hash_arrays(hashes, num_items)
        ser = Series(h, index=obj.index, dtype='uint64', copy=False)
    else:
        raise TypeError(f'Unexpected type for hashing {type(obj)}')
    return ser","""""""Return a data hash of the Index/Series/DataFrame.

Parameters
----------
obj : Index, Series, or DataFrame
index : bool, default True
    Include the index in the hash (if Series/DataFrame).
encoding : str, default 'utf8'
    Encoding for data & key when strings.
hash_key : str, default _default_hash_key
    Hash_key for string key to encode.
categorize : bool, default True
    Whether to first categorize object arrays before hashing. This is more
    efficient when the array contains duplicate values.

Returns
-------
Series of uint64, same length as the object

Examples
--------
>>> pd.util.hash_pandas_object(pd.Series([1, 2, 3]))
0    14639053686158035780
1     3869563279212530728
2      393322362522515241
dtype: uint64"""""""
pandas/core/util/hashing.py,"def hash_tuples(vals: MultiIndex | Iterable[tuple[Hashable, ...]], encoding: str='utf8', hash_key: str=_default_hash_key) -> npt.NDArray[np.uint64]:
    if not is_list_like(vals):
        raise TypeError('must be convertible to a list-of-tuples')
    from pandas import Categorical, MultiIndex
    if not isinstance(vals, ABCMultiIndex):
        mi = MultiIndex.from_tuples(vals)
    else:
        mi = vals
    cat_vals = [Categorical._simple_new(mi.codes[level], CategoricalDtype(categories=mi.levels[level], ordered=False)) for level in range(mi.nlevels)]
    hashes = (cat._hash_pandas_object(encoding=encoding, hash_key=hash_key, categorize=False) for cat in cat_vals)
    h = combine_hash_arrays(hashes, len(cat_vals))
    return h","""""""Hash an MultiIndex / listlike-of-tuples efficiently.

Parameters
----------
vals : MultiIndex or listlike-of-tuples
encoding : str, default 'utf8'
hash_key : str, default _default_hash_key

Returns
-------
ndarray[np.uint64] of hashed values"""""""
pandas/core/util/hashing.py,"def hash_array(vals: ArrayLike, encoding: str='utf8', hash_key: str=_default_hash_key, categorize: bool=True) -> npt.NDArray[np.uint64]:
    if not hasattr(vals, 'dtype'):
        raise TypeError('must pass a ndarray-like')
    if isinstance(vals, ABCExtensionArray):
        return vals._hash_pandas_object(encoding=encoding, hash_key=hash_key, categorize=categorize)
    if not isinstance(vals, np.ndarray):
        raise TypeError(f'hash_array requires np.ndarray or ExtensionArray, not {type(vals).__name__}. Use hash_pandas_object instead.')
    return _hash_ndarray(vals, encoding, hash_key, categorize)","""""""Given a 1d array, return an array of deterministic integers.

Parameters
----------
vals : ndarray or ExtensionArray
encoding : str, default 'utf8'
    Encoding for data & key when strings.
hash_key : str, default _default_hash_key
    Hash_key for string key to encode.
categorize : bool, default True
    Whether to first categorize object arrays before hashing. This is more
    efficient when the array contains duplicate values.

Returns
-------
ndarray[np.uint64, ndim=1]
    Hashed values, same length as the vals.

Examples
--------
>>> pd.util.hash_array(np.array([1, 2, 3]))
array([ 6238072747940578789, 15839785061582574730,  2185194620014831856],
  dtype=uint64)"""""""
pandas/core/util/hashing.py,"def _hash_ndarray(vals: np.ndarray, encoding: str='utf8', hash_key: str=_default_hash_key, categorize: bool=True) -> npt.NDArray[np.uint64]:
    dtype = vals.dtype
    if np.issubdtype(dtype, np.complex128):
        hash_real = _hash_ndarray(vals.real, encoding, hash_key, categorize)
        hash_imag = _hash_ndarray(vals.imag, encoding, hash_key, categorize)
        return hash_real + 23 * hash_imag
    if dtype == bool:
        vals = vals.astype('u8')
    elif issubclass(dtype.type, (np.datetime64, np.timedelta64)):
        vals = vals.view('i8').astype('u8', copy=False)
    elif issubclass(dtype.type, np.number) and dtype.itemsize <= 8:
        vals = vals.view(f'u{vals.dtype.itemsize}').astype('u8')
    else:
        if categorize:
            from pandas import Categorical, Index, factorize
            (codes, categories) = factorize(vals, sort=False)
            dtype = CategoricalDtype(categories=Index(categories), ordered=False)
            cat = Categorical._simple_new(codes, dtype)
            return cat._hash_pandas_object(encoding=encoding, hash_key=hash_key, categorize=False)
        try:
            vals = hash_object_array(vals, hash_key, encoding)
        except TypeError:
            vals = hash_object_array(vals.astype(str).astype(object), hash_key, encoding)
    vals ^= vals >> 30
    vals *= np.uint64(13787848793156543929)
    vals ^= vals >> 27
    vals *= np.uint64(10723151780598845931)
    vals ^= vals >> 31
    return vals","""""""See hash_array.__doc__."""""""
pandas/core/util/numba_.py,"def maybe_use_numba(engine: str | None) -> bool:
    return engine == 'numba' or (engine is None and GLOBAL_USE_NUMBA)","""""""Signal whether to use numba routines."""""""
pandas/core/util/numba_.py,"def get_jit_arguments(engine_kwargs: dict[str, bool] | None=None, kwargs: dict | None=None) -> dict[str, bool]:
    if engine_kwargs is None:
        engine_kwargs = {}
    nopython = engine_kwargs.get('nopython', True)
    if kwargs and nopython:
        raise NumbaUtilError('numba does not support kwargs with nopython=True: https://github.com/numba/numba/issues/2916')
    nogil = engine_kwargs.get('nogil', False)
    parallel = engine_kwargs.get('parallel', False)
    return {'nopython': nopython, 'nogil': nogil, 'parallel': parallel}","""""""Return arguments to pass to numba.JIT, falling back on pandas default JIT settings.

Parameters
----------
engine_kwargs : dict, default None
    user passed keyword arguments for numba.JIT
kwargs : dict, default None
    user passed keyword arguments to pass into the JITed function

Returns
-------
dict[str, bool]
    nopython, nogil, parallel

Raises
------
NumbaUtilError"""""""
pandas/core/util/numba_.py,"def jit_user_function(func: Callable) -> Callable:
    if TYPE_CHECKING:
        import numba
    else:
        numba = import_optional_dependency('numba')
    if numba.extending.is_jitted(func):
        numba_func = func
    else:
        numba_func = numba.extending.register_jitable(func)
    return numba_func","""""""If user function is not jitted already, mark the user's function
as jitable.

Parameters
----------
func : function
    user defined function

Returns
-------
function
    Numba JITed function, or function marked as JITable by numba"""""""
pandas/core/window/doc.py,"def create_section_header(header: str) -> str:
    return f""{header}\n{'-' * len(header)}\n""","""""""Create numpydoc section header"""""""
pandas/core/window/ewm.py,"def _calculate_deltas(times: np.ndarray | NDFrame, halflife: float | TimedeltaConvertibleTypes | None) -> np.ndarray:
    _times = np.asarray(times.view(np.int64), dtype=np.float64)
    _halflife = float(Timedelta(halflife).as_unit('ns')._value)
    return np.diff(_times) / _halflife","""""""Return the diff of the times divided by the half-life. These values are used in
the calculation of the ewm mean.

Parameters
----------
times : np.ndarray, Series
    Times corresponding to the observations. Must be monotonically increasing
    and ``datetime64[ns]`` dtype.
halflife : float, str, timedelta, optional
    Half-life specifying the decay

Returns
-------
np.ndarray
    Diff of the times divided by the half-life"""""""
pandas/core/window/numba_.py,"@functools.cache
def generate_numba_apply_func(func: Callable[..., Scalar], nopython: bool, nogil: bool, parallel: bool):
    numba_func = jit_user_function(func)
    if TYPE_CHECKING:
        import numba
    else:
        numba = import_optional_dependency('numba')

    @numba.jit(nopython=nopython, nogil=nogil, parallel=parallel)
    def roll_apply(values: np.ndarray, begin: np.ndarray, end: np.ndarray, minimum_periods: int, *args: Any) -> np.ndarray:
        result = np.empty(len(begin))
        for i in numba.prange(len(result)):
            start = begin[i]
            stop = end[i]
            window = values[start:stop]
            count_nan = np.sum(np.isnan(window))
            if len(window) - count_nan >= minimum_periods:
                result[i] = numba_func(window, *args)
            else:
                result[i] = np.nan
        return result
    return roll_apply","""""""Generate a numba jitted apply function specified by values from engine_kwargs.

1. jit the user's function
2. Return a rolling apply function with the jitted function inline

Configurations specified in engine_kwargs apply to both the user's
function _AND_ the rolling apply function.

Parameters
----------
func : function
    function to be applied to each window and will be JITed
nopython : bool
    nopython to be passed into numba.jit
nogil : bool
    nogil to be passed into numba.jit
parallel : bool
    parallel to be passed into numba.jit

Returns
-------
Numba function"""""""
pandas/core/window/numba_.py,"@functools.cache
def generate_numba_ewm_func(nopython: bool, nogil: bool, parallel: bool, com: float, adjust: bool, ignore_na: bool, deltas: tuple, normalize: bool):
    if TYPE_CHECKING:
        import numba
    else:
        numba = import_optional_dependency('numba')

    @numba.jit(nopython=nopython, nogil=nogil, parallel=parallel)
    def ewm(values: np.ndarray, begin: np.ndarray, end: np.ndarray, minimum_periods: int) -> np.ndarray:
        result = np.empty(len(values))
        alpha = 1.0 / (1.0 + com)
        old_wt_factor = 1.0 - alpha
        new_wt = 1.0 if adjust else alpha
        for i in numba.prange(len(begin)):
            start = begin[i]
            stop = end[i]
            window = values[start:stop]
            sub_result = np.empty(len(window))
            weighted = window[0]
            nobs = int(not np.isnan(weighted))
            sub_result[0] = weighted if nobs >= minimum_periods else np.nan
            old_wt = 1.0
            for j in range(1, len(window)):
                cur = window[j]
                is_observation = not np.isnan(cur)
                nobs += is_observation
                if not np.isnan(weighted):
                    if is_observation or not ignore_na:
                        if normalize:
                            old_wt *= old_wt_factor ** deltas[start + j - 1]
                        else:
                            weighted = old_wt_factor * weighted
                        if is_observation:
                            if normalize:
                                if weighted != cur:
                                    weighted = old_wt * weighted + new_wt * cur
                                    if normalize:
                                        weighted = weighted / (old_wt + new_wt)
                                if adjust:
                                    old_wt += new_wt
                                else:
                                    old_wt = 1.0
                            else:
                                weighted += cur
                elif is_observation:
                    weighted = cur
                sub_result[j] = weighted if nobs >= minimum_periods else np.nan
            result[start:stop] = sub_result
        return result
    return ewm","""""""Generate a numba jitted ewm mean or sum function specified by values
from engine_kwargs.

Parameters
----------
nopython : bool
    nopython to be passed into numba.jit
nogil : bool
    nogil to be passed into numba.jit
parallel : bool
    parallel to be passed into numba.jit
com : float
adjust : bool
ignore_na : bool
deltas : tuple
normalize : bool

Returns
-------
Numba function"""""""
pandas/core/window/numba_.py,"@functools.cache
def generate_numba_table_func(func: Callable[..., np.ndarray], nopython: bool, nogil: bool, parallel: bool):
    numba_func = jit_user_function(func)
    if TYPE_CHECKING:
        import numba
    else:
        numba = import_optional_dependency('numba')

    @numba.jit(nopython=nopython, nogil=nogil, parallel=parallel)
    def roll_table(values: np.ndarray, begin: np.ndarray, end: np.ndarray, minimum_periods: int, *args: Any):
        result = np.empty((len(begin), values.shape[1]))
        min_periods_mask = np.empty(result.shape)
        for i in numba.prange(len(result)):
            start = begin[i]
            stop = end[i]
            window = values[start:stop]
            count_nan = np.sum(np.isnan(window), axis=0)
            sub_result = numba_func(window, *args)
            nan_mask = len(window) - count_nan >= minimum_periods
            min_periods_mask[i, :] = nan_mask
            result[i, :] = sub_result
        result = np.where(min_periods_mask, result, np.nan)
        return result
    return roll_table","""""""Generate a numba jitted function to apply window calculations table-wise.

Func will be passed a M window size x N number of columns array, and
must return a 1 x N number of columns array. Func is intended to operate
row-wise, but the result will be transposed for axis=1.

1. jit the user's function
2. Return a rolling apply function with the jitted function inline

Parameters
----------
func : function
    function to be applied to each window and will be JITed
nopython : bool
    nopython to be passed into numba.jit
nogil : bool
    nogil to be passed into numba.jit
parallel : bool
    parallel to be passed into numba.jit

Returns
-------
Numba function"""""""
pandas/core/window/numba_.py,"@functools.cache
def generate_numba_ewm_table_func(nopython: bool, nogil: bool, parallel: bool, com: float, adjust: bool, ignore_na: bool, deltas: tuple, normalize: bool):
    if TYPE_CHECKING:
        import numba
    else:
        numba = import_optional_dependency('numba')

    @numba.jit(nopython=nopython, nogil=nogil, parallel=parallel)
    def ewm_table(values: np.ndarray, begin: np.ndarray, end: np.ndarray, minimum_periods: int) -> np.ndarray:
        alpha = 1.0 / (1.0 + com)
        old_wt_factor = 1.0 - alpha
        new_wt = 1.0 if adjust else alpha
        old_wt = np.ones(values.shape[1])
        result = np.empty(values.shape)
        weighted = values[0].copy()
        nobs = (~np.isnan(weighted)).astype(np.int64)
        result[0] = np.where(nobs >= minimum_periods, weighted, np.nan)
        for i in range(1, len(values)):
            cur = values[i]
            is_observations = ~np.isnan(cur)
            nobs += is_observations.astype(np.int64)
            for j in numba.prange(len(cur)):
                if not np.isnan(weighted[j]):
                    if is_observations[j] or not ignore_na:
                        if normalize:
                            old_wt[j] *= old_wt_factor ** deltas[i - 1]
                        else:
                            weighted[j] = old_wt_factor * weighted[j]
                        if is_observations[j]:
                            if normalize:
                                if weighted[j] != cur[j]:
                                    weighted[j] = old_wt[j] * weighted[j] + new_wt * cur[j]
                                    if normalize:
                                        weighted[j] = weighted[j] / (old_wt[j] + new_wt)
                                if adjust:
                                    old_wt[j] += new_wt
                                else:
                                    old_wt[j] = 1.0
                            else:
                                weighted[j] += cur[j]
                elif is_observations[j]:
                    weighted[j] = cur[j]
            result[i] = np.where(nobs >= minimum_periods, weighted, np.nan)
        return result
    return ewm_table","""""""Generate a numba jitted ewm mean or sum function applied table wise specified
by values from engine_kwargs.

Parameters
----------
nopython : bool
    nopython to be passed into numba.jit
nogil : bool
    nogil to be passed into numba.jit
parallel : bool
    parallel to be passed into numba.jit
com : float
adjust : bool
ignore_na : bool
deltas : tuple
normalize: bool

Returns
-------
Numba function"""""""
pandas/core/window/online.py,"def generate_online_numba_ewma_func(nopython: bool, nogil: bool, parallel: bool):
    if TYPE_CHECKING:
        import numba
    else:
        numba = import_optional_dependency('numba')

    @numba.jit(nopython=nopython, nogil=nogil, parallel=parallel)
    def online_ewma(values: np.ndarray, deltas: np.ndarray, minimum_periods: int, old_wt_factor: float, new_wt: float, old_wt: np.ndarray, adjust: bool, ignore_na: bool):
        """"""
        Compute online exponentially weighted mean per column over 2D values.

        Takes the first observation as is, then computes the subsequent
        exponentially weighted mean accounting minimum periods.
        """"""
        result = np.empty(values.shape)
        weighted_avg = values[0]
        nobs = (~np.isnan(weighted_avg)).astype(np.int64)
        result[0] = np.where(nobs >= minimum_periods, weighted_avg, np.nan)
        for i in range(1, len(values)):
            cur = values[i]
            is_observations = ~np.isnan(cur)
            nobs += is_observations.astype(np.int64)
            for j in numba.prange(len(cur)):
                if not np.isnan(weighted_avg[j]):
                    if is_observations[j] or not ignore_na:
                        old_wt[j] *= old_wt_factor ** deltas[j - 1]
                        if is_observations[j]:
                            if weighted_avg[j] != cur[j]:
                                weighted_avg[j] = (old_wt[j] * weighted_avg[j] + new_wt * cur[j]) / (old_wt[j] + new_wt)
                            if adjust:
                                old_wt[j] += new_wt
                            else:
                                old_wt[j] = 1.0
                elif is_observations[j]:
                    weighted_avg[j] = cur[j]
            result[i] = np.where(nobs >= minimum_periods, weighted_avg, np.nan)
        return (result, old_wt)
    return online_ewma","""""""Generate a numba jitted groupby ewma function specified by values
from engine_kwargs.

Parameters
----------
nopython : bool
    nopython to be passed into numba.jit
nogil : bool
    nogil to be passed into numba.jit
parallel : bool
    parallel to be passed into numba.jit

Returns
-------
Numba function"""""""
pandas/io/clipboard/__init__.py,"def determine_clipboard():
    global Foundation, AppKit, qtpy, PyQt4, PyQt5
    if 'cygwin' in platform.system().lower():
        if os.path.exists('/dev/clipboard'):
            warnings.warn(""Pyperclip's support for Cygwin is not perfect, see https://github.com/asweigart/pyperclip/issues/55"", stacklevel=find_stack_level())
            return init_dev_clipboard_clipboard()
    elif os.name == 'nt' or platform.system() == 'Windows':
        return init_windows_clipboard()
    if platform.system() == 'Linux':
        if _executable_exists('wslconfig.exe'):
            return init_wsl_clipboard()
    if os.name == 'mac' or platform.system() == 'Darwin':
        try:
            import AppKit
            import Foundation
        except ImportError:
            return init_osx_pbcopy_clipboard()
        else:
            return init_osx_pyobjc_clipboard()
    if HAS_DISPLAY:
        if os.environ.get('WAYLAND_DISPLAY') and _executable_exists('wl-copy'):
            return init_wl_clipboard()
        if _executable_exists('xsel'):
            return init_xsel_clipboard()
        if _executable_exists('xclip'):
            return init_xclip_clipboard()
        if _executable_exists('klipper') and _executable_exists('qdbus'):
            return init_klipper_clipboard()
        try:
            import qtpy
        except ImportError:
            try:
                import PyQt5
            except ImportError:
                try:
                    import PyQt4
                except ImportError:
                    pass
                else:
                    return init_qt_clipboard()
            else:
                return init_qt_clipboard()
        else:
            return init_qt_clipboard()
    return init_no_clipboard()","""""""Determine the OS/platform and set the copy() and paste() functions
accordingly."""""""
pandas/io/clipboard/__init__.py,"def set_clipboard(clipboard):
    global copy, paste
    clipboard_types = {'pbcopy': init_osx_pbcopy_clipboard, 'pyobjc': init_osx_pyobjc_clipboard, 'qt': init_qt_clipboard, 'xclip': init_xclip_clipboard, 'xsel': init_xsel_clipboard, 'wl-clipboard': init_wl_clipboard, 'klipper': init_klipper_clipboard, 'windows': init_windows_clipboard, 'no': init_no_clipboard}
    if clipboard not in clipboard_types:
        allowed_clipboard_types = [repr(_) for _ in clipboard_types]
        raise ValueError(f""Argument must be one of {', '.join(allowed_clipboard_types)}"")
    (copy, paste) = clipboard_types[clipboard]()","""""""Explicitly sets the clipboard mechanism. The ""clipboard mechanism"" is how
the copy() and paste() functions interact with the operating system to
implement the copy/paste feature. The clipboard parameter must be one of:
    - pbcopy
    - pyobjc (default on macOS)
    - qt
    - xclip
    - xsel
    - klipper
    - windows (default on Windows)
    - no (this is what is set when no clipboard mechanism can be found)"""""""
pandas/io/clipboard/__init__.py,"def lazy_load_stub_copy(text):
    global copy, paste
    (copy, paste) = determine_clipboard()
    return copy(text)","""""""A stub function for copy(), which will load the real copy() function when
called so that the real copy() function is used for later calls.

This allows users to import pyperclip without having determine_clipboard()
automatically run, which will automatically select a clipboard mechanism.
This could be a problem if it selects, say, the memory-heavy PyQt4 module
but the user was just going to immediately call set_clipboard() to use a
different clipboard mechanism.

The lazy loading this stub function implements gives the user a chance to
call set_clipboard() to pick another clipboard mechanism. Or, if the user
simply calls copy() or paste() without calling set_clipboard() first,
will fall back on whatever clipboard mechanism that determine_clipboard()
automatically chooses."""""""
pandas/io/clipboard/__init__.py,"def lazy_load_stub_paste():
    global copy, paste
    (copy, paste) = determine_clipboard()
    return paste()","""""""A stub function for paste(), which will load the real paste() function when
called so that the real paste() function is used for later calls.

This allows users to import pyperclip without having determine_clipboard()
automatically run, which will automatically select a clipboard mechanism.
This could be a problem if it selects, say, the memory-heavy PyQt4 module
but the user was just going to immediately call set_clipboard() to use a
different clipboard mechanism.

The lazy loading this stub function implements gives the user a chance to
call set_clipboard() to pick another clipboard mechanism. Or, if the user
simply calls copy() or paste() without calling set_clipboard() first,
will fall back on whatever clipboard mechanism that determine_clipboard()
automatically chooses."""""""
pandas/io/clipboard/__init__.py,"def waitForPaste(timeout=None):
    startTime = time.time()
    while True:
        clipboardText = paste()
        if clipboardText != '':
            return clipboardText
        time.sleep(0.01)
        if timeout is not None and time.time() > startTime + timeout:
            raise PyperclipTimeoutException('waitForPaste() timed out after ' + str(timeout) + ' seconds.')","""""""This function call blocks until a non-empty text string exists on the
clipboard. It returns this text.

This function raises PyperclipTimeoutException if timeout was set to
a number of seconds that has elapsed without non-empty text being put on
the clipboard."""""""
pandas/io/clipboard/__init__.py,"def waitForNewPaste(timeout=None):
    startTime = time.time()
    originalText = paste()
    while True:
        currentText = paste()
        if currentText != originalText:
            return currentText
        time.sleep(0.01)
        if timeout is not None and time.time() > startTime + timeout:
            raise PyperclipTimeoutException('waitForNewPaste() timed out after ' + str(timeout) + ' seconds.')","""""""This function call blocks until a new text string exists on the
clipboard that is different from the text that was there when the function
was first called. It returns this text.

This function raises PyperclipTimeoutException if timeout was set to
a number of seconds that has elapsed without non-empty text being put on
the clipboard."""""""
pandas/io/clipboards.py,"def read_clipboard(sep: str='\\s+', dtype_backend: DtypeBackend | lib.NoDefault=lib.no_default, **kwargs):
    encoding = kwargs.pop('encoding', 'utf-8')
    if encoding is not None and encoding.lower().replace('-', '') != 'utf8':
        raise NotImplementedError('reading from clipboard only supports utf-8 encoding')
    check_dtype_backend(dtype_backend)
    from pandas.io.clipboard import clipboard_get
    from pandas.io.parsers import read_csv
    text = clipboard_get()
    try:
        text = text.decode(kwargs.get('encoding') or get_option('display.encoding'))
    except AttributeError:
        pass
    lines = text[:10000].split('\n')[:-1][:10]
    counts = {x.lstrip(' ').count('\t') for x in lines}
    if len(lines) > 1 and len(counts) == 1 and (counts.pop() != 0):
        sep = '\t'
        index_length = len(lines[0]) - len(lines[0].lstrip(' \t'))
        if index_length != 0:
            kwargs.setdefault('index_col', list(range(index_length)))
    if sep is None and kwargs.get('delim_whitespace') is None:
        sep = '\\s+'
    if len(sep) > 1 and kwargs.get('engine') is None:
        kwargs['engine'] = 'python'
    elif len(sep) > 1 and kwargs.get('engine') == 'c':
        warnings.warn('read_clipboard with regex separator does not work properly with c engine.', stacklevel=find_stack_level())
    return read_csv(StringIO(text), sep=sep, dtype_backend=dtype_backend, **kwargs)","""""""Read text from clipboard and pass to :func:`~pandas.read_csv`.

Parses clipboard contents similar to how CSV files are parsed
using :func:`~pandas.read_csv`.

Parameters
----------
sep : str, default '\\s+'
    A string or regex delimiter. The default of ``'\\s+'`` denotes
    one or more whitespace characters.

dtype_backend : {'numpy_nullable', 'pyarrow'}, default 'numpy_nullable'
    Back-end data type applied to the resultant :class:`DataFrame`
    (still experimental). Behaviour is as follows:

    * ``""numpy_nullable""``: returns nullable-dtype-backed :class:`DataFrame`
      (default).
    * ``""pyarrow""``: returns pyarrow-backed nullable :class:`ArrowDtype`
      DataFrame.

    .. versionadded:: 2.0

**kwargs
    See :func:`~pandas.read_csv` for the full argument list.

Returns
-------
DataFrame
    A parsed :class:`~pandas.DataFrame` object.

See Also
--------
DataFrame.to_clipboard : Copy object to the system clipboard.
read_csv : Read a comma-separated values (csv) file into DataFrame.
read_fwf : Read a table of fixed-width formatted lines into DataFrame.

Examples
--------
>>> df = pd.DataFrame([[1, 2, 3], [4, 5, 6]], columns=['A', 'B', 'C'])
>>> df.to_clipboard()  # doctest: +SKIP
>>> pd.read_clipboard()  # doctest: +SKIP
     A  B  C
0    1  2  3
1    4  5  6"""""""
pandas/io/clipboards.py,"def to_clipboard(obj, excel: bool | None=True, sep: str | None=None, **kwargs) -> None:
    encoding = kwargs.pop('encoding', 'utf-8')
    if encoding is not None and encoding.lower().replace('-', '') != 'utf8':
        raise ValueError('clipboard only supports utf-8 encoding')
    from pandas.io.clipboard import clipboard_set
    if excel is None:
        excel = True
    if excel:
        try:
            if sep is None:
                sep = '\t'
            buf = StringIO()
            obj.to_csv(buf, sep=sep, encoding='utf-8', **kwargs)
            text = buf.getvalue()
            clipboard_set(text)
            return
        except TypeError:
            warnings.warn('to_clipboard in excel mode requires a single character separator.', stacklevel=find_stack_level())
    elif sep is not None:
        warnings.warn('to_clipboard with excel=False ignores the sep argument.', stacklevel=find_stack_level())
    if isinstance(obj, ABCDataFrame):
        with option_context('display.max_colwidth', None):
            objstr = obj.to_string(**kwargs)
    else:
        objstr = str(obj)
    clipboard_set(objstr)","""""""Attempt to write text representation of object to the system clipboard
The clipboard can be then pasted into Excel for example.

Parameters
----------
obj : the object to write to the clipboard
excel : bool, defaults to True
        if True, use the provided separator, writing in a csv
        format for allowing easy pasting into excel.
        if False, write a string representation of the object
        to the clipboard
sep : optional, defaults to tab
other keywords are passed to to_csv

Notes
-----
Requirements for your platform
  - Linux: xclip, or xsel (with PyQt4 modules)
  - Windows:
  - OS X:"""""""
pandas/io/common.py,"def is_url(url: object) -> bool:
    if not isinstance(url, str):
        return False
    return parse_url(url).scheme in _VALID_URLS","""""""Check to see if a URL has a valid protocol.

Parameters
----------
url : str or unicode

Returns
-------
isurl : bool
    If `url` has a valid protocol return True otherwise False."""""""
pandas/io/common.py,"def _expand_user(filepath_or_buffer: str | BaseBufferT) -> str | BaseBufferT:
    if isinstance(filepath_or_buffer, str):
        return os.path.expanduser(filepath_or_buffer)
    return filepath_or_buffer","""""""Return the argument with an initial component of ~ or ~user
replaced by that user's home directory.

Parameters
----------
filepath_or_buffer : object to be converted if possible

Returns
-------
expanded_filepath_or_buffer : an expanded filepath or the
                              input if not expandable"""""""
pandas/io/common.py,"def stringify_path(filepath_or_buffer: FilePath | BaseBufferT, convert_file_like: bool=False) -> str | BaseBufferT:
    if not convert_file_like and is_file_like(filepath_or_buffer):
        return cast(BaseBufferT, filepath_or_buffer)
    if isinstance(filepath_or_buffer, os.PathLike):
        filepath_or_buffer = filepath_or_buffer.__fspath__()
    return _expand_user(filepath_or_buffer)","""""""Attempt to convert a path-like object to a string.

Parameters
----------
filepath_or_buffer : object to be converted

Returns
-------
str_filepath_or_buffer : maybe a string version of the object

Notes
-----
Objects supporting the fspath protocol are coerced
according to its __fspath__ method.

Any other object is passed through unchanged, which includes bytes,
strings, buffers, or anything else that's not even path-like."""""""
pandas/io/common.py,"def urlopen(*args, **kwargs):
    import urllib.request
    return urllib.request.urlopen(*args, **kwargs)","""""""Lazy-import wrapper for stdlib urlopen, as that imports a big chunk of
the stdlib."""""""
pandas/io/common.py,"def is_fsspec_url(url: FilePath | BaseBuffer) -> bool:
    return isinstance(url, str) and bool(_RFC_3986_PATTERN.match(url)) and (not url.startswith(('http://', 'https://')))","""""""Returns true if the given URL looks like
something fsspec can handle"""""""
pandas/io/common.py,"@doc(storage_options=_shared_docs['storage_options'], compression_options=_shared_docs['compression_options'] % 'filepath_or_buffer')
def _get_filepath_or_buffer(filepath_or_buffer: FilePath | BaseBuffer, encoding: str='utf-8', compression: CompressionOptions | None=None, mode: str='r', storage_options: StorageOptions | None=None) -> IOArgs:
    filepath_or_buffer = stringify_path(filepath_or_buffer)
    (compression_method, compression) = get_compression_method(compression)
    compression_method = infer_compression(filepath_or_buffer, compression_method)
    if compression_method and hasattr(filepath_or_buffer, 'write') and ('b' not in mode):
        warnings.warn('compression has no effect when passing a non-binary object as input.', RuntimeWarning, stacklevel=find_stack_level())
        compression_method = None
    compression = dict(compression, method=compression_method)
    if 'w' in mode and compression_method in ['bz2', 'xz'] and (encoding in ['utf-16', 'utf-32']):
        warnings.warn(f'{compression} will not write the byte order mark for {encoding}', UnicodeWarning, stacklevel=find_stack_level())
    fsspec_mode = mode
    if 't' not in fsspec_mode and 'b' not in fsspec_mode:
        fsspec_mode += 'b'
    if isinstance(filepath_or_buffer, str) and is_url(filepath_or_buffer):
        storage_options = storage_options or {}
        import urllib.request
        req_info = urllib.request.Request(filepath_or_buffer, headers=storage_options)
        with urlopen(req_info) as req:
            content_encoding = req.headers.get('Content-Encoding', None)
            if content_encoding == 'gzip':
                compression = {'method': 'gzip'}
            reader = BytesIO(req.read())
        return IOArgs(filepath_or_buffer=reader, encoding=encoding, compression=compression, should_close=True, mode=fsspec_mode)
    if is_fsspec_url(filepath_or_buffer):
        assert isinstance(filepath_or_buffer, str)
        if filepath_or_buffer.startswith('s3a://'):
            filepath_or_buffer = filepath_or_buffer.replace('s3a://', 's3://')
        if filepath_or_buffer.startswith('s3n://'):
            filepath_or_buffer = filepath_or_buffer.replace('s3n://', 's3://')
        fsspec = import_optional_dependency('fsspec')
        err_types_to_retry_with_anon: list[Any] = []
        try:
            import_optional_dependency('botocore')
            from botocore.exceptions import ClientError, NoCredentialsError
            err_types_to_retry_with_anon = [ClientError, NoCredentialsError, PermissionError]
        except ImportError:
            pass
        try:
            file_obj = fsspec.open(filepath_or_buffer, mode=fsspec_mode, **storage_options or {}).open()
        except tuple(err_types_to_retry_with_anon):
            if storage_options is None:
                storage_options = {'anon': True}
            else:
                storage_options = dict(storage_options)
                storage_options['anon'] = True
            file_obj = fsspec.open(filepath_or_buffer, mode=fsspec_mode, **storage_options or {}).open()
        return IOArgs(filepath_or_buffer=file_obj, encoding=encoding, compression=compression, should_close=True, mode=fsspec_mode)
    elif storage_options:
        raise ValueError('storage_options passed with file object or non-fsspec file path')
    if isinstance(filepath_or_buffer, (str, bytes, mmap.mmap)):
        return IOArgs(filepath_or_buffer=_expand_user(filepath_or_buffer), encoding=encoding, compression=compression, should_close=False, mode=mode)
    if not (hasattr(filepath_or_buffer, 'read') or hasattr(filepath_or_buffer, 'write')):
        msg = f'Invalid file path or buffer object type: {type(filepath_or_buffer)}'
        raise ValueError(msg)
    return IOArgs(filepath_or_buffer=filepath_or_buffer, encoding=encoding, compression=compression, should_close=False, mode=mode)","""""""If the filepath_or_buffer is a url, translate and return the buffer.
Otherwise passthrough.

Parameters
----------
filepath_or_buffer : a url, filepath (str, py.path.local or pathlib.Path),
                     or buffer
{compression_options}

    .. versionchanged:: 1.4.0 Zstandard support.

encoding : the encoding to use to decode bytes, default is 'utf-8'
mode : str, optional

{storage_options}

    .. versionadded:: 1.2.0

..versionchange:: 1.2.0

  Returns the dataclass IOArgs."""""""
pandas/io/common.py,"def file_path_to_url(path: str) -> str:
    from urllib.request import pathname2url
    return urljoin('file:', pathname2url(path))","""""""converts an absolute native path to a FILE URL.

Parameters
----------
path : a path in native format

Returns
-------
a valid FILE URL"""""""
pandas/io/common.py,"def get_compression_method(compression: CompressionOptions) -> tuple[str | None, CompressionDict]:
    compression_method: str | None
    if isinstance(compression, Mapping):
        compression_args = dict(compression)
        try:
            compression_method = compression_args.pop('method')
        except KeyError as err:
            raise ValueError(""If mapping, compression must have key 'method'"") from err
    else:
        compression_args = {}
        compression_method = compression
    return (compression_method, compression_args)","""""""Simplifies a compression argument to a compression method string and
a mapping containing additional arguments.

Parameters
----------
compression : str or mapping
    If string, specifies the compression method. If mapping, value at key
    'method' specifies compression method.

Returns
-------
tuple of ({compression method}, Optional[str]
          {compression arguments}, Dict[str, Any])

Raises
------
ValueError on mapping missing 'method' key"""""""
pandas/io/common.py,"@doc(compression_options=_shared_docs['compression_options'] % 'filepath_or_buffer')
def infer_compression(filepath_or_buffer: FilePath | BaseBuffer, compression: str | None) -> str | None:
    if compression is None:
        return None
    if compression == 'infer':
        filepath_or_buffer = stringify_path(filepath_or_buffer, convert_file_like=True)
        if not isinstance(filepath_or_buffer, str):
            return None
        for (extension, compression) in extension_to_compression.items():
            if filepath_or_buffer.lower().endswith(extension):
                return compression
        return None
    if compression in _supported_compressions:
        return compression
    valid = ['infer', None] + sorted(_supported_compressions)
    msg = f'Unrecognized compression type: {compression}\nValid compression types are {valid}'
    raise ValueError(msg)","""""""Get the compression method for filepath_or_buffer. If compression='infer',
the inferred compression method is returned. Otherwise, the input
compression method is returned unchanged, unless it's invalid, in which
case an error is raised.

Parameters
----------
filepath_or_buffer : str or file handle
    File path or object.
{compression_options}

    .. versionchanged:: 1.4.0 Zstandard support.

Returns
-------
string or None

Raises
------
ValueError on invalid compression specified."""""""
pandas/io/common.py,"def check_parent_directory(path: Path | str) -> None:
    parent = Path(path).parent
    if not parent.is_dir():
        raise OSError(f""Cannot save file into a non-existent directory: '{parent}'"")","""""""Check if parent directory of a file exists, raise OSError if it does not

Parameters
----------
path: Path or str
    Path to check parent directory of"""""""
pandas/io/common.py,"@doc(compression_options=_shared_docs['compression_options'] % 'path_or_buf')
def get_handle(path_or_buf: FilePath | BaseBuffer, mode: str, *, encoding: str | None=None, compression: CompressionOptions | None=None, memory_map: bool=False, is_text: bool=True, errors: str | None=None, storage_options: StorageOptions | None=None) -> IOHandles[str] | IOHandles[bytes]:
    encoding = encoding or 'utf-8'
    errors = errors or 'strict'
    if _is_binary_mode(path_or_buf, mode) and 'b' not in mode:
        mode += 'b'
    codecs.lookup(encoding)
    if isinstance(errors, str):
        codecs.lookup_error(errors)
    ioargs = _get_filepath_or_buffer(path_or_buf, encoding=encoding, compression=compression, mode=mode, storage_options=storage_options)
    handle = ioargs.filepath_or_buffer
    handles: list[BaseBuffer]
    (handle, memory_map, handles) = _maybe_memory_map(handle, memory_map)
    is_path = isinstance(handle, str)
    compression_args = dict(ioargs.compression)
    compression = compression_args.pop('method')
    if 'r' not in mode and is_path:
        check_parent_directory(str(handle))
    if compression:
        if compression != 'zstd':
            ioargs.mode = ioargs.mode.replace('t', '')
        elif compression == 'zstd' and 'b' not in ioargs.mode:
            ioargs.mode += 'b'
        if compression == 'gzip':
            if isinstance(handle, str):
                handle = gzip.GzipFile(filename=handle, mode=ioargs.mode, **compression_args)
            else:
                handle = gzip.GzipFile(fileobj=handle, mode=ioargs.mode, **compression_args)
        elif compression == 'bz2':
            handle = get_bz2_file()(handle, mode=ioargs.mode, **compression_args)
        elif compression == 'zip':
            handle = _BytesZipFile(handle, ioargs.mode, **compression_args)
            if handle.buffer.mode == 'r':
                handles.append(handle)
                zip_names = handle.buffer.namelist()
                if len(zip_names) == 1:
                    handle = handle.buffer.open(zip_names.pop())
                elif not zip_names:
                    raise ValueError(f'Zero files found in ZIP file {path_or_buf}')
                else:
                    raise ValueError(f'Multiple files found in ZIP file. Only one file per ZIP: {zip_names}')
        elif compression == 'tar':
            compression_args.setdefault('mode', ioargs.mode)
            if isinstance(handle, str):
                handle = _BytesTarFile(name=handle, **compression_args)
            else:
                handle = _BytesTarFile(fileobj=handle, **compression_args)
            assert isinstance(handle, _BytesTarFile)
            if 'r' in handle.buffer.mode:
                handles.append(handle)
                files = handle.buffer.getnames()
                if len(files) == 1:
                    file = handle.buffer.extractfile(files[0])
                    assert file is not None
                    handle = file
                elif not files:
                    raise ValueError(f'Zero files found in TAR archive {path_or_buf}')
                else:
                    raise ValueError(f'Multiple files found in TAR archive. Only one file per TAR archive: {files}')
        elif compression == 'xz':
            handle = get_lzma_file()(handle, ioargs.mode, **compression_args)
        elif compression == 'zstd':
            zstd = import_optional_dependency('zstandard')
            if 'r' in ioargs.mode:
                open_args = {'dctx': zstd.ZstdDecompressor(**compression_args)}
            else:
                open_args = {'cctx': zstd.ZstdCompressor(**compression_args)}
            handle = zstd.open(handle, mode=ioargs.mode, **open_args)
        else:
            msg = f'Unrecognized compression type: {compression}'
            raise ValueError(msg)
        assert not isinstance(handle, str)
        handles.append(handle)
    elif isinstance(handle, str):
        if ioargs.encoding and 'b' not in ioargs.mode:
            handle = open(handle, ioargs.mode, encoding=ioargs.encoding, errors=errors, newline='')
        else:
            handle = open(handle, ioargs.mode)
        handles.append(handle)
    is_wrapped = False
    if not is_text and ioargs.mode == 'rb' and isinstance(handle, TextIOBase):
        handle = _BytesIOWrapper(handle, encoding=ioargs.encoding)
    elif is_text and (compression or memory_map or _is_binary_mode(handle, ioargs.mode)):
        if not hasattr(handle, 'readable') or not hasattr(handle, 'writable') or (not hasattr(handle, 'seekable')):
            handle = _IOWrapper(handle)
        handle = TextIOWrapper(handle, encoding=ioargs.encoding, errors=errors, newline='')
        handles.append(handle)
        is_wrapped = not (isinstance(ioargs.filepath_or_buffer, str) or ioargs.should_close)
    if 'r' in ioargs.mode and (not hasattr(handle, 'read')):
        raise TypeError(f'Expected file path name or file-like object, got {type(ioargs.filepath_or_buffer)} type')
    handles.reverse()
    if ioargs.should_close:
        assert not isinstance(ioargs.filepath_or_buffer, str)
        handles.append(ioargs.filepath_or_buffer)
    return IOHandles(handle=handle, created_handles=handles, is_wrapped=is_wrapped, compression=ioargs.compression)","""""""Get file handle for given path/buffer and mode.

Parameters
----------
path_or_buf : str or file handle
    File path or object.
mode : str
    Mode to open path_or_buf with.
encoding : str or None
    Encoding to use.
{compression_options}

       May be a dict with key 'method' as compression mode
       and other keys as compression options if compression
       mode is 'zip'.

       Passing compression options as keys in dict is
       supported for compression modes 'gzip', 'bz2', 'zstd' and 'zip'.

    .. versionchanged:: 1.4.0 Zstandard support.

memory_map : bool, default False
    See parsers._parser_params for more information. Only used by read_csv.
is_text : bool, default True
    Whether the type of the content passed to the file/buffer is string or
    bytes. This is not the same as `""b"" not in mode`. If a string content is
    passed to a binary file/buffer, a wrapper is inserted.
errors : str, default 'strict'
    Specifies how encoding and decoding errors are to be handled.
    See the errors argument for :func:`open` for a full list
    of options.
storage_options: StorageOptions = None
    Passed to _get_filepath_or_buffer

.. versionchanged:: 1.2.0

Returns the dataclass IOHandles"""""""
pandas/io/common.py,"def _maybe_memory_map(handle: str | BaseBuffer, memory_map: bool) -> tuple[str | BaseBuffer, bool, list[BaseBuffer]]:
    handles: list[BaseBuffer] = []
    memory_map &= hasattr(handle, 'fileno') or isinstance(handle, str)
    if not memory_map:
        return (handle, memory_map, handles)
    handle = cast(ReadCsvBuffer, handle)
    if isinstance(handle, str):
        handle = open(handle, 'rb')
        handles.append(handle)
    try:
        wrapped = _IOWrapper(mmap.mmap(handle.fileno(), 0, access=mmap.ACCESS_READ))
    finally:
        for handle in reversed(handles):
            handle.close()
    return (wrapped, memory_map, [wrapped])","""""""Try to memory map file/buffer."""""""
pandas/io/common.py,"def file_exists(filepath_or_buffer: FilePath | BaseBuffer) -> bool:
    exists = False
    filepath_or_buffer = stringify_path(filepath_or_buffer)
    if not isinstance(filepath_or_buffer, str):
        return exists
    try:
        exists = os.path.exists(filepath_or_buffer)
    except (TypeError, ValueError):
        pass
    return exists","""""""Test whether file exists."""""""
pandas/io/common.py,"def _is_binary_mode(handle: FilePath | BaseBuffer, mode: str) -> bool:
    if 't' in mode or 'b' in mode:
        return 'b' in mode
    text_classes = (codecs.StreamWriter, codecs.StreamReader, codecs.StreamReaderWriter)
    if issubclass(type(handle), text_classes):
        return False
    return isinstance(handle, _get_binary_io_classes()) or 'b' in getattr(handle, 'mode', mode)","""""""Whether the handle is opened in binary mode"""""""
pandas/io/common.py,"@functools.lru_cache
def _get_binary_io_classes() -> tuple[type, ...]:
    binary_classes: tuple[type, ...] = (BufferedIOBase, RawIOBase)
    zstd = import_optional_dependency('zstandard', errors='ignore')
    if zstd is not None:
        with zstd.ZstdDecompressor().stream_reader(b'') as reader:
            binary_classes += (type(reader),)
    return binary_classes","""""""IO classes that that expect bytes"""""""
pandas/io/common.py,"def is_potential_multi_index(columns: Sequence[Hashable] | MultiIndex, index_col: bool | Sequence[int] | None=None) -> bool:
    if index_col is None or isinstance(index_col, bool):
        index_col = []
    return bool(len(columns) and (not isinstance(columns, MultiIndex)) and all((isinstance(c, tuple) for c in columns if c not in list(index_col))))","""""""Check whether or not the `columns` parameter
could be converted into a MultiIndex.

Parameters
----------
columns : array-like
    Object which may or may not be convertible into a MultiIndex
index_col : None, bool or list, optional
    Column or columns to use as the (possibly hierarchical) index

Returns
-------
bool : Whether or not columns could become a MultiIndex"""""""
pandas/io/common.py,"def dedup_names(names: Sequence[Hashable], is_potential_multiindex: bool) -> Sequence[Hashable]:
    names = list(names)
    counts: DefaultDict[Hashable, int] = defaultdict(int)
    for (i, col) in enumerate(names):
        cur_count = counts[col]
        while cur_count > 0:
            counts[col] = cur_count + 1
            if is_potential_multiindex:
                assert isinstance(col, tuple)
                col = col[:-1] + (f'{col[-1]}.{cur_count}',)
            else:
                col = f'{col}.{cur_count}'
            cur_count = counts[col]
        names[i] = col
        counts[col] = cur_count + 1
    return names","""""""Rename column names if duplicates exist.

Currently the renaming is done by appending a period and an autonumeric,
but a custom pattern may be supported in the future.

Examples
--------
>>> dedup_names([""x"", ""y"", ""x"", ""x""], is_potential_multiindex=False)
['x', 'y', 'x.1', 'x.2']"""""""
pandas/io/excel/_base.py,"@doc(storage_options=_shared_docs['storage_options'])
def inspect_excel_format(content_or_path: FilePath | ReadBuffer[bytes], storage_options: StorageOptions | None=None) -> str | None:
    if isinstance(content_or_path, bytes):
        content_or_path = BytesIO(content_or_path)
    with get_handle(content_or_path, 'rb', storage_options=storage_options, is_text=False) as handle:
        stream = handle.handle
        stream.seek(0)
        buf = stream.read(PEEK_SIZE)
        if buf is None:
            raise ValueError('stream is empty')
        assert isinstance(buf, bytes)
        peek = buf
        stream.seek(0)
        if any((peek.startswith(sig) for sig in XLS_SIGNATURES)):
            return 'xls'
        elif not peek.startswith(ZIP_SIGNATURE):
            return None
        with zipfile.ZipFile(stream) as zf:
            component_names = [name.replace('\\', '/').lower() for name in zf.namelist()]
        if 'xl/workbook.xml' in component_names:
            return 'xlsx'
        if 'xl/workbook.bin' in component_names:
            return 'xlsb'
        if 'content.xml' in component_names:
            return 'ods'
        return 'zip'","""""""Inspect the path or content of an excel file and get its format.

Adopted from xlrd: https://github.com/python-excel/xlrd.

Parameters
----------
content_or_path : str or file-like object
    Path to file or content of file to inspect. May be a URL.
{storage_options}

Returns
-------
str or None
    Format of file if it can be determined.

Raises
------
ValueError
    If resulting stream is empty.
BadZipFile
    If resulting stream does not have an XLS signature and is not a valid zipfile."""""""
pandas/io/excel/_util.py,"def register_writer(klass: ExcelWriter_t) -> None:
    if not callable(klass):
        raise ValueError('Can only register callables as engines')
    engine_name = klass._engine
    _writers[engine_name] = klass","""""""Add engine to the excel writer registry.io.excel.

You must use this method to integrate with ``to_excel``.

Parameters
----------
klass : ExcelWriter"""""""
pandas/io/excel/_util.py,"def get_default_engine(ext: str, mode: Literal['reader', 'writer']='reader') -> str:
    _default_readers = {'xlsx': 'openpyxl', 'xlsm': 'openpyxl', 'xlsb': 'pyxlsb', 'xls': 'xlrd', 'ods': 'odf'}
    _default_writers = {'xlsx': 'openpyxl', 'xlsm': 'openpyxl', 'xlsb': 'pyxlsb', 'ods': 'odf'}
    assert mode in ['reader', 'writer']
    if mode == 'writer':
        xlsxwriter = import_optional_dependency('xlsxwriter', errors='warn')
        if xlsxwriter:
            _default_writers['xlsx'] = 'xlsxwriter'
        return _default_writers[ext]
    else:
        return _default_readers[ext]","""""""Return the default reader/writer for the given extension.

Parameters
----------
ext : str
    The excel file extension for which to get the default engine.
mode : str {'reader', 'writer'}
    Whether to get the default engine for reading or writing.
    Either 'reader' or 'writer'

Returns
-------
str
    The default engine for the extension."""""""
pandas/io/excel/_util.py,"def _excel2num(x: str) -> int:
    index = 0
    for c in x.upper().strip():
        cp = ord(c)
        if cp < ord('A') or cp > ord('Z'):
            raise ValueError(f'Invalid column name: {x}')
        index = index * 26 + cp - ord('A') + 1
    return index - 1","""""""Convert Excel column name like 'AB' to 0-based column index.

Parameters
----------
x : str
    The Excel column name to convert to a 0-based column index.

Returns
-------
num : int
    The column index corresponding to the name.

Raises
------
ValueError
    Part of the Excel column name was invalid."""""""
pandas/io/excel/_util.py,"def _range2cols(areas: str) -> list[int]:
    cols: list[int] = []
    for rng in areas.split(','):
        if ':' in rng:
            rngs = rng.split(':')
            cols.extend(range(_excel2num(rngs[0]), _excel2num(rngs[1]) + 1))
        else:
            cols.append(_excel2num(rng))
    return cols","""""""Convert comma separated list of column names and ranges to indices.

Parameters
----------
areas : str
    A string containing a sequence of column ranges (or areas).

Returns
-------
cols : list
    A list of 0-based column indices.

Examples
--------
>>> _range2cols('A:E')
[0, 1, 2, 3, 4]
>>> _range2cols('A,C,Z:AB')
[0, 2, 25, 26, 27]"""""""
pandas/io/excel/_util.py,"def maybe_convert_usecols(usecols: str | list[int] | list[str] | usecols_func | None) -> None | list[int] | list[str] | usecols_func:
    if usecols is None:
        return usecols
    if is_integer(usecols):
        raise ValueError('Passing an integer for `usecols` is no longer supported.  Please pass in a list of int from 0 to `usecols` inclusive instead.')
    if isinstance(usecols, str):
        return _range2cols(usecols)
    return usecols","""""""Convert `usecols` into a compatible format for parsing in `parsers.py`.

Parameters
----------
usecols : object
    The use-columns object to potentially convert.

Returns
-------
converted : object
    The compatible format of `usecols`."""""""
pandas/io/excel/_util.py,"def fill_mi_header(row: list[Hashable], control_row: list[bool]) -> tuple[list[Hashable], list[bool]]:
    last = row[0]
    for i in range(1, len(row)):
        if not control_row[i]:
            last = row[i]
        if row[i] == '' or row[i] is None:
            row[i] = last
        else:
            control_row[i] = False
            last = row[i]
    return (row, control_row)","""""""Forward fill blank entries in row but only inside the same parent index.

Used for creating headers in Multiindex.

Parameters
----------
row : list
    List of items in a single row.
control_row : list of bool
    Helps to determine if particular column is in same parent index as the
    previous value. Used to stop propagation of empty cells between
    different indexes.

Returns
-------
Returns changed row and control_row"""""""
pandas/io/excel/_util.py,"def pop_header_name(row: list[Hashable], index_col: int | Sequence[int]) -> tuple[Hashable | None, list[Hashable]]:
    if is_list_like(index_col):
        assert isinstance(index_col, Iterable)
        i = max(index_col)
    else:
        assert not isinstance(index_col, Iterable)
        i = index_col
    header_name = row[i]
    header_name = None if header_name == '' else header_name
    return (header_name, row[:i] + [''] + row[i + 1:])","""""""Pop the header name for MultiIndex parsing.

Parameters
----------
row : list
    The data row to parse for the header name.
index_col : int, list
    The index columns for our data. Assumed to be non-null.

Returns
-------
header_name : str
    The extracted header name.
trimmed_row : list
    The original data row with the header name removed."""""""
pandas/io/excel/_util.py,"def combine_kwargs(engine_kwargs: dict[str, Any] | None, kwargs: dict) -> dict:
    if engine_kwargs is None:
        result = {}
    else:
        result = engine_kwargs.copy()
    result.update(kwargs)
    return result","""""""Used to combine two sources of kwargs for the backend engine.

Use of kwargs is deprecated, this function is solely for use in 1.3 and should
be removed in 1.4/2.0. Also _base.ExcelWriter.__new__ ensures either engine_kwargs
or kwargs must be None or empty respectively.

Parameters
----------
engine_kwargs: dict
    kwargs to be passed through to the engine.
kwargs: dict
    kwargs to be psased through to the engine (deprecated)

Returns
-------
engine_kwargs combined with kwargs"""""""
pandas/io/feather_format.py,"@doc(storage_options=_shared_docs['storage_options'])
def to_feather(df: DataFrame, path: FilePath | WriteBuffer[bytes], storage_options: StorageOptions | None=None, **kwargs: Any) -> None:
    import_optional_dependency('pyarrow')
    from pyarrow import feather
    if not isinstance(df, DataFrame):
        raise ValueError('feather only support IO with DataFrames')
    with get_handle(path, 'wb', storage_options=storage_options, is_text=False) as handles:
        feather.write_feather(df, handles.handle, **kwargs)","""""""Write a DataFrame to the binary Feather format.

Parameters
----------
df : DataFrame
path : str, path object, or file-like object
{storage_options}

    .. versionadded:: 1.2.0

**kwargs :
    Additional keywords passed to `pyarrow.feather.write_feather`."""""""
pandas/io/feather_format.py,"@doc(storage_options=_shared_docs['storage_options'])
def read_feather(path: FilePath | ReadBuffer[bytes], columns: Sequence[Hashable] | None=None, use_threads: bool=True, storage_options: StorageOptions | None=None, dtype_backend: DtypeBackend | lib.NoDefault=lib.no_default) -> DataFrame:
    import_optional_dependency('pyarrow')
    from pyarrow import feather
    check_dtype_backend(dtype_backend)
    with get_handle(path, 'rb', storage_options=storage_options, is_text=False) as handles:
        if dtype_backend is lib.no_default and (not using_pyarrow_string_dtype()):
            return feather.read_feather(handles.handle, columns=columns, use_threads=bool(use_threads))
        pa_table = feather.read_table(handles.handle, columns=columns, use_threads=bool(use_threads))
        if dtype_backend == 'numpy_nullable':
            from pandas.io._util import _arrow_dtype_mapping
            return pa_table.to_pandas(types_mapper=_arrow_dtype_mapping().get)
        elif dtype_backend == 'pyarrow':
            return pa_table.to_pandas(types_mapper=pd.ArrowDtype)
        elif using_pyarrow_string_dtype():
            return pa_table.to_pandas(types_mapper=arrow_string_types_mapper())
        else:
            raise NotImplementedError","""""""Load a feather-format object from the file path.

Parameters
----------
path : str, path object, or file-like object
    String, path object (implementing ``os.PathLike[str]``), or file-like
    object implementing a binary ``read()`` function. The string could be a URL.
    Valid URL schemes include http, ftp, s3, and file. For file URLs, a host is
    expected. A local file could be: ``file://localhost/path/to/table.feather``.
columns : sequence, default None
    If not provided, all columns are read.
use_threads : bool, default True
    Whether to parallelize reading using multiple threads.
{storage_options}

    .. versionadded:: 1.2.0

dtype_backend : {{'numpy_nullable', 'pyarrow'}}, default 'numpy_nullable'
    Back-end data type applied to the resultant :class:`DataFrame`
    (still experimental). Behaviour is as follows:

    * ``""numpy_nullable""``: returns nullable-dtype-backed :class:`DataFrame`
      (default).
    * ``""pyarrow""``: returns pyarrow-backed nullable :class:`ArrowDtype`
      DataFrame.

    .. versionadded:: 2.0

Returns
-------
type of object stored in file

Examples
--------
>>> df = pd.read_feather(""path/to/file.feather"")  # doctest: +SKIP"""""""
pandas/io/formats/console.py,"def get_console_size() -> tuple[int | None, int | None]:
    from pandas import get_option
    display_width = get_option('display.width')
    display_height = get_option('display.max_rows')
    if in_interactive_session():
        if in_ipython_frontend():
            from pandas._config.config import get_default_val
            terminal_width = get_default_val('display.width')
            terminal_height = get_default_val('display.max_rows')
        else:
            (terminal_width, terminal_height) = get_terminal_size()
    else:
        (terminal_width, terminal_height) = (None, None)
    return (display_width or terminal_width, display_height or terminal_height)","""""""Return console size as tuple = (width, height).

Returns (None,None) in non-interactive session."""""""
pandas/io/formats/console.py,"def in_interactive_session() -> bool:
    from pandas import get_option

    def check_main():
        try:
            import __main__ as main
        except ModuleNotFoundError:
            return get_option('mode.sim_interactive')
        return not hasattr(main, '__file__') or get_option('mode.sim_interactive')
    try:
        return __IPYTHON__ or check_main()
    except NameError:
        return check_main()","""""""Check if we're running in an interactive shell.

Returns
-------
bool
    True if running under python/ipython interactive shell."""""""
pandas/io/formats/console.py,"def in_ipython_frontend() -> bool:
    try:
        ip = get_ipython()
        return 'zmq' in str(type(ip)).lower()
    except NameError:
        pass
    return False","""""""Check if we're inside an IPython zmq frontend.

Returns
-------
bool"""""""
pandas/io/formats/css.py,"def _side_expander(prop_fmt: str) -> Callable:

    def expand(self, prop, value: str) -> Generator[tuple[str, str], None, None]:
        """"""
        Expand shorthand property into side-specific property (top, right, bottom, left)

        Parameters
        ----------
            prop (str): CSS property name
            value (str): String token for property

        Yields
        ------
            Tuple (str, str): Expanded property, value
        """"""
        tokens = value.split()
        try:
            mapping = self.SIDE_SHORTHANDS[len(tokens)]
        except KeyError:
            warnings.warn(f'Could not expand ""{prop}: {value}""', CSSWarning, stacklevel=find_stack_level())
            return
        for (key, idx) in zip(self.SIDES, mapping):
            yield (prop_fmt.format(key), tokens[idx])
    return expand","""""""Wrapper to expand shorthand property into top, right, bottom, left properties

Parameters
----------
side : str
    The border side to expand into properties

Returns
-------
    function: Return to call when a 'border(-{side}): {value}' string is encountered"""""""
pandas/io/formats/css.py,"def _border_expander(side: str='') -> Callable:
    if side != '':
        side = f'-{side}'

    def expand(self, prop, value: str) -> Generator[tuple[str, str], None, None]:
        """"""
        Expand border into color, style, and width tuples

        Parameters
        ----------
            prop : str
                CSS property name passed to styler
            value : str
                Value passed to styler for property

        Yields
        ------
            Tuple (str, str): Expanded property, value
        """"""
        tokens = value.split()
        if len(tokens) == 0 or len(tokens) > 3:
            warnings.warn(f'Too many tokens provided to ""{prop}"" (expected 1-3)', CSSWarning, stacklevel=find_stack_level())
        border_declarations = {f'border{side}-color': 'black', f'border{side}-style': 'none', f'border{side}-width': 'medium'}
        for token in tokens:
            if token.lower() in self.BORDER_STYLES:
                border_declarations[f'border{side}-style'] = token
            elif any((ratio in token.lower() for ratio in self.BORDER_WIDTH_RATIOS)):
                border_declarations[f'border{side}-width'] = token
            else:
                border_declarations[f'border{side}-color'] = token
        yield from self.atomize(border_declarations.items())
    return expand","""""""Wrapper to expand 'border' property into border color, style, and width properties

Parameters
----------
side : str
    The border side to expand into properties

Returns
-------
    function: Return to call when a 'border(-{side}): {value}' string is encountered"""""""
pandas/io/formats/format.py,"def get_dataframe_repr_params() -> dict[str, Any]:
    from pandas.io.formats import console
    if get_option('display.expand_frame_repr'):
        (line_width, _) = console.get_console_size()
    else:
        line_width = None
    return {'max_rows': get_option('display.max_rows'), 'min_rows': get_option('display.min_rows'), 'max_cols': get_option('display.max_columns'), 'max_colwidth': get_option('display.max_colwidth'), 'show_dimensions': get_option('display.show_dimensions'), 'line_width': line_width}","""""""Get the parameters used to repr(dataFrame) calls using DataFrame.to_string.

Supplying these parameters to DataFrame.to_string is equivalent to calling
``repr(DataFrame)``. This is useful if you want to adjust the repr output.

.. versionadded:: 1.4.0

Example
-------
>>> import pandas as pd
>>>
>>> df = pd.DataFrame([[1, 2], [3, 4]])
>>> repr_params = pd.io.formats.format.get_dataframe_repr_params()
>>> repr(df) == df.to_string(**repr_params)
True"""""""
pandas/io/formats/format.py,"def get_series_repr_params() -> dict[str, Any]:
    (width, height) = get_terminal_size()
    max_rows = height if get_option('display.max_rows') == 0 else get_option('display.max_rows')
    min_rows = height if get_option('display.max_rows') == 0 else get_option('display.min_rows')
    return {'name': True, 'dtype': True, 'min_rows': min_rows, 'max_rows': max_rows, 'length': get_option('display.show_dimensions')}","""""""Get the parameters used to repr(Series) calls using Series.to_string.

Supplying these parameters to Series.to_string is equivalent to calling
``repr(series)``. This is useful if you want to adjust the series repr output.

.. versionadded:: 1.4.0

Example
-------
>>> import pandas as pd
>>>
>>> ser = pd.Series([1, 2, 3, 4])
>>> repr_params = pd.io.formats.format.get_series_repr_params()
>>> repr(ser) == ser.to_string(**repr_params)
True"""""""
pandas/io/formats/format.py,"def save_to_buffer(string: str, buf: FilePath | WriteBuffer[str] | None=None, encoding: str | None=None) -> str | None:
    with get_buffer(buf, encoding=encoding) as f:
        f.write(string)
        if buf is None:
            return f.getvalue()
        return None","""""""Perform serialization. Write to buf or return as string if buf is None."""""""
pandas/io/formats/format.py,"@contextmanager
def get_buffer(buf: FilePath | WriteBuffer[str] | None, encoding: str | None=None) -> Generator[WriteBuffer[str], None, None] | Generator[StringIO, None, None]:
    if buf is not None:
        buf = stringify_path(buf)
    else:
        buf = StringIO()
    if encoding is None:
        encoding = 'utf-8'
    elif not isinstance(buf, str):
        raise ValueError('buf is not a file name and encoding is specified.')
    if hasattr(buf, 'write'):
        yield buf
    elif isinstance(buf, str):
        check_parent_directory(str(buf))
        with open(buf, 'w', encoding=encoding, newline='') as f:
            yield f
    else:
        raise TypeError('buf is not a file name and it has no write method')","""""""Context manager to open, yield and close buffer for filenames or Path-like
objects, otherwise yield buf unchanged."""""""
pandas/io/formats/format.py,"def format_array(values: ArrayLike, formatter: Callable | None, float_format: FloatFormatType | None=None, na_rep: str='NaN', digits: int | None=None, space: str | int | None=None, justify: str='right', decimal: str='.', leading_space: bool | None=True, quoting: int | None=None, fallback_formatter: Callable | None=None) -> list[str]:
    fmt_klass: type[GenericArrayFormatter]
    if lib.is_np_dtype(values.dtype, 'M'):
        fmt_klass = Datetime64Formatter
        values = cast(DatetimeArray, values)
    elif isinstance(values.dtype, DatetimeTZDtype):
        fmt_klass = Datetime64TZFormatter
        values = cast(DatetimeArray, values)
    elif lib.is_np_dtype(values.dtype, 'm'):
        fmt_klass = Timedelta64Formatter
        values = cast(TimedeltaArray, values)
    elif isinstance(values.dtype, ExtensionDtype):
        fmt_klass = ExtensionArrayFormatter
    elif lib.is_np_dtype(values.dtype, 'fc'):
        fmt_klass = FloatArrayFormatter
    elif lib.is_np_dtype(values.dtype, 'iu'):
        fmt_klass = IntArrayFormatter
    else:
        fmt_klass = GenericArrayFormatter
    if space is None:
        space = 12
    if float_format is None:
        float_format = get_option('display.float_format')
    if digits is None:
        digits = get_option('display.precision')
    fmt_obj = fmt_klass(values, digits=digits, na_rep=na_rep, float_format=float_format, formatter=formatter, space=space, justify=justify, decimal=decimal, leading_space=leading_space, quoting=quoting, fallback_formatter=fallback_formatter)
    return fmt_obj.get_result()","""""""Format an array for printing.

Parameters
----------
values : np.ndarray or ExtensionArray
formatter
float_format
na_rep
digits
space
justify
decimal
leading_space : bool, optional, default True
    Whether the array should be formatted with a leading space.
    When an array as a column of a Series or DataFrame, we do want
    the leading space to pad between columns.

    When formatting an Index subclass
    (e.g. IntervalIndex._format_native_types), we don't want the
    leading space since it should be left-aligned.
fallback_formatter

Returns
-------
List[str]"""""""
pandas/io/formats/format.py,"def format_percentiles(percentiles: np.ndarray | Sequence[float]) -> list[str]:
    percentiles = np.asarray(percentiles)
    if not is_numeric_dtype(percentiles) or not np.all(percentiles >= 0) or (not np.all(percentiles <= 1)):
        raise ValueError('percentiles should all be in the interval [0,1]')
    percentiles = 100 * percentiles
    percentiles_round_type = percentiles.round().astype(int)
    int_idx = np.isclose(percentiles_round_type, percentiles)
    if np.all(int_idx):
        out = percentiles_round_type.astype(str)
        return [i + '%' for i in out]
    unique_pcts = np.unique(percentiles)
    to_begin = unique_pcts[0] if unique_pcts[0] > 0 else None
    to_end = 100 - unique_pcts[-1] if unique_pcts[-1] < 100 else None
    prec = -np.floor(np.log10(np.min(np.ediff1d(unique_pcts, to_begin=to_begin, to_end=to_end)))).astype(int)
    prec = max(1, prec)
    out = np.empty_like(percentiles, dtype=object)
    out[int_idx] = percentiles[int_idx].round().astype(int).astype(str)
    out[~int_idx] = percentiles[~int_idx].round(prec).astype(str)
    return [i + '%' for i in out]","""""""Outputs rounded and formatted percentiles.

Parameters
----------
percentiles : list-like, containing floats from interval [0,1]

Returns
-------
formatted : list of strings

Notes
-----
Rounding precision is chosen so that: (1) if any two elements of
``percentiles`` differ, they remain different after rounding
(2) no entry is *rounded* to 0% or 100%.
Any non-integer is always rounded to at least 1 decimal place.

Examples
--------
Keeps all entries different after rounding:

>>> format_percentiles([0.01999, 0.02001, 0.5, 0.666666, 0.9999])
['1.999%', '2.001%', '50%', '66.667%', '99.99%']

No element is rounded to 0% or 100% (unless already equal to it).
Duplicates are allowed:

>>> format_percentiles([0, 0.5, 0.02001, 0.5, 0.666666, 0.9999])
['0%', '50%', '2.0%', '50%', '66.67%', '99.99%']"""""""
pandas/io/formats/format.py,"def get_format_datetime64(is_dates_only_: bool, nat_rep: str='NaT', date_format: str | None=None) -> Callable:
    if is_dates_only_:
        return lambda x: _format_datetime64_dateonly(x, nat_rep=nat_rep, date_format=date_format)
    else:
        return lambda x: _format_datetime64(x, nat_rep=nat_rep)","""""""Return a formatter callable taking a datetime64 as input and providing
a string as output"""""""
pandas/io/formats/format.py,"def get_format_datetime64_from_values(values: DatetimeArray, date_format: str | None) -> str | None:
    assert isinstance(values, DatetimeArray)
    ido = is_dates_only(values)
    if ido:
        return date_format or '%Y-%m-%d'
    return date_format","""""""given values and a date_format, return a string format"""""""
pandas/io/formats/format.py,"def get_format_timedelta64(values: TimedeltaArray, nat_rep: str | float='NaT', box: bool=False) -> Callable:
    values_int = values.view(np.int64)
    values_int = cast('npt.NDArray[np.int64]', values_int)
    consider_values = values_int != iNaT
    one_day_nanos = 86400 * 10 ** 9
    not_midnight = values_int % one_day_nanos != 0
    both = np.logical_and(consider_values, not_midnight)
    even_days = both.sum() == 0
    if even_days:
        format = None
    else:
        format = 'long'

    def _formatter(x):
        if x is None or (is_scalar(x) and isna(x)):
            return nat_rep
        if not isinstance(x, Timedelta):
            x = Timedelta(x)
        result = x._repr_base(format=format)
        if box:
            result = f""'{result}'""
        return result
    return _formatter","""""""Return a formatter function for a range of timedeltas.
These will all have the same format argument

If box, then show the return in quotes"""""""
pandas/io/formats/format.py,"def _trim_zeros_complex(str_complexes: ArrayLike, decimal: str='.') -> list[str]:
    (real_part, imag_part) = ([], [])
    for x in str_complexes:
        trimmed = re.split('([j+-])', x)
        real_part.append(''.join(trimmed[:-4]))
        imag_part.append(''.join(trimmed[-4:-2]))
    n = len(str_complexes)
    padded_parts = _trim_zeros_float(real_part + imag_part, decimal)
    if len(padded_parts) == 0:
        return []
    padded_length = max((len(part) for part in padded_parts)) - 1
    padded = [real_pt + imag_pt[0] + f'{imag_pt[1:]:>{padded_length}}' + 'j' for (real_pt, imag_pt) in zip(padded_parts[:n], padded_parts[n:])]
    return padded","""""""Separates the real and imaginary parts from the complex number, and
executes the _trim_zeros_float method on each of those."""""""
pandas/io/formats/format.py,"def _trim_zeros_single_float(str_float: str) -> str:
    str_float = str_float.rstrip('0')
    if str_float.endswith('.'):
        str_float += '0'
    return str_float","""""""Trims trailing zeros after a decimal point,
leaving just one if necessary."""""""
pandas/io/formats/format.py,"def _trim_zeros_float(str_floats: ArrayLike | list[str], decimal: str='.') -> list[str]:
    trimmed = str_floats
    number_regex = re.compile(f'^\\s*[\\+-]?[0-9]+\\{decimal}[0-9]*$')

    def is_number_with_decimal(x) -> bool:
        return re.match(number_regex, x) is not None

    def should_trim(values: ArrayLike | list[str]) -> bool:
        """"""
        Determine if an array of strings should be trimmed.

        Returns True if all numbers containing decimals (defined by the
        above regular expression) within the array end in a zero, otherwise
        returns False.
        """"""
        numbers = [x for x in values if is_number_with_decimal(x)]
        return len(numbers) > 0 and all((x.endswith('0') for x in numbers))
    while should_trim(trimmed):
        trimmed = [x[:-1] if is_number_with_decimal(x) else x for x in trimmed]
    result = [x + '0' if is_number_with_decimal(x) and x.endswith(decimal) else x for x in trimmed]
    return result","""""""Trims the maximum number of trailing zeros equally from
all numbers containing decimals, leaving just one if
necessary."""""""
pandas/io/formats/format.py,"def set_eng_float_format(accuracy: int=3, use_eng_prefix: bool=False) -> None:
    set_option('display.float_format', EngFormatter(accuracy, use_eng_prefix))","""""""Format float representation in DataFrame with SI notation.

Parameters
----------
accuracy : int, default 3
    Number of decimal digits after the floating point.
use_eng_prefix : bool, default False
    Whether to represent a value with SI prefixes.

Returns
-------
None

Examples
--------
>>> df = pd.DataFrame([1e-9, 1e-3, 1, 1e3, 1e6])
>>> df
              0
0  1.000000e-09
1  1.000000e-03
2  1.000000e+00
3  1.000000e+03
4  1.000000e+06

>>> pd.set_eng_float_format(accuracy=1)
>>> df
         0
0  1.0E-09
1  1.0E-03
2  1.0E+00
3  1.0E+03
4  1.0E+06

>>> pd.set_eng_float_format(use_eng_prefix=True)
>>> df
        0
0  1.000n
1  1.000m
2   1.000
3  1.000k
4  1.000M

>>> pd.set_eng_float_format(accuracy=1, use_eng_prefix=True)
>>> df
      0
0  1.0n
1  1.0m
2   1.0
3  1.0k
4  1.0M

>>> pd.set_option(""display.float_format"", None)  # unset option"""""""
pandas/io/formats/format.py,"def get_level_lengths(levels: Any, sentinel: bool | object | str='') -> list[dict[int, int]]:
    if len(levels) == 0:
        return []
    control = [True] * len(levels[0])
    result = []
    for level in levels:
        last_index = 0
        lengths = {}
        for (i, key) in enumerate(level):
            if control[i] and key == sentinel:
                pass
            else:
                control[i] = False
                lengths[last_index] = i - last_index
                last_index = i
        lengths[last_index] = len(level) - last_index
        result.append(lengths)
    return result","""""""For each index in each level the function returns lengths of indexes.

Parameters
----------
levels : list of lists
    List of values on for level.
sentinel : string, optional
    Value which states that no new index starts on there.

Returns
-------
Returns list of maps. For each level returns map of indexes (key is index
in row and value is length of index)."""""""
pandas/io/formats/format.py,"def buffer_put_lines(buf: WriteBuffer[str], lines: list[str]) -> None:
    if any((isinstance(x, str) for x in lines)):
        lines = [str(x) for x in lines]
    buf.write('\n'.join(lines))","""""""Appends lines to a buffer.

Parameters
----------
buf
    The buffer to write to
lines
    The lines to append."""""""
pandas/io/formats/info.py,"def _put_str(s: str | Dtype, space: int) -> str:
    return str(s)[:space].ljust(space)","""""""Make string of specified length, padding to the right if necessary.

Parameters
----------
s : Union[str, Dtype]
    String to be formatted.
space : int
    Length to force string to be of.

Returns
-------
str
    String coerced to given length.

Examples
--------
>>> pd.io.formats.info._put_str(""panda"", 6)
'panda '
>>> pd.io.formats.info._put_str(""panda"", 4)
'pand'"""""""
pandas/io/formats/info.py,"def _sizeof_fmt(num: float, size_qualifier: str) -> str:
    for x in ['bytes', 'KB', 'MB', 'GB', 'TB']:
        if num < 1024.0:
            return f'{num:3.1f}{size_qualifier} {x}'
        num /= 1024.0
    return f'{num:3.1f}{size_qualifier} PB'","""""""Return size in human readable format.

Parameters
----------
num : int
    Size in bytes.
size_qualifier : str
    Either empty, or '+' (if lower bound).

Returns
-------
str
    Size in human readable format.

Examples
--------
>>> _sizeof_fmt(23028, '')
'22.5 KB'

>>> _sizeof_fmt(23028, '+')
'22.5+ KB'"""""""
pandas/io/formats/info.py,"def _initialize_memory_usage(memory_usage: bool | str | None=None) -> bool | str:
    if memory_usage is None:
        memory_usage = get_option('display.memory_usage')
    return memory_usage","""""""Get memory usage based on inputs and display options."""""""
pandas/io/formats/info.py,"def _get_dataframe_dtype_counts(df: DataFrame) -> Mapping[str, int]:
    return df.dtypes.value_counts().groupby(lambda x: x.name).sum()","""""""Create mapping between datatypes and their number of occurrences."""""""
pandas/io/formats/printing.py,"def adjoin(space: int, *lists: list[str], **kwargs) -> str:
    strlen = kwargs.pop('strlen', len)
    justfunc = kwargs.pop('justfunc', justify)
    newLists = []
    lengths = [max(map(strlen, x)) + space for x in lists[:-1]]
    lengths.append(max(map(len, lists[-1])))
    maxLen = max(map(len, lists))
    for (i, lst) in enumerate(lists):
        nl = justfunc(lst, lengths[i], mode='left')
        nl = [' ' * lengths[i]] * (maxLen - len(lst)) + nl
        newLists.append(nl)
    toJoin = zip(*newLists)
    return '\n'.join((''.join(lines) for lines in toJoin))","""""""Glues together two sets of strings using the amount of space requested.
The idea is to prettify.

----------
space : int
    number of spaces for padding
lists : str
    list of str which being joined
strlen : callable
    function used to calculate the length of each str. Needed for unicode
    handling.
justfunc : callable
    function used to justify str. Needed for unicode handling."""""""
pandas/io/formats/printing.py,"def justify(texts: Iterable[str], max_len: int, mode: str='right') -> list[str]:
    if mode == 'left':
        return [x.ljust(max_len) for x in texts]
    elif mode == 'center':
        return [x.center(max_len) for x in texts]
    else:
        return [x.rjust(max_len) for x in texts]","""""""Perform ljust, center, rjust against string or list-like"""""""
pandas/io/formats/printing.py,"def _pprint_seq(seq: Sequence, _nest_lvl: int=0, max_seq_items: int | None=None, **kwds) -> str:
    if isinstance(seq, set):
        fmt = '{{{body}}}'
    else:
        fmt = '[{body}]' if hasattr(seq, '__setitem__') else '({body})'
    if max_seq_items is False:
        nitems = len(seq)
    else:
        nitems = max_seq_items or get_option('max_seq_items') or len(seq)
    s = iter(seq)
    r = [pprint_thing(next(s), _nest_lvl + 1, max_seq_items=max_seq_items, **kwds) for i in range(min(nitems, len(seq)))]
    body = ', '.join(r)
    if nitems < len(seq):
        body += ', ...'
    elif isinstance(seq, tuple) and len(seq) == 1:
        body += ','
    return fmt.format(body=body)","""""""internal. pprinter for iterables. you should probably use pprint_thing()
rather than calling this directly.

bounds length of printed sequence, depending on options"""""""
pandas/io/formats/printing.py,"def _pprint_dict(seq: Mapping, _nest_lvl: int=0, max_seq_items: int | None=None, **kwds) -> str:
    fmt = '{{{things}}}'
    pairs = []
    pfmt = '{key}: {val}'
    if max_seq_items is False:
        nitems = len(seq)
    else:
        nitems = max_seq_items or get_option('max_seq_items') or len(seq)
    for (k, v) in list(seq.items())[:nitems]:
        pairs.append(pfmt.format(key=pprint_thing(k, _nest_lvl + 1, max_seq_items=max_seq_items, **kwds), val=pprint_thing(v, _nest_lvl + 1, max_seq_items=max_seq_items, **kwds)))
    if nitems < len(seq):
        return fmt.format(things=', '.join(pairs) + ', ...')
    else:
        return fmt.format(things=', '.join(pairs))","""""""internal. pprinter for iterables. you should probably use pprint_thing()
rather than calling this directly."""""""
pandas/io/formats/printing.py,"def pprint_thing(thing: Any, _nest_lvl: int=0, escape_chars: EscapeChars | None=None, default_escapes: bool=False, quote_strings: bool=False, max_seq_items: int | None=None) -> str:

    def as_escaped_string(thing: Any, escape_chars: EscapeChars | None=escape_chars) -> str:
        translate = {'\t': '\\t', '\n': '\\n', '\r': '\\r'}
        if isinstance(escape_chars, dict):
            if default_escapes:
                translate.update(escape_chars)
            else:
                translate = escape_chars
            escape_chars = list(escape_chars.keys())
        else:
            escape_chars = escape_chars or ()
        result = str(thing)
        for c in escape_chars:
            result = result.replace(c, translate[c])
        return result
    if hasattr(thing, '__next__'):
        return str(thing)
    elif isinstance(thing, dict) and _nest_lvl < get_option('display.pprint_nest_depth'):
        result = _pprint_dict(thing, _nest_lvl, quote_strings=True, max_seq_items=max_seq_items)
    elif is_sequence(thing) and _nest_lvl < get_option('display.pprint_nest_depth'):
        result = _pprint_seq(thing, _nest_lvl, escape_chars=escape_chars, quote_strings=quote_strings, max_seq_items=max_seq_items)
    elif isinstance(thing, str) and quote_strings:
        result = f""'{as_escaped_string(thing)}'""
    else:
        result = as_escaped_string(thing)
    return result","""""""This function is the sanctioned way of converting objects
to a string representation and properly handles nested sequences.

Parameters
----------
thing : anything to be formatted
_nest_lvl : internal use only. pprint_thing() is mutually-recursive
    with pprint_sequence, this argument is used to keep track of the
    current nesting level, and limit it.
escape_chars : list or dict, optional
    Characters to escape. If a dict is passed the values are the
    replacements
default_escapes : bool, default False
    Whether the input escape characters replaces or adds to the defaults
max_seq_items : int or None, default None
    Pass through to other pretty printers to limit sequence printing

Returns
-------
str"""""""
pandas/io/formats/printing.py,"def format_object_summary(obj, formatter: Callable, is_justify: bool=True, name: str | None=None, indent_for_name: bool=True, line_break_each_value: bool=False) -> str:
    from pandas.io.formats.console import get_console_size
    from pandas.io.formats.format import get_adjustment
    (display_width, _) = get_console_size()
    if display_width is None:
        display_width = get_option('display.width') or 80
    if name is None:
        name = type(obj).__name__
    if indent_for_name:
        name_len = len(name)
        space1 = f""\n{' ' * (name_len + 1)}""
        space2 = f""\n{' ' * (name_len + 2)}""
    else:
        space1 = '\n'
        space2 = '\n '
    n = len(obj)
    if line_break_each_value:
        sep = ',\n ' + ' ' * len(name)
    else:
        sep = ','
    max_seq_items = get_option('display.max_seq_items') or n
    is_truncated = n > max_seq_items
    adj = get_adjustment()

    def _extend_line(s: str, line: str, value: str, display_width: int, next_line_prefix: str) -> tuple[str, str]:
        if adj.len(line.rstrip()) + adj.len(value.rstrip()) >= display_width:
            s += line.rstrip()
            line = next_line_prefix
        line += value
        return (s, line)

    def best_len(values: list[str]) -> int:
        if values:
            return max((adj.len(x) for x in values))
        else:
            return 0
    close = ', '
    if n == 0:
        summary = f'[]{close}'
    elif n == 1 and (not line_break_each_value):
        first = formatter(obj[0])
        summary = f'[{first}]{close}'
    elif n == 2 and (not line_break_each_value):
        first = formatter(obj[0])
        last = formatter(obj[-1])
        summary = f'[{first}, {last}]{close}'
    else:
        if max_seq_items == 1:
            head = []
            tail = [formatter(x) for x in obj[-1:]]
        elif n > max_seq_items:
            n = min(max_seq_items // 2, 10)
            head = [formatter(x) for x in obj[:n]]
            tail = [formatter(x) for x in obj[-n:]]
        else:
            head = []
            tail = [formatter(x) for x in obj]
        if is_justify:
            if line_break_each_value:
                (head, tail) = _justify(head, tail)
            elif is_truncated or not (len(', '.join(head)) < display_width and len(', '.join(tail)) < display_width):
                max_length = max(best_len(head), best_len(tail))
                head = [x.rjust(max_length) for x in head]
                tail = [x.rjust(max_length) for x in tail]
        if line_break_each_value:
            max_space = display_width - len(space2)
            value = tail[0]
            max_items = 1
            for num_items in reversed(range(1, len(value) + 1)):
                pprinted_seq = _pprint_seq(value, max_seq_items=num_items)
                if len(pprinted_seq) < max_space:
                    max_items = num_items
                    break
            head = [_pprint_seq(x, max_seq_items=max_items) for x in head]
            tail = [_pprint_seq(x, max_seq_items=max_items) for x in tail]
        summary = ''
        line = space2
        for head_value in head:
            word = head_value + sep + ' '
            (summary, line) = _extend_line(summary, line, word, display_width, space2)
        if is_truncated:
            summary += line.rstrip() + space2 + '...'
            line = space2
        for tail_item in tail[:-1]:
            word = tail_item + sep + ' '
            (summary, line) = _extend_line(summary, line, word, display_width, space2)
        (summary, line) = _extend_line(summary, line, tail[-1], display_width - 2, space2)
        summary += line
        close = ']' + close.rstrip(' ')
        summary += close
        if len(summary) > display_width or line_break_each_value:
            summary += space1
        else:
            summary += ' '
        summary = '[' + summary[len(space2):]
    return summary","""""""Return the formatted obj as a unicode string

Parameters
----------
obj : object
    must be iterable and support __getitem__
formatter : callable
    string formatter for an element
is_justify : bool
    should justify the display
name : name, optional
    defaults to the class name of the obj
indent_for_name : bool, default True
    Whether subsequent lines should be indented to
    align with the name.
line_break_each_value : bool, default False
    If True, inserts a line break for each value of ``obj``.
    If False, only break lines when the a line of values gets wider
    than the display width.

Returns
-------
summary string"""""""
pandas/io/formats/printing.py,"def _justify(head: list[Sequence[str]], tail: list[Sequence[str]]) -> tuple[list[tuple[str, ...]], list[tuple[str, ...]]]:
    combined = head + tail
    max_length = [0] * len(combined[0])
    for inner_seq in combined:
        length = [len(item) for item in inner_seq]
        max_length = [max(x, y) for (x, y) in zip(max_length, length)]
    head_tuples = [tuple((x.rjust(max_len) for (x, max_len) in zip(seq, max_length))) for seq in head]
    tail_tuples = [tuple((x.rjust(max_len) for (x, max_len) in zip(seq, max_length))) for seq in tail]
    return (head_tuples, tail_tuples)","""""""Justify items in head and tail, so they are right-aligned when stacked.

Parameters
----------
head : list-like of list-likes of strings
tail : list-like of list-likes of strings

Returns
-------
tuple of list of tuples of strings
    Same as head and tail, but items are right aligned when stacked
    vertically.

Examples
--------
>>> _justify([['a', 'b']], [['abc', 'abcd']])
([('  a', '   b')], [('abc', 'abcd')])"""""""
pandas/io/formats/style.py,"def _validate_apply_axis_arg(arg: NDFrame | Sequence | np.ndarray, arg_name: str, dtype: Any | None, data: NDFrame) -> np.ndarray:
    dtype = {'dtype': dtype} if dtype else {}
    if isinstance(arg, Series) and isinstance(data, DataFrame):
        raise ValueError(f""'{arg_name}' is a Series but underlying data for operations is a DataFrame since 'axis=None'"")
    if isinstance(arg, DataFrame) and isinstance(data, Series):
        raise ValueError(f""'{arg_name}' is a DataFrame but underlying data for operations is a Series with 'axis in [0,1]'"")
    if isinstance(arg, (Series, DataFrame)):
        arg = arg.reindex_like(data, method=None).to_numpy(**dtype)
    else:
        arg = np.asarray(arg, **dtype)
        assert isinstance(arg, np.ndarray)
        if arg.shape != data.shape:
            raise ValueError(f""supplied '{arg_name}' is not correct shape for data over selected 'axis': got {arg.shape}, expected {data.shape}"")
    return arg","""""""For the apply-type methods, ``axis=None`` creates ``data`` as DataFrame, and for
``axis=[1,0]`` it creates a Series. Where ``arg`` is expected as an element
of some operator with ``data`` we must make sure that the two are compatible shapes,
or raise.

Parameters
----------
arg : sequence, Series or DataFrame
    the user input arg
arg_name : string
    name of the arg for use in error messages
dtype : numpy dtype, optional
    forced numpy dtype if given
data : Series or DataFrame
    underling subset of Styler data on which operations are performed

Returns
-------
ndarray"""""""
pandas/io/formats/style.py,"def _background_gradient(data, cmap: str | Colormap='PuBu', low: float=0, high: float=0, text_color_threshold: float=0.408, vmin: float | None=None, vmax: float | None=None, gmap: Sequence | np.ndarray | DataFrame | Series | None=None, text_only: bool=False):
    if gmap is None:
        gmap = data.to_numpy(dtype=float, na_value=np.nan)
    else:
        gmap = _validate_apply_axis_arg(gmap, 'gmap', float, data)
    with _mpl(Styler.background_gradient) as (_, _matplotlib):
        smin = np.nanmin(gmap) if vmin is None else vmin
        smax = np.nanmax(gmap) if vmax is None else vmax
        rng = smax - smin
        norm = _matplotlib.colors.Normalize(smin - rng * low, smax + rng * high)
        if cmap is None:
            rgbas = _matplotlib.colormaps[_matplotlib.rcParams['image.cmap']](norm(gmap))
        else:
            rgbas = _matplotlib.colormaps.get_cmap(cmap)(norm(gmap))

        def relative_luminance(rgba) -> float:
            """"""
            Calculate relative luminance of a color.

            The calculation adheres to the W3C standards
            (https://www.w3.org/WAI/GL/wiki/Relative_luminance)

            Parameters
            ----------
            color : rgb or rgba tuple

            Returns
            -------
            float
                The relative luminance as a value from 0 to 1
            """"""
            (r, g, b) = (x / 12.92 if x <= 0.04045 else ((x + 0.055) / 1.055) ** 2.4 for x in rgba[:3])
            return 0.2126 * r + 0.7152 * g + 0.0722 * b

        def css(rgba, text_only) -> str:
            if not text_only:
                dark = relative_luminance(rgba) < text_color_threshold
                text_color = '#f1f1f1' if dark else '#000000'
                return f'background-color: {_matplotlib.colors.rgb2hex(rgba)};color: {text_color};'
            else:
                return f'color: {_matplotlib.colors.rgb2hex(rgba)};'
        if data.ndim == 1:
            return [css(rgba, text_only) for rgba in rgbas]
        else:
            return DataFrame([[css(rgba, text_only) for rgba in row] for row in rgbas], index=data.index, columns=data.columns)","""""""Color background in a range according to the data or a gradient map"""""""
pandas/io/formats/style.py,"def _highlight_between(data: NDFrame, props: str, left: Scalar | Sequence | np.ndarray | NDFrame | None=None, right: Scalar | Sequence | np.ndarray | NDFrame | None=None, inclusive: bool | str=True) -> np.ndarray:
    if np.iterable(left) and (not isinstance(left, str)):
        left = _validate_apply_axis_arg(left, 'left', None, data)
    if np.iterable(right) and (not isinstance(right, str)):
        right = _validate_apply_axis_arg(right, 'right', None, data)
    if inclusive == 'both':
        ops = (operator.ge, operator.le)
    elif inclusive == 'neither':
        ops = (operator.gt, operator.lt)
    elif inclusive == 'left':
        ops = (operator.ge, operator.lt)
    elif inclusive == 'right':
        ops = (operator.gt, operator.le)
    else:
        raise ValueError(f""'inclusive' values can be 'both', 'left', 'right', or 'neither' got {inclusive}"")
    g_left = ops[0](data, left) if left is not None else np.full(data.shape, True, dtype=bool)
    if isinstance(g_left, (DataFrame, Series)):
        g_left = g_left.where(pd.notna(g_left), False)
    l_right = ops[1](data, right) if right is not None else np.full(data.shape, True, dtype=bool)
    if isinstance(l_right, (DataFrame, Series)):
        l_right = l_right.where(pd.notna(l_right), False)
    return np.where(g_left & l_right, props, '')","""""""Return an array of css props based on condition of data values within given range."""""""
pandas/io/formats/style.py,"def _highlight_value(data: DataFrame | Series, op: str, props: str) -> np.ndarray:
    value = getattr(data, op)(skipna=True)
    if isinstance(data, DataFrame):
        value = getattr(value, op)(skipna=True)
    cond = data == value
    cond = cond.where(pd.notna(cond), False)
    return np.where(cond, props, '')","""""""Return an array of css strings based on the condition of values matching an op."""""""
pandas/io/formats/style.py,"def _bar(data: NDFrame, align: str | float | Callable, colors: str | list | tuple, cmap: Any, width: float, height: float, vmin: float | None, vmax: float | None, base_css: str):

    def css_bar(start: float, end: float, color: str) -> str:
        """"""
        Generate CSS code to draw a bar from start to end in a table cell.

        Uses linear-gradient.

        Parameters
        ----------
        start : float
            Relative positional start of bar coloring in [0,1]
        end : float
            Relative positional end of the bar coloring in [0,1]
        color : str
            CSS valid color to apply.

        Returns
        -------
        str : The CSS applicable to the cell.

        Notes
        -----
        Uses ``base_css`` from outer scope.
        """"""
        cell_css = base_css
        if end > start:
            cell_css += 'background: linear-gradient(90deg,'
            if start > 0:
                cell_css += f' transparent {start * 100:.1f}%, {color} {start * 100:.1f}%,'
            cell_css += f' {color} {end * 100:.1f}%, transparent {end * 100:.1f}%)'
        return cell_css

    def css_calc(x, left: float, right: float, align: str, color: str | list | tuple):
        """"""
        Return the correct CSS for bar placement based on calculated values.

        Parameters
        ----------
        x : float
            Value which determines the bar placement.
        left : float
            Value marking the left side of calculation, usually minimum of data.
        right : float
            Value marking the right side of the calculation, usually maximum of data
            (left < right).
        align : {""left"", ""right"", ""zero"", ""mid""}
            How the bars will be positioned.
            ""left"", ""right"", ""zero"" can be used with any values for ``left``, ``right``.
            ""mid"" can only be used where ``left <= 0`` and ``right >= 0``.
            ""zero"" is used to specify a center when all values ``x``, ``left``,
            ``right`` are translated, e.g. by say a mean or median.

        Returns
        -------
        str : Resultant CSS with linear gradient.

        Notes
        -----
        Uses ``colors``, ``width`` and ``height`` from outer scope.
        """"""
        if pd.isna(x):
            return base_css
        if isinstance(color, (list, tuple)):
            color = color[0] if x < 0 else color[1]
        assert isinstance(color, str)
        x = left if x < left else x
        x = right if x > right else x
        start: float = 0
        end: float = 1
        if align == 'left':
            end = (x - left) / (right - left)
        elif align == 'right':
            start = (x - left) / (right - left)
        else:
            z_frac: float = 0.5
            if align == 'zero':
                limit: float = max(abs(left), abs(right))
                (left, right) = (-limit, limit)
            elif align == 'mid':
                mid: float = (left + right) / 2
                z_frac = -mid / (right - left) + 0.5 if mid < 0 else -left / (right - left)
            if x < 0:
                (start, end) = ((x - left) / (right - left), z_frac)
            else:
                (start, end) = (z_frac, (x - left) / (right - left))
        ret = css_bar(start * width, end * width, color)
        if height < 1 and 'background: linear-gradient(' in ret:
            return ret + f' no-repeat center; background-size: 100% {height * 100:.1f}%;'
        else:
            return ret
    values = data.to_numpy()
    left = np.nanmin(values) if vmin is None else vmin
    right = np.nanmax(values) if vmax is None else vmax
    z: float = 0
    if align == 'mid':
        if left >= 0:
            (align, left) = ('left', 0 if vmin is None else vmin)
        elif right <= 0:
            (align, right) = ('right', 0 if vmax is None else vmax)
    elif align == 'mean':
        (z, align) = (np.nanmean(values), 'zero')
    elif callable(align):
        (z, align) = (align(values), 'zero')
    elif isinstance(align, (float, int)):
        (z, align) = (float(align), 'zero')
    elif align not in ('left', 'right', 'zero'):
        raise ValueError(""`align` should be in {'left', 'right', 'mid', 'mean', 'zero'} or be a value defining the center line or a callable that returns a float"")
    rgbas = None
    if cmap is not None:
        with _mpl(Styler.bar) as (_, _matplotlib):
            cmap = _matplotlib.colormaps[cmap] if isinstance(cmap, str) else cmap
            norm = _matplotlib.colors.Normalize(left, right)
            rgbas = cmap(norm(values))
            if data.ndim == 1:
                rgbas = [_matplotlib.colors.rgb2hex(rgba) for rgba in rgbas]
            else:
                rgbas = [[_matplotlib.colors.rgb2hex(rgba) for rgba in row] for row in rgbas]
    assert isinstance(align, str)
    if data.ndim == 1:
        return [css_calc(x - z, left - z, right - z, align, colors if rgbas is None else rgbas[i]) for (i, x) in enumerate(values)]
    else:
        return np.array([[css_calc(x - z, left - z, right - z, align, colors if rgbas is None else rgbas[i][j]) for (j, x) in enumerate(row)] for (i, row) in enumerate(values)])","""""""Draw bar chart in data cells using HTML CSS linear gradient.

Parameters
----------
data : Series or DataFrame
    Underling subset of Styler data on which operations are performed.
align : str in {""left"", ""right"", ""mid"", ""zero"", ""mean""}, int, float, callable
    Method for how bars are structured or scalar value of centre point.
colors : list-like of str
    Two listed colors as string in valid CSS.
width : float in [0,1]
    The percentage of the cell, measured from left, where drawn bars will reside.
height : float in [0,1]
    The percentage of the cell's height where drawn bars will reside, centrally
    aligned.
vmin : float, optional
    Overwrite the minimum value of the window.
vmax : float, optional
    Overwrite the maximum value of the window.
base_css : str
    Additional CSS that is included in the cell before bars are drawn."""""""
pandas/io/formats/style_render.py,"def _element(html_element: str, html_class: str | None, value: Any, is_visible: bool, **kwargs) -> dict:
    if 'display_value' not in kwargs:
        kwargs['display_value'] = value
    return {'type': html_element, 'value': value, 'class': html_class, 'is_visible': is_visible, **kwargs}","""""""Template to return container with information for a <td></td> or <th></th> element."""""""
pandas/io/formats/style_render.py,"def _get_trimming_maximums(rn, cn, max_elements, max_rows=None, max_cols=None, scaling_factor: float=0.8) -> tuple[int, int]:

    def scale_down(rn, cn):
        if cn >= rn:
            return (rn, int(cn * scaling_factor))
        else:
            return (int(rn * scaling_factor), cn)
    if max_rows:
        rn = max_rows if rn > max_rows else rn
    if max_cols:
        cn = max_cols if cn > max_cols else cn
    while rn * cn > max_elements:
        (rn, cn) = scale_down(rn, cn)
    return (rn, cn)","""""""Recursively reduce the number of rows and columns to satisfy max elements.

Parameters
----------
rn, cn : int
    The number of input rows / columns
max_elements : int
    The number of allowable elements
max_rows, max_cols : int, optional
    Directly specify an initial maximum rows or columns before compression.
scaling_factor : float
    Factor at which to reduce the number of rows / columns to fit.

Returns
-------
rn, cn : tuple
    New rn and cn values that satisfy the max_elements constraint"""""""
pandas/io/formats/style_render.py,"def _get_level_lengths(index: Index, sparsify: bool, max_index: int, hidden_elements: Sequence[int] | None=None):
    if isinstance(index, MultiIndex):
        levels = index.format(sparsify=lib.no_default, adjoin=False)
    else:
        levels = index.format()
    if hidden_elements is None:
        hidden_elements = []
    lengths = {}
    if not isinstance(index, MultiIndex):
        for (i, value) in enumerate(levels):
            if i not in hidden_elements:
                lengths[0, i] = 1
        return lengths
    for (i, lvl) in enumerate(levels):
        visible_row_count = 0
        for (j, row) in enumerate(lvl):
            if visible_row_count > max_index:
                break
            if not sparsify:
                if j not in hidden_elements:
                    lengths[i, j] = 1
                    visible_row_count += 1
            elif row is not lib.no_default and j not in hidden_elements:
                last_label = j
                lengths[i, last_label] = 1
                visible_row_count += 1
            elif row is not lib.no_default:
                last_label = j
                lengths[i, last_label] = 0
            elif j not in hidden_elements:
                visible_row_count += 1
                if visible_row_count > max_index:
                    break
                if lengths[i, last_label] == 0:
                    last_label = j
                    lengths[i, last_label] = 1
                else:
                    lengths[i, last_label] += 1
    non_zero_lengths = {element: length for (element, length) in lengths.items() if length >= 1}
    return non_zero_lengths","""""""Given an index, find the level length for each element.

Parameters
----------
index : Index
    Index or columns to determine lengths of each element
sparsify : bool
    Whether to hide or show each distinct element in a MultiIndex
max_index : int
    The maximum number of elements to analyse along the index due to trimming
hidden_elements : sequence of int
    Index positions of elements hidden from display in the index affecting
    length

Returns
-------
Dict :
    Result is a dictionary of (level, initial_position): span"""""""
pandas/io/formats/style_render.py,"def _is_visible(idx_row, idx_col, lengths) -> bool:
    return (idx_col, idx_row) in lengths","""""""Index -> {(idx_row, idx_col): bool})."""""""
pandas/io/formats/style_render.py,"def format_table_styles(styles: CSSStyles) -> CSSStyles:
    return [{'selector': selector, 'props': css_dict['props']} for css_dict in styles for selector in css_dict['selector'].split(',')]","""""""looks for multiple CSS selectors and separates them:
[{'selector': 'td, th', 'props': 'a:v;'}]
    ---> [{'selector': 'td', 'props': 'a:v;'},
          {'selector': 'th', 'props': 'a:v;'}]"""""""
pandas/io/formats/style_render.py,"def _default_formatter(x: Any, precision: int, thousands: bool=False) -> Any:
    if is_float(x) or is_complex(x):
        return f'{x:,.{precision}f}' if thousands else f'{x:.{precision}f}'
    elif is_integer(x):
        return f'{x:,}' if thousands else str(x)
    return x","""""""Format the display of a value

Parameters
----------
x : Any
    Input variable to be formatted
precision : Int
    Floating point precision used if ``x`` is float or complex.
thousands : bool, default False
    Whether to group digits with thousands separated with "","".

Returns
-------
value : Any
    Matches input type, or string if input is float or complex or int with sep."""""""
pandas/io/formats/style_render.py,"def _wrap_decimal_thousands(formatter: Callable, decimal: str, thousands: str | None) -> Callable:

    def wrapper(x):
        if is_float(x) or is_integer(x) or is_complex(x):
            if decimal != '.' and thousands is not None and (thousands != ','):
                return formatter(x).replace(',', '_-').replace('.', decimal).replace('_-', thousands)
            elif decimal != '.' and (thousands is None or thousands == ','):
                return formatter(x).replace('.', decimal)
            elif decimal == '.' and thousands is not None and (thousands != ','):
                return formatter(x).replace(',', thousands)
        return formatter(x)
    return wrapper","""""""Takes a string formatting function and wraps logic to deal with thousands and
decimal parameters, in the case that they are non-standard and that the input
is a (float, complex, int)."""""""
pandas/io/formats/style_render.py,"def _str_escape(x, escape):
    if isinstance(x, str):
        if escape == 'html':
            return escape_html(x)
        elif escape == 'latex':
            return _escape_latex(x)
        elif escape == 'latex-math':
            return _escape_latex_math(x)
        else:
            raise ValueError(f""`escape` only permitted in {{'html', 'latex', 'latex-math'}}, got {escape}"")
    return x","""""""if escaping: only use on str, else return input"""""""
pandas/io/formats/style_render.py,"def _render_href(x, format):
    if isinstance(x, str):
        if format == 'html':
            href = '<a href=""{0}"" target=""_blank"">{0}</a>'
        elif format == 'latex':
            href = '\\href{{{0}}}{{{0}}}'
        else:
            raise ValueError(""``hyperlinks`` format can only be 'html' or 'latex'"")
        pat = ""((http|ftp)s?:\\/\\/|www.)[\\w/\\-?=%.:@]+\\.[\\w/\\-&?=%.,':;~!@#$*()\\[\\]]+""
        return re.sub(pat, lambda m: href.format(m.group(0)), x)
    return x","""""""uses regex to detect a common URL pattern and converts to href tag in format."""""""
pandas/io/formats/style_render.py,"def _maybe_wrap_formatter(formatter: BaseFormatter | None=None, na_rep: str | None=None, precision: int | None=None, decimal: str='.', thousands: str | None=None, escape: str | None=None, hyperlinks: str | None=None) -> Callable:
    if isinstance(formatter, str):
        func_0 = lambda x: formatter.format(x)
    elif callable(formatter):
        func_0 = formatter
    elif formatter is None:
        precision = get_option('styler.format.precision') if precision is None else precision
        func_0 = partial(_default_formatter, precision=precision, thousands=thousands is not None)
    else:
        raise TypeError(f""'formatter' expected str or callable, got {type(formatter)}"")
    if escape is not None:
        func_1 = lambda x: func_0(_str_escape(x, escape=escape))
    else:
        func_1 = func_0
    if decimal != '.' or (thousands is not None and thousands != ','):
        func_2 = _wrap_decimal_thousands(func_1, decimal=decimal, thousands=thousands)
    else:
        func_2 = func_1
    if hyperlinks is not None:
        func_3 = lambda x: func_2(_render_href(x, format=hyperlinks))
    else:
        func_3 = func_2
    if na_rep is None:
        return func_3
    else:
        return lambda x: na_rep if isna(x) is True else func_3(x)","""""""Allows formatters to be expressed as str, callable or None, where None returns
a default formatting function. wraps with na_rep, and precision where they are
available."""""""
pandas/io/formats/style_render.py,"def non_reducing_slice(slice_: Subset):
    kinds = (ABCSeries, np.ndarray, Index, list, str)
    if isinstance(slice_, kinds):
        slice_ = IndexSlice[:, slice_]

    def pred(part) -> bool:
        """"""
        Returns
        -------
        bool
            True if slice does *not* reduce,
            False if `part` is a tuple.
        """"""
        if isinstance(part, tuple):
            return any((isinstance(s, slice) or is_list_like(s) for s in part))
        else:
            return isinstance(part, slice) or is_list_like(part)
    if not is_list_like(slice_):
        if not isinstance(slice_, slice):
            slice_ = [[slice_]]
        else:
            slice_ = [slice_]
    else:
        slice_ = [p if pred(p) else [p] for p in slice_]
    return tuple(slice_)","""""""Ensure that a slice doesn't reduce to a Series or Scalar.

Any user-passed `subset` should have this called on it
to make sure we're always working with DataFrames."""""""
pandas/io/formats/style_render.py,"def maybe_convert_css_to_tuples(style: CSSProperties) -> CSSList:
    if isinstance(style, str):
        s = style.split(';')
        try:
            return [(x.split(':')[0].strip(), x.split(':')[1].strip()) for x in s if x.strip() != '']
        except IndexError:
            raise ValueError(f""Styles supplied as string must follow CSS rule formats, for example 'attr: val;'. '{style}' was given."")
    return style","""""""Convert css-string to sequence of tuples format if needed.
'color:red; border:1px solid black;' -> [('color', 'red'),
                                         ('border','1px solid red')]"""""""
pandas/io/formats/style_render.py,"def refactor_levels(level: Level | list[Level] | None, obj: Index) -> list[int]:
    if level is None:
        levels_: list[int] = list(range(obj.nlevels))
    elif isinstance(level, int):
        levels_ = [level]
    elif isinstance(level, str):
        levels_ = [obj._get_level_number(level)]
    elif isinstance(level, list):
        levels_ = [obj._get_level_number(lev) if not isinstance(lev, int) else lev for lev in level]
    else:
        raise ValueError('`level` must be of type `int`, `str` or list of such')
    return levels_","""""""Returns a consistent levels arg for use in ``hide_index`` or ``hide_columns``.

Parameters
----------
level : int, str, list
    Original ``level`` arg supplied to above methods.
obj:
    Either ``self.index`` or ``self.columns``

Returns
-------
list : refactored arg with a list of levels to hide"""""""
pandas/io/formats/style_render.py,"def _parse_latex_table_wrapping(table_styles: CSSStyles, caption: str | None) -> bool:
    IGNORED_WRAPPERS = ['toprule', 'midrule', 'bottomrule', 'column_format']
    return table_styles is not None and any((d['selector'] not in IGNORED_WRAPPERS for d in table_styles)) or caption is not None","""""""Indicate whether LaTeX {tabular} should be wrapped with a {table} environment.

Parses the `table_styles` and detects any selectors which must be included outside
of {tabular}, i.e. indicating that wrapping must occur, and therefore return True,
or if a caption exists and requires similar."""""""
pandas/io/formats/style_render.py,"def _parse_latex_table_styles(table_styles: CSSStyles, selector: str) -> str | None:
    for style in table_styles[::-1]:
        if style['selector'] == selector:
            return str(style['props'][0][1]).replace('', ':')
    return None","""""""Return the first 'props' 'value' from ``tables_styles`` identified by ``selector``.

Examples
--------
>>> table_styles = [{'selector': 'foo', 'props': [('attr','value')]},
...                 {'selector': 'bar', 'props': [('attr', 'overwritten')]},
...                 {'selector': 'bar', 'props': [('a1', 'baz'), ('a2', 'ignore')]}]
>>> _parse_latex_table_styles(table_styles, selector='bar')
'baz'

Notes
-----
The replacement of """" with "":"" is to avoid the CSS problem where "":"" has structural
significance and cannot be used in LaTeX labels, but is often required by them."""""""
pandas/io/formats/style_render.py,"def _parse_latex_cell_styles(latex_styles: CSSList, display_value: str, convert_css: bool=False) -> str:
    if convert_css:
        latex_styles = _parse_latex_css_conversion(latex_styles)
    for (command, options) in latex_styles[::-1]:
        formatter = {'--wrap': f'{{\\{command}--to_parse {display_value}}}', '--nowrap': f'\\{command}--to_parse {display_value}', '--lwrap': f'{{\\{command}--to_parse}} {display_value}', '--rwrap': f'\\{command}--to_parse{{{display_value}}}', '--dwrap': f'{{\\{command}--to_parse}}{{{display_value}}}'}
        display_value = f'\\{command}{options} {display_value}'
        for arg in ['--nowrap', '--wrap', '--lwrap', '--rwrap', '--dwrap']:
            if arg in str(options):
                display_value = formatter[arg].replace('--to_parse', _parse_latex_options_strip(value=options, arg=arg))
                break
    return display_value","""""""Mutate the ``display_value`` string including LaTeX commands from ``latex_styles``.

This method builds a recursive latex chain of commands based on the
CSSList input, nested around ``display_value``.

If a CSS style is given as ('<command>', '<options>') this is translated to
'\<command><options>{display_value}', and this value is treated as the
display value for the next iteration.

The most recent style forms the inner component, for example for styles:
`[('c1', 'o1'), ('c2', 'o2')]` this returns: `\c1o1{\c2o2{display_value}}`

Sometimes latex commands have to be wrapped with curly braces in different ways:
We create some parsing flags to identify the different behaviours:

 - `--rwrap`        : `\<command><options>{<display_value>}`
 - `--wrap`         : `{\<command><options> <display_value>}`
 - `--nowrap`       : `\<command><options> <display_value>`
 - `--lwrap`        : `{\<command><options>} <display_value>`
 - `--dwrap`        : `{\<command><options>}{<display_value>}`

For example for styles:
`[('c1', 'o1--wrap'), ('c2', 'o2')]` this returns: `{\c1o1 \c2o2{display_value}}"""""""
pandas/io/formats/style_render.py,"def _parse_latex_header_span(cell: dict[str, Any], multirow_align: str, multicol_align: str, wrap: bool=False, convert_css: bool=False) -> str:
    display_val = _parse_latex_cell_styles(cell['cellstyle'], cell['display_value'], convert_css)
    if 'attributes' in cell:
        attrs = cell['attributes']
        if 'colspan=""' in attrs:
            colspan = attrs[attrs.find('colspan=""') + 9:]
            colspan = int(colspan[:colspan.find('""')])
            if 'naive-l' == multicol_align:
                out = f'{{{display_val}}}' if wrap else f'{display_val}'
                blanks = ' & {}' if wrap else ' &'
                return out + blanks * (colspan - 1)
            elif 'naive-r' == multicol_align:
                out = f'{{{display_val}}}' if wrap else f'{display_val}'
                blanks = '{} & ' if wrap else '& '
                return blanks * (colspan - 1) + out
            return f'\\multicolumn{{{colspan}}}{{{multicol_align}}}{{{display_val}}}'
        elif 'rowspan=""' in attrs:
            if multirow_align == 'naive':
                return display_val
            rowspan = attrs[attrs.find('rowspan=""') + 9:]
            rowspan = int(rowspan[:rowspan.find('""')])
            return f'\\multirow[{multirow_align}]{{{rowspan}}}{{*}}{{{display_val}}}'
    if wrap:
        return f'{{{display_val}}}'
    else:
        return display_val","""""""Refactor the cell `display_value` if a 'colspan' or 'rowspan' attribute is present.

'rowspan' and 'colspan' do not occur simultaneouly. If they are detected then
the `display_value` is altered to a LaTeX `multirow` or `multicol` command
respectively, with the appropriate cell-span.

``wrap`` is used to enclose the `display_value` in braces which is needed for
column headers using an siunitx package.

Requires the package {multirow}, whereas multicol support is usually built in
to the {tabular} environment.

Examples
--------
>>> cell = {'cellstyle': '', 'display_value':'text', 'attributes': 'colspan=""3""'}
>>> _parse_latex_header_span(cell, 't', 'c')
'\\multicolumn{3}{c}{text}'"""""""
pandas/io/formats/style_render.py,"def _parse_latex_options_strip(value: str | float, arg: str) -> str:
    return str(value).replace(arg, '').replace('/*', '').replace('*/', '').strip()","""""""Strip a css_value which may have latex wrapping arguments, css comment identifiers,
and whitespaces, to a valid string for latex options parsing.

For example: 'red /* --wrap */  ' --> 'red'"""""""
pandas/io/formats/style_render.py,"def _parse_latex_css_conversion(styles: CSSList) -> CSSList:

    def font_weight(value, arg):
        if value in ('bold', 'bolder'):
            return ('bfseries', f'{arg}')
        return None

    def font_style(value, arg):
        if value == 'italic':
            return ('itshape', f'{arg}')
        if value == 'oblique':
            return ('slshape', f'{arg}')
        return None

    def color(value, user_arg, command, comm_arg):
        """"""
        CSS colors have 5 formats to process:

         - 6 digit hex code: ""#ff23ee""     --> [HTML]{FF23EE}
         - 3 digit hex code: ""#f0e""        --> [HTML]{FF00EE}
         - rgba: rgba(128, 255, 0, 0.5)    --> [rgb]{0.502, 1.000, 0.000}
         - rgb: rgb(128, 255, 0,)          --> [rbg]{0.502, 1.000, 0.000}
         - string: red                     --> {red}

        Additionally rgb or rgba can be expressed in % which is also parsed.
        """"""
        arg = user_arg if user_arg != '' else comm_arg
        if value[0] == '#' and len(value) == 7:
            return (command, f'[HTML]{{{value[1:].upper()}}}{arg}')
        if value[0] == '#' and len(value) == 4:
            val = f'{value[1].upper() * 2}{value[2].upper() * 2}{value[3].upper() * 2}'
            return (command, f'[HTML]{{{val}}}{arg}')
        elif value[:3] == 'rgb':
            r = re.findall('(?<=\\()[0-9\\s%]+(?=,)', value)[0].strip()
            r = float(r[:-1]) / 100 if '%' in r else int(r) / 255
            g = re.findall('(?<=,)[0-9\\s%]+(?=,)', value)[0].strip()
            g = float(g[:-1]) / 100 if '%' in g else int(g) / 255
            if value[3] == 'a':
                b = re.findall('(?<=,)[0-9\\s%]+(?=,)', value)[1].strip()
            else:
                b = re.findall('(?<=,)[0-9\\s%]+(?=\\))', value)[0].strip()
            b = float(b[:-1]) / 100 if '%' in b else int(b) / 255
            return (command, f'[rgb]{{{r:.3f}, {g:.3f}, {b:.3f}}}{arg}')
        else:
            return (command, f'{{{value}}}{arg}')
    CONVERTED_ATTRIBUTES: dict[str, Callable] = {'font-weight': font_weight, 'background-color': partial(color, command='cellcolor', comm_arg='--lwrap'), 'color': partial(color, command='color', comm_arg=''), 'font-style': font_style}
    latex_styles: CSSList = []
    for (attribute, value) in styles:
        if isinstance(value, str) and '--latex' in value:
            latex_styles.append((attribute, value.replace('--latex', '')))
        if attribute in CONVERTED_ATTRIBUTES:
            arg = ''
            for x in ['--wrap', '--nowrap', '--lwrap', '--dwrap', '--rwrap']:
                if x in str(value):
                    (arg, value) = (x, _parse_latex_options_strip(value, x))
                    break
            latex_style = CONVERTED_ATTRIBUTES[attribute](value, arg)
            if latex_style is not None:
                latex_styles.extend([latex_style])
    return latex_styles","""""""Convert CSS (attribute,value) pairs to equivalent LaTeX (command,options) pairs.

Ignore conversion if tagged with `--latex` option, skipped if no conversion found."""""""
pandas/io/formats/style_render.py,"def _escape_latex(s):
    return s.replace('\\', 'ab2=8yz').replace('ab2=8yz ', 'ab2=8yz\\space ').replace('&', '\\&').replace('%', '\\%').replace('$', '\\$').replace('#', '\\#').replace('_', '\\_').replace('{', '\\{').replace('}', '\\}').replace('~ ', '~\\space ').replace('~', '\\textasciitilde ').replace('^ ', '^\\space ').replace('^', '\\textasciicircum ').replace('ab2=8yz', '\\textbackslash ')","""""""Replace the characters ``&``, ``%``, ``$``, ``#``, ``_``, ``{``, ``}``,
``~``, ``^``, and ``\`` in the string with LaTeX-safe sequences.

Use this if you need to display text that might contain such characters in LaTeX.

Parameters
----------
s : str
    Input to be escaped

Return
------
str :
    Escaped string"""""""
pandas/io/formats/style_render.py,"def _math_mode_with_dollar(s):
    s = s.replace('\\$', 'rt8=7wz')
    pattern = re.compile('\\$.*?\\$')
    pos = 0
    ps = pattern.search(s, pos)
    res = []
    while ps:
        res.append(_escape_latex(s[pos:ps.span()[0]]))
        res.append(ps.group())
        pos = ps.span()[1]
        ps = pattern.search(s, pos)
    res.append(_escape_latex(s[pos:len(s)]))
    return ''.join(res).replace('rt8=7wz', '\\$')","""""""All characters in LaTeX math mode are preserved.

The substrings in LaTeX math mode, which start with
the character ``$`` and end with ``$``, are preserved
without escaping. Otherwise regular LaTeX escaping applies.

Parameters
----------
s : str
    Input to be escaped

Return
------
str :
    Escaped string"""""""
pandas/io/formats/style_render.py,"def _math_mode_with_parentheses(s):
    s = s.replace('\\(', 'LEFT=6yzLEFT').replace('\\)', 'RIGHTab5=RIGHT')
    res = []
    for item in re.split('LEFT=6yz|ab5=RIGHT', s):
        if item.startswith('LEFT') and item.endswith('RIGHT'):
            res.append(item.replace('LEFT', '\\(').replace('RIGHT', '\\)'))
        elif 'LEFT' in item and 'RIGHT' in item:
            res.append(_escape_latex(item).replace('LEFT', '\\(').replace('RIGHT', '\\)'))
        else:
            res.append(_escape_latex(item).replace('LEFT', '\\textbackslash (').replace('RIGHT', '\\textbackslash )'))
    return ''.join(res)","""""""All characters in LaTeX math mode are preserved.

The substrings in LaTeX math mode, which start with
the character ``\(`` and end with ``\)``, are preserved
without escaping. Otherwise regular LaTeX escaping applies.

Parameters
----------
s : str
    Input to be escaped

Return
------
str :
    Escaped string"""""""
pandas/io/formats/style_render.py,"def _escape_latex_math(s):
    s = s.replace('\\$', 'rt8=7wz')
    ps_d = re.compile('\\$.*?\\$').search(s, 0)
    ps_p = re.compile('\\(.*?\\)').search(s, 0)
    mode = []
    if ps_d:
        mode.append(ps_d.span()[0])
    if ps_p:
        mode.append(ps_p.span()[0])
    if len(mode) == 0:
        return _escape_latex(s.replace('rt8=7wz', '\\$'))
    if s[mode[0]] == '$':
        return _math_mode_with_dollar(s.replace('rt8=7wz', '\\$'))
    if s[mode[0] - 1:mode[0] + 1] == '\\(':
        return _math_mode_with_parentheses(s.replace('rt8=7wz', '\\$'))
    else:
        return _escape_latex(s.replace('rt8=7wz', '\\$'))","""""""All characters in LaTeX math mode are preserved.

The substrings in LaTeX math mode, which either are surrounded
by two characters ``$`` or start with the character ``\(`` and end with ``\)``,
are preserved without escaping. Otherwise regular LaTeX escaping applies.

Parameters
----------
s : str
    Input to be escaped

Return
------
str :
    Escaped string"""""""
pandas/io/gbq.py,"def read_gbq(query: str, project_id: str | None=None, index_col: str | None=None, col_order: list[str] | None=None, reauth: bool=False, auth_local_webserver: bool=True, dialect: str | None=None, location: str | None=None, configuration: dict[str, Any] | None=None, credentials: google.auth.credentials.Credentials | None=None, use_bqstorage_api: bool | None=None, max_results: int | None=None, progress_bar_type: str | None=None) -> DataFrame:
    pandas_gbq = _try_import()
    kwargs: dict[str, str | bool | int | None] = {}
    if use_bqstorage_api is not None:
        kwargs['use_bqstorage_api'] = use_bqstorage_api
    if max_results is not None:
        kwargs['max_results'] = max_results
    kwargs['progress_bar_type'] = progress_bar_type
    return pandas_gbq.read_gbq(query, project_id=project_id, index_col=index_col, col_order=col_order, reauth=reauth, auth_local_webserver=auth_local_webserver, dialect=dialect, location=location, configuration=configuration, credentials=credentials, **kwargs)","""""""Load data from Google BigQuery.

This function requires the `pandas-gbq package
<https://pandas-gbq.readthedocs.io>`__.

See the `How to authenticate with Google BigQuery
<https://pandas-gbq.readthedocs.io/en/latest/howto/authentication.html>`__
guide for authentication instructions.

Parameters
----------
query : str
    SQL-Like Query to return data values.
project_id : str, optional
    Google BigQuery Account project ID. Optional when available from
    the environment.
index_col : str, optional
    Name of result column to use for index in results DataFrame.
col_order : list(str), optional
    List of BigQuery column names in the desired order for results
    DataFrame.
reauth : bool, default False
    Force Google BigQuery to re-authenticate the user. This is useful
    if multiple accounts are used.
auth_local_webserver : bool, default True
    Use the `local webserver flow`_ instead of the `console flow`_
    when getting user credentials.

    .. _local webserver flow:
        https://google-auth-oauthlib.readthedocs.io/en/latest/reference/google_auth_oauthlib.flow.html#google_auth_oauthlib.flow.InstalledAppFlow.run_local_server
    .. _console flow:
        https://google-auth-oauthlib.readthedocs.io/en/latest/reference/google_auth_oauthlib.flow.html#google_auth_oauthlib.flow.InstalledAppFlow.run_console

    *New in version 0.2.0 of pandas-gbq*.

    .. versionchanged:: 1.5.0
       Default value is changed to ``True``. Google has deprecated the
       ``auth_local_webserver = False`` `""out of band"" (copy-paste)
       flow
       <https://developers.googleblog.com/2022/02/making-oauth-flows-safer.html?m=1#disallowed-oob>`_.
dialect : str, default 'legacy'
    Note: The default value is changing to 'standard' in a future version.

    SQL syntax dialect to use. Value can be one of:

    ``'legacy'``
        Use BigQuery's legacy SQL dialect. For more information see
        `BigQuery Legacy SQL Reference
        <https://cloud.google.com/bigquery/docs/reference/legacy-sql>`__.
    ``'standard'``
        Use BigQuery's standard SQL, which is
        compliant with the SQL 2011 standard. For more information
        see `BigQuery Standard SQL Reference
        <https://cloud.google.com/bigquery/docs/reference/standard-sql/>`__.
location : str, optional
    Location where the query job should run. See the `BigQuery locations
    documentation
    <https://cloud.google.com/bigquery/docs/dataset-locations>`__ for a
    list of available locations. The location must match that of any
    datasets used in the query.

    *New in version 0.5.0 of pandas-gbq*.
configuration : dict, optional
    Query config parameters for job processing.
    For example:

        configuration = {'query': {'useQueryCache': False}}

    For more information see `BigQuery REST API Reference
    <https://cloud.google.com/bigquery/docs/reference/rest/v2/jobs#configuration.query>`__.
credentials : google.auth.credentials.Credentials, optional
    Credentials for accessing Google APIs. Use this parameter to override
    default credentials, such as to use Compute Engine
    :class:`google.auth.compute_engine.Credentials` or Service Account
    :class:`google.oauth2.service_account.Credentials` directly.

    *New in version 0.8.0 of pandas-gbq*.
use_bqstorage_api : bool, default False
    Use the `BigQuery Storage API
    <https://cloud.google.com/bigquery/docs/reference/storage/>`__ to
    download query results quickly, but at an increased cost. To use this
    API, first `enable it in the Cloud Console
    <https://console.cloud.google.com/apis/library/bigquerystorage.googleapis.com>`__.
    You must also have the `bigquery.readsessions.create
    <https://cloud.google.com/bigquery/docs/access-control#roles>`__
    permission on the project you are billing queries to.

    This feature requires version 0.10.0 or later of the ``pandas-gbq``
    package. It also requires the ``google-cloud-bigquery-storage`` and
    ``fastavro`` packages.

max_results : int, optional
    If set, limit the maximum number of rows to fetch from the query
    results.

progress_bar_type : Optional, str
    If set, use the `tqdm <https://tqdm.github.io/>`__ library to
    display a progress bar while the data downloads. Install the
    ``tqdm`` package to use this feature.

    Possible values of ``progress_bar_type`` include:

    ``None``
        No progress bar.
    ``'tqdm'``
        Use the :func:`tqdm.tqdm` function to print a progress bar
        to :data:`sys.stderr`.
    ``'tqdm_notebook'``
        Use the :func:`tqdm.tqdm_notebook` function to display a
        progress bar as a Jupyter notebook widget.
    ``'tqdm_gui'``
        Use the :func:`tqdm.tqdm_gui` function to display a
        progress bar as a graphical dialog box.

Returns
-------
df: DataFrame
    DataFrame representing results of query.

See Also
--------
pandas_gbq.read_gbq : This function in the pandas-gbq library.
DataFrame.to_gbq : Write a DataFrame to Google BigQuery.

Examples
--------
Example taken from `Google BigQuery documentation
<https://cloud.google.com/bigquery/docs/pandas-gbq-migration>`_

>>> sql = ""SELECT name FROM table_name WHERE state = 'TX' LIMIT 100;""
>>> df = pd.read_gbq(sql, dialect=""standard"")  # doctest: +SKIP
>>> project_id = ""your-project-id""  # doctest: +SKIP
>>> df = pd.read_gbq(sql,
...                  project_id=project_id,
...                  dialect=""standard""
...                  )  # doctest: +SKIP"""""""
pandas/io/html.py,"def _remove_whitespace(s: str, regex: Pattern=_RE_WHITESPACE) -> str:
    return regex.sub(' ', s.strip())","""""""Replace extra whitespace inside of a string with a single space.

Parameters
----------
s : str or unicode
    The string from which to remove extra whitespace.
regex : re.Pattern
    The regular expression to use to remove extra whitespace.

Returns
-------
subd : str or unicode
    `s` with all extra whitespace replaced with a single space."""""""
pandas/io/html.py,"def _get_skiprows(skiprows: int | Sequence[int] | slice | None) -> int | Sequence[int]:
    if isinstance(skiprows, slice):
        (start, step) = (skiprows.start or 0, skiprows.step or 1)
        return list(range(start, skiprows.stop, step))
    elif isinstance(skiprows, numbers.Integral) or is_list_like(skiprows):
        return cast('int | Sequence[int]', skiprows)
    elif skiprows is None:
        return 0
    raise TypeError(f'{type(skiprows).__name__} is not a valid type for skipping rows')","""""""Get an iterator given an integer, slice or container.

Parameters
----------
skiprows : int, slice, container
    The iterator to use to skip rows; can also be a slice.

Raises
------
TypeError
    * If `skiprows` is not a slice, integer, or Container

Returns
-------
it : iterable
    A proper iterator to use to skip rows of a DataFrame."""""""
pandas/io/html.py,"def _read(obj: FilePath | BaseBuffer, encoding: str | None, storage_options: StorageOptions | None) -> str | bytes:
    text: str | bytes
    if is_url(obj) or hasattr(obj, 'read') or (isinstance(obj, str) and file_exists(obj)):
        with get_handle(obj, 'r', encoding=encoding, storage_options=storage_options) as handles:
            text = handles.handle.read()
    elif isinstance(obj, (str, bytes)):
        text = obj
    else:
        raise TypeError(f""Cannot read object of type '{type(obj).__name__}'"")
    return text","""""""Try to read from a url, file or string.

Parameters
----------
obj : str, unicode, path object, or file-like object

Returns
-------
raw_text : str"""""""
pandas/io/html.py,"def _build_xpath_expr(attrs) -> str:
    if 'class_' in attrs:
        attrs['class'] = attrs.pop('class_')
    s = ' and '.join([f'@{k}={repr(v)}' for (k, v) in attrs.items()])
    return f'[{s}]'","""""""Build an xpath expression to simulate bs4's ability to pass in kwargs to
search for attributes when using the lxml parser.

Parameters
----------
attrs : dict
    A dict of HTML attributes. These are NOT checked for validity.

Returns
-------
expr : unicode
    An XPath expression that checks for the given HTML attributes."""""""
pandas/io/html.py,"def _parser_dispatch(flavor: str | None) -> type[_HtmlFrameParser]:
    valid_parsers = list(_valid_parsers.keys())
    if flavor not in valid_parsers:
        raise ValueError(f'{repr(flavor)} is not a valid flavor, valid flavors are {valid_parsers}')
    if flavor in ('bs4', 'html5lib'):
        import_optional_dependency('html5lib')
        import_optional_dependency('bs4')
    else:
        import_optional_dependency('lxml.etree')
    return _valid_parsers[flavor]","""""""Choose the parser based on the input flavor.

Parameters
----------
flavor : str
    The type of parser to use. This must be a valid backend.

Returns
-------
cls : _HtmlFrameParser subclass
    The parser class based on the requested input flavor.

Raises
------
ValueError
    * If `flavor` is not a valid backend.
ImportError
    * If you do not have the requested `flavor`"""""""
pandas/io/html.py,"@doc(storage_options=_shared_docs['storage_options'])
def read_html(io: FilePath | ReadBuffer[str], *, match: str | Pattern='.+', flavor: str | Sequence[str] | None=None, header: int | Sequence[int] | None=None, index_col: int | Sequence[int] | None=None, skiprows: int | Sequence[int] | slice | None=None, attrs: dict[str, str] | None=None, parse_dates: bool=False, thousands: str | None=',', encoding: str | None=None, decimal: str='.', converters: dict | None=None, na_values: Iterable[object] | None=None, keep_default_na: bool=True, displayed_only: bool=True, extract_links: Literal[None, 'header', 'footer', 'body', 'all']=None, dtype_backend: DtypeBackend | lib.NoDefault=lib.no_default, storage_options: StorageOptions=None) -> list[DataFrame]:
    if isinstance(skiprows, numbers.Integral) and skiprows < 0:
        raise ValueError('cannot skip rows starting from the end of the data (you passed a negative value)')
    if extract_links not in [None, 'header', 'footer', 'body', 'all']:
        raise ValueError(f'`extract_links` must be one of {{None, ""header"", ""footer"", ""body"", ""all""}}, got ""{extract_links}""')
    validate_header_arg(header)
    check_dtype_backend(dtype_backend)
    io = stringify_path(io)
    if isinstance(io, str) and (not any([is_file_like(io), file_exists(io), is_url(io), is_fsspec_url(io)])):
        warnings.warn(""Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object."", FutureWarning, stacklevel=find_stack_level())
    return _parse(flavor=flavor, io=io, match=match, header=header, index_col=index_col, skiprows=skiprows, parse_dates=parse_dates, thousands=thousands, attrs=attrs, encoding=encoding, decimal=decimal, converters=converters, na_values=na_values, keep_default_na=keep_default_na, displayed_only=displayed_only, extract_links=extract_links, dtype_backend=dtype_backend, storage_options=storage_options)","""""""Read HTML tables into a ``list`` of ``DataFrame`` objects.

Parameters
----------
io : str, path object, or file-like object
    String, path object (implementing ``os.PathLike[str]``), or file-like
    object implementing a string ``read()`` function.
    The string can represent a URL or the HTML itself. Note that
    lxml only accepts the http, ftp and file url protocols. If you have a
    URL that starts with ``'https'`` you might try removing the ``'s'``.

    .. deprecated:: 2.1.0
        Passing html literal strings is deprecated.
        Wrap literal string/bytes input in ``io.StringIO``/``io.BytesIO`` instead.

match : str or compiled regular expression, optional
    The set of tables containing text matching this regex or string will be
    returned. Unless the HTML is extremely simple you will probably need to
    pass a non-empty string here. Defaults to '.+' (match any non-empty
    string). The default value will return all tables contained on a page.
    This value is converted to a regular expression so that there is
    consistent behavior between Beautiful Soup and lxml.

flavor : str or list-like, optional
    The parsing engine (or list of parsing engines) to use. 'bs4' and
    'html5lib' are synonymous with each other, they are both there for
    backwards compatibility. The default of ``None`` tries to use ``lxml``
    to parse and if that fails it falls back on ``bs4`` + ``html5lib``.

header : int or list-like, optional
    The row (or list of rows for a :class:`~pandas.MultiIndex`) to use to
    make the columns headers.

index_col : int or list-like, optional
    The column (or list of columns) to use to create the index.

skiprows : int, list-like or slice, optional
    Number of rows to skip after parsing the column integer. 0-based. If a
    sequence of integers or a slice is given, will skip the rows indexed by
    that sequence.  Note that a single element sequence means 'skip the nth
    row' whereas an integer means 'skip n rows'.

attrs : dict, optional
    This is a dictionary of attributes that you can pass to use to identify
    the table in the HTML. These are not checked for validity before being
    passed to lxml or Beautiful Soup. However, these attributes must be
    valid HTML table attributes to work correctly. For example, ::

        attrs = {{'id': 'table'}}

    is a valid attribute dictionary because the 'id' HTML tag attribute is
    a valid HTML attribute for *any* HTML tag as per `this document
    <https://html.spec.whatwg.org/multipage/dom.html#global-attributes>`__. ::

        attrs = {{'asdf': 'table'}}

    is *not* a valid attribute dictionary because 'asdf' is not a valid
    HTML attribute even if it is a valid XML attribute.  Valid HTML 4.01
    table attributes can be found `here
    <http://www.w3.org/TR/REC-html40/struct/tables.html#h-11.2>`__. A
    working draft of the HTML 5 spec can be found `here
    <https://html.spec.whatwg.org/multipage/tables.html>`__. It contains the
    latest information on table attributes for the modern web.

parse_dates : bool, optional
    See :func:`~read_csv` for more details.

thousands : str, optional
    Separator to use to parse thousands. Defaults to ``','``.

encoding : str, optional
    The encoding used to decode the web page. Defaults to ``None``.``None``
    preserves the previous encoding behavior, which depends on the
    underlying parser library (e.g., the parser library will try to use
    the encoding provided by the document).

decimal : str, default '.'
    Character to recognize as decimal point (e.g. use ',' for European
    data).

converters : dict, default None
    Dict of functions for converting values in certain columns. Keys can
    either be integers or column labels, values are functions that take one
    input argument, the cell (not column) content, and return the
    transformed content.

na_values : iterable, default None
    Custom NA values.

keep_default_na : bool, default True
    If na_values are specified and keep_default_na is False the default NaN
    values are overridden, otherwise they're appended to.

displayed_only : bool, default True
    Whether elements with ""display: none"" should be parsed.

extract_links : {{None, ""all"", ""header"", ""body"", ""footer""}}
    Table elements in the specified section(s) with <a> tags will have their
    href extracted.

    .. versionadded:: 1.5.0

dtype_backend : {{'numpy_nullable', 'pyarrow'}}, default 'numpy_nullable'
    Back-end data type applied to the resultant :class:`DataFrame`
    (still experimental). Behaviour is as follows:

    * ``""numpy_nullable""``: returns nullable-dtype-backed :class:`DataFrame`
      (default).
    * ``""pyarrow""``: returns pyarrow-backed nullable :class:`ArrowDtype`
      DataFrame.

    .. versionadded:: 2.0

{storage_options}

    .. versionadded:: 2.1.0

Returns
-------
dfs
    A list of DataFrames.

See Also
--------
read_csv : Read a comma-separated values (csv) file into DataFrame.

Notes
-----
Before using this function you should read the :ref:`gotchas about the
HTML parsing libraries <io.html.gotchas>`.

Expect to do some cleanup after you call this function. For example, you
might need to manually assign column names if the column names are
converted to NaN when you pass the `header=0` argument. We try to assume as
little as possible about the structure of the table and push the
idiosyncrasies of the HTML contained in the table to the user.

This function searches for ``<table>`` elements and only for ``<tr>``
and ``<th>`` rows and ``<td>`` elements within each ``<tr>`` or ``<th>``
element in the table. ``<td>`` stands for ""table data"". This function
attempts to properly handle ``colspan`` and ``rowspan`` attributes.
If the function has a ``<thead>`` argument, it is used to construct
the header, otherwise the function attempts to find the header within
the body (by putting rows with only ``<th>`` elements into the header).

Similar to :func:`~read_csv` the `header` argument is applied
**after** `skiprows` is applied.

This function will *always* return a list of :class:`DataFrame` *or*
it will fail, e.g., it will *not* return an empty list.

Examples
--------
See the :ref:`read_html documentation in the IO section of the docs
<io.read_html>` for some examples of reading in HTML tables."""""""
pandas/io/json/_json.py,"@doc(storage_options=_shared_docs['storage_options'], decompression_options=_shared_docs['decompression_options'] % 'path_or_buf')
def read_json(path_or_buf: FilePath | ReadBuffer[str] | ReadBuffer[bytes], *, orient: str | None=None, typ: Literal['frame', 'series']='frame', dtype: DtypeArg | None=None, convert_axes: bool | None=None, convert_dates: bool | list[str]=True, keep_default_dates: bool=True, precise_float: bool=False, date_unit: str | None=None, encoding: str | None=None, encoding_errors: str | None='strict', lines: bool=False, chunksize: int | None=None, compression: CompressionOptions='infer', nrows: int | None=None, storage_options: StorageOptions | None=None, dtype_backend: DtypeBackend | lib.NoDefault=lib.no_default, engine: JSONEngine='ujson') -> DataFrame | Series | JsonReader:
    if orient == 'table' and dtype:
        raise ValueError(""cannot pass both dtype and orient='table'"")
    if orient == 'table' and convert_axes:
        raise ValueError(""cannot pass both convert_axes and orient='table'"")
    check_dtype_backend(dtype_backend)
    if dtype is None and orient != 'table':
        dtype = True
    if convert_axes is None and orient != 'table':
        convert_axes = True
    json_reader = JsonReader(path_or_buf, orient=orient, typ=typ, dtype=dtype, convert_axes=convert_axes, convert_dates=convert_dates, keep_default_dates=keep_default_dates, precise_float=precise_float, date_unit=date_unit, encoding=encoding, lines=lines, chunksize=chunksize, compression=compression, nrows=nrows, storage_options=storage_options, encoding_errors=encoding_errors, dtype_backend=dtype_backend, engine=engine)
    if chunksize:
        return json_reader
    else:
        return json_reader.read()","""""""Convert a JSON string to pandas object.

Parameters
----------
path_or_buf : a valid JSON str, path object or file-like object
    Any valid string path is acceptable. The string could be a URL. Valid
    URL schemes include http, ftp, s3, and file. For file URLs, a host is
    expected. A local file could be:
    ``file://localhost/path/to/table.json``.

    If you want to pass in a path object, pandas accepts any
    ``os.PathLike``.

    By file-like object, we refer to objects with a ``read()`` method,
    such as a file handle (e.g. via builtin ``open`` function)
    or ``StringIO``.

    .. deprecated:: 2.1.0
        Passing json literal strings is deprecated.

orient : str, optional
    Indication of expected JSON string format.
    Compatible JSON strings can be produced by ``to_json()`` with a
    corresponding orient value.
    The set of possible orients is:

    - ``'split'`` : dict like
      ``{{index -> [index], columns -> [columns], data -> [values]}}``
    - ``'records'`` : list like
      ``[{{column -> value}}, ... , {{column -> value}}]``
    - ``'index'`` : dict like ``{{index -> {{column -> value}}}}``
    - ``'columns'`` : dict like ``{{column -> {{index -> value}}}}``
    - ``'values'`` : just the values array
    - ``'table'`` : dict like ``{{'schema': {{schema}}, 'data': {{data}}}}``

    The allowed and default values depend on the value
    of the `typ` parameter.

    * when ``typ == 'series'``,

      - allowed orients are ``{{'split','records','index'}}``
      - default is ``'index'``
      - The Series index must be unique for orient ``'index'``.

    * when ``typ == 'frame'``,

      - allowed orients are ``{{'split','records','index',
        'columns','values', 'table'}}``
      - default is ``'columns'``
      - The DataFrame index must be unique for orients ``'index'`` and
        ``'columns'``.
      - The DataFrame columns must be unique for orients ``'index'``,
        ``'columns'``, and ``'records'``.

typ : {{'frame', 'series'}}, default 'frame'
    The type of object to recover.

dtype : bool or dict, default None
    If True, infer dtypes; if a dict of column to dtype, then use those;
    if False, then don't infer dtypes at all, applies only to the data.

    For all ``orient`` values except ``'table'``, default is True.

convert_axes : bool, default None
    Try to convert the axes to the proper dtypes.

    For all ``orient`` values except ``'table'``, default is True.

convert_dates : bool or list of str, default True
    If True then default datelike columns may be converted (depending on
    keep_default_dates).
    If False, no dates will be converted.
    If a list of column names, then those columns will be converted and
    default datelike columns may also be converted (depending on
    keep_default_dates).

keep_default_dates : bool, default True
    If parsing dates (convert_dates is not False), then try to parse the
    default datelike columns.
    A column label is datelike if

    * it ends with ``'_at'``,

    * it ends with ``'_time'``,

    * it begins with ``'timestamp'``,

    * it is ``'modified'``, or

    * it is ``'date'``.

precise_float : bool, default False
    Set to enable usage of higher precision (strtod) function when
    decoding string to double values. Default (False) is to use fast but
    less precise builtin functionality.

date_unit : str, default None
    The timestamp unit to detect if converting dates. The default behaviour
    is to try and detect the correct precision, but if this is not desired
    then pass one of 's', 'ms', 'us' or 'ns' to force parsing only seconds,
    milliseconds, microseconds or nanoseconds respectively.

encoding : str, default is 'utf-8'
    The encoding to use to decode py3 bytes.

encoding_errors : str, optional, default ""strict""
    How encoding errors are treated. `List of possible values
    <https://docs.python.org/3/library/codecs.html#error-handlers>`_ .

    .. versionadded:: 1.3.0

lines : bool, default False
    Read the file as a json object per line.

chunksize : int, optional
    Return JsonReader object for iteration.
    See the `line-delimited json docs
    <https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html#line-delimited-json>`_
    for more information on ``chunksize``.
    This can only be passed if `lines=True`.
    If this is None, the file will be read into memory all at once.

    .. versionchanged:: 1.2

       ``JsonReader`` is a context manager.

{decompression_options}

    .. versionchanged:: 1.4.0 Zstandard support.

nrows : int, optional
    The number of lines from the line-delimited jsonfile that has to be read.
    This can only be passed if `lines=True`.
    If this is None, all the rows will be returned.

{storage_options}

    .. versionadded:: 1.2.0

dtype_backend : {{'numpy_nullable', 'pyarrow'}}, default 'numpy_nullable'
    Back-end data type applied to the resultant :class:`DataFrame`
    (still experimental). Behaviour is as follows:

    * ``""numpy_nullable""``: returns nullable-dtype-backed :class:`DataFrame`
      (default).
    * ``""pyarrow""``: returns pyarrow-backed nullable :class:`ArrowDtype`
      DataFrame.

    .. versionadded:: 2.0

engine : {{""ujson"", ""pyarrow""}}, default ""ujson""
    Parser engine to use. The ``""pyarrow""`` engine is only available when
    ``lines=True``.

    .. versionadded:: 2.0

Returns
-------
Series, DataFrame, or pandas.api.typing.JsonReader
    A JsonReader is returned when ``chunksize`` is not ``0`` or ``None``.
    Otherwise, the type returned depends on the value of ``typ``.

See Also
--------
DataFrame.to_json : Convert a DataFrame to a JSON string.
Series.to_json : Convert a Series to a JSON string.
json_normalize : Normalize semi-structured JSON data into a flat table.

Notes
-----
Specific to ``orient='table'``, if a :class:`DataFrame` with a literal
:class:`Index` name of `index` gets written with :func:`to_json`, the
subsequent read operation will incorrectly set the :class:`Index` name to
``None``. This is because `index` is also used by :func:`DataFrame.to_json`
to denote a missing :class:`Index` name, and the subsequent
:func:`read_json` operation cannot distinguish between the two. The same
limitation is encountered with a :class:`MultiIndex` and any names
beginning with ``'level_'``.

Examples
--------
>>> from io import StringIO
>>> df = pd.DataFrame([['a', 'b'], ['c', 'd']],
...                   index=['row 1', 'row 2'],
...                   columns=['col 1', 'col 2'])

Encoding/decoding a Dataframe using ``'split'`` formatted JSON:

>>> df.to_json(orient='split')
    '{{""columns"":[""col 1"",""col 2""],""index"":[""row 1"",""row 2""],""data"":[[""a"",""b""],[""c"",""d""]]}}'
>>> pd.read_json(StringIO(_), orient='split')
      col 1 col 2
row 1     a     b
row 2     c     d

Encoding/decoding a Dataframe using ``'index'`` formatted JSON:

>>> df.to_json(orient='index')
'{{""row 1"":{{""col 1"":""a"",""col 2"":""b""}},""row 2"":{{""col 1"":""c"",""col 2"":""d""}}}}'

>>> pd.read_json(StringIO(_), orient='index')
      col 1 col 2
row 1     a     b
row 2     c     d

Encoding/decoding a Dataframe using ``'records'`` formatted JSON.
Note that index labels are not preserved with this encoding.

>>> df.to_json(orient='records')
'[{{""col 1"":""a"",""col 2"":""b""}},{{""col 1"":""c"",""col 2"":""d""}}]'
>>> pd.read_json(StringIO(_), orient='records')
  col 1 col 2
0     a     b
1     c     d

Encoding with Table Schema

>>> df.to_json(orient='table')
    '{{""schema"":{{""fields"":[{{""name"":""index"",""type"":""string""}},{{""name"":""col 1"",""type"":""string""}},{{""name"":""col 2"",""type"":""string""}}],""primaryKey"":[""index""],""pandas_version"":""1.4.0""}},""data"":[{{""index"":""row 1"",""col 1"":""a"",""col 2"":""b""}},{{""index"":""row 2"",""col 1"":""c"",""col 2"":""d""}}]}}'"""""""
pandas/io/json/_normalize.py,"def convert_to_line_delimits(s: str) -> str:
    if not s[0] == '[' and s[-1] == ']':
        return s
    s = s[1:-1]
    return convert_json_to_lines(s)","""""""Helper function that converts JSON lists to line delimited JSON."""""""
pandas/io/json/_normalize.py,"def nested_to_record(ds, prefix: str='', sep: str='.', level: int=0, max_level: int | None=None):
    singleton = False
    if isinstance(ds, dict):
        ds = [ds]
        singleton = True
    new_ds = []
    for d in ds:
        new_d = copy.deepcopy(d)
        for (k, v) in d.items():
            if not isinstance(k, str):
                k = str(k)
            if level == 0:
                newkey = k
            else:
                newkey = prefix + sep + k
            if not isinstance(v, dict) or (max_level is not None and level >= max_level):
                if level != 0:
                    v = new_d.pop(k)
                    new_d[newkey] = v
                continue
            v = new_d.pop(k)
            new_d.update(nested_to_record(v, newkey, sep, level + 1, max_level))
        new_ds.append(new_d)
    if singleton:
        return new_ds[0]
    return new_ds","""""""A simplified json_normalize

Converts a nested dict into a flat dict (""record""), unlike json_normalize,
it does not attempt to extract a subset of the data.

Parameters
----------
ds : dict or list of dicts
prefix: the prefix, optional, default: """"
sep : str, default '.'
    Nested records will generate names separated by sep,
    e.g., for sep='.', { 'foo' : { 'bar' : 0 } } -> foo.bar
level: int, optional, default: 0
    The number of levels in the json string.

max_level: int, optional, default: None
    The max depth to normalize.

Returns
-------
d - dict or list of dicts, matching `ds`

Examples
--------
>>> nested_to_record(
...     dict(flat1=1, dict1=dict(c=1, d=2), nested=dict(e=dict(c=1, d=2), d=2))
... )
{'flat1': 1, 'dict1.c': 1, 'dict1.d': 2, 'nested.e.c': 1, 'nested.e.d': 2, 'nested.d': 2}"""""""
pandas/io/json/_normalize.py,"def _normalise_json(data: Any, key_string: str, normalized_dict: dict[str, Any], separator: str) -> dict[str, Any]:
    if isinstance(data, dict):
        for (key, value) in data.items():
            new_key = f'{key_string}{separator}{key}'
            if not key_string:
                new_key = new_key.removeprefix(separator)
            _normalise_json(data=value, key_string=new_key, normalized_dict=normalized_dict, separator=separator)
    else:
        normalized_dict[key_string] = data
    return normalized_dict","""""""Main recursive function
Designed for the most basic use case of pd.json_normalize(data)
intended as a performance improvement, see #15621

Parameters
----------
data : Any
    Type dependent on types contained within nested Json
key_string : str
    New key (with separator(s) in) for data
normalized_dict : dict
    The new normalized/flattened Json dict
separator : str, default '.'
    Nested records will generate names separated by sep,
    e.g., for sep='.', { 'foo' : { 'bar' : 0 } } -> foo.bar"""""""
pandas/io/json/_normalize.py,"def _normalise_json_ordered(data: dict[str, Any], separator: str) -> dict[str, Any]:
    top_dict_ = {k: v for (k, v) in data.items() if not isinstance(v, dict)}
    nested_dict_ = _normalise_json(data={k: v for (k, v) in data.items() if isinstance(v, dict)}, key_string='', normalized_dict={}, separator=separator)
    return {**top_dict_, **nested_dict_}","""""""Order the top level keys and then recursively go to depth

Parameters
----------
data : dict or list of dicts
separator : str, default '.'
    Nested records will generate names separated by sep,
    e.g., for sep='.', { 'foo' : { 'bar' : 0 } } -> foo.bar

Returns
-------
dict or list of dicts, matching `normalised_json_object`"""""""
pandas/io/json/_normalize.py,"def _simple_json_normalize(ds: dict | list[dict], sep: str='.') -> dict | list[dict] | Any:
    normalised_json_object = {}
    if isinstance(ds, dict):
        normalised_json_object = _normalise_json_ordered(data=ds, separator=sep)
    elif isinstance(ds, list):
        normalised_json_list = [_simple_json_normalize(row, sep=sep) for row in ds]
        return normalised_json_list
    return normalised_json_object","""""""A optimized basic json_normalize

Converts a nested dict into a flat dict (""record""), unlike
json_normalize and nested_to_record it doesn't do anything clever.
But for the most basic use cases it enhances performance.
E.g. pd.json_normalize(data)

Parameters
----------
ds : dict or list of dicts
sep : str, default '.'
    Nested records will generate names separated by sep,
    e.g., for sep='.', { 'foo' : { 'bar' : 0 } } -> foo.bar

Returns
-------
frame : DataFrame
d - dict or list of dicts, matching `normalised_json_object`

Examples
--------
>>> _simple_json_normalize(
...     {
...         ""flat1"": 1,
...         ""dict1"": {""c"": 1, ""d"": 2},
...         ""nested"": {""e"": {""c"": 1, ""d"": 2}, ""d"": 2},
...     }
... )
{'flat1': 1, 'dict1.c': 1, 'dict1.d': 2, 'nested.e.c': 1, 'nested.e.d': 2, 'nested.d': 2}"""""""
pandas/io/json/_normalize.py,"def json_normalize(data: dict | list[dict], record_path: str | list | None=None, meta: str | list[str | list[str]] | None=None, meta_prefix: str | None=None, record_prefix: str | None=None, errors: IgnoreRaise='raise', sep: str='.', max_level: int | None=None) -> DataFrame:

    def _pull_field(js: dict[str, Any], spec: list | str, extract_record: bool=False) -> Scalar | Iterable:
        """"""Internal function to pull field""""""
        result = js
        try:
            if isinstance(spec, list):
                for field in spec:
                    if result is None:
                        raise KeyError(field)
                    result = result[field]
            else:
                result = result[spec]
        except KeyError as e:
            if extract_record:
                raise KeyError(f'Key {e} not found. If specifying a record_path, all elements of data should have the path.') from e
            if errors == 'ignore':
                return np.nan
            else:
                raise KeyError(f""Key {e} not found. To replace missing values of {e} with np.nan, pass in errors='ignore'"") from e
        return result

    def _pull_records(js: dict[str, Any], spec: list | str) -> list:
        """"""
        Internal function to pull field for records, and similar to
        _pull_field, but require to return list. And will raise error
        if has non iterable value.
        """"""
        result = _pull_field(js, spec, extract_record=True)
        if not isinstance(result, list):
            if pd.isnull(result):
                result = []
            else:
                raise TypeError(f'{js} has non list value {result} for path {spec}. Must be list or null.')
        return result
    if isinstance(data, list) and (not data):
        return DataFrame()
    elif isinstance(data, dict):
        data = [data]
    elif isinstance(data, abc.Iterable) and (not isinstance(data, str)):
        data = list(data)
    else:
        raise NotImplementedError
    if record_path is None and meta is None and (meta_prefix is None) and (record_prefix is None) and (max_level is None):
        return DataFrame(_simple_json_normalize(data, sep=sep))
    if record_path is None:
        if any(([isinstance(x, dict) for x in y.values()] for y in data)):
            data = nested_to_record(data, sep=sep, max_level=max_level)
        return DataFrame(data)
    elif not isinstance(record_path, list):
        record_path = [record_path]
    if meta is None:
        meta = []
    elif not isinstance(meta, list):
        meta = [meta]
    _meta = [m if isinstance(m, list) else [m] for m in meta]
    records: list = []
    lengths = []
    meta_vals: DefaultDict = defaultdict(list)
    meta_keys = [sep.join(val) for val in _meta]

    def _recursive_extract(data, path, seen_meta, level: int=0) -> None:
        if isinstance(data, dict):
            data = [data]
        if len(path) > 1:
            for obj in data:
                for (val, key) in zip(_meta, meta_keys):
                    if level + 1 == len(val):
                        seen_meta[key] = _pull_field(obj, val[-1])
                _recursive_extract(obj[path[0]], path[1:], seen_meta, level=level + 1)
        else:
            for obj in data:
                recs = _pull_records(obj, path[0])
                recs = [nested_to_record(r, sep=sep, max_level=max_level) if isinstance(r, dict) else r for r in recs]
                lengths.append(len(recs))
                for (val, key) in zip(_meta, meta_keys):
                    if level + 1 > len(val):
                        meta_val = seen_meta[key]
                    else:
                        meta_val = _pull_field(obj, val[level:])
                    meta_vals[key].append(meta_val)
                records.extend(recs)
    _recursive_extract(data, record_path, {}, level=0)
    result = DataFrame(records)
    if record_prefix is not None:
        result = result.rename(columns=lambda x: f'{record_prefix}{x}')
    for (k, v) in meta_vals.items():
        if meta_prefix is not None:
            k = meta_prefix + k
        if k in result:
            raise ValueError(f'Conflicting metadata name {k}, need distinguishing prefix ')
        values = np.array(v, dtype=object)
        if values.ndim > 1:
            values = np.empty((len(v),), dtype=object)
            for (i, v) in enumerate(v):
                values[i] = v
        result[k] = values.repeat(lengths)
    return result","""""""Normalize semi-structured JSON data into a flat table.

Parameters
----------
data : dict or list of dicts
    Unserialized JSON objects.
record_path : str or list of str, default None
    Path in each object to list of records. If not passed, data will be
    assumed to be an array of records.
meta : list of paths (str or list of str), default None
    Fields to use as metadata for each record in resulting table.
meta_prefix : str, default None
    If True, prefix records with dotted (?) path, e.g. foo.bar.field if
    meta is ['foo', 'bar'].
record_prefix : str, default None
    If True, prefix records with dotted (?) path, e.g. foo.bar.field if
    path to records is ['foo', 'bar'].
errors : {'raise', 'ignore'}, default 'raise'
    Configures error handling.

    * 'ignore' : will ignore KeyError if keys listed in meta are not
      always present.
    * 'raise' : will raise KeyError if keys listed in meta are not
      always present.
sep : str, default '.'
    Nested records will generate names separated by sep.
    e.g., for sep='.', {'foo': {'bar': 0}} -> foo.bar.
max_level : int, default None
    Max number of levels(depth of dict) to normalize.
    if None, normalizes all levels.

Returns
-------
frame : DataFrame
Normalize semi-structured JSON data into a flat table.

Examples
--------
>>> data = [
...     {""id"": 1, ""name"": {""first"": ""Coleen"", ""last"": ""Volk""}},
...     {""name"": {""given"": ""Mark"", ""family"": ""Regner""}},
...     {""id"": 2, ""name"": ""Faye Raker""},
... ]
>>> pd.json_normalize(data)
    id name.first name.last name.given name.family        name
0  1.0     Coleen      Volk        NaN         NaN         NaN
1  NaN        NaN       NaN       Mark      Regner         NaN
2  2.0        NaN       NaN        NaN         NaN  Faye Raker

>>> data = [
...     {
...         ""id"": 1,
...         ""name"": ""Cole Volk"",
...         ""fitness"": {""height"": 130, ""weight"": 60},
...     },
...     {""name"": ""Mark Reg"", ""fitness"": {""height"": 130, ""weight"": 60}},
...     {
...         ""id"": 2,
...         ""name"": ""Faye Raker"",
...         ""fitness"": {""height"": 130, ""weight"": 60},
...     },
... ]
>>> pd.json_normalize(data, max_level=0)
    id        name                        fitness
0  1.0   Cole Volk  {'height': 130, 'weight': 60}
1  NaN    Mark Reg  {'height': 130, 'weight': 60}
2  2.0  Faye Raker  {'height': 130, 'weight': 60}

Normalizes nested data up to level 1.

>>> data = [
...     {
...         ""id"": 1,
...         ""name"": ""Cole Volk"",
...         ""fitness"": {""height"": 130, ""weight"": 60},
...     },
...     {""name"": ""Mark Reg"", ""fitness"": {""height"": 130, ""weight"": 60}},
...     {
...         ""id"": 2,
...         ""name"": ""Faye Raker"",
...         ""fitness"": {""height"": 130, ""weight"": 60},
...     },
... ]
>>> pd.json_normalize(data, max_level=1)
    id        name  fitness.height  fitness.weight
0  1.0   Cole Volk             130              60
1  NaN    Mark Reg             130              60
2  2.0  Faye Raker             130              60

>>> data = [
...     {
...         ""state"": ""Florida"",
...         ""shortname"": ""FL"",
...         ""info"": {""governor"": ""Rick Scott""},
...         ""counties"": [
...             {""name"": ""Dade"", ""population"": 12345},
...             {""name"": ""Broward"", ""population"": 40000},
...             {""name"": ""Palm Beach"", ""population"": 60000},
...         ],
...     },
...     {
...         ""state"": ""Ohio"",
...         ""shortname"": ""OH"",
...         ""info"": {""governor"": ""John Kasich""},
...         ""counties"": [
...             {""name"": ""Summit"", ""population"": 1234},
...             {""name"": ""Cuyahoga"", ""population"": 1337},
...         ],
...     },
... ]
>>> result = pd.json_normalize(
...     data, ""counties"", [""state"", ""shortname"", [""info"", ""governor""]]
... )
>>> result
         name  population    state shortname info.governor
0        Dade       12345   Florida    FL    Rick Scott
1     Broward       40000   Florida    FL    Rick Scott
2  Palm Beach       60000   Florida    FL    Rick Scott
3      Summit        1234   Ohio       OH    John Kasich
4    Cuyahoga        1337   Ohio       OH    John Kasich

>>> data = {""A"": [1, 2]}
>>> pd.json_normalize(data, ""A"", record_prefix=""Prefix."")
    Prefix.0
0          1
1          2

Returns normalized data with columns prefixed with the given string."""""""
pandas/io/json/_table_schema.py,"def as_json_table_type(x: DtypeObj) -> str:
    if is_integer_dtype(x):
        return 'integer'
    elif is_bool_dtype(x):
        return 'boolean'
    elif is_numeric_dtype(x):
        return 'number'
    elif lib.is_np_dtype(x, 'M') or isinstance(x, (DatetimeTZDtype, PeriodDtype)):
        return 'datetime'
    elif lib.is_np_dtype(x, 'm'):
        return 'duration'
    elif isinstance(x, ExtensionDtype):
        return 'any'
    elif is_string_dtype(x):
        return 'string'
    else:
        return 'any'","""""""Convert a NumPy / pandas type to its corresponding json_table.

Parameters
----------
x : np.dtype or ExtensionDtype

Returns
-------
str
    the Table Schema data types

Notes
-----
This table shows the relationship between NumPy / pandas dtypes,
and Table Schema dtypes.

==============  =================
Pandas type     Table Schema type
==============  =================
int64           integer
float64         number
bool            boolean
datetime64[ns]  datetime
timedelta64[ns] duration
object          str
categorical     any
=============== ================="""""""
pandas/io/json/_table_schema.py,"def set_default_names(data):
    if com.all_not_none(*data.index.names):
        nms = data.index.names
        if len(nms) == 1 and data.index.name == 'index':
            warnings.warn(""Index name of 'index' is not round-trippable."", stacklevel=find_stack_level())
        elif len(nms) > 1 and any((x.startswith('level_') for x in nms)):
            warnings.warn(""Index names beginning with 'level_' are not round-trippable."", stacklevel=find_stack_level())
        return data
    data = data.copy()
    if data.index.nlevels > 1:
        data.index.names = com.fill_missing_names(data.index.names)
    else:
        data.index.name = data.index.name or 'index'
    return data","""""""Sets index names to 'index' for regular, or 'level_x' for Multi"""""""
pandas/io/json/_table_schema.py,"def convert_json_field_to_pandas_type(field) -> str | CategoricalDtype:
    typ = field['type']
    if typ == 'string':
        return 'object'
    elif typ == 'integer':
        return field.get('extDtype', 'int64')
    elif typ == 'number':
        return field.get('extDtype', 'float64')
    elif typ == 'boolean':
        return field.get('extDtype', 'bool')
    elif typ == 'duration':
        return 'timedelta64'
    elif typ == 'datetime':
        if field.get('tz'):
            return f""datetime64[ns, {field['tz']}]""
        elif field.get('freq'):
            return f""period[{field['freq']}]""
        else:
            return 'datetime64[ns]'
    elif typ == 'any':
        if 'constraints' in field and 'ordered' in field:
            return CategoricalDtype(categories=field['constraints']['enum'], ordered=field['ordered'])
        elif 'extDtype' in field:
            return registry.find(field['extDtype'])
        else:
            return 'object'
    raise ValueError(f'Unsupported or invalid field type: {typ}')","""""""Converts a JSON field descriptor into its corresponding NumPy / pandas type

Parameters
----------
field
    A JSON field descriptor

Returns
-------
dtype

Raises
------
ValueError
    If the type of the provided field is unknown or currently unsupported

Examples
--------
>>> convert_json_field_to_pandas_type({""name"": ""an_int"", ""type"": ""integer""})
'int64'

>>> convert_json_field_to_pandas_type(
...     {
...         ""name"": ""a_categorical"",
...         ""type"": ""any"",
...         ""constraints"": {""enum"": [""a"", ""b"", ""c""]},
...         ""ordered"": True,
...     }
... )
CategoricalDtype(categories=['a', 'b', 'c'], ordered=True, categories_dtype=object)

>>> convert_json_field_to_pandas_type({""name"": ""a_datetime"", ""type"": ""datetime""})
'datetime64[ns]'

>>> convert_json_field_to_pandas_type(
...     {""name"": ""a_datetime_with_tz"", ""type"": ""datetime"", ""tz"": ""US/Central""}
... )
'datetime64[ns, US/Central]'"""""""
pandas/io/json/_table_schema.py,"def build_table_schema(data: DataFrame | Series, index: bool=True, primary_key: bool | None=None, version: bool=True) -> dict[str, JSONSerializable]:
    if index is True:
        data = set_default_names(data)
    schema: dict[str, Any] = {}
    fields = []
    if index:
        if data.index.nlevels > 1:
            data.index = cast('MultiIndex', data.index)
            for (level, name) in zip(data.index.levels, data.index.names):
                new_field = convert_pandas_type_to_json_field(level)
                new_field['name'] = name
                fields.append(new_field)
        else:
            fields.append(convert_pandas_type_to_json_field(data.index))
    if data.ndim > 1:
        for (column, s) in data.items():
            fields.append(convert_pandas_type_to_json_field(s))
    else:
        fields.append(convert_pandas_type_to_json_field(data))
    schema['fields'] = fields
    if index and data.index.is_unique and (primary_key is None):
        if data.index.nlevels == 1:
            schema['primaryKey'] = [data.index.name]
        else:
            schema['primaryKey'] = data.index.names
    elif primary_key is not None:
        schema['primaryKey'] = primary_key
    if version:
        schema['pandas_version'] = TABLE_SCHEMA_VERSION
    return schema","""""""Create a Table schema from ``data``.

Parameters
----------
data : Series, DataFrame
index : bool, default True
    Whether to include ``data.index`` in the schema.
primary_key : bool or None, default True
    Column names to designate as the primary key.
    The default `None` will set `'primaryKey'` to the index
    level or levels if the index is unique.
version : bool, default True
    Whether to include a field `pandas_version` with the version
    of pandas that last revised the table schema. This version
    can be different from the installed pandas version.

Returns
-------
dict

Notes
-----
See `Table Schema
<https://pandas.pydata.org/docs/user_guide/io.html#table-schema>`__ for
conversion types.
Timedeltas as converted to ISO8601 duration format with
9 decimal places after the seconds field for nanosecond precision.

Categoricals are converted to the `any` dtype, and use the `enum` field
constraint to list the allowed values. The `ordered` attribute is included
in an `ordered` field.

Examples
--------
>>> from pandas.io.json._table_schema import build_table_schema
>>> df = pd.DataFrame(
...     {'A': [1, 2, 3],
...      'B': ['a', 'b', 'c'],
...      'C': pd.date_range('2016-01-01', freq='d', periods=3),
...     }, index=pd.Index(range(3), name='idx'))
>>> build_table_schema(df)
{'fields': [{'name': 'idx', 'type': 'integer'}, {'name': 'A', 'type': 'integer'}, {'name': 'B', 'type': 'string'}, {'name': 'C', 'type': 'datetime'}], 'primaryKey': ['idx'], 'pandas_version': '1.4.0'}"""""""
pandas/io/json/_table_schema.py,"def parse_table_schema(json, precise_float: bool) -> DataFrame:
    table = ujson_loads(json, precise_float=precise_float)
    col_order = [field['name'] for field in table['schema']['fields']]
    df = DataFrame(table['data'], columns=col_order)[col_order]
    dtypes = {field['name']: convert_json_field_to_pandas_type(field) for field in table['schema']['fields']}
    if 'timedelta64' in dtypes.values():
        raise NotImplementedError('table=""orient"" can not yet read ISO-formatted Timedelta data')
    df = df.astype(dtypes)
    if 'primaryKey' in table['schema']:
        df = df.set_index(table['schema']['primaryKey'])
        if len(df.index.names) == 1:
            if df.index.name == 'index':
                df.index.name = None
        else:
            df.index.names = [None if x.startswith('level_') else x for x in df.index.names]
    return df","""""""Builds a DataFrame from a given schema

Parameters
----------
json :
    A JSON table schema
precise_float : bool
    Flag controlling precision when decoding string to double values, as
    dictated by ``read_json``

Returns
-------
df : DataFrame

Raises
------
NotImplementedError
    If the JSON table schema contains either timezone or timedelta data

Notes
-----
    Because :func:`DataFrame.to_json` uses the string 'index' to denote a
    name-less :class:`Index`, this function sets the name of the returned
    :class:`DataFrame` to ``None`` when said string is encountered with a
    normal :class:`Index`. For a :class:`MultiIndex`, the same limitation
    applies to any strings beginning with 'level_'. Therefore, an
    :class:`Index` name of 'index'  and :class:`MultiIndex` names starting
    with 'level_' are not supported.

See Also
--------
build_table_schema : Inverse function.
pandas.read_json"""""""
pandas/io/orc.py,"def read_orc(path: FilePath | ReadBuffer[bytes], columns: list[str] | None=None, dtype_backend: DtypeBackend | lib.NoDefault=lib.no_default, filesystem: pyarrow.fs.FileSystem | fsspec.spec.AbstractFileSystem | None=None, **kwargs: Any) -> DataFrame:
    orc = import_optional_dependency('pyarrow.orc')
    check_dtype_backend(dtype_backend)
    with get_handle(path, 'rb', is_text=False) as handles:
        source = handles.handle
        if is_fsspec_url(path) and filesystem is None:
            pa = import_optional_dependency('pyarrow')
            pa_fs = import_optional_dependency('pyarrow.fs')
            try:
                (filesystem, source) = pa_fs.FileSystem.from_uri(path)
            except (TypeError, pa.ArrowInvalid):
                pass
        pa_table = orc.read_table(source=source, columns=columns, filesystem=filesystem, **kwargs)
    if dtype_backend is not lib.no_default:
        if dtype_backend == 'pyarrow':
            df = pa_table.to_pandas(types_mapper=pd.ArrowDtype)
        else:
            from pandas.io._util import _arrow_dtype_mapping
            mapping = _arrow_dtype_mapping()
            df = pa_table.to_pandas(types_mapper=mapping.get)
        return df
    else:
        if using_pyarrow_string_dtype():
            types_mapper = arrow_string_types_mapper()
        else:
            types_mapper = None
        return pa_table.to_pandas(types_mapper=types_mapper)","""""""Load an ORC object from the file path, returning a DataFrame.

Parameters
----------
path : str, path object, or file-like object
    String, path object (implementing ``os.PathLike[str]``), or file-like
    object implementing a binary ``read()`` function. The string could be a URL.
    Valid URL schemes include http, ftp, s3, and file. For file URLs, a host is
    expected. A local file could be:
    ``file://localhost/path/to/table.orc``.
columns : list, default None
    If not None, only these columns will be read from the file.
    Output always follows the ordering of the file and not the columns list.
    This mirrors the original behaviour of
    :external+pyarrow:py:meth:`pyarrow.orc.ORCFile.read`.
dtype_backend : {'numpy_nullable', 'pyarrow'}, default 'numpy_nullable'
    Back-end data type applied to the resultant :class:`DataFrame`
    (still experimental). Behaviour is as follows:

    * ``""numpy_nullable""``: returns nullable-dtype-backed :class:`DataFrame`
      (default).
    * ``""pyarrow""``: returns pyarrow-backed nullable :class:`ArrowDtype`
      DataFrame.

    .. versionadded:: 2.0

filesystem : fsspec or pyarrow filesystem, default None
    Filesystem object to use when reading the parquet file.

    .. versionadded:: 2.1.0

**kwargs
    Any additional kwargs are passed to pyarrow.

Returns
-------
DataFrame

Notes
-----
Before using this function you should read the :ref:`user guide about ORC <io.orc>`
and :ref:`install optional dependencies <install.warn_orc>`.

If ``path`` is a URI scheme pointing to a local or remote file (e.g. ""s3://""),
a ``pyarrow.fs`` filesystem will be attempted to read the file. You can also pass a
pyarrow or fsspec filesystem object into the filesystem keyword to override this
behavior.

Examples
--------
>>> result = pd.read_orc(""example_pa.orc"")  # doctest: +SKIP"""""""
pandas/io/orc.py,"def to_orc(df: DataFrame, path: FilePath | WriteBuffer[bytes] | None=None, *, engine: Literal['pyarrow']='pyarrow', index: bool | None=None, engine_kwargs: dict[str, Any] | None=None) -> bytes | None:
    if index is None:
        index = df.index.names[0] is not None
    if engine_kwargs is None:
        engine_kwargs = {}
    if not df.index.equals(default_index(len(df))):
        raise ValueError('orc does not support serializing a non-default index for the index; you can .reset_index() to make the index into column(s)')
    if df.index.name is not None:
        raise ValueError('orc does not serialize index meta-data on a default index')
    if pa_version_under8p0:
        for dtype in df.dtypes:
            if isinstance(dtype, (IntervalDtype, CategoricalDtype, PeriodDtype)) or is_unsigned_integer_dtype(dtype):
                raise NotImplementedError('The dtype of one or more columns is not supported yet.')
    if engine != 'pyarrow':
        raise ValueError(""engine must be 'pyarrow'"")
    engine = import_optional_dependency(engine, min_version='7.0.0')
    pa = import_optional_dependency('pyarrow')
    orc = import_optional_dependency('pyarrow.orc')
    was_none = path is None
    if was_none:
        path = io.BytesIO()
    assert path is not None
    with get_handle(path, 'wb', is_text=False) as handles:
        assert isinstance(engine, ModuleType)
        try:
            orc.write_table(engine.Table.from_pandas(df, preserve_index=index), handles.handle, **engine_kwargs)
        except (TypeError, pa.ArrowNotImplementedError) as e:
            raise NotImplementedError('The dtype of one or more columns is not supported yet.') from e
    if was_none:
        assert isinstance(path, io.BytesIO)
        return path.getvalue()
    return None","""""""Write a DataFrame to the ORC format.

.. versionadded:: 1.5.0

Parameters
----------
df : DataFrame
    The dataframe to be written to ORC. Raises NotImplementedError
    if dtype of one or more columns is category, unsigned integers,
    intervals, periods or sparse.
path : str, file-like object or None, default None
    If a string, it will be used as Root Directory path
    when writing a partitioned dataset. By file-like object,
    we refer to objects with a write() method, such as a file handle
    (e.g. via builtin open function). If path is None,
    a bytes object is returned.
engine : str, default 'pyarrow'
    ORC library to use. Pyarrow must be >= 7.0.0.
index : bool, optional
    If ``True``, include the dataframe's index(es) in the file output. If
    ``False``, they will not be written to the file.
    If ``None``, similar to ``infer`` the dataframe's index(es)
    will be saved. However, instead of being saved as values,
    the RangeIndex will be stored as a range in the metadata so it
    doesn't require much space and is faster. Other indexes will
    be included as columns in the file output.
engine_kwargs : dict[str, Any] or None, default None
    Additional keyword arguments passed to :func:`pyarrow.orc.write_table`.

Returns
-------
bytes if no path argument is provided else None

Raises
------
NotImplementedError
    Dtype of one or more columns is category, unsigned integers, interval,
    period or sparse.
ValueError
    engine is not pyarrow.

Notes
-----
* Before using this function you should read the
  :ref:`user guide about ORC <io.orc>` and
  :ref:`install optional dependencies <install.warn_orc>`.
* This function requires `pyarrow <https://arrow.apache.org/docs/python/>`_
  library.
* For supported dtypes please refer to `supported ORC features in Arrow
  <https://arrow.apache.org/docs/cpp/orc.html#data-types>`__.
* Currently timezones in datetime columns are not preserved when a
  dataframe is converted into ORC files."""""""
pandas/io/parquet.py,"def get_engine(engine: str) -> BaseImpl:
    if engine == 'auto':
        engine = get_option('io.parquet.engine')
    if engine == 'auto':
        engine_classes = [PyArrowImpl, FastParquetImpl]
        error_msgs = ''
        for engine_class in engine_classes:
            try:
                return engine_class()
            except ImportError as err:
                error_msgs += '\n - ' + str(err)
        raise ImportError(f""Unable to find a usable engine; tried using: 'pyarrow', 'fastparquet'.\nA suitable version of pyarrow or fastparquet is required for parquet support.\nTrying to import the above resulted in these errors:{error_msgs}"")
    if engine == 'pyarrow':
        return PyArrowImpl()
    elif engine == 'fastparquet':
        return FastParquetImpl()
    raise ValueError(""engine must be one of 'pyarrow', 'fastparquet'"")","""""""return our implementation"""""""
pandas/io/parquet.py,"def _get_path_or_handle(path: FilePath | ReadBuffer[bytes] | WriteBuffer[bytes], fs: Any, storage_options: StorageOptions | None=None, mode: str='rb', is_dir: bool=False) -> tuple[FilePath | ReadBuffer[bytes] | WriteBuffer[bytes], IOHandles[bytes] | None, Any]:
    path_or_handle = stringify_path(path)
    if fs is not None:
        pa_fs = import_optional_dependency('pyarrow.fs', errors='ignore')
        fsspec = import_optional_dependency('fsspec', errors='ignore')
        if pa_fs is not None and isinstance(fs, pa_fs.FileSystem):
            if storage_options:
                raise NotImplementedError('storage_options not supported with a pyarrow FileSystem.')
        elif fsspec is not None and isinstance(fs, fsspec.spec.AbstractFileSystem):
            pass
        else:
            raise ValueError(f'filesystem must be a pyarrow or fsspec FileSystem, not a {type(fs).__name__}')
    if is_fsspec_url(path_or_handle) and fs is None:
        if storage_options is None:
            pa = import_optional_dependency('pyarrow')
            pa_fs = import_optional_dependency('pyarrow.fs')
            try:
                (fs, path_or_handle) = pa_fs.FileSystem.from_uri(path)
            except (TypeError, pa.ArrowInvalid):
                pass
        if fs is None:
            fsspec = import_optional_dependency('fsspec')
            (fs, path_or_handle) = fsspec.core.url_to_fs(path_or_handle, **storage_options or {})
    elif storage_options and (not is_url(path_or_handle) or mode != 'rb'):
        raise ValueError('storage_options passed with buffer, or non-supported URL')
    handles = None
    if not fs and (not is_dir) and isinstance(path_or_handle, str) and (not os.path.isdir(path_or_handle)):
        handles = get_handle(path_or_handle, mode, is_text=False, storage_options=storage_options)
        fs = None
        path_or_handle = handles.handle
    return (path_or_handle, handles, fs)","""""""File handling for PyArrow."""""""
pandas/io/parquet.py,"@doc(storage_options=_shared_docs['storage_options'])
def to_parquet(df: DataFrame, path: FilePath | WriteBuffer[bytes] | None=None, engine: str='auto', compression: str | None='snappy', index: bool | None=None, storage_options: StorageOptions | None=None, partition_cols: list[str] | None=None, filesystem: Any=None, **kwargs) -> bytes | None:
    if isinstance(partition_cols, str):
        partition_cols = [partition_cols]
    impl = get_engine(engine)
    path_or_buf: FilePath | WriteBuffer[bytes] = io.BytesIO() if path is None else path
    impl.write(df, path_or_buf, compression=compression, index=index, partition_cols=partition_cols, storage_options=storage_options, filesystem=filesystem, **kwargs)
    if path is None:
        assert isinstance(path_or_buf, io.BytesIO)
        return path_or_buf.getvalue()
    else:
        return None","""""""Write a DataFrame to the parquet format.

Parameters
----------
df : DataFrame
path : str, path object, file-like object, or None, default None
    String, path object (implementing ``os.PathLike[str]``), or file-like
    object implementing a binary ``write()`` function. If None, the result is
    returned as bytes. If a string, it will be used as Root Directory path
    when writing a partitioned dataset. The engine fastparquet does not
    accept file-like objects.

    .. versionchanged:: 1.2.0

engine : {{'auto', 'pyarrow', 'fastparquet'}}, default 'auto'
    Parquet library to use. If 'auto', then the option
    ``io.parquet.engine`` is used. The default ``io.parquet.engine``
    behavior is to try 'pyarrow', falling back to 'fastparquet' if
    'pyarrow' is unavailable.

    When using the ``'pyarrow'`` engine and no storage options are provided
    and a filesystem is implemented by both ``pyarrow.fs`` and ``fsspec``
    (e.g. ""s3://""), then the ``pyarrow.fs`` filesystem is attempted first.
    Use the filesystem keyword with an instantiated fsspec filesystem
    if you wish to use its implementation.
compression : {{'snappy', 'gzip', 'brotli', 'lz4', 'zstd', None}},
    default 'snappy'. Name of the compression to use. Use ``None``
    for no compression.
index : bool, default None
    If ``True``, include the dataframe's index(es) in the file output. If
    ``False``, they will not be written to the file.
    If ``None``, similar to ``True`` the dataframe's index(es)
    will be saved. However, instead of being saved as values,
    the RangeIndex will be stored as a range in the metadata so it
    doesn't require much space and is faster. Other indexes will
    be included as columns in the file output.
partition_cols : str or list, optional, default None
    Column names by which to partition the dataset.
    Columns are partitioned in the order they are given.
    Must be None if path is not a string.
{storage_options}

    .. versionadded:: 1.2.0

filesystem : fsspec or pyarrow filesystem, default None
    Filesystem object to use when reading the parquet file. Only implemented
    for ``engine=""pyarrow""``.

    .. versionadded:: 2.1.0

kwargs
    Additional keyword arguments passed to the engine

Returns
-------
bytes if no path argument is provided else None"""""""
pandas/io/parquet.py,"@doc(storage_options=_shared_docs['storage_options'])
def read_parquet(path: FilePath | ReadBuffer[bytes], engine: str='auto', columns: list[str] | None=None, storage_options: StorageOptions | None=None, use_nullable_dtypes: bool | lib.NoDefault=lib.no_default, dtype_backend: DtypeBackend | lib.NoDefault=lib.no_default, filesystem: Any=None, filters: list[tuple] | list[list[tuple]] | None=None, **kwargs) -> DataFrame:
    impl = get_engine(engine)
    if use_nullable_dtypes is not lib.no_default:
        msg = ""The argument 'use_nullable_dtypes' is deprecated and will be removed in a future version.""
        if use_nullable_dtypes is True:
            msg += ""Use dtype_backend='numpy_nullable' instead of use_nullable_dtype=True.""
        warnings.warn(msg, FutureWarning, stacklevel=find_stack_level())
    else:
        use_nullable_dtypes = False
    check_dtype_backend(dtype_backend)
    return impl.read(path, columns=columns, filters=filters, storage_options=storage_options, use_nullable_dtypes=use_nullable_dtypes, dtype_backend=dtype_backend, filesystem=filesystem, **kwargs)","""""""Load a parquet object from the file path, returning a DataFrame.

Parameters
----------
path : str, path object or file-like object
    String, path object (implementing ``os.PathLike[str]``), or file-like
    object implementing a binary ``read()`` function.
    The string could be a URL. Valid URL schemes include http, ftp, s3,
    gs, and file. For file URLs, a host is expected. A local file could be:
    ``file://localhost/path/to/table.parquet``.
    A file URL can also be a path to a directory that contains multiple
    partitioned parquet files. Both pyarrow and fastparquet support
    paths to directories as well as file URLs. A directory path could be:
    ``file://localhost/path/to/tables`` or ``s3://bucket/partition_dir``.
engine : {{'auto', 'pyarrow', 'fastparquet'}}, default 'auto'
    Parquet library to use. If 'auto', then the option
    ``io.parquet.engine`` is used. The default ``io.parquet.engine``
    behavior is to try 'pyarrow', falling back to 'fastparquet' if
    'pyarrow' is unavailable.

    When using the ``'pyarrow'`` engine and no storage options are provided
    and a filesystem is implemented by both ``pyarrow.fs`` and ``fsspec``
    (e.g. ""s3://""), then the ``pyarrow.fs`` filesystem is attempted first.
    Use the filesystem keyword with an instantiated fsspec filesystem
    if you wish to use its implementation.
columns : list, default=None
    If not None, only these columns will be read from the file.
{storage_options}

    .. versionadded:: 1.3.0

use_nullable_dtypes : bool, default False
    If True, use dtypes that use ``pd.NA`` as missing value indicator
    for the resulting DataFrame. (only applicable for the ``pyarrow``
    engine)
    As new dtypes are added that support ``pd.NA`` in the future, the
    output with this option will change to use those dtypes.
    Note: this is an experimental option, and behaviour (e.g. additional
    support dtypes) may change without notice.

    .. deprecated:: 2.0

dtype_backend : {{'numpy_nullable', 'pyarrow'}}, default 'numpy_nullable'
    Back-end data type applied to the resultant :class:`DataFrame`
    (still experimental). Behaviour is as follows:

    * ``""numpy_nullable""``: returns nullable-dtype-backed :class:`DataFrame`
      (default).
    * ``""pyarrow""``: returns pyarrow-backed nullable :class:`ArrowDtype`
      DataFrame.

    .. versionadded:: 2.0

filesystem : fsspec or pyarrow filesystem, default None
    Filesystem object to use when reading the parquet file. Only implemented
    for ``engine=""pyarrow""``.

    .. versionadded:: 2.1.0

filters : List[Tuple] or List[List[Tuple]], default None
    To filter out data.
    Filter syntax: [[(column, op, val), ...],...]
    where op is [==, =, >, >=, <, <=, !=, in, not in]
    The innermost tuples are transposed into a set of filters applied
    through an `AND` operation.
    The outer list combines these sets of filters through an `OR`
    operation.
    A single list of tuples can also be used, meaning that no `OR`
    operation between set of filters is to be conducted.

    Using this argument will NOT result in row-wise filtering of the final
    partitions unless ``engine=""pyarrow""`` is also specified.  For
    other engines, filtering is only performed at the partition level, that is,
    to prevent the loading of some row-groups and/or files.

    .. versionadded:: 2.1.0

**kwargs
    Any additional kwargs are passed to the engine.

Returns
-------
DataFrame

See Also
--------
DataFrame.to_parquet : Create a parquet object that serializes a DataFrame.

Examples
--------
>>> original_df = pd.DataFrame(
...     {{""foo"": range(5), ""bar"": range(5, 10)}}
...    )
>>> original_df
   foo  bar
0    0    5
1    1    6
2    2    7
3    3    8
4    4    9
>>> df_parquet_bytes = original_df.to_parquet()
>>> from io import BytesIO
>>> restored_df = pd.read_parquet(BytesIO(df_parquet_bytes))
>>> restored_df
   foo  bar
0    0    5
1    1    6
2    2    7
3    3    8
4    4    9
>>> restored_df.equals(original_df)
True
>>> restored_bar = pd.read_parquet(BytesIO(df_parquet_bytes), columns=[""bar""])
>>> restored_bar
    bar
0    5
1    6
2    7
3    8
4    9
>>> restored_bar.equals(original_df[['bar']])
True

The function uses `kwargs` that are passed directly to the engine.
In the following example, we use the `filters` argument of the pyarrow
engine to filter the rows of the DataFrame.

Since `pyarrow` is the default engine, we can omit the `engine` argument.
Note that the `filters` argument is implemented by the `pyarrow` engine,
which can benefit from multithreading and also potentially be more
economical in terms of memory.

>>> sel = [(""foo"", "">"", 2)]
>>> restored_part = pd.read_parquet(BytesIO(df_parquet_bytes), filters=sel)
>>> restored_part
    foo  bar
0    3    8
1    4    9"""""""
pandas/io/parsers/base_parser.py,"def _get_na_values(col, na_values, na_fvalues, keep_default_na: bool):
    if isinstance(na_values, dict):
        if col in na_values:
            return (na_values[col], na_fvalues[col])
        else:
            if keep_default_na:
                return (STR_NA_VALUES, set())
            return (set(), set())
    else:
        return (na_values, na_fvalues)","""""""Get the NaN values for a given column.

Parameters
----------
col : str
    The name of the column.
na_values : array-like, dict
    The object listing the NaN values as strings.
na_fvalues : array-like, dict
    The object listing the NaN values as floats.
keep_default_na : bool
    If `na_values` is a dict, and the column is not mapped in the
    dictionary, whether to return the default NaN values or the empty set.

Returns
-------
nan_tuple : A length-two tuple composed of

    1) na_values : the string NaN values for that column.
    2) na_fvalues : the float NaN values for that column."""""""
pandas/io/parsers/base_parser.py,"def _validate_parse_dates_arg(parse_dates):
    msg = ""Only booleans, lists, and dictionaries are accepted for the 'parse_dates' parameter""
    if not (parse_dates is None or lib.is_bool(parse_dates) or isinstance(parse_dates, (list, dict))):
        raise TypeError(msg)
    return parse_dates","""""""Check whether or not the 'parse_dates' parameter
is a non-boolean scalar. Raises a ValueError if
that is the case."""""""
pandas/io/parsers/c_parser_wrapper.py,"def _concatenate_chunks(chunks: list[dict[int, ArrayLike]]) -> dict:
    names = list(chunks[0].keys())
    warning_columns = []
    result: dict = {}
    for name in names:
        arrs = [chunk.pop(name) for chunk in chunks]
        dtypes = {a.dtype for a in arrs}
        non_cat_dtypes = {x for x in dtypes if not isinstance(x, CategoricalDtype)}
        dtype = dtypes.pop()
        if isinstance(dtype, CategoricalDtype):
            result[name] = union_categoricals(arrs, sort_categories=False)
        else:
            result[name] = concat_compat(arrs)
            if len(non_cat_dtypes) > 1 and result[name].dtype == np.dtype(object):
                warning_columns.append(str(name))
    if warning_columns:
        warning_names = ','.join(warning_columns)
        warning_message = ' '.join([f'Columns ({warning_names}) have mixed types. Specify dtype option on import or set low_memory=False.'])
        warnings.warn(warning_message, DtypeWarning, stacklevel=find_stack_level())
    return result","""""""Concatenate chunks of data read with low_memory=True.

The tricky part is handling Categoricals, where different chunks
may have different inferred categories."""""""
pandas/io/parsers/c_parser_wrapper.py,"def ensure_dtype_objs(dtype: DtypeArg | dict[Hashable, DtypeArg] | None) -> DtypeObj | dict[Hashable, DtypeObj] | None:
    if isinstance(dtype, defaultdict):
        default_dtype = pandas_dtype(dtype.default_factory())
        dtype_converted: defaultdict = defaultdict(lambda : default_dtype)
        for key in dtype.keys():
            dtype_converted[key] = pandas_dtype(dtype[key])
        return dtype_converted
    elif isinstance(dtype, dict):
        return {k: pandas_dtype(dtype[k]) for k in dtype}
    elif dtype is not None:
        return pandas_dtype(dtype)
    return dtype","""""""Ensure we have either None, a dtype object, or a dictionary mapping to
dtype objects."""""""
pandas/io/parsers/python_parser.py,"def _validate_skipfooter_arg(skipfooter: int) -> int:
    if not is_integer(skipfooter):
        raise ValueError('skipfooter must be an integer')
    if skipfooter < 0:
        raise ValueError('skipfooter cannot be negative')
    return skipfooter","""""""Validate the 'skipfooter' parameter.

Checks whether 'skipfooter' is a non-negative integer.
Raises a ValueError if that is not the case.

Parameters
----------
skipfooter : non-negative integer
    The number of rows to skip at the end of the file.

Returns
-------
validated_skipfooter : non-negative integer
    The original input if the validation succeeds.

Raises
------
ValueError : 'skipfooter' was not a non-negative integer."""""""
pandas/io/parsers/readers.py,"def validate_integer(name: str, val: int | float | None, min_val: int=0) -> int | None:
    if val is None:
        return val
    msg = f""'{name:s}' must be an integer >={min_val:d}""
    if is_float(val):
        if int(val) != val:
            raise ValueError(msg)
        val = int(val)
    elif not (is_integer(val) and val >= min_val):
        raise ValueError(msg)
    return int(val)","""""""Checks whether the 'name' parameter for parsing is either
an integer OR float that can SAFELY be cast to an integer
without losing accuracy. Raises a ValueError if that is
not the case.

Parameters
----------
name : str
    Parameter name (used for error reporting)
val : int or float
    The value to check
min_val : int
    Minimum allowed value (val < min_val will result in a ValueError)"""""""
pandas/io/parsers/readers.py,"def _validate_names(names: Sequence[Hashable] | None) -> None:
    if names is not None:
        if len(names) != len(set(names)):
            raise ValueError('Duplicate names are not allowed.')
        if not (is_list_like(names, allow_sets=False) or isinstance(names, abc.KeysView)):
            raise ValueError('Names should be an ordered collection.')","""""""Raise ValueError if the `names` parameter contains duplicates or has an
invalid data type.

Parameters
----------
names : array-like or None
    An array containing a list of the names used for the output DataFrame.

Raises
------
ValueError
    If names are not unique or are not ordered (e.g. set)."""""""
pandas/io/parsers/readers.py,"def _read(filepath_or_buffer: FilePath | ReadCsvBuffer[bytes] | ReadCsvBuffer[str], kwds) -> DataFrame | TextFileReader:
    if kwds.get('parse_dates', None) is None:
        if kwds.get('date_parser', lib.no_default) is lib.no_default and kwds.get('date_format', None) is None:
            kwds['parse_dates'] = False
        else:
            kwds['parse_dates'] = True
    iterator = kwds.get('iterator', False)
    chunksize = kwds.get('chunksize', None)
    if kwds.get('engine') == 'pyarrow':
        if iterator:
            raise ValueError(""The 'iterator' option is not supported with the 'pyarrow' engine"")
        if chunksize is not None:
            raise ValueError(""The 'chunksize' option is not supported with the 'pyarrow' engine"")
    else:
        chunksize = validate_integer('chunksize', chunksize, 1)
    nrows = kwds.get('nrows', None)
    _validate_names(kwds.get('names', None))
    parser = TextFileReader(filepath_or_buffer, **kwds)
    if chunksize or iterator:
        return parser
    with parser:
        return parser.read(nrows)","""""""Generic reader of line files."""""""
pandas/io/parsers/readers.py,"def read_fwf(filepath_or_buffer: FilePath | ReadCsvBuffer[bytes] | ReadCsvBuffer[str], *, colspecs: Sequence[tuple[int, int]] | str | None='infer', widths: Sequence[int] | None=None, infer_nrows: int=100, dtype_backend: DtypeBackend | lib.NoDefault=lib.no_default, iterator: bool=False, chunksize: int | None=None, **kwds) -> DataFrame | TextFileReader:
    if colspecs is None and widths is None:
        raise ValueError('Must specify either colspecs or widths')
    if colspecs not in (None, 'infer') and widths is not None:
        raise ValueError(""You must specify only one of 'widths' and 'colspecs'"")
    if widths is not None:
        (colspecs, col) = ([], 0)
        for w in widths:
            colspecs.append((col, col + w))
            col += w
    assert colspecs is not None
    names = kwds.get('names')
    if names is not None:
        if len(names) != len(colspecs) and colspecs != 'infer':
            len_index = 0
            if kwds.get('index_col') is not None:
                index_col: Any = kwds.get('index_col')
                if index_col is not False:
                    if not is_list_like(index_col):
                        len_index = 1
                    else:
                        len_index = len(index_col)
            if kwds.get('usecols') is None and len(names) + len_index != len(colspecs):
                raise ValueError('Length of colspecs must match length of names')
    kwds['colspecs'] = colspecs
    kwds['infer_nrows'] = infer_nrows
    kwds['engine'] = 'python-fwf'
    kwds['iterator'] = iterator
    kwds['chunksize'] = chunksize
    check_dtype_backend(dtype_backend)
    kwds['dtype_backend'] = dtype_backend
    return _read(filepath_or_buffer, kwds)","""""""Read a table of fixed-width formatted lines into DataFrame.

Also supports optionally iterating or breaking of the file
into chunks.

Additional help can be found in the `online docs for IO Tools
<https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html>`_.

Parameters
----------
filepath_or_buffer : str, path object, or file-like object
    String, path object (implementing ``os.PathLike[str]``), or file-like
    object implementing a text ``read()`` function.The string could be a URL.
    Valid URL schemes include http, ftp, s3, and file. For file URLs, a host is
    expected. A local file could be:
    ``file://localhost/path/to/table.csv``.
colspecs : list of tuple (int, int) or 'infer'. optional
    A list of tuples giving the extents of the fixed-width
    fields of each line as half-open intervals (i.e.,  [from, to[ ).
    String value 'infer' can be used to instruct the parser to try
    detecting the column specifications from the first 100 rows of
    the data which are not being skipped via skiprows (default='infer').
widths : list of int, optional
    A list of field widths which can be used instead of 'colspecs' if
    the intervals are contiguous.
infer_nrows : int, default 100
    The number of rows to consider when letting the parser determine the
    `colspecs`.
dtype_backend : {'numpy_nullable', 'pyarrow'}, default 'numpy_nullable'
    Back-end data type applied to the resultant :class:`DataFrame`
    (still experimental). Behaviour is as follows:

    * ``""numpy_nullable""``: returns nullable-dtype-backed :class:`DataFrame`
      (default).
    * ``""pyarrow""``: returns pyarrow-backed nullable :class:`ArrowDtype`
      DataFrame.

    .. versionadded:: 2.0

**kwds : optional
    Optional keyword arguments can be passed to ``TextFileReader``.

Returns
-------
DataFrame or TextFileReader
    A comma-separated values (csv) file is returned as two-dimensional
    data structure with labeled axes.

See Also
--------
DataFrame.to_csv : Write DataFrame to a comma-separated values (csv) file.
read_csv : Read a comma-separated values (csv) file into DataFrame.

Examples
--------
>>> pd.read_fwf('data.csv')  # doctest: +SKIP"""""""
pandas/io/parsers/readers.py,"def TextParser(*args, **kwds) -> TextFileReader:
    kwds['engine'] = 'python'
    return TextFileReader(*args, **kwds)","""""""Converts lists of lists/tuples into DataFrames with proper type inference
and optional (e.g. string to datetime) conversion. Also enables iterating
lazily over chunks of large files

Parameters
----------
data : file-like object or list
delimiter : separator character to use
dialect : str or csv.Dialect instance, optional
    Ignored if delimiter is longer than 1 character
names : sequence, default
header : int, default 0
    Row to use to parse column labels. Defaults to the first row. Prior
    rows will be discarded
index_col : int or list, optional
    Column or columns to use as the (possibly hierarchical) index
has_index_names: bool, default False
    True if the cols defined in index_col have an index name and are
    not in the header.
na_values : scalar, str, list-like, or dict, optional
    Additional strings to recognize as NA/NaN.
keep_default_na : bool, default True
thousands : str, optional
    Thousands separator
comment : str, optional
    Comment out remainder of line
parse_dates : bool, default False
keep_date_col : bool, default False
date_parser : function, optional

    .. deprecated:: 2.0.0
date_format : str or dict of column -> format, default ``None``

    .. versionadded:: 2.0.0
skiprows : list of integers
    Row numbers to skip
skipfooter : int
    Number of line at bottom of file to skip
converters : dict, optional
    Dict of functions for converting values in certain columns. Keys can
    either be integers or column labels, values are functions that take one
    input argument, the cell (not column) content, and return the
    transformed content.
encoding : str, optional
    Encoding to use for UTF when reading/writing (ex. 'utf-8')
float_precision : str, optional
    Specifies which converter the C engine should use for floating-point
    values. The options are `None` or `high` for the ordinary converter,
    `legacy` for the original lower precision pandas converter, and
    `round_trip` for the round-trip converter.

    .. versionchanged:: 1.2"""""""
pandas/io/parsers/readers.py,"def _stringify_na_values(na_values):
    result: list[str | float] = []
    for x in na_values:
        result.append(str(x))
        result.append(x)
        try:
            v = float(x)
            if v == int(v):
                v = int(v)
                result.append(f'{v}.0')
                result.append(str(v))
            result.append(v)
        except (TypeError, ValueError, OverflowError):
            pass
        try:
            result.append(int(x))
        except (TypeError, ValueError, OverflowError):
            pass
    return set(result)","""""""return a stringified and numeric for these values"""""""
pandas/io/parsers/readers.py,"def _refine_defaults_read(dialect: str | csv.Dialect | None, delimiter: str | None | lib.NoDefault, delim_whitespace: bool, engine: CSVEngine | None, sep: str | None | lib.NoDefault, on_bad_lines: str | Callable, names: Sequence[Hashable] | None | lib.NoDefault, defaults: dict[str, Any], dtype_backend: DtypeBackend | lib.NoDefault):
    delim_default = defaults['delimiter']
    kwds: dict[str, Any] = {}
    if dialect is not None:
        kwds['sep_override'] = delimiter is None and (sep is lib.no_default or sep == delim_default)
    if delimiter and sep is not lib.no_default:
        raise ValueError('Specified a sep and a delimiter; you can only specify one.')
    kwds['names'] = None if names is lib.no_default else names
    if delimiter is None:
        delimiter = sep
    if delim_whitespace and delimiter is not lib.no_default:
        raise ValueError('Specified a delimiter with both sep and delim_whitespace=True; you can only specify one.')
    if delimiter == '\n':
        raise ValueError('Specified \\n as separator or delimiter. This forces the python engine which does not accept a line terminator. Hence it is not allowed to use the line terminator as separator.')
    if delimiter is lib.no_default:
        kwds['delimiter'] = delim_default
    else:
        kwds['delimiter'] = delimiter
    if engine is not None:
        kwds['engine_specified'] = True
    else:
        kwds['engine'] = 'c'
        kwds['engine_specified'] = False
    if on_bad_lines == 'error':
        kwds['on_bad_lines'] = ParserBase.BadLineHandleMethod.ERROR
    elif on_bad_lines == 'warn':
        kwds['on_bad_lines'] = ParserBase.BadLineHandleMethod.WARN
    elif on_bad_lines == 'skip':
        kwds['on_bad_lines'] = ParserBase.BadLineHandleMethod.SKIP
    elif callable(on_bad_lines):
        if engine not in ['python', 'pyarrow']:
            raise ValueError(""on_bad_line can only be a callable function if engine='python' or 'pyarrow'"")
        kwds['on_bad_lines'] = on_bad_lines
    else:
        raise ValueError(f'Argument {on_bad_lines} is invalid for on_bad_lines')
    check_dtype_backend(dtype_backend)
    kwds['dtype_backend'] = dtype_backend
    return kwds","""""""Validate/refine default values of input parameters of read_csv, read_table.

Parameters
----------
dialect : str or csv.Dialect
    If provided, this parameter will override values (default or not) for the
    following parameters: `delimiter`, `doublequote`, `escapechar`,
    `skipinitialspace`, `quotechar`, and `quoting`. If it is necessary to
    override values, a ParserWarning will be issued. See csv.Dialect
    documentation for more details.
delimiter : str or object
    Alias for sep.
delim_whitespace : bool
    Specifies whether or not whitespace (e.g. ``' '`` or ``'        '``) will be
    used as the sep. Equivalent to setting ``sep='\s+'``. If this option
    is set to True, nothing should be passed in for the ``delimiter``
    parameter.
engine : {{'c', 'python'}}
    Parser engine to use. The C engine is faster while the python engine is
    currently more feature-complete.
sep : str or object
    A delimiter provided by the user (str) or a sentinel value, i.e.
    pandas._libs.lib.no_default.
on_bad_lines : str, callable
    An option for handling bad lines or a sentinel value(None).
names : array-like, optional
    List of column names to use. If the file contains a header row,
    then you should explicitly pass ``header=0`` to override the column names.
    Duplicates in this list are not allowed.
defaults: dict
    Default values of input parameters.

Returns
-------
kwds : dict
    Input parameters with correct values.

Raises
------
ValueError :
    If a delimiter was specified with ``sep`` (or ``delimiter``) and
    ``delim_whitespace=True``."""""""
pandas/io/parsers/readers.py,"def _extract_dialect(kwds: dict[str, Any]) -> csv.Dialect | None:
    if kwds.get('dialect') is None:
        return None
    dialect = kwds['dialect']
    if dialect in csv.list_dialects():
        dialect = csv.get_dialect(dialect)
    _validate_dialect(dialect)
    return dialect","""""""Extract concrete csv dialect instance.

Returns
-------
csv.Dialect or None"""""""
pandas/io/parsers/readers.py,"def _validate_dialect(dialect: csv.Dialect) -> None:
    for param in MANDATORY_DIALECT_ATTRS:
        if not hasattr(dialect, param):
            raise ValueError(f'Invalid dialect {dialect} provided')","""""""Validate csv dialect instance.

Raises
------
ValueError
    If incorrect dialect is provided."""""""
pandas/io/parsers/readers.py,"def _merge_with_dialect_properties(dialect: csv.Dialect, defaults: dict[str, Any]) -> dict[str, Any]:
    kwds = defaults.copy()
    for param in MANDATORY_DIALECT_ATTRS:
        dialect_val = getattr(dialect, param)
        parser_default = parser_defaults[param]
        provided = kwds.get(param, parser_default)
        conflict_msgs = []
        if provided not in (parser_default, dialect_val):
            msg = f""Conflicting values for '{param}': '{provided}' was provided, but the dialect specifies '{dialect_val}'. Using the dialect-specified value.""
            if not (param == 'delimiter' and kwds.pop('sep_override', False)):
                conflict_msgs.append(msg)
        if conflict_msgs:
            warnings.warn('\n\n'.join(conflict_msgs), ParserWarning, stacklevel=find_stack_level())
        kwds[param] = dialect_val
    return kwds","""""""Merge default kwargs in TextFileReader with dialect parameters.

Parameters
----------
dialect : csv.Dialect
    Concrete csv dialect. See csv.Dialect documentation for more details.
defaults : dict
    Keyword arguments passed to TextFileReader.

Returns
-------
kwds : dict
    Updated keyword arguments, merged with dialect parameters."""""""
pandas/io/parsers/readers.py,"def _validate_skipfooter(kwds: dict[str, Any]) -> None:
    if kwds.get('skipfooter'):
        if kwds.get('iterator') or kwds.get('chunksize'):
            raise ValueError(""'skipfooter' not supported for iteration"")
        if kwds.get('nrows'):
            raise ValueError(""'skipfooter' not supported with 'nrows'"")","""""""Check whether skipfooter is compatible with other kwargs in TextFileReader.

Parameters
----------
kwds : dict
    Keyword arguments passed to TextFileReader.

Raises
------
ValueError
    If skipfooter is not compatible with other parameters."""""""
pandas/io/pickle.py,"@doc(storage_options=_shared_docs['storage_options'], compression_options=_shared_docs['compression_options'] % 'filepath_or_buffer')
def to_pickle(obj: Any, filepath_or_buffer: FilePath | WriteBuffer[bytes], compression: CompressionOptions='infer', protocol: int=pickle.HIGHEST_PROTOCOL, storage_options: StorageOptions | None=None) -> None:
    if protocol < 0:
        protocol = pickle.HIGHEST_PROTOCOL
    with get_handle(filepath_or_buffer, 'wb', compression=compression, is_text=False, storage_options=storage_options) as handles:
        pickle.dump(obj, handles.handle, protocol=protocol)","""""""Pickle (serialize) object to file.

Parameters
----------
obj : any object
    Any python object.
filepath_or_buffer : str, path object, or file-like object
    String, path object (implementing ``os.PathLike[str]``), or file-like
    object implementing a binary ``write()`` function.
    Also accepts URL. URL has to be of S3 or GCS.
{compression_options}

    .. versionchanged:: 1.4.0 Zstandard support.

protocol : int
    Int which indicates which protocol should be used by the pickler,
    default HIGHEST_PROTOCOL (see [1], paragraph 12.1.2). The possible
    values for this parameter depend on the version of Python. For Python
    2.x, possible values are 0, 1, 2. For Python>=3.0, 3 is a valid value.
    For Python >= 3.4, 4 is a valid value. A negative value for the
    protocol parameter is equivalent to setting its value to
    HIGHEST_PROTOCOL.

{storage_options}

    .. versionadded:: 1.2.0

    .. [1] https://docs.python.org/3/library/pickle.html

See Also
--------
read_pickle : Load pickled pandas object (or any object) from file.
DataFrame.to_hdf : Write DataFrame to an HDF5 file.
DataFrame.to_sql : Write DataFrame to a SQL database.
DataFrame.to_parquet : Write a DataFrame to the binary parquet format.

Examples
--------
>>> original_df = pd.DataFrame({{""foo"": range(5), ""bar"": range(5, 10)}})  # doctest: +SKIP
>>> original_df  # doctest: +SKIP
   foo  bar
0    0    5
1    1    6
2    2    7
3    3    8
4    4    9
>>> pd.to_pickle(original_df, ""./dummy.pkl"")  # doctest: +SKIP

>>> unpickled_df = pd.read_pickle(""./dummy.pkl"")  # doctest: +SKIP
>>> unpickled_df  # doctest: +SKIP
   foo  bar
0    0    5
1    1    6
2    2    7
3    3    8
4    4    9"""""""
pandas/io/pickle.py,"@doc(storage_options=_shared_docs['storage_options'], decompression_options=_shared_docs['decompression_options'] % 'filepath_or_buffer')
def read_pickle(filepath_or_buffer: FilePath | ReadPickleBuffer, compression: CompressionOptions='infer', storage_options: StorageOptions | None=None) -> DataFrame | Series:
    excs_to_catch = (AttributeError, ImportError, ModuleNotFoundError, TypeError)
    with get_handle(filepath_or_buffer, 'rb', compression=compression, is_text=False, storage_options=storage_options) as handles:
        try:
            try:
                with warnings.catch_warnings(record=True):
                    warnings.simplefilter('ignore', Warning)
                    return pickle.load(handles.handle)
            except excs_to_catch:
                return pc.load(handles.handle, encoding=None)
        except UnicodeDecodeError:
            return pc.load(handles.handle, encoding='latin-1')","""""""Load pickled pandas object (or any object) from file.

.. warning::

   Loading pickled data received from untrusted sources can be
   unsafe. See `here <https://docs.python.org/3/library/pickle.html>`__.

Parameters
----------
filepath_or_buffer : str, path object, or file-like object
    String, path object (implementing ``os.PathLike[str]``), or file-like
    object implementing a binary ``readlines()`` function.
    Also accepts URL. URL is not limited to S3 and GCS.

{decompression_options}

    .. versionchanged:: 1.4.0 Zstandard support.

{storage_options}

    .. versionadded:: 1.2.0

Returns
-------
same type as object stored in file

See Also
--------
DataFrame.to_pickle : Pickle (serialize) DataFrame object to file.
Series.to_pickle : Pickle (serialize) Series object to file.
read_hdf : Read HDF5 file into a DataFrame.
read_sql : Read SQL query or database table into a DataFrame.
read_parquet : Load a parquet object, returning a DataFrame.

Notes
-----
read_pickle is only guaranteed to be backwards compatible to pandas 0.20.3
provided the object was serialized with to_pickle.

Examples
--------
>>> original_df = pd.DataFrame(
...     {{""foo"": range(5), ""bar"": range(5, 10)}}
...    )  # doctest: +SKIP
>>> original_df  # doctest: +SKIP
   foo  bar
0    0    5
1    1    6
2    2    7
3    3    8
4    4    9
>>> pd.to_pickle(original_df, ""./dummy.pkl"")  # doctest: +SKIP

>>> unpickled_df = pd.read_pickle(""./dummy.pkl"")  # doctest: +SKIP
>>> unpickled_df  # doctest: +SKIP
   foo  bar
0    0    5
1    1    6
2    2    7
3    3    8
4    4    9"""""""
pandas/io/pytables.py,"def _ensure_decoded(s):
    if isinstance(s, np.bytes_):
        s = s.decode('UTF-8')
    return s","""""""if we have bytes, decode them to unicode"""""""
pandas/io/pytables.py,"def _ensure_str(name):
    if isinstance(name, str):
        name = str(name)
    return name","""""""Ensure that an index / column name is a str (python 3); otherwise they
may be np.string dtype. Non-string dtypes are passed through unchanged.

https://github.com/pandas-dev/pandas/issues/13492"""""""
pandas/io/pytables.py,"def _ensure_term(where, scope_level: int):
    level = scope_level + 1
    if isinstance(where, (list, tuple)):
        where = [Term(term, scope_level=level + 1) if maybe_expression(term) else term for term in where if term is not None]
    elif maybe_expression(where):
        where = Term(where, scope_level=level)
    return where if where is None or len(where) else None","""""""Ensure that the where is a Term or a list of Term.

This makes sure that we are capturing the scope of variables that are
passed create the terms here with a frame_level=2 (we are 2 levels down)"""""""
pandas/io/pytables.py,"def to_hdf(path_or_buf: FilePath | HDFStore, key: str, value: DataFrame | Series, mode: str='a', complevel: int | None=None, complib: str | None=None, append: bool=False, format: str | None=None, index: bool=True, min_itemsize: int | dict[str, int] | None=None, nan_rep=None, dropna: bool | None=None, data_columns: Literal[True] | list[str] | None=None, errors: str='strict', encoding: str='UTF-8') -> None:
    if append:
        f = lambda store: store.append(key, value, format=format, index=index, min_itemsize=min_itemsize, nan_rep=nan_rep, dropna=dropna, data_columns=data_columns, errors=errors, encoding=encoding)
    else:
        f = lambda store: store.put(key, value, format=format, index=index, min_itemsize=min_itemsize, nan_rep=nan_rep, data_columns=data_columns, errors=errors, encoding=encoding, dropna=dropna)
    path_or_buf = stringify_path(path_or_buf)
    if isinstance(path_or_buf, str):
        with HDFStore(path_or_buf, mode=mode, complevel=complevel, complib=complib) as store:
            f(store)
    else:
        f(path_or_buf)","""""""store this object, close it if we opened it"""""""
pandas/io/pytables.py,"def read_hdf(path_or_buf: FilePath | HDFStore, key=None, mode: str='r', errors: str='strict', where: str | list | None=None, start: int | None=None, stop: int | None=None, columns: list[str] | None=None, iterator: bool=False, chunksize: int | None=None, **kwargs):
    if mode not in ['r', 'r+', 'a']:
        raise ValueError(f'mode {mode} is not allowed while performing a read. Allowed modes are r, r+ and a.')
    if where is not None:
        where = _ensure_term(where, scope_level=1)
    if isinstance(path_or_buf, HDFStore):
        if not path_or_buf.is_open:
            raise OSError('The HDFStore must be open for reading.')
        store = path_or_buf
        auto_close = False
    else:
        path_or_buf = stringify_path(path_or_buf)
        if not isinstance(path_or_buf, str):
            raise NotImplementedError('Support for generic buffers has not been implemented.')
        try:
            exists = os.path.exists(path_or_buf)
        except (TypeError, ValueError):
            exists = False
        if not exists:
            raise FileNotFoundError(f'File {path_or_buf} does not exist')
        store = HDFStore(path_or_buf, mode=mode, errors=errors, **kwargs)
        auto_close = True
    try:
        if key is None:
            groups = store.groups()
            if len(groups) == 0:
                raise ValueError('Dataset(s) incompatible with Pandas data types, not table, or no datasets found in HDF5 file.')
            candidate_only_group = groups[0]
            for group_to_check in groups[1:]:
                if not _is_metadata_of(group_to_check, candidate_only_group):
                    raise ValueError('key must be provided when HDF5 file contains multiple datasets.')
            key = candidate_only_group._v_pathname
        return store.select(key, where=where, start=start, stop=stop, columns=columns, iterator=iterator, chunksize=chunksize, auto_close=auto_close)
    except (ValueError, TypeError, LookupError):
        if not isinstance(path_or_buf, HDFStore):
            with suppress(AttributeError):
                store.close()
        raise","""""""Read from the store, close it if we opened it.

Retrieve pandas object stored in file, optionally based on where
criteria.

.. warning::

   Pandas uses PyTables for reading and writing HDF5 files, which allows
   serializing object-dtype data with pickle when using the ""fixed"" format.
   Loading pickled data received from untrusted sources can be unsafe.

   See: https://docs.python.org/3/library/pickle.html for more.

Parameters
----------
path_or_buf : str, path object, pandas.HDFStore
    Any valid string path is acceptable. Only supports the local file system,
    remote URLs and file-like objects are not supported.

    If you want to pass in a path object, pandas accepts any
    ``os.PathLike``.

    Alternatively, pandas accepts an open :class:`pandas.HDFStore` object.

key : object, optional
    The group identifier in the store. Can be omitted if the HDF file
    contains a single pandas object.
mode : {'r', 'r+', 'a'}, default 'r'
    Mode to use when opening the file. Ignored if path_or_buf is a
    :class:`pandas.HDFStore`. Default is 'r'.
errors : str, default 'strict'
    Specifies how encoding and decoding errors are to be handled.
    See the errors argument for :func:`open` for a full list
    of options.
where : list, optional
    A list of Term (or convertible) objects.
start : int, optional
    Row number to start selection.
stop  : int, optional
    Row number to stop selection.
columns : list, optional
    A list of columns names to return.
iterator : bool, optional
    Return an iterator object.
chunksize : int, optional
    Number of rows to include in an iteration when using an iterator.
**kwargs
    Additional keyword arguments passed to HDFStore.

Returns
-------
object
    The selected object. Return type depends on the object stored.

See Also
--------
DataFrame.to_hdf : Write a HDF file from a DataFrame.
HDFStore : Low-level access to HDF files.

Examples
--------
>>> df = pd.DataFrame([[1, 1.0, 'a']], columns=['x', 'y', 'z'])  # doctest: +SKIP
>>> df.to_hdf('./store.h5', 'data')  # doctest: +SKIP
>>> reread = pd.read_hdf('./store.h5')  # doctest: +SKIP"""""""
pandas/io/pytables.py,"def _is_metadata_of(group: Node, parent_group: Node) -> bool:
    if group._v_depth <= parent_group._v_depth:
        return False
    current = group
    while current._v_depth > 1:
        parent = current._v_parent
        if parent == parent_group and current._v_name == 'meta':
            return True
        current = current._v_parent
    return False","""""""Check if a given group is a metadata group for a given parent_group."""""""
pandas/io/pytables.py,"def _get_tz(tz: tzinfo) -> str | tzinfo:
    zone = timezones.get_timezone(tz)
    return zone","""""""for a tz-aware type, return an encoded zone"""""""
pandas/io/pytables.py,"def _set_tz(values: np.ndarray | Index, tz: str | tzinfo | None, coerce: bool=False) -> np.ndarray | DatetimeIndex:
    if isinstance(values, DatetimeIndex):
        assert values.tz is None or values.tz == tz
    if tz is not None:
        if isinstance(values, DatetimeIndex):
            name = values.name
            values = values.asi8
        else:
            name = None
            values = values.ravel()
        tz = _ensure_decoded(tz)
        values = DatetimeIndex(values, name=name)
        values = values.tz_localize('UTC').tz_convert(tz)
    elif coerce:
        values = np.asarray(values, dtype='M8[ns]')
    return values","""""""coerce the values to a DatetimeIndex if tz is set
preserve the input shape if possible

Parameters
----------
values : ndarray or Index
tz : str or tzinfo
coerce : if we do not have a passed timezone, coerce to M8[ns] ndarray"""""""
pandas/io/pytables.py,"def _convert_string_array(data: np.ndarray, encoding: str, errors: str) -> np.ndarray:
    if len(data):
        data = Series(data.ravel(), copy=False).str.encode(encoding, errors)._values.reshape(data.shape)
    ensured = ensure_object(data.ravel())
    itemsize = max(1, libwriters.max_len_string_array(ensured))
    data = np.asarray(data, dtype=f'S{itemsize}')
    return data","""""""Take a string-like that is object dtype and coerce to a fixed size string type.

Parameters
----------
data : np.ndarray[object]
encoding : str
errors : str
    Handler for encoding errors.

Returns
-------
np.ndarray[fixed-length-string]"""""""
pandas/io/pytables.py,"def _unconvert_string_array(data: np.ndarray, nan_rep, encoding: str, errors: str) -> np.ndarray:
    shape = data.shape
    data = np.asarray(data.ravel(), dtype=object)
    if len(data):
        itemsize = libwriters.max_len_string_array(ensure_object(data))
        dtype = f'U{itemsize}'
        if isinstance(data[0], bytes):
            data = Series(data, copy=False).str.decode(encoding, errors=errors)._values
        else:
            data = data.astype(dtype, copy=False).astype(object, copy=False)
    if nan_rep is None:
        nan_rep = 'nan'
    libwriters.string_array_replace_from_nan_rep(data, nan_rep)
    return data.reshape(shape)","""""""Inverse of _convert_string_array.

Parameters
----------
data : np.ndarray[fixed-length-string]
nan_rep : the storage repr of NaN
encoding : str
errors : str
    Handler for encoding errors.

Returns
-------
np.ndarray[object]
    Decoded data."""""""
pandas/io/pytables.py,"def _maybe_adjust_name(name: str, version: Sequence[int]) -> str:
    if isinstance(version, str) or len(version) < 3:
        raise ValueError('Version is incorrect, expected sequence of 3 integers.')
    if version[0] == 0 and version[1] <= 10 and (version[2] == 0):
        m = re.search('values_block_(\\d+)', name)
        if m:
            grp = m.groups()[0]
            name = f'values_{grp}'
    return name","""""""Prior to 0.10.1, we named values blocks like: values_block_0 an the
name values_0, adjust the given name if necessary.

Parameters
----------
name : str
version : Tuple[int, int, int]

Returns
-------
str"""""""
pandas/io/pytables.py,"def _dtype_to_kind(dtype_str: str) -> str:
    dtype_str = _ensure_decoded(dtype_str)
    if dtype_str.startswith(('string', 'bytes')):
        kind = 'string'
    elif dtype_str.startswith('float'):
        kind = 'float'
    elif dtype_str.startswith('complex'):
        kind = 'complex'
    elif dtype_str.startswith(('int', 'uint')):
        kind = 'integer'
    elif dtype_str.startswith('datetime64'):
        kind = 'datetime64'
    elif dtype_str.startswith('timedelta'):
        kind = 'timedelta64'
    elif dtype_str.startswith('bool'):
        kind = 'bool'
    elif dtype_str.startswith('category'):
        kind = 'category'
    elif dtype_str.startswith('period'):
        kind = 'integer'
    elif dtype_str == 'object':
        kind = 'object'
    else:
        raise ValueError(f'cannot interpret dtype of [{dtype_str}]')
    return kind","""""""Find the ""kind"" string describing the given dtype name."""""""
pandas/io/pytables.py,"def _get_data_and_dtype_name(data: ArrayLike):
    if isinstance(data, Categorical):
        data = data.codes
    dtype_name = data.dtype.name.split('[')[0]
    if data.dtype.kind in 'mM':
        data = np.asarray(data.view('i8'))
    elif isinstance(data, PeriodIndex):
        data = data.asi8
    data = np.asarray(data)
    return (data, dtype_name)","""""""Convert the passed data into a storable form and a dtype string."""""""
pandas/io/sas/sas7bdat.py,"def _convert_datetimes(sas_datetimes: pd.Series, unit: str) -> pd.Series:
    try:
        return pd.to_datetime(sas_datetimes, unit=unit, origin='1960-01-01')
    except OutOfBoundsDatetime:
        s_series = sas_datetimes.apply(_parse_datetime, unit=unit)
        s_series = cast(pd.Series, s_series)
        return s_series","""""""Convert to Timestamp if possible, otherwise to datetime.datetime.
SAS float64 lacks precision for more than ms resolution so the fit
to datetime.datetime is ok.

Parameters
----------
sas_datetimes : {Series, Sequence[float]}
   Dates or datetimes in SAS
unit : {str}
   ""d"" if the floats represent dates, ""s"" for datetimes

Returns
-------
Series
   Series of datetime64 dtype or datetime.datetime."""""""
pandas/io/sas/sas_xport.py,"def _parse_date(datestr: str) -> DatetimeNaTType:
    try:
        return datetime.strptime(datestr, '%d%b%y:%H:%M:%S')
    except ValueError:
        return pd.NaT","""""""Given a date in xport format, return Python date."""""""
pandas/io/sas/sas_xport.py,"def _split_line(s: str, parts):
    out = {}
    start = 0
    for (name, length) in parts:
        out[name] = s[start:start + length].strip()
        start += length
    del out['_']
    return out","""""""Parameters
----------
s: str
    Fixed-length string to split
parts: list of (name, length) pairs
    Used to break up string, name '_' will be filtered from output.

Returns
-------
Dict of name:contents of string at given location."""""""
pandas/io/sas/sas_xport.py,"def _parse_float_vec(vec):
    dtype = np.dtype('>u4,>u4')
    vec1 = vec.view(dtype=dtype)
    xport1 = vec1['f0']
    xport2 = vec1['f1']
    ieee1 = xport1 & 16777215
    shift = np.zeros(len(vec), dtype=np.uint8)
    shift[np.where(xport1 & 2097152)] = 1
    shift[np.where(xport1 & 4194304)] = 2
    shift[np.where(xport1 & 8388608)] = 3
    ieee1 >>= shift
    ieee2 = xport2 >> shift | (xport1 & 7) << 29 + (3 - shift)
    ieee1 &= 4293918719
    ieee1 |= ((xport1 >> 24 & 127) - 65 << 2) + shift + 1023 << 20 | xport1 & 2147483648
    ieee = np.empty((len(ieee1),), dtype='>u4,>u4')
    ieee['f0'] = ieee1
    ieee['f1'] = ieee2
    ieee = ieee.view(dtype='>f8')
    ieee = ieee.astype('f8')
    return ieee","""""""Parse a vector of float values representing IBM 8 byte floats into
native 8 byte floats."""""""
pandas/io/sas/sasreader.py,"@doc(decompression_options=_shared_docs['decompression_options'] % 'filepath_or_buffer')
def read_sas(filepath_or_buffer: FilePath | ReadBuffer[bytes], *, format: str | None=None, index: Hashable | None=None, encoding: str | None=None, chunksize: int | None=None, iterator: bool=False, compression: CompressionOptions='infer') -> DataFrame | ReaderBase:
    if format is None:
        buffer_error_msg = 'If this is a buffer object rather than a string name, you must specify a format string'
        filepath_or_buffer = stringify_path(filepath_or_buffer)
        if not isinstance(filepath_or_buffer, str):
            raise ValueError(buffer_error_msg)
        fname = filepath_or_buffer.lower()
        if '.xpt' in fname:
            format = 'xport'
        elif '.sas7bdat' in fname:
            format = 'sas7bdat'
        else:
            raise ValueError(f'unable to infer format of SAS file from filename: {repr(fname)}')
    reader: ReaderBase
    if format.lower() == 'xport':
        from pandas.io.sas.sas_xport import XportReader
        reader = XportReader(filepath_or_buffer, index=index, encoding=encoding, chunksize=chunksize, compression=compression)
    elif format.lower() == 'sas7bdat':
        from pandas.io.sas.sas7bdat import SAS7BDATReader
        reader = SAS7BDATReader(filepath_or_buffer, index=index, encoding=encoding, chunksize=chunksize, compression=compression)
    else:
        raise ValueError('unknown SAS format')
    if iterator or chunksize:
        return reader
    with reader:
        return reader.read()","""""""Read SAS files stored as either XPORT or SAS7BDAT format files.

Parameters
----------
filepath_or_buffer : str, path object, or file-like object
    String, path object (implementing ``os.PathLike[str]``), or file-like
    object implementing a binary ``read()`` function. The string could be a URL.
    Valid URL schemes include http, ftp, s3, and file. For file URLs, a host is
    expected. A local file could be:
    ``file://localhost/path/to/table.sas7bdat``.
format : str {{'xport', 'sas7bdat'}} or None
    If None, file format is inferred from file extension. If 'xport' or
    'sas7bdat', uses the corresponding format.
index : identifier of index column, defaults to None
    Identifier of column that should be used as index of the DataFrame.
encoding : str, default is None
    Encoding for text data.  If None, text data are stored as raw bytes.
chunksize : int
    Read file `chunksize` lines at a time, returns iterator.

    .. versionchanged:: 1.2

        ``TextFileReader`` is a context manager.
iterator : bool, defaults to False
    If True, returns an iterator for reading the file incrementally.

    .. versionchanged:: 1.2

        ``TextFileReader`` is a context manager.
{decompression_options}

Returns
-------
DataFrame if iterator=False and chunksize=None, else SAS7BDATReader
or XportReader

Examples
--------
>>> df = pd.read_sas(""sas_data.sas7bdat"")  # doctest: +SKIP"""""""
pandas/io/spss.py,"def read_spss(path: str | Path, usecols: Sequence[str] | None=None, convert_categoricals: bool=True, dtype_backend: DtypeBackend | lib.NoDefault=lib.no_default) -> DataFrame:
    pyreadstat = import_optional_dependency('pyreadstat')
    check_dtype_backend(dtype_backend)
    if usecols is not None:
        if not is_list_like(usecols):
            raise TypeError('usecols must be list-like.')
        usecols = list(usecols)
    (df, _) = pyreadstat.read_sav(stringify_path(path), usecols=usecols, apply_value_formats=convert_categoricals)
    if dtype_backend is not lib.no_default:
        df = df.convert_dtypes(dtype_backend=dtype_backend)
    return df","""""""Load an SPSS file from the file path, returning a DataFrame.

Parameters
----------
path : str or Path
    File path.
usecols : list-like, optional
    Return a subset of the columns. If None, return all columns.
convert_categoricals : bool, default is True
    Convert categorical columns into pd.Categorical.
dtype_backend : {'numpy_nullable', 'pyarrow'}, default 'numpy_nullable'
    Back-end data type applied to the resultant :class:`DataFrame`
    (still experimental). Behaviour is as follows:

    * ``""numpy_nullable""``: returns nullable-dtype-backed :class:`DataFrame`
      (default).
    * ``""pyarrow""``: returns pyarrow-backed nullable :class:`ArrowDtype`
      DataFrame.

    .. versionadded:: 2.0

Returns
-------
DataFrame

Examples
--------
>>> df = pd.read_spss(""spss_data.sav"")  # doctest: +SKIP"""""""
pandas/io/sql.py,"def _process_parse_dates_argument(parse_dates):
    if parse_dates is True or parse_dates is None or parse_dates is False:
        parse_dates = []
    elif not hasattr(parse_dates, '__iter__'):
        parse_dates = [parse_dates]
    return parse_dates","""""""Process parse_dates argument for read_sql functions"""""""
pandas/io/sql.py,"def _parse_date_columns(data_frame, parse_dates):
    parse_dates = _process_parse_dates_argument(parse_dates)
    for (i, (col_name, df_col)) in enumerate(data_frame.items()):
        if isinstance(df_col.dtype, DatetimeTZDtype) or col_name in parse_dates:
            try:
                fmt = parse_dates[col_name]
            except (KeyError, TypeError):
                fmt = None
            data_frame.isetitem(i, _handle_date_column(df_col, format=fmt))
    return data_frame","""""""Force non-datetime columns to be read as such.
Supports both string formatted and integer timestamp columns."""""""
pandas/io/sql.py,"def _wrap_result(data, columns, index_col=None, coerce_float: bool=True, parse_dates=None, dtype: DtypeArg | None=None, dtype_backend: DtypeBackend | Literal['numpy']='numpy'):
    frame = _convert_arrays_to_dataframe(data, columns, coerce_float, dtype_backend)
    if dtype:
        frame = frame.astype(dtype)
    frame = _parse_date_columns(frame, parse_dates)
    if index_col is not None:
        frame = frame.set_index(index_col)
    return frame","""""""Wrap result set of query in a DataFrame."""""""
pandas/io/sql.py,"def execute(sql, con, params=None):
    warnings.warn('`pandas.io.sql.execute` is deprecated and will be removed in the future version.', FutureWarning, stacklevel=find_stack_level())
    sqlalchemy = import_optional_dependency('sqlalchemy', errors='ignore')
    if sqlalchemy is not None and isinstance(con, (str, sqlalchemy.engine.Engine)):
        raise TypeError('pandas.io.sql.execute requires a connection')
    with pandasSQL_builder(con, need_transaction=True) as pandas_sql:
        return pandas_sql.execute(sql, params)","""""""Execute the given SQL query using the provided connection object.

Parameters
----------
sql : string
    SQL query to be executed.
con : SQLAlchemy connection or sqlite3 connection
    If a DBAPI2 object, only sqlite3 is supported.
params : list or tuple, optional, default: None
    List of parameters to pass to execute method.

Returns
-------
Results Iterable"""""""
pandas/io/sql.py,"def read_sql_table(table_name: str, con, schema: str | None=None, index_col: str | list[str] | None=None, coerce_float: bool=True, parse_dates: list[str] | dict[str, str] | None=None, columns: list[str] | None=None, chunksize: int | None=None, dtype_backend: DtypeBackend | lib.NoDefault=lib.no_default) -> DataFrame | Iterator[DataFrame]:
    check_dtype_backend(dtype_backend)
    if dtype_backend is lib.no_default:
        dtype_backend = 'numpy'
    assert dtype_backend is not lib.no_default
    with pandasSQL_builder(con, schema=schema, need_transaction=True) as pandas_sql:
        if not pandas_sql.has_table(table_name):
            raise ValueError(f'Table {table_name} not found')
        table = pandas_sql.read_table(table_name, index_col=index_col, coerce_float=coerce_float, parse_dates=parse_dates, columns=columns, chunksize=chunksize, dtype_backend=dtype_backend)
    if table is not None:
        return table
    else:
        raise ValueError(f'Table {table_name} not found', con)","""""""Read SQL database table into a DataFrame.

Given a table name and a SQLAlchemy connectable, returns a DataFrame.
This function does not support DBAPI connections.

Parameters
----------
table_name : str
    Name of SQL table in database.
con : SQLAlchemy connectable or str
    A database URI could be provided as str.
    SQLite DBAPI connection mode not supported.
schema : str, default None
    Name of SQL schema in database to query (if database flavor
    supports this). Uses default schema if None (default).
index_col : str or list of str, optional, default: None
    Column(s) to set as index(MultiIndex).
coerce_float : bool, default True
    Attempts to convert values of non-string, non-numeric objects (like
    decimal.Decimal) to floating point. Can result in loss of Precision.
parse_dates : list or dict, default None
    - List of column names to parse as dates.
    - Dict of ``{column_name: format string}`` where format string is
      strftime compatible in case of parsing string times or is one of
      (D, s, ns, ms, us) in case of parsing integer timestamps.
    - Dict of ``{column_name: arg dict}``, where the arg dict corresponds
      to the keyword arguments of :func:`pandas.to_datetime`
      Especially useful with databases without native Datetime support,
      such as SQLite.
columns : list, default None
    List of column names to select from SQL table.
chunksize : int, default None
    If specified, returns an iterator where `chunksize` is the number of
    rows to include in each chunk.
dtype_backend : {'numpy_nullable', 'pyarrow'}, default 'numpy_nullable'
    Back-end data type applied to the resultant :class:`DataFrame`
    (still experimental). Behaviour is as follows:

    * ``""numpy_nullable""``: returns nullable-dtype-backed :class:`DataFrame`
      (default).
    * ``""pyarrow""``: returns pyarrow-backed nullable :class:`ArrowDtype`
      DataFrame.

    .. versionadded:: 2.0

Returns
-------
DataFrame or Iterator[DataFrame]
    A SQL table is returned as two-dimensional data structure with labeled
    axes.

See Also
--------
read_sql_query : Read SQL query into a DataFrame.
read_sql : Read SQL query or database table into a DataFrame.

Notes
-----
Any datetime values with time zone information will be converted to UTC.

Examples
--------
>>> pd.read_sql_table('table_name', 'postgres:///db_name')  # doctest:+SKIP"""""""
pandas/io/sql.py,"def read_sql_query(sql, con, index_col: str | list[str] | None=None, coerce_float: bool=True, params: list[Any] | Mapping[str, Any] | None=None, parse_dates: list[str] | dict[str, str] | None=None, chunksize: int | None=None, dtype: DtypeArg | None=None, dtype_backend: DtypeBackend | lib.NoDefault=lib.no_default) -> DataFrame | Iterator[DataFrame]:
    check_dtype_backend(dtype_backend)
    if dtype_backend is lib.no_default:
        dtype_backend = 'numpy'
    assert dtype_backend is not lib.no_default
    with pandasSQL_builder(con) as pandas_sql:
        return pandas_sql.read_query(sql, index_col=index_col, params=params, coerce_float=coerce_float, parse_dates=parse_dates, chunksize=chunksize, dtype=dtype, dtype_backend=dtype_backend)","""""""Read SQL query into a DataFrame.

Returns a DataFrame corresponding to the result set of the query
string. Optionally provide an `index_col` parameter to use one of the
columns as the index, otherwise default integer index will be used.

Parameters
----------
sql : str SQL query or SQLAlchemy Selectable (select or text object)
    SQL query to be executed.
con : SQLAlchemy connectable, str, or sqlite3 connection
    Using SQLAlchemy makes it possible to use any DB supported by that
    library. If a DBAPI2 object, only sqlite3 is supported.
index_col : str or list of str, optional, default: None
    Column(s) to set as index(MultiIndex).
coerce_float : bool, default True
    Attempts to convert values of non-string, non-numeric objects (like
    decimal.Decimal) to floating point. Useful for SQL result sets.
params : list, tuple or mapping, optional, default: None
    List of parameters to pass to execute method.  The syntax used
    to pass parameters is database driver dependent. Check your
    database driver documentation for which of the five syntax styles,
    described in PEP 249's paramstyle, is supported.
    Eg. for psycopg2, uses %(name)s so use params={'name' : 'value'}.
parse_dates : list or dict, default: None
    - List of column names to parse as dates.
    - Dict of ``{column_name: format string}`` where format string is
      strftime compatible in case of parsing string times, or is one of
      (D, s, ns, ms, us) in case of parsing integer timestamps.
    - Dict of ``{column_name: arg dict}``, where the arg dict corresponds
      to the keyword arguments of :func:`pandas.to_datetime`
      Especially useful with databases without native Datetime support,
      such as SQLite.
chunksize : int, default None
    If specified, return an iterator where `chunksize` is the number of
    rows to include in each chunk.
dtype : Type name or dict of columns
    Data type for data or columns. E.g. np.float64 or
    {'a': np.float64, 'b': np.int32, 'c': 'Int64'}.

    .. versionadded:: 1.3.0
dtype_backend : {'numpy_nullable', 'pyarrow'}, default 'numpy_nullable'
    Back-end data type applied to the resultant :class:`DataFrame`
    (still experimental). Behaviour is as follows:

    * ``""numpy_nullable""``: returns nullable-dtype-backed :class:`DataFrame`
      (default).
    * ``""pyarrow""``: returns pyarrow-backed nullable :class:`ArrowDtype`
      DataFrame.

    .. versionadded:: 2.0

Returns
-------
DataFrame or Iterator[DataFrame]

See Also
--------
read_sql_table : Read SQL database table into a DataFrame.
read_sql : Read SQL query or database table into a DataFrame.

Notes
-----
Any datetime values with time zone information parsed via the `parse_dates`
parameter will be converted to UTC.

Examples
--------
>>> from sqlalchemy import create_engine  # doctest: +SKIP
>>> engine = create_engine(""sqlite:///database.db"")  # doctest: +SKIP
>>> with engine.connect() as conn, conn.begin():  # doctest: +SKIP
...     data = pd.read_sql_table(""data"", conn)  # doctest: +SKIP"""""""
pandas/io/sql.py,"def read_sql(sql, con, index_col: str | list[str] | None=None, coerce_float: bool=True, params=None, parse_dates=None, columns: list[str] | None=None, chunksize: int | None=None, dtype_backend: DtypeBackend | lib.NoDefault=lib.no_default, dtype: DtypeArg | None=None) -> DataFrame | Iterator[DataFrame]:
    check_dtype_backend(dtype_backend)
    if dtype_backend is lib.no_default:
        dtype_backend = 'numpy'
    assert dtype_backend is not lib.no_default
    with pandasSQL_builder(con) as pandas_sql:
        if isinstance(pandas_sql, SQLiteDatabase):
            return pandas_sql.read_query(sql, index_col=index_col, params=params, coerce_float=coerce_float, parse_dates=parse_dates, chunksize=chunksize, dtype_backend=dtype_backend, dtype=dtype)
        try:
            _is_table_name = pandas_sql.has_table(sql)
        except Exception:
            _is_table_name = False
        if _is_table_name:
            return pandas_sql.read_table(sql, index_col=index_col, coerce_float=coerce_float, parse_dates=parse_dates, columns=columns, chunksize=chunksize, dtype_backend=dtype_backend)
        else:
            return pandas_sql.read_query(sql, index_col=index_col, params=params, coerce_float=coerce_float, parse_dates=parse_dates, chunksize=chunksize, dtype_backend=dtype_backend, dtype=dtype)","""""""Read SQL query or database table into a DataFrame.

This function is a convenience wrapper around ``read_sql_table`` and
``read_sql_query`` (for backward compatibility). It will delegate
to the specific function depending on the provided input. A SQL query
will be routed to ``read_sql_query``, while a database table name will
be routed to ``read_sql_table``. Note that the delegated function might
have more specific notes about their functionality not listed here.

Parameters
----------
sql : str or SQLAlchemy Selectable (select or text object)
    SQL query to be executed or a table name.
con : SQLAlchemy connectable, str, or sqlite3 connection
    Using SQLAlchemy makes it possible to use any DB supported by that
    library. If a DBAPI2 object, only sqlite3 is supported. The user is responsible
    for engine disposal and connection closure for the SQLAlchemy connectable; str
    connections are closed automatically. See
    `here <https://docs.sqlalchemy.org/en/13/core/connections.html>`_.
index_col : str or list of str, optional, default: None
    Column(s) to set as index(MultiIndex).
coerce_float : bool, default True
    Attempts to convert values of non-string, non-numeric objects (like
    decimal.Decimal) to floating point, useful for SQL result sets.
params : list, tuple or dict, optional, default: None
    List of parameters to pass to execute method.  The syntax used
    to pass parameters is database driver dependent. Check your
    database driver documentation for which of the five syntax styles,
    described in PEP 249's paramstyle, is supported.
    Eg. for psycopg2, uses %(name)s so use params={'name' : 'value'}.
parse_dates : list or dict, default: None
    - List of column names to parse as dates.
    - Dict of ``{column_name: format string}`` where format string is
      strftime compatible in case of parsing string times, or is one of
      (D, s, ns, ms, us) in case of parsing integer timestamps.
    - Dict of ``{column_name: arg dict}``, where the arg dict corresponds
      to the keyword arguments of :func:`pandas.to_datetime`
      Especially useful with databases without native Datetime support,
      such as SQLite.
columns : list, default: None
    List of column names to select from SQL table (only used when reading
    a table).
chunksize : int, default None
    If specified, return an iterator where `chunksize` is the
    number of rows to include in each chunk.
dtype_backend : {'numpy_nullable', 'pyarrow'}, default 'numpy_nullable'
    Back-end data type applied to the resultant :class:`DataFrame`
    (still experimental). Behaviour is as follows:

    * ``""numpy_nullable""``: returns nullable-dtype-backed :class:`DataFrame`
      (default).
    * ``""pyarrow""``: returns pyarrow-backed nullable :class:`ArrowDtype`
      DataFrame.

    .. versionadded:: 2.0
dtype : Type name or dict of columns
    Data type for data or columns. E.g. np.float64 or
    {'a': np.float64, 'b': np.int32, 'c': 'Int64'}.
    The argument is ignored if a table is passed instead of a query.

    .. versionadded:: 2.0.0

Returns
-------
DataFrame or Iterator[DataFrame]

See Also
--------
read_sql_table : Read SQL database table into a DataFrame.
read_sql_query : Read SQL query into a DataFrame.

Examples
--------
Read data from SQL via either a SQL query or a SQL tablename.
When using a SQLite database only SQL queries are accepted,
providing only the SQL tablename will result in an error.

>>> from sqlite3 import connect
>>> conn = connect(':memory:')
>>> df = pd.DataFrame(data=[[0, '10/11/12'], [1, '12/11/10']],
...                   columns=['int_column', 'date_column'])
>>> df.to_sql(name='test_data', con=conn)
2

>>> pd.read_sql('SELECT int_column, date_column FROM test_data', conn)
   int_column date_column
0           0    10/11/12
1           1    12/11/10

>>> pd.read_sql('test_data', 'postgres:///db_name')  # doctest:+SKIP

Apply date parsing to columns through the ``parse_dates`` argument
The ``parse_dates`` argument calls ``pd.to_datetime`` on the provided columns.
Custom argument values for applying ``pd.to_datetime`` on a column are specified
via a dictionary format:

>>> pd.read_sql('SELECT int_column, date_column FROM test_data',
...             conn,
...             parse_dates={""date_column"": {""format"": ""%d/%m/%y""}})
   int_column date_column
0           0  2012-11-10
1           1  2010-11-12"""""""
pandas/io/sql.py,"def to_sql(frame, name: str, con, schema: str | None=None, if_exists: Literal['fail', 'replace', 'append']='fail', index: bool=True, index_label: IndexLabel | None=None, chunksize: int | None=None, dtype: DtypeArg | None=None, method: Literal['multi'] | Callable | None=None, engine: str='auto', **engine_kwargs) -> int | None:
    if if_exists not in ('fail', 'replace', 'append'):
        raise ValueError(f""'{if_exists}' is not valid for if_exists"")
    if isinstance(frame, Series):
        frame = frame.to_frame()
    elif not isinstance(frame, DataFrame):
        raise NotImplementedError(""'frame' argument should be either a Series or a DataFrame"")
    with pandasSQL_builder(con, schema=schema, need_transaction=True) as pandas_sql:
        return pandas_sql.to_sql(frame, name, if_exists=if_exists, index=index, index_label=index_label, schema=schema, chunksize=chunksize, dtype=dtype, method=method, engine=engine, **engine_kwargs)","""""""Write records stored in a DataFrame to a SQL database.

Parameters
----------
frame : DataFrame, Series
name : str
    Name of SQL table.
con : SQLAlchemy connectable(engine/connection) or database string URI
    or sqlite3 DBAPI2 connection
    Using SQLAlchemy makes it possible to use any DB supported by that
    library.
    If a DBAPI2 object, only sqlite3 is supported.
schema : str, optional
    Name of SQL schema in database to write to (if database flavor
    supports this). If None, use default schema (default).
if_exists : {'fail', 'replace', 'append'}, default 'fail'
    - fail: If table exists, do nothing.
    - replace: If table exists, drop it, recreate it, and insert data.
    - append: If table exists, insert data. Create if does not exist.
index : bool, default True
    Write DataFrame index as a column.
index_label : str or sequence, optional
    Column label for index column(s). If None is given (default) and
    `index` is True, then the index names are used.
    A sequence should be given if the DataFrame uses MultiIndex.
chunksize : int, optional
    Specify the number of rows in each batch to be written at a time.
    By default, all rows will be written at once.
dtype : dict or scalar, optional
    Specifying the datatype for columns. If a dictionary is used, the
    keys should be the column names and the values should be the
    SQLAlchemy types or strings for the sqlite3 fallback mode. If a
    scalar is provided, it will be applied to all columns.
method : {None, 'multi', callable}, optional
    Controls the SQL insertion clause used:

    - None : Uses standard SQL ``INSERT`` clause (one per row).
    - ``'multi'``: Pass multiple values in a single ``INSERT`` clause.
    - callable with signature ``(pd_table, conn, keys, data_iter) -> int | None``.

    Details and a sample callable implementation can be found in the
    section :ref:`insert method <io.sql.method>`.
engine : {'auto', 'sqlalchemy'}, default 'auto'
    SQL engine library to use. If 'auto', then the option
    ``io.sql.engine`` is used. The default ``io.sql.engine``
    behavior is 'sqlalchemy'

    .. versionadded:: 1.3.0

**engine_kwargs
    Any additional kwargs are passed to the engine.

Returns
-------
None or int
    Number of rows affected by to_sql. None is returned if the callable
    passed into ``method`` does not return an integer number of rows.

    .. versionadded:: 1.4.0

Notes
-----
The returned rows affected is the sum of the ``rowcount`` attribute of ``sqlite3.Cursor``
or SQLAlchemy connectable. The returned value may not reflect the exact number of written
rows as stipulated in the
`sqlite3 <https://docs.python.org/3/library/sqlite3.html#sqlite3.Cursor.rowcount>`__ or
`SQLAlchemy <https://docs.sqlalchemy.org/en/14/core/connections.html#sqlalchemy.engine.BaseCursorResult.rowcount>`__"""""""
pandas/io/sql.py,"def has_table(table_name: str, con, schema: str | None=None) -> bool:
    with pandasSQL_builder(con, schema=schema) as pandas_sql:
        return pandas_sql.has_table(table_name)","""""""Check if DataBase has named table.

Parameters
----------
table_name: string
    Name of SQL table.
con: SQLAlchemy connectable(engine/connection) or sqlite3 DBAPI2 connection
    Using SQLAlchemy makes it possible to use any DB supported by that
    library.
    If a DBAPI2 object, only sqlite3 is supported.
schema : string, default None
    Name of SQL schema in database to write to (if database flavor supports
    this). If None, use default schema (default).

Returns
-------
boolean"""""""
pandas/io/sql.py,"def pandasSQL_builder(con, schema: str | None=None, need_transaction: bool=False) -> PandasSQL:
    import sqlite3
    if isinstance(con, sqlite3.Connection) or con is None:
        return SQLiteDatabase(con)
    sqlalchemy = import_optional_dependency('sqlalchemy', errors='ignore')
    if isinstance(con, str) and sqlalchemy is None:
        raise ImportError('Using URI string without sqlalchemy installed.')
    if sqlalchemy is not None and isinstance(con, (str, sqlalchemy.engine.Connectable)):
        return SQLDatabase(con, schema, need_transaction)
    warnings.warn('pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.', UserWarning, stacklevel=find_stack_level())
    return SQLiteDatabase(con)","""""""Convenience function to return the correct PandasSQL subclass based on the
provided parameters.  Also creates a sqlalchemy connection and transaction
if necessary."""""""
pandas/io/sql.py,"def get_engine(engine: str) -> BaseEngine:
    if engine == 'auto':
        engine = get_option('io.sql.engine')
    if engine == 'auto':
        engine_classes = [SQLAlchemyEngine]
        error_msgs = ''
        for engine_class in engine_classes:
            try:
                return engine_class()
            except ImportError as err:
                error_msgs += '\n - ' + str(err)
        raise ImportError(f""Unable to find a usable engine; tried using: 'sqlalchemy'.\nA suitable version of sqlalchemy is required for sql I/O support.\nTrying to import the above resulted in these errors:{error_msgs}"")
    if engine == 'sqlalchemy':
        return SQLAlchemyEngine()
    raise ValueError(""engine must be one of 'auto', 'sqlalchemy'"")","""""""return our implementation"""""""
pandas/io/sql.py,"def get_schema(frame, name: str, keys=None, con=None, dtype: DtypeArg | None=None, schema: str | None=None) -> str:
    with pandasSQL_builder(con=con) as pandas_sql:
        return pandas_sql._create_sql_schema(frame, name, keys=keys, dtype=dtype, schema=schema)","""""""Get the SQL db table schema for the given frame.

Parameters
----------
frame : DataFrame
name : str
    name of SQL table
keys : string or sequence, default: None
    columns to use a primary key
con: an open SQL database connection object or a SQLAlchemy connectable
    Using SQLAlchemy makes it possible to use any DB supported by that
    library, default: None
    If a DBAPI2 object, only sqlite3 is supported.
dtype : dict of column name to SQL type, default None
    Optional specifying the datatype for columns. The SQL type should
    be a SQLAlchemy type, or a string for sqlite3 fallback connection.
schema: str, default: None
    Optional specifying the schema to be used in creating the table.

    .. versionadded:: 1.2.0"""""""
pandas/io/stata.py,"def _stata_elapsed_date_to_datetime_vec(dates, fmt) -> Series:
    (MIN_YEAR, MAX_YEAR) = (Timestamp.min.year, Timestamp.max.year)
    MAX_DAY_DELTA = (Timestamp.max - datetime(1960, 1, 1)).days
    MIN_DAY_DELTA = (Timestamp.min - datetime(1960, 1, 1)).days
    MIN_MS_DELTA = MIN_DAY_DELTA * 24 * 3600 * 1000
    MAX_MS_DELTA = MAX_DAY_DELTA * 24 * 3600 * 1000

    def convert_year_month_safe(year, month) -> Series:
        """"""
        Convert year and month to datetimes, using pandas vectorized versions
        when the date range falls within the range supported by pandas.
        Otherwise it falls back to a slower but more robust method
        using datetime.
        """"""
        if year.max() < MAX_YEAR and year.min() > MIN_YEAR:
            return to_datetime(100 * year + month, format='%Y%m')
        else:
            index = getattr(year, 'index', None)
            return Series([datetime(y, m, 1) for (y, m) in zip(year, month)], index=index)

    def convert_year_days_safe(year, days) -> Series:
        """"""
        Converts year (e.g. 1999) and days since the start of the year to a
        datetime or datetime64 Series
        """"""
        if year.max() < MAX_YEAR - 1 and year.min() > MIN_YEAR:
            return to_datetime(year, format='%Y') + to_timedelta(days, unit='d')
        else:
            index = getattr(year, 'index', None)
            value = [datetime(y, 1, 1) + timedelta(days=int(d)) for (y, d) in zip(year, days)]
            return Series(value, index=index)

    def convert_delta_safe(base, deltas, unit) -> Series:
        """"""
        Convert base dates and deltas to datetimes, using pandas vectorized
        versions if the deltas satisfy restrictions required to be expressed
        as dates in pandas.
        """"""
        index = getattr(deltas, 'index', None)
        if unit == 'd':
            if deltas.max() > MAX_DAY_DELTA or deltas.min() < MIN_DAY_DELTA:
                values = [base + timedelta(days=int(d)) for d in deltas]
                return Series(values, index=index)
        elif unit == 'ms':
            if deltas.max() > MAX_MS_DELTA or deltas.min() < MIN_MS_DELTA:
                values = [base + timedelta(microseconds=int(d) * 1000) for d in deltas]
                return Series(values, index=index)
        else:
            raise ValueError('format not understood')
        base = to_datetime(base)
        deltas = to_timedelta(deltas, unit=unit)
        return base + deltas
    bad_locs = np.isnan(dates)
    has_bad_values = False
    if bad_locs.any():
        has_bad_values = True
        dates._reset_cacher()
        dates[bad_locs] = 1.0
    dates = dates.astype(np.int64)
    if fmt.startswith(('%tc', 'tc')):
        base = stata_epoch
        ms = dates
        conv_dates = convert_delta_safe(base, ms, 'ms')
    elif fmt.startswith(('%tC', 'tC')):
        warnings.warn('Encountered %tC format. Leaving in Stata Internal Format.', stacklevel=find_stack_level())
        conv_dates = Series(dates, dtype=object)
        if has_bad_values:
            conv_dates[bad_locs] = NaT
        return conv_dates
    elif fmt.startswith(('%td', 'td', '%d', 'd')):
        base = stata_epoch
        days = dates
        conv_dates = convert_delta_safe(base, days, 'd')
    elif fmt.startswith(('%tw', 'tw')):
        year = stata_epoch.year + dates // 52
        days = dates % 52 * 7
        conv_dates = convert_year_days_safe(year, days)
    elif fmt.startswith(('%tm', 'tm')):
        year = stata_epoch.year + dates // 12
        month = dates % 12 + 1
        conv_dates = convert_year_month_safe(year, month)
    elif fmt.startswith(('%tq', 'tq')):
        year = stata_epoch.year + dates // 4
        quarter_month = dates % 4 * 3 + 1
        conv_dates = convert_year_month_safe(year, quarter_month)
    elif fmt.startswith(('%th', 'th')):
        year = stata_epoch.year + dates // 2
        month = dates % 2 * 6 + 1
        conv_dates = convert_year_month_safe(year, month)
    elif fmt.startswith(('%ty', 'ty')):
        year = dates
        first_month = np.ones_like(dates)
        conv_dates = convert_year_month_safe(year, first_month)
    else:
        raise ValueError(f'Date fmt {fmt} not understood')
    if has_bad_values:
        conv_dates[bad_locs] = NaT
    return conv_dates","""""""Convert from SIF to datetime. https://www.stata.com/help.cgi?datetime

Parameters
----------
dates : Series
    The Stata Internal Format date to convert to datetime according to fmt
fmt : str
    The format to convert to. Can be, tc, td, tw, tm, tq, th, ty
    Returns

Returns
-------
converted : Series
    The converted dates

Examples
--------
>>> dates = pd.Series([52])
>>> _stata_elapsed_date_to_datetime_vec(dates , ""%tw"")
0   1961-01-01
dtype: datetime64[ns]

Notes
-----
datetime/c - tc
    milliseconds since 01jan1960 00:00:00.000, assuming 86,400 s/day
datetime/C - tC - NOT IMPLEMENTED
    milliseconds since 01jan1960 00:00:00.000, adjusted for leap seconds
date - td
    days since 01jan1960 (01jan1960 = 0)
weekly date - tw
    weeks since 1960w1
    This assumes 52 weeks in a year, then adds 7 * remainder of the weeks.
    The datetime value is the start of the week in terms of days in the
    year, not ISO calendar weeks.
monthly date - tm
    months since 1960m1
quarterly date - tq
    quarters since 1960q1
half-yearly date - th
    half-years since 1960h1 yearly
date - ty
    years since 0000"""""""
pandas/io/stata.py,"def _datetime_to_stata_elapsed_vec(dates: Series, fmt: str) -> Series:
    index = dates.index
    NS_PER_DAY = 24 * 3600 * 1000 * 1000 * 1000
    US_PER_DAY = NS_PER_DAY / 1000

    def parse_dates_safe(dates: Series, delta: bool=False, year: bool=False, days: bool=False):
        d = {}
        if lib.is_np_dtype(dates.dtype, 'M'):
            if delta:
                time_delta = dates - Timestamp(stata_epoch).as_unit('ns')
                d['delta'] = time_delta._values.view(np.int64) // 1000
            if days or year:
                date_index = DatetimeIndex(dates)
                d['year'] = date_index._data.year
                d['month'] = date_index._data.month
            if days:
                days_in_ns = dates.view(np.int64) - to_datetime(d['year'], format='%Y').view(np.int64)
                d['days'] = days_in_ns // NS_PER_DAY
        elif infer_dtype(dates, skipna=False) == 'datetime':
            if delta:
                delta = dates._values - stata_epoch

                def f(x: timedelta) -> float:
                    return US_PER_DAY * x.days + 1000000 * x.seconds + x.microseconds
                v = np.vectorize(f)
                d['delta'] = v(delta)
            if year:
                year_month = dates.apply(lambda x: 100 * x.year + x.month)
                d['year'] = year_month._values // 100
                d['month'] = year_month._values - d['year'] * 100
            if days:

                def g(x: datetime) -> int:
                    return (x - datetime(x.year, 1, 1)).days
                v = np.vectorize(g)
                d['days'] = v(dates)
        else:
            raise ValueError('Columns containing dates must contain either datetime64, datetime or null values.')
        return DataFrame(d, index=index)
    bad_loc = isna(dates)
    index = dates.index
    if bad_loc.any():
        dates = Series(dates)
        if lib.is_np_dtype(dates.dtype, 'M'):
            dates[bad_loc] = to_datetime(stata_epoch)
        else:
            dates[bad_loc] = stata_epoch
    if fmt in ['%tc', 'tc']:
        d = parse_dates_safe(dates, delta=True)
        conv_dates = d.delta / 1000
    elif fmt in ['%tC', 'tC']:
        warnings.warn('Stata Internal Format tC not supported.', stacklevel=find_stack_level())
        conv_dates = dates
    elif fmt in ['%td', 'td']:
        d = parse_dates_safe(dates, delta=True)
        conv_dates = d.delta // US_PER_DAY
    elif fmt in ['%tw', 'tw']:
        d = parse_dates_safe(dates, year=True, days=True)
        conv_dates = 52 * (d.year - stata_epoch.year) + d.days // 7
    elif fmt in ['%tm', 'tm']:
        d = parse_dates_safe(dates, year=True)
        conv_dates = 12 * (d.year - stata_epoch.year) + d.month - 1
    elif fmt in ['%tq', 'tq']:
        d = parse_dates_safe(dates, year=True)
        conv_dates = 4 * (d.year - stata_epoch.year) + (d.month - 1) // 3
    elif fmt in ['%th', 'th']:
        d = parse_dates_safe(dates, year=True)
        conv_dates = 2 * (d.year - stata_epoch.year) + (d.month > 6).astype(int)
    elif fmt in ['%ty', 'ty']:
        d = parse_dates_safe(dates, year=True)
        conv_dates = d.year
    else:
        raise ValueError(f'Format {fmt} is not a known Stata date format')
    conv_dates = Series(conv_dates, dtype=np.float64)
    missing_value = struct.unpack('<d', b'\x00\x00\x00\x00\x00\x00\xe0\x7f')[0]
    conv_dates[bad_loc] = missing_value
    return Series(conv_dates, index=index)","""""""Convert from datetime to SIF. https://www.stata.com/help.cgi?datetime

Parameters
----------
dates : Series
    Series or array containing datetime or datetime64[ns] to
    convert to the Stata Internal Format given by fmt
fmt : str
    The format to convert to. Can be, tc, td, tw, tm, tq, th, ty"""""""
pandas/io/stata.py,"def _cast_to_stata_types(data: DataFrame) -> DataFrame:
    ws = ''
    conversion_data: tuple[tuple[type, type, type], tuple[type, type, type], tuple[type, type, type], tuple[type, type, type], tuple[type, type, type]] = ((np.bool_, np.int8, np.int8), (np.uint8, np.int8, np.int16), (np.uint16, np.int16, np.int32), (np.uint32, np.int32, np.int64), (np.uint64, np.int64, np.float64))
    float32_max = struct.unpack('<f', b'\xff\xff\xff~')[0]
    float64_max = struct.unpack('<d', b'\xff\xff\xff\xff\xff\xff\xdf\x7f')[0]
    for col in data:
        is_nullable_int = isinstance(data[col].dtype, (IntegerDtype, BooleanDtype))
        orig = data[col]
        orig_missing = orig.isna()
        if is_nullable_int:
            missing_loc = data[col].isna()
            if missing_loc.any():
                fv = 0 if isinstance(data[col].dtype, IntegerDtype) else False
                data.loc[missing_loc, col] = fv
            data[col] = data[col].astype(data[col].dtype.numpy_dtype)
        dtype = data[col].dtype
        empty_df = data.shape[0] == 0
        for c_data in conversion_data:
            if dtype == c_data[0]:
                if empty_df or data[col].max() <= np.iinfo(c_data[1]).max:
                    dtype = c_data[1]
                else:
                    dtype = c_data[2]
                if c_data[2] == np.int64:
                    if data[col].max() >= 2 ** 53:
                        ws = precision_loss_doc.format('uint64', 'float64')
                data[col] = data[col].astype(dtype)
        if dtype == np.int8 and (not empty_df):
            if data[col].max() > 100 or data[col].min() < -127:
                data[col] = data[col].astype(np.int16)
        elif dtype == np.int16 and (not empty_df):
            if data[col].max() > 32740 or data[col].min() < -32767:
                data[col] = data[col].astype(np.int32)
        elif dtype == np.int64:
            if empty_df or (data[col].max() <= 2147483620 and data[col].min() >= -2147483647):
                data[col] = data[col].astype(np.int32)
            else:
                data[col] = data[col].astype(np.float64)
                if data[col].max() >= 2 ** 53 or data[col].min() <= -2 ** 53:
                    ws = precision_loss_doc.format('int64', 'float64')
        elif dtype in (np.float32, np.float64):
            if np.isinf(data[col]).any():
                raise ValueError(f'Column {col} contains infinity or -infinitywhich is outside the range supported by Stata.')
            value = data[col].max()
            if dtype == np.float32 and value > float32_max:
                data[col] = data[col].astype(np.float64)
            elif dtype == np.float64:
                if value > float64_max:
                    raise ValueError(f'Column {col} has a maximum value ({value}) outside the range supported by Stata ({float64_max})')
        if is_nullable_int:
            if orig_missing.any():
                sentinel = StataMissingValue.BASE_MISSING_VALUES[data[col].dtype.name]
                data.loc[orig_missing, col] = sentinel
    if ws:
        warnings.warn(ws, PossiblePrecisionLoss, stacklevel=find_stack_level())
    return data","""""""Checks the dtypes of the columns of a pandas DataFrame for
compatibility with the data types and ranges supported by Stata, and
converts if necessary.

Parameters
----------
data : DataFrame
    The DataFrame to check and convert

Notes
-----
Numeric columns in Stata must be one of int8, int16, int32, float32 or
float64, with some additional value restrictions.  int8 and int16 columns
are checked for violations of the value restrictions and upcast if needed.
int64 data is not usable in Stata, and so it is downcast to int32 whenever
the value are in the int32 range, and sidecast to float64 when larger than
this range.  If the int64 values are outside of the range of those
perfectly representable as float64 values, a warning is raised.

bool columns are cast to int8.  uint columns are converted to int of the
same size if there is no loss in precision, otherwise are upcast to a
larger type.  uint64 is currently not supported since it is concerted to
object in a DataFrame."""""""
pandas/io/stata.py,"def _pad_bytes(name: AnyStr, length: int) -> AnyStr:
    if isinstance(name, bytes):
        return name + b'\x00' * (length - len(name))
    return name + '\x00' * (length - len(name))","""""""Take a char string and pads it with null bytes until it's length chars."""""""
pandas/io/stata.py,"def _convert_datetime_to_stata_type(fmt: str) -> np.dtype:
    if fmt in ['tc', '%tc', 'td', '%td', 'tw', '%tw', 'tm', '%tm', 'tq', '%tq', 'th', '%th', 'ty', '%ty']:
        return np.dtype(np.float64)
    else:
        raise NotImplementedError(f'Format {fmt} not implemented')","""""""Convert from one of the stata date formats to a type in TYPE_MAP."""""""
pandas/io/stata.py,"def _dtype_to_stata_type(dtype: np.dtype, column: Series) -> int:
    if dtype.type is np.object_:
        itemsize = max_len_string_array(ensure_object(column._values))
        return max(itemsize, 1)
    elif dtype.type is np.float64:
        return 255
    elif dtype.type is np.float32:
        return 254
    elif dtype.type is np.int32:
        return 253
    elif dtype.type is np.int16:
        return 252
    elif dtype.type is np.int8:
        return 251
    else:
        raise NotImplementedError(f'Data type {dtype} not supported.')","""""""Convert dtype types to stata types. Returns the byte of the given ordinal.
See TYPE_MAP and comments for an explanation. This is also explained in
the dta spec.
1 - 244 are strings of this length
                     Pandas    Stata
251 - for int8      byte
252 - for int16     int
253 - for int32     long
254 - for float32   float
255 - for double    double

If there are dates to convert, then dtype will already have the correct
type inserted."""""""
pandas/io/stata.py,"def _dtype_to_default_stata_fmt(dtype, column: Series, dta_version: int=114, force_strl: bool=False) -> str:
    if dta_version < 117:
        max_str_len = 244
    else:
        max_str_len = 2045
        if force_strl:
            return '%9s'
    if dtype.type is np.object_:
        itemsize = max_len_string_array(ensure_object(column._values))
        if itemsize > max_str_len:
            if dta_version >= 117:
                return '%9s'
            else:
                raise ValueError(excessive_string_length_error.format(column.name))
        return '%' + str(max(itemsize, 1)) + 's'
    elif dtype == np.float64:
        return '%10.0g'
    elif dtype == np.float32:
        return '%9.0g'
    elif dtype == np.int32:
        return '%12.0g'
    elif dtype in (np.int8, np.int16):
        return '%8.0g'
    else:
        raise NotImplementedError(f'Data type {dtype} not supported.')","""""""Map numpy dtype to stata's default format for this type. Not terribly
important since users can change this in Stata. Semantics are

object  -> ""%DDs"" where DD is the length of the string.  If not a string,
            raise ValueError
float64 -> ""%10.0g""
float32 -> ""%9.0g""
int64   -> ""%9.0g""
int32   -> ""%12.0g""
int16   -> ""%8.0g""
int8    -> ""%8.0g""
strl    -> ""%9s"""""""""
pandas/io/stata.py,"def _dtype_to_stata_type_117(dtype: np.dtype, column: Series, force_strl: bool) -> int:
    if force_strl:
        return 32768
    if dtype.type is np.object_:
        itemsize = max_len_string_array(ensure_object(column._values))
        itemsize = max(itemsize, 1)
        if itemsize <= 2045:
            return itemsize
        return 32768
    elif dtype.type is np.float64:
        return 65526
    elif dtype.type is np.float32:
        return 65527
    elif dtype.type is np.int32:
        return 65528
    elif dtype.type is np.int16:
        return 65529
    elif dtype.type is np.int8:
        return 65530
    else:
        raise NotImplementedError(f'Data type {dtype} not supported.')","""""""Converts dtype types to stata types. Returns the byte of the given ordinal.
See TYPE_MAP and comments for an explanation. This is also explained in
the dta spec.
1 - 2045 are strings of this length
            Pandas    Stata
32768 - for object    strL
65526 - for int8      byte
65527 - for int16     int
65528 - for int32     long
65529 - for float32   float
65530 - for double    double

If there are dates to convert, then dtype will already have the correct
type inserted."""""""
pandas/io/stata.py,"def _pad_bytes_new(name: str | bytes, length: int) -> bytes:
    if isinstance(name, str):
        name = bytes(name, 'utf-8')
    return name + b'\x00' * (length - len(name))","""""""Takes a bytes instance and pads it with null bytes until it's length chars."""""""
pandas/io/xml.py,"def get_data_from_filepath(filepath_or_buffer: FilePath | bytes | ReadBuffer[bytes] | ReadBuffer[str], encoding: str | None, compression: CompressionOptions, storage_options: StorageOptions) -> str | bytes | ReadBuffer[bytes] | ReadBuffer[str]:
    if not isinstance(filepath_or_buffer, bytes):
        filepath_or_buffer = stringify_path(filepath_or_buffer)
    if (isinstance(filepath_or_buffer, str) and (not filepath_or_buffer.startswith(('<?xml', '<')))) and (not isinstance(filepath_or_buffer, str) or is_url(filepath_or_buffer) or is_fsspec_url(filepath_or_buffer) or file_exists(filepath_or_buffer)):
        with get_handle(filepath_or_buffer, 'r', encoding=encoding, compression=compression, storage_options=storage_options) as handle_obj:
            filepath_or_buffer = handle_obj.handle.read() if hasattr(handle_obj.handle, 'read') else handle_obj.handle
    return filepath_or_buffer","""""""Extract raw XML data.

The method accepts three input types:
    1. filepath (string-like)
    2. file-like object (e.g. open file object, StringIO)
    3. XML string or bytes

This method turns (1) into (2) to simplify the rest of the processing.
It returns input types (2) and (3) unchanged."""""""
pandas/io/xml.py,"def preprocess_data(data) -> io.StringIO | io.BytesIO:
    if isinstance(data, str):
        data = io.StringIO(data)
    elif isinstance(data, bytes):
        data = io.BytesIO(data)
    return data","""""""Convert extracted raw data.

This method will return underlying data of extracted XML content.
The data either has a `read` attribute (e.g. a file object or a
StringIO/BytesIO) or is a string or bytes that is an XML document."""""""
pandas/io/xml.py,"def _data_to_frame(data, **kwargs) -> DataFrame:
    tags = next(iter(data))
    nodes = [list(d.values()) for d in data]
    try:
        with TextParser(nodes, names=tags, **kwargs) as tp:
            return tp.read()
    except ParserError:
        raise ParserError('XML document may be too complex for import. Try to flatten document and use distinct element and attribute names.')","""""""Convert parsed data to Data Frame.

This method will bind xml dictionary data of keys and values
into named columns of Data Frame using the built-in TextParser
class that build Data Frame and infers specific dtypes."""""""
pandas/io/xml.py,"def _parse(path_or_buffer: FilePath | ReadBuffer[bytes] | ReadBuffer[str], xpath: str, namespaces: dict[str, str] | None, elems_only: bool, attrs_only: bool, names: Sequence[str] | None, dtype: DtypeArg | None, converters: ConvertersArg | None, parse_dates: ParseDatesArg | None, encoding: str | None, parser: XMLParsers, stylesheet: FilePath | ReadBuffer[bytes] | ReadBuffer[str] | None, iterparse: dict[str, list[str]] | None, compression: CompressionOptions, storage_options: StorageOptions, dtype_backend: DtypeBackend | lib.NoDefault=lib.no_default, **kwargs) -> DataFrame:
    p: _EtreeFrameParser | _LxmlFrameParser
    if isinstance(path_or_buffer, str) and (not any([is_file_like(path_or_buffer), file_exists(path_or_buffer), is_url(path_or_buffer), is_fsspec_url(path_or_buffer)])):
        warnings.warn(""Passing literal xml to 'read_xml' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object."", FutureWarning, stacklevel=find_stack_level())
    if parser == 'lxml':
        lxml = import_optional_dependency('lxml.etree', errors='ignore')
        if lxml is not None:
            p = _LxmlFrameParser(path_or_buffer, xpath, namespaces, elems_only, attrs_only, names, dtype, converters, parse_dates, encoding, stylesheet, iterparse, compression, storage_options)
        else:
            raise ImportError('lxml not found, please install or use the etree parser.')
    elif parser == 'etree':
        p = _EtreeFrameParser(path_or_buffer, xpath, namespaces, elems_only, attrs_only, names, dtype, converters, parse_dates, encoding, stylesheet, iterparse, compression, storage_options)
    else:
        raise ValueError('Values for parser can only be lxml or etree.')
    data_dicts = p.parse_data()
    return _data_to_frame(data=data_dicts, dtype=dtype, converters=converters, parse_dates=parse_dates, dtype_backend=dtype_backend, **kwargs)","""""""Call internal parsers.

This method will conditionally call internal parsers:
LxmlFrameParser and/or EtreeParser.

Raises
------
ImportError
    * If lxml is not installed if selected as parser.

ValueError
    * If parser is not lxml or etree."""""""
pandas/io/xml.py,"@doc(storage_options=_shared_docs['storage_options'], decompression_options=_shared_docs['decompression_options'] % 'path_or_buffer')
def read_xml(path_or_buffer: FilePath | ReadBuffer[bytes] | ReadBuffer[str], *, xpath: str='./*', namespaces: dict[str, str] | None=None, elems_only: bool=False, attrs_only: bool=False, names: Sequence[str] | None=None, dtype: DtypeArg | None=None, converters: ConvertersArg | None=None, parse_dates: ParseDatesArg | None=None, encoding: str | None='utf-8', parser: XMLParsers='lxml', stylesheet: FilePath | ReadBuffer[bytes] | ReadBuffer[str] | None=None, iterparse: dict[str, list[str]] | None=None, compression: CompressionOptions='infer', storage_options: StorageOptions | None=None, dtype_backend: DtypeBackend | lib.NoDefault=lib.no_default) -> DataFrame:
    check_dtype_backend(dtype_backend)
    return _parse(path_or_buffer=path_or_buffer, xpath=xpath, namespaces=namespaces, elems_only=elems_only, attrs_only=attrs_only, names=names, dtype=dtype, converters=converters, parse_dates=parse_dates, encoding=encoding, parser=parser, stylesheet=stylesheet, iterparse=iterparse, compression=compression, storage_options=storage_options, dtype_backend=dtype_backend)","""""""Read XML document into a :class:`~pandas.DataFrame` object.

.. versionadded:: 1.3.0

Parameters
----------
path_or_buffer : str, path object, or file-like object
    String, path object (implementing ``os.PathLike[str]``), or file-like
    object implementing a ``read()`` function. The string can be any valid XML
    string or a path. The string can further be a URL. Valid URL schemes
    include http, ftp, s3, and file.

    .. deprecated:: 2.1.0
        Passing xml literal strings is deprecated.
        Wrap literal xml input in ``io.StringIO`` or ``io.BytesIO`` instead.

xpath : str, optional, default './\*'
    The ``XPath`` to parse required set of nodes for migration to
    :class:`~pandas.DataFrame`.``XPath`` should return a collection of elements
    and not a single element. Note: The ``etree`` parser supports limited ``XPath``
    expressions. For more complex ``XPath``, use ``lxml`` which requires
    installation.

namespaces : dict, optional
    The namespaces defined in XML document as dicts with key being
    namespace prefix and value the URI. There is no need to include all
    namespaces in XML, only the ones used in ``xpath`` expression.
    Note: if XML document uses default namespace denoted as
    `xmlns='<URI>'` without a prefix, you must assign any temporary
    namespace prefix such as 'doc' to the URI in order to parse
    underlying nodes and/or attributes. For example, ::

        namespaces = {{""doc"": ""https://example.com""}}

elems_only : bool, optional, default False
    Parse only the child elements at the specified ``xpath``. By default,
    all child elements and non-empty text nodes are returned.

attrs_only :  bool, optional, default False
    Parse only the attributes at the specified ``xpath``.
    By default, all attributes are returned.

names :  list-like, optional
    Column names for DataFrame of parsed XML data. Use this parameter to
    rename original element names and distinguish same named elements and
    attributes.

dtype : Type name or dict of column -> type, optional
    Data type for data or columns. E.g. {{'a': np.float64, 'b': np.int32,
    'c': 'Int64'}}
    Use `str` or `object` together with suitable `na_values` settings
    to preserve and not interpret dtype.
    If converters are specified, they will be applied INSTEAD
    of dtype conversion.

    .. versionadded:: 1.5.0

converters : dict, optional
    Dict of functions for converting values in certain columns. Keys can either
    be integers or column labels.

    .. versionadded:: 1.5.0

parse_dates : bool or list of int or names or list of lists or dict, default False
    Identifiers to parse index or columns to datetime. The behavior is as follows:

    * boolean. If True -> try parsing the index.
    * list of int or names. e.g. If [1, 2, 3] -> try parsing columns 1, 2, 3
      each as a separate date column.
    * list of lists. e.g.  If [[1, 3]] -> combine columns 1 and 3 and parse as
      a single date column.
    * dict, e.g. {{'foo' : [1, 3]}} -> parse columns 1, 3 as date and call
      result 'foo'

    .. versionadded:: 1.5.0

encoding : str, optional, default 'utf-8'
    Encoding of XML document.

parser : {{'lxml','etree'}}, default 'lxml'
    Parser module to use for retrieval of data. Only 'lxml' and
    'etree' are supported. With 'lxml' more complex ``XPath`` searches
    and ability to use XSLT stylesheet are supported.

stylesheet : str, path object or file-like object
    A URL, file-like object, or a raw string containing an XSLT script.
    This stylesheet should flatten complex, deeply nested XML documents
    for easier parsing. To use this feature you must have ``lxml`` module
    installed and specify 'lxml' as ``parser``. The ``xpath`` must
    reference nodes of transformed XML document generated after XSLT
    transformation and not the original XML document. Only XSLT 1.0
    scripts and not later versions is currently supported.

iterparse : dict, optional
    The nodes or attributes to retrieve in iterparsing of XML document
    as a dict with key being the name of repeating element and value being
    list of elements or attribute names that are descendants of the repeated
    element. Note: If this option is used, it will replace ``xpath`` parsing
    and unlike ``xpath``, descendants do not need to relate to each other but can
    exist any where in document under the repeating element. This memory-
    efficient method should be used for very large XML files (500MB, 1GB, or 5GB+).
    For example, ::

        iterparse = {{""row_element"": [""child_elem"", ""attr"", ""grandchild_elem""]}}

    .. versionadded:: 1.5.0

{decompression_options}

    .. versionchanged:: 1.4.0 Zstandard support.

{storage_options}

dtype_backend : {{'numpy_nullable', 'pyarrow'}}, default 'numpy_nullable'
    Back-end data type applied to the resultant :class:`DataFrame`
    (still experimental). Behaviour is as follows:

    * ``""numpy_nullable""``: returns nullable-dtype-backed :class:`DataFrame`
      (default).
    * ``""pyarrow""``: returns pyarrow-backed nullable :class:`ArrowDtype`
      DataFrame.

    .. versionadded:: 2.0

Returns
-------
df
    A DataFrame.

See Also
--------
read_json : Convert a JSON string to pandas object.
read_html : Read HTML tables into a list of DataFrame objects.

Notes
-----
This method is best designed to import shallow XML documents in
following format which is the ideal fit for the two-dimensions of a
``DataFrame`` (row by column). ::

        <root>
            <row>
              <column1>data</column1>
              <column2>data</column2>
              <column3>data</column3>
              ...
           </row>
           <row>
              ...
           </row>
           ...
        </root>

As a file format, XML documents can be designed any way including
layout of elements and attributes as long as it conforms to W3C
specifications. Therefore, this method is a convenience handler for
a specific flatter design and not all possible XML structures.

However, for more complex XML documents, ``stylesheet`` allows you to
temporarily redesign original document with XSLT (a special purpose
language) for a flatter version for migration to a DataFrame.

This function will *always* return a single :class:`DataFrame` or raise
exceptions due to issues with XML document, ``xpath``, or other
parameters.

See the :ref:`read_xml documentation in the IO section of the docs
<io.read_xml>` for more information in using this method to parse XML
files to DataFrames.

Examples
--------
>>> import io
>>> xml = '''<?xml version='1.0' encoding='utf-8'?>
... <data xmlns=""http://example.com"">
...  <row>
...    <shape>square</shape>
...    <degrees>360</degrees>
...    <sides>4.0</sides>
...  </row>
...  <row>
...    <shape>circle</shape>
...    <degrees>360</degrees>
...    <sides/>
...  </row>
...  <row>
...    <shape>triangle</shape>
...    <degrees>180</degrees>
...    <sides>3.0</sides>
...  </row>
... </data>'''

>>> df = pd.read_xml(io.StringIO(xml))
>>> df
      shape  degrees  sides
0    square      360    4.0
1    circle      360    NaN
2  triangle      180    3.0

>>> xml = '''<?xml version='1.0' encoding='utf-8'?>
... <data>
...   <row shape=""square"" degrees=""360"" sides=""4.0""/>
...   <row shape=""circle"" degrees=""360""/>
...   <row shape=""triangle"" degrees=""180"" sides=""3.0""/>
... </data>'''

>>> df = pd.read_xml(io.StringIO(xml), xpath="".//row"")
>>> df
      shape  degrees  sides
0    square      360    4.0
1    circle      360    NaN
2  triangle      180    3.0

>>> xml = '''<?xml version='1.0' encoding='utf-8'?>
... <doc:data xmlns:doc=""https://example.com"">
...   <doc:row>
...     <doc:shape>square</doc:shape>
...     <doc:degrees>360</doc:degrees>
...     <doc:sides>4.0</doc:sides>
...   </doc:row>
...   <doc:row>
...     <doc:shape>circle</doc:shape>
...     <doc:degrees>360</doc:degrees>
...     <doc:sides/>
...   </doc:row>
...   <doc:row>
...     <doc:shape>triangle</doc:shape>
...     <doc:degrees>180</doc:degrees>
...     <doc:sides>3.0</doc:sides>
...   </doc:row>
... </doc:data>'''

>>> df = pd.read_xml(io.StringIO(xml),
...                  xpath=""//doc:row"",
...                  namespaces={{""doc"": ""https://example.com""}})
>>> df
      shape  degrees  sides
0    square      360    4.0
1    circle      360    NaN
2  triangle      180    3.0"""""""
pandas/plotting/_core.py,"def hist_series(self, by=None, ax=None, grid: bool=True, xlabelsize: int | None=None, xrot: float | None=None, ylabelsize: int | None=None, yrot: float | None=None, figsize: tuple[int, int] | None=None, bins: int | Sequence[int]=10, backend: str | None=None, legend: bool=False, **kwargs):
    plot_backend = _get_plot_backend(backend)
    return plot_backend.hist_series(self, by=by, ax=ax, grid=grid, xlabelsize=xlabelsize, xrot=xrot, ylabelsize=ylabelsize, yrot=yrot, figsize=figsize, bins=bins, legend=legend, **kwargs)","""""""Draw histogram of the input series using matplotlib.

Parameters
----------
by : object, optional
    If passed, then used to form histograms for separate groups.
ax : matplotlib axis object
    If not passed, uses gca().
grid : bool, default True
    Whether to show axis grid lines.
xlabelsize : int, default None
    If specified changes the x-axis label size.
xrot : float, default None
    Rotation of x axis labels.
ylabelsize : int, default None
    If specified changes the y-axis label size.
yrot : float, default None
    Rotation of y axis labels.
figsize : tuple, default None
    Figure size in inches by default.
bins : int or sequence, default 10
    Number of histogram bins to be used. If an integer is given, bins + 1
    bin edges are calculated and returned. If bins is a sequence, gives
    bin edges, including left edge of first bin and right edge of last
    bin. In this case, bins is returned unmodified.
backend : str, default None
    Backend to use instead of the backend specified in the option
    ``plotting.backend``. For instance, 'matplotlib'. Alternatively, to
    specify the ``plotting.backend`` for the whole session, set
    ``pd.options.plotting.backend``.
legend : bool, default False
    Whether to show the legend.

**kwargs
    To be passed to the actual plotting function.

Returns
-------
matplotlib.AxesSubplot
    A histogram plot.

See Also
--------
matplotlib.axes.Axes.hist : Plot a histogram using matplotlib.

Examples
--------
For Series:

.. plot::
    :context: close-figs

    >>> lst = ['a', 'a', 'a', 'b', 'b', 'b']
    >>> ser = pd.Series([1, 2, 2, 4, 6, 6], index=lst)
    >>> hist = ser.hist()

For Groupby:

.. plot::
    :context: close-figs

    >>> lst = ['a', 'a', 'a', 'b', 'b', 'b']
    >>> ser = pd.Series([1, 2, 2, 4, 6, 6], index=lst)
    >>> hist = ser.groupby(level=0).hist()"""""""
pandas/plotting/_core.py,"def hist_frame(data: DataFrame, column: IndexLabel | None=None, by=None, grid: bool=True, xlabelsize: int | None=None, xrot: float | None=None, ylabelsize: int | None=None, yrot: float | None=None, ax=None, sharex: bool=False, sharey: bool=False, figsize: tuple[int, int] | None=None, layout: tuple[int, int] | None=None, bins: int | Sequence[int]=10, backend: str | None=None, legend: bool=False, **kwargs):
    plot_backend = _get_plot_backend(backend)
    return plot_backend.hist_frame(data, column=column, by=by, grid=grid, xlabelsize=xlabelsize, xrot=xrot, ylabelsize=ylabelsize, yrot=yrot, ax=ax, sharex=sharex, sharey=sharey, figsize=figsize, layout=layout, legend=legend, bins=bins, **kwargs)","""""""Make a histogram of the DataFrame's columns.

A `histogram`_ is a representation of the distribution of data.
This function calls :meth:`matplotlib.pyplot.hist`, on each series in
the DataFrame, resulting in one histogram per column.

.. _histogram: https://en.wikipedia.org/wiki/Histogram

Parameters
----------
data : DataFrame
    The pandas object holding the data.
column : str or sequence, optional
    If passed, will be used to limit data to a subset of columns.
by : object, optional
    If passed, then used to form histograms for separate groups.
grid : bool, default True
    Whether to show axis grid lines.
xlabelsize : int, default None
    If specified changes the x-axis label size.
xrot : float, default None
    Rotation of x axis labels. For example, a value of 90 displays the
    x labels rotated 90 degrees clockwise.
ylabelsize : int, default None
    If specified changes the y-axis label size.
yrot : float, default None
    Rotation of y axis labels. For example, a value of 90 displays the
    y labels rotated 90 degrees clockwise.
ax : Matplotlib axes object, default None
    The axes to plot the histogram on.
sharex : bool, default True if ax is None else False
    In case subplots=True, share x axis and set some x axis labels to
    invisible; defaults to True if ax is None otherwise False if an ax
    is passed in.
    Note that passing in both an ax and sharex=True will alter all x axis
    labels for all subplots in a figure.
sharey : bool, default False
    In case subplots=True, share y axis and set some y axis labels to
    invisible.
figsize : tuple, optional
    The size in inches of the figure to create. Uses the value in
    `matplotlib.rcParams` by default.
layout : tuple, optional
    Tuple of (rows, columns) for the layout of the histograms.
bins : int or sequence, default 10
    Number of histogram bins to be used. If an integer is given, bins + 1
    bin edges are calculated and returned. If bins is a sequence, gives
    bin edges, including left edge of first bin and right edge of last
    bin. In this case, bins is returned unmodified.

backend : str, default None
    Backend to use instead of the backend specified in the option
    ``plotting.backend``. For instance, 'matplotlib'. Alternatively, to
    specify the ``plotting.backend`` for the whole session, set
    ``pd.options.plotting.backend``.

legend : bool, default False
    Whether to show the legend.

**kwargs
    All other plotting keyword arguments to be passed to
    :meth:`matplotlib.pyplot.hist`.

Returns
-------
matplotlib.AxesSubplot or numpy.ndarray of them

See Also
--------
matplotlib.pyplot.hist : Plot a histogram using matplotlib.

Examples
--------
This example draws a histogram based on the length and width of
some animals, displayed in three bins

.. plot::
    :context: close-figs

    >>> df = pd.DataFrame({
    ...     'length': [1.5, 0.5, 1.2, 0.9, 3],
    ...     'width': [0.7, 0.2, 0.15, 0.2, 1.1]
    ...     }, index=['pig', 'rabbit', 'duck', 'chicken', 'horse'])
    >>> hist = df.hist(bins=3)"""""""
pandas/plotting/_core.py,"def boxplot_frame_groupby(grouped, subplots: bool=True, column=None, fontsize: int | None=None, rot: int=0, grid: bool=True, ax=None, figsize: tuple[float, float] | None=None, layout=None, sharex: bool=False, sharey: bool=True, backend=None, **kwargs):
    plot_backend = _get_plot_backend(backend)
    return plot_backend.boxplot_frame_groupby(grouped, subplots=subplots, column=column, fontsize=fontsize, rot=rot, grid=grid, ax=ax, figsize=figsize, layout=layout, sharex=sharex, sharey=sharey, **kwargs)","""""""Make box plots from DataFrameGroupBy data.

Parameters
----------
grouped : Grouped DataFrame
subplots : bool
    * ``False`` - no subplots will be used
    * ``True`` - create a subplot for each group.

column : column name or list of names, or vector
    Can be any valid input to groupby.
fontsize : float or str
rot : label rotation angle
grid : Setting this to True will show the grid
ax : Matplotlib axis object, default None
figsize : A tuple (width, height) in inches
layout : tuple (optional)
    The layout of the plot: (rows, columns).
sharex : bool, default False
    Whether x-axes will be shared among subplots.
sharey : bool, default True
    Whether y-axes will be shared among subplots.
backend : str, default None
    Backend to use instead of the backend specified in the option
    ``plotting.backend``. For instance, 'matplotlib'. Alternatively, to
    specify the ``plotting.backend`` for the whole session, set
    ``pd.options.plotting.backend``.
**kwargs
    All other plotting keyword arguments to be passed to
    matplotlib's boxplot function.

Returns
-------
dict of key/value = group key/DataFrame.boxplot return value
or DataFrame.boxplot return value in case subplots=figures=False

Examples
--------
You can create boxplots for grouped data and show them as separate subplots:

.. plot::
    :context: close-figs

    >>> import itertools
    >>> tuples = [t for t in itertools.product(range(1000), range(4))]
    >>> index = pd.MultiIndex.from_tuples(tuples, names=['lvl0', 'lvl1'])
    >>> data = np.random.randn(len(index),4)
    >>> df = pd.DataFrame(data, columns=list('ABCD'), index=index)
    >>> grouped = df.groupby(level='lvl1')
    >>> grouped.boxplot(rot=45, fontsize=12, figsize=(8,10))  # doctest: +SKIP

The ``subplots=False`` option shows the boxplots in a single figure.

.. plot::
    :context: close-figs

    >>> grouped.boxplot(subplots=False, rot=45, fontsize=12)  # doctest: +SKIP"""""""
pandas/plotting/_core.py,"def _load_backend(backend: str) -> types.ModuleType:
    from importlib.metadata import entry_points
    if backend == 'matplotlib':
        try:
            module = importlib.import_module('pandas.plotting._matplotlib')
        except ImportError:
            raise ImportError('matplotlib is required for plotting when the default backend ""matplotlib"" is selected.') from None
        return module
    found_backend = False
    eps = entry_points()
    key = 'pandas_plotting_backends'
    if hasattr(eps, 'select'):
        entry = eps.select(group=key)
    else:
        entry = eps.get(key, ())
    for entry_point in entry:
        found_backend = entry_point.name == backend
        if found_backend:
            module = entry_point.load()
            break
    if not found_backend:
        try:
            module = importlib.import_module(backend)
            found_backend = True
        except ImportError:
            pass
    if found_backend:
        if hasattr(module, 'plot'):
            return module
    raise ValueError(f""Could not find plotting backend '{backend}'. Ensure that you've installed the package providing the '{backend}' entrypoint, or that the package has a top-level `.plot` method."")","""""""Load a pandas plotting backend.

Parameters
----------
backend : str
    The identifier for the backend. Either an entrypoint item registered
    with importlib.metadata, ""matplotlib"", or a module name.

Returns
-------
types.ModuleType
    The imported backend."""""""
pandas/plotting/_core.py,"def _get_plot_backend(backend: str | None=None):
    backend_str: str = backend or get_option('plotting.backend')
    if backend_str in _backends:
        return _backends[backend_str]
    module = _load_backend(backend_str)
    _backends[backend_str] = module
    return module","""""""Return the plotting backend to use (e.g. `pandas.plotting._matplotlib`).

The plotting system of pandas uses matplotlib by default, but the idea here
is that it can also work with other third-party backends. This function
returns the module which provides a top-level `.plot` method that will
actually do the plotting. The backend is specified from a string, which
either comes from the keyword argument `backend`, or, if not specified, from
the option `pandas.options.plotting.backend`. All the rest of the code in
this file uses the backend specified there for the plotting.

The backend is imported lazily, as matplotlib is a soft dependency, and
pandas can be used without it being installed.

Notes
-----
Modifies `_backends` with imported backend as a side effect."""""""
pandas/plotting/_matplotlib/boxplot.py,"def _set_ticklabels(ax: Axes, labels: list[str], is_vertical: bool, **kwargs) -> None:
    ticks = ax.get_xticks() if is_vertical else ax.get_yticks()
    if len(ticks) != len(labels):
        (i, remainder) = divmod(len(ticks), len(labels))
        assert remainder == 0, remainder
        labels *= i
    if is_vertical:
        ax.set_xticklabels(labels, **kwargs)
    else:
        ax.set_yticklabels(labels, **kwargs)","""""""Set the tick labels of a given axis.

Due to https://github.com/matplotlib/matplotlib/pull/17266, we need to handle the
case of repeated ticks (due to `FixedLocator`) and thus we duplicate the number of
labels."""""""
pandas/plotting/_matplotlib/converter.py,"def register_pandas_matplotlib_converters(func: F) -> F:

    @functools.wraps(func)
    def wrapper(*args, **kwargs):
        with pandas_converters():
            return func(*args, **kwargs)
    return cast(F, wrapper)","""""""Decorator applying pandas_converters."""""""
pandas/plotting/_matplotlib/converter.py,"@contextlib.contextmanager
def pandas_converters() -> Generator[None, None, None]:
    value = get_option('plotting.matplotlib.register_converters')
    if value:
        register()
    try:
        yield
    finally:
        if value == 'auto':
            deregister()","""""""Context manager registering pandas' converters for a plot.

See Also
--------
register_pandas_matplotlib_converters : Decorator that applies this."""""""
pandas/plotting/_matplotlib/converter.py,"def _get_default_annual_spacing(nyears) -> tuple[int, int]:
    if nyears < 11:
        (min_spacing, maj_spacing) = (1, 1)
    elif nyears < 20:
        (min_spacing, maj_spacing) = (1, 2)
    elif nyears < 50:
        (min_spacing, maj_spacing) = (1, 5)
    elif nyears < 100:
        (min_spacing, maj_spacing) = (5, 10)
    elif nyears < 200:
        (min_spacing, maj_spacing) = (5, 25)
    elif nyears < 600:
        (min_spacing, maj_spacing) = (10, 50)
    else:
        factor = nyears // 1000 + 1
        (min_spacing, maj_spacing) = (factor * 20, factor * 100)
    return (min_spacing, maj_spacing)","""""""Returns a default spacing between consecutive ticks for annual data."""""""
pandas/plotting/_matplotlib/converter.py,"def _period_break(dates: PeriodIndex, period: str) -> npt.NDArray[np.intp]:
    mask = _period_break_mask(dates, period)
    return np.nonzero(mask)[0]","""""""Returns the indices where the given period changes.

Parameters
----------
dates : PeriodIndex
    Array of intervals to monitor.
period : str
    Name of the period to monitor."""""""
pandas/plotting/_matplotlib/converter.py,"def has_level_label(label_flags: npt.NDArray[np.intp], vmin: float) -> bool:
    if label_flags.size == 0 or (label_flags.size == 1 and label_flags[0] == 0 and (vmin % 1 > 0.0)):
        return False
    else:
        return True","""""""Returns true if the ``label_flags`` indicate there is at least one label
for this level.

if the minimum view limit is not an exact integer, then the first tick
label won't be shown, so we must adjust for that."""""""
pandas/plotting/_matplotlib/core.py,"def _color_in_style(style: str) -> bool:
    from matplotlib.colors import BASE_COLORS
    return not set(BASE_COLORS).isdisjoint(style)","""""""Check if there is a color letter in the style string."""""""
pandas/plotting/_matplotlib/groupby.py,"def create_iter_data_given_by(data: DataFrame, kind: str='hist') -> dict[str, DataFrame | Series]:
    if kind == 'hist':
        level = 0
    else:
        level = 1
    assert isinstance(data.columns, MultiIndex)
    return {col: data.loc[:, data.columns.get_level_values(level) == col] for col in data.columns.levels[level]}","""""""Create data for iteration given `by` is assigned or not, and it is only
used in both hist and boxplot.

If `by` is assigned, return a dictionary of DataFrames in which the key of
dictionary is the values in groups.
If `by` is not assigned, return input as is, and this preserves current
status of iter_data.

Parameters
----------
data : reformatted grouped data from `_compute_plot_data` method.
kind : str, plot kind. This function is only used for `hist` and `box` plots.

Returns
-------
iter_data : DataFrame or Dictionary of DataFrames

Examples
--------
If `by` is assigned:

>>> import numpy as np
>>> tuples = [('h1', 'a'), ('h1', 'b'), ('h2', 'a'), ('h2', 'b')]
>>> mi = MultiIndex.from_tuples(tuples)
>>> value = [[1, 3, np.nan, np.nan],
...          [3, 4, np.nan, np.nan], [np.nan, np.nan, 5, 6]]
>>> data = DataFrame(value, columns=mi)
>>> create_iter_data_given_by(data)
{'h1':     h1
     a    b
0  1.0  3.0
1  3.0  4.0
2  NaN  NaN, 'h2':     h2
     a    b
0  NaN  NaN
1  NaN  NaN
2  5.0  6.0}"""""""
pandas/plotting/_matplotlib/groupby.py,"def reconstruct_data_with_by(data: DataFrame, by: IndexLabel, cols: IndexLabel) -> DataFrame:
    by_modified = unpack_single_str_list(by)
    grouped = data.groupby(by_modified)
    data_list = []
    for (key, group) in grouped:
        columns = MultiIndex.from_product([[key], cols])
        sub_group = group[cols]
        sub_group.columns = columns
        data_list.append(sub_group)
    data = concat(data_list, axis=1)
    return data","""""""Internal function to group data, and reassign multiindex column names onto the
result in order to let grouped data be used in _compute_plot_data method.

Parameters
----------
data : Original DataFrame to plot
by : grouped `by` parameter selected by users
cols : columns of data set (excluding columns used in `by`)

Returns
-------
Output is the reconstructed DataFrame with MultiIndex columns. The first level
of MI is unique values of groups, and second level of MI is the columns
selected by users.

Examples
--------
>>> d = {'h': ['h1', 'h1', 'h2'], 'a': [1, 3, 5], 'b': [3, 4, 6]}
>>> df = DataFrame(d)
>>> reconstruct_data_with_by(df, by='h', cols=['a', 'b'])
   h1      h2
   a     b     a     b
0  1.0   3.0   NaN   NaN
1  3.0   4.0   NaN   NaN
2  NaN   NaN   5.0   6.0"""""""
pandas/plotting/_matplotlib/groupby.py,"def reformat_hist_y_given_by(y: Series | np.ndarray, by: IndexLabel | None) -> Series | np.ndarray:
    if by is not None and len(y.shape) > 1:
        return np.array([remove_na_arraylike(col) for col in y.T]).T
    return remove_na_arraylike(y)","""""""Internal function to reformat y given `by` is applied or not for hist plot.

If by is None, input y is 1-d with NaN removed; and if by is not None, groupby
will take place and input y is multi-dimensional array."""""""
pandas/plotting/_matplotlib/hist.py,"def _grouped_hist(data, column=None, by=None, ax=None, bins: int=50, figsize: tuple[float, float] | None=None, layout=None, sharex: bool=False, sharey: bool=False, rot: float=90, grid: bool=True, xlabelsize: int | None=None, xrot=None, ylabelsize: int | None=None, yrot=None, legend: bool=False, **kwargs):
    if legend:
        assert 'label' not in kwargs
        if data.ndim == 1:
            kwargs['label'] = data.name
        elif column is None:
            kwargs['label'] = data.columns
        else:
            kwargs['label'] = column

    def plot_group(group, ax) -> None:
        ax.hist(group.dropna().values, bins=bins, **kwargs)
        if legend:
            ax.legend()
    if xrot is None:
        xrot = rot
    (fig, axes) = _grouped_plot(plot_group, data, column=column, by=by, sharex=sharex, sharey=sharey, ax=ax, figsize=figsize, layout=layout, rot=rot)
    set_ticks_props(axes, xlabelsize=xlabelsize, xrot=xrot, ylabelsize=ylabelsize, yrot=yrot)
    maybe_adjust_figure(fig, bottom=0.15, top=0.9, left=0.1, right=0.9, hspace=0.5, wspace=0.3)
    return axes","""""""Grouped histogram

Parameters
----------
data : Series/DataFrame
column : object, optional
by : object, optional
ax : axes, optional
bins : int, default 50
figsize : tuple, optional
layout : optional
sharex : bool, default False
sharey : bool, default False
rot : float, default 90
grid : bool, default True
legend: : bool, default False
kwargs : dict, keyword arguments passed to matplotlib.Axes.hist

Returns
-------
collection of Matplotlib Axes"""""""
pandas/plotting/_matplotlib/style.py,"def get_standard_colors(num_colors: int, colormap: Colormap | None=None, color_type: str='default', color: dict[str, Color] | Color | Collection[Color] | None=None):
    if isinstance(color, dict):
        return color
    colors = _derive_colors(color=color, colormap=colormap, color_type=color_type, num_colors=num_colors)
    return list(_cycle_colors(colors, num_colors=num_colors))","""""""Get standard colors based on `colormap`, `color_type` or `color` inputs.

Parameters
----------
num_colors : int
    Minimum number of colors to be returned.
    Ignored if `color` is a dictionary.
colormap : :py:class:`matplotlib.colors.Colormap`, optional
    Matplotlib colormap.
    When provided, the resulting colors will be derived from the colormap.
color_type : {""default"", ""random""}, optional
    Type of colors to derive. Used if provided `color` and `colormap` are None.
    Ignored if either `color` or `colormap` are not None.
color : dict or str or sequence, optional
    Color(s) to be used for deriving sequence of colors.
    Can be either be a dictionary, or a single color (single color string,
    or sequence of floats representing a single color),
    or a sequence of colors.

Returns
-------
dict or list
    Standard colors. Can either be a mapping if `color` was a dictionary,
    or a list of colors with a length of `num_colors` or more.

Warns
-----
UserWarning
    If both `colormap` and `color` are provided.
    Parameter `color` will override."""""""
pandas/plotting/_matplotlib/style.py,"def _derive_colors(*, color: Color | Collection[Color] | None, colormap: str | Colormap | None, color_type: str, num_colors: int) -> list[Color]:
    if color is None and colormap is not None:
        return _get_colors_from_colormap(colormap, num_colors=num_colors)
    elif color is not None:
        if colormap is not None:
            warnings.warn(""'color' and 'colormap' cannot be used simultaneously. Using 'color'"", stacklevel=find_stack_level())
        return _get_colors_from_color(color)
    else:
        return _get_colors_from_color_type(color_type, num_colors=num_colors)","""""""Derive colors from either `colormap`, `color_type` or `color` inputs.

Get a list of colors either from `colormap`, or from `color`,
or from `color_type` (if both `colormap` and `color` are None).

Parameters
----------
color : str or sequence, optional
    Color(s) to be used for deriving sequence of colors.
    Can be either be a single color (single color string, or sequence of floats
    representing a single color), or a sequence of colors.
colormap : :py:class:`matplotlib.colors.Colormap`, optional
    Matplotlib colormap.
    When provided, the resulting colors will be derived from the colormap.
color_type : {""default"", ""random""}, optional
    Type of colors to derive. Used if provided `color` and `colormap` are None.
    Ignored if either `color` or `colormap`` are not None.
num_colors : int
    Number of colors to be extracted.

Returns
-------
list
    List of colors extracted.

Warns
-----
UserWarning
    If both `colormap` and `color` are provided.
    Parameter `color` will override."""""""
pandas/plotting/_matplotlib/style.py,"def _cycle_colors(colors: list[Color], num_colors: int) -> Iterator[Color]:
    max_colors = max(num_colors, len(colors))
    yield from itertools.islice(itertools.cycle(colors), max_colors)","""""""Cycle colors until achieving max of `num_colors` or length of `colors`.

Extra colors will be ignored by matplotlib if there are more colors
than needed and nothing needs to be done here."""""""
pandas/plotting/_matplotlib/style.py,"def _get_colors_from_colormap(colormap: str | Colormap, num_colors: int) -> list[Color]:
    cmap = _get_cmap_instance(colormap)
    return [cmap(num) for num in np.linspace(0, 1, num=num_colors)]","""""""Get colors from colormap."""""""
pandas/plotting/_matplotlib/style.py,"def _get_cmap_instance(colormap: str | Colormap) -> Colormap:
    if isinstance(colormap, str):
        cmap = colormap
        colormap = mpl.colormaps[colormap]
        if colormap is None:
            raise ValueError(f'Colormap {cmap} is not recognized')
    return colormap","""""""Get instance of matplotlib colormap."""""""
pandas/plotting/_matplotlib/style.py,"def _get_colors_from_color(color: Color | Collection[Color]) -> list[Color]:
    if len(color) == 0:
        raise ValueError(f'Invalid color argument: {color}')
    if _is_single_color(color):
        color = cast(Color, color)
        return [color]
    color = cast(Collection[Color], color)
    return list(_gen_list_of_colors_from_iterable(color))","""""""Get colors from user input color."""""""
pandas/plotting/_matplotlib/style.py,"def _is_single_color(color: Color | Collection[Color]) -> bool:
    if isinstance(color, str) and _is_single_string_color(color):
        return True
    if _is_floats_color(color):
        return True
    return False","""""""Check if `color` is a single color, not a sequence of colors.

Single color is of these kinds:
    - Named color ""red"", ""C0"", ""firebrick""
    - Alias ""g""
    - Sequence of floats, such as (0.1, 0.2, 0.3) or (0.1, 0.2, 0.3, 0.4).

See Also
--------
_is_single_string_color"""""""
pandas/plotting/_matplotlib/style.py,"def _gen_list_of_colors_from_iterable(color: Collection[Color]) -> Iterator[Color]:
    for x in color:
        if _is_single_color(x):
            yield x
        else:
            raise ValueError(f'Invalid color {x}')","""""""Yield colors from string of several letters or from collection of colors."""""""
pandas/plotting/_matplotlib/style.py,"def _is_floats_color(color: Color | Collection[Color]) -> bool:
    return bool(is_list_like(color) and (len(color) == 3 or len(color) == 4) and all((isinstance(x, (int, float)) for x in color)))","""""""Check if color comprises a sequence of floats representing color."""""""
pandas/plotting/_matplotlib/style.py,"def _get_colors_from_color_type(color_type: str, num_colors: int) -> list[Color]:
    if color_type == 'default':
        return _get_default_colors(num_colors)
    elif color_type == 'random':
        return _get_random_colors(num_colors)
    else:
        raise ValueError(""color_type must be either 'default' or 'random'"")","""""""Get colors from user input color type."""""""
pandas/plotting/_matplotlib/style.py,"def _get_default_colors(num_colors: int) -> list[Color]:
    import matplotlib.pyplot as plt
    colors = [c['color'] for c in plt.rcParams['axes.prop_cycle']]
    return colors[0:num_colors]","""""""Get `num_colors` of default colors from matplotlib rc params."""""""
pandas/plotting/_matplotlib/style.py,"def _get_random_colors(num_colors: int) -> list[Color]:
    return [_random_color(num) for num in range(num_colors)]","""""""Get `num_colors` of random colors."""""""
pandas/plotting/_matplotlib/style.py,"def _random_color(column: int) -> list[float]:
    rs = com.random_state(column)
    return rs.rand(3).tolist()","""""""Get a random color represented as a list of length 3"""""""
pandas/plotting/_matplotlib/style.py,"def _is_single_string_color(color: Color) -> bool:
    conv = matplotlib.colors.ColorConverter()
    try:
        conv.to_rgba(color)
    except ValueError:
        return False
    else:
        return True","""""""Check if `color` is a single string color.

Examples of single string colors:
    - 'r'
    - 'g'
    - 'red'
    - 'green'
    - 'C3'
    - 'firebrick'

Parameters
----------
color : Color
    Color string or sequence of floats.

Returns
-------
bool
    True if `color` looks like a valid color.
    False otherwise."""""""
pandas/plotting/_matplotlib/timeseries.py,"def decorate_axes(ax: Axes, freq, kwargs) -> None:
    if not hasattr(ax, '_plot_data'):
        ax._plot_data = []
    ax.freq = freq
    xaxis = ax.get_xaxis()
    xaxis.freq = freq
    if not hasattr(ax, 'legendlabels'):
        ax.legendlabels = [kwargs.get('label', None)]
    else:
        ax.legendlabels.append(kwargs.get('label', None))
    ax.view_interval = None
    ax.date_axis_info = None","""""""Initialize axes for time-series plotting"""""""
pandas/plotting/_matplotlib/timeseries.py,"def _get_ax_freq(ax: Axes):
    ax_freq = getattr(ax, 'freq', None)
    if ax_freq is None:
        if hasattr(ax, 'left_ax'):
            ax_freq = getattr(ax.left_ax, 'freq', None)
        elif hasattr(ax, 'right_ax'):
            ax_freq = getattr(ax.right_ax, 'freq', None)
    if ax_freq is None:
        shared_axes = ax.get_shared_x_axes().get_siblings(ax)
        if len(shared_axes) > 1:
            for shared_ax in shared_axes:
                ax_freq = getattr(shared_ax, 'freq', None)
                if ax_freq is not None:
                    break
    return ax_freq","""""""Get the freq attribute of the ax object if set.
Also checks shared axes (eg when using secondary yaxis, sharex=True
or twinx)"""""""
pandas/plotting/_matplotlib/timeseries.py,"def format_dateaxis(subplot, freq, index) -> None:
    from matplotlib import pylab
    if isinstance(index, ABCPeriodIndex):
        majlocator = TimeSeries_DateLocator(freq, dynamic_mode=True, minor_locator=False, plot_obj=subplot)
        minlocator = TimeSeries_DateLocator(freq, dynamic_mode=True, minor_locator=True, plot_obj=subplot)
        subplot.xaxis.set_major_locator(majlocator)
        subplot.xaxis.set_minor_locator(minlocator)
        majformatter = TimeSeries_DateFormatter(freq, dynamic_mode=True, minor_locator=False, plot_obj=subplot)
        minformatter = TimeSeries_DateFormatter(freq, dynamic_mode=True, minor_locator=True, plot_obj=subplot)
        subplot.xaxis.set_major_formatter(majformatter)
        subplot.xaxis.set_minor_formatter(minformatter)
        subplot.format_coord = functools.partial(_format_coord, freq)
    elif isinstance(index, ABCTimedeltaIndex):
        subplot.xaxis.set_major_formatter(TimeSeries_TimedeltaFormatter())
    else:
        raise TypeError('index type not supported')
    pylab.draw_if_interactive()","""""""Pretty-formats the date axis (x-axis).

Major and minor ticks are automatically set for the frequency of the
current underlying series.  As the dynamic mode is activated by
default, changing the limits of the x axis will intelligently change
the positions of the ticks."""""""
pandas/plotting/_matplotlib/tools.py,"def do_adjust_figure(fig: Figure) -> bool:
    if not hasattr(fig, 'get_constrained_layout'):
        return False
    return not fig.get_constrained_layout()","""""""Whether fig has constrained_layout enabled."""""""
pandas/plotting/_matplotlib/tools.py,"def maybe_adjust_figure(fig: Figure, *args, **kwargs) -> None:
    if do_adjust_figure(fig):
        fig.subplots_adjust(*args, **kwargs)","""""""Call fig.subplots_adjust unless fig has constrained_layout enabled."""""""
pandas/plotting/_matplotlib/tools.py,"def create_subplots(naxes: int, sharex: bool=False, sharey: bool=False, squeeze: bool=True, subplot_kw=None, ax=None, layout=None, layout_type: str='box', **fig_kw):
    import matplotlib.pyplot as plt
    if subplot_kw is None:
        subplot_kw = {}
    if ax is None:
        fig = plt.figure(**fig_kw)
    else:
        if is_list_like(ax):
            if squeeze:
                ax = flatten_axes(ax)
            if layout is not None:
                warnings.warn('When passing multiple axes, layout keyword is ignored.', UserWarning, stacklevel=find_stack_level())
            if sharex or sharey:
                warnings.warn('When passing multiple axes, sharex and sharey are ignored. These settings must be specified when creating axes.', UserWarning, stacklevel=find_stack_level())
            if ax.size == naxes:
                fig = ax.flat[0].get_figure()
                return (fig, ax)
            else:
                raise ValueError(f'The number of passed axes must be {naxes}, the same as the output plot')
        fig = ax.get_figure()
        if naxes == 1:
            if squeeze:
                return (fig, ax)
            else:
                return (fig, flatten_axes(ax))
        else:
            warnings.warn('To output multiple subplots, the figure containing the passed axes is being cleared.', UserWarning, stacklevel=find_stack_level())
            fig.clear()
    (nrows, ncols) = _get_layout(naxes, layout=layout, layout_type=layout_type)
    nplots = nrows * ncols
    axarr = np.empty(nplots, dtype=object)
    ax0 = fig.add_subplot(nrows, ncols, 1, **subplot_kw)
    if sharex:
        subplot_kw['sharex'] = ax0
    if sharey:
        subplot_kw['sharey'] = ax0
    axarr[0] = ax0
    for i in range(1, nplots):
        kwds = subplot_kw.copy()
        if i >= naxes:
            kwds['sharex'] = None
            kwds['sharey'] = None
        ax = fig.add_subplot(nrows, ncols, i + 1, **kwds)
        axarr[i] = ax
    if naxes != nplots:
        for ax in axarr[naxes:]:
            ax.set_visible(False)
    handle_shared_axes(axarr, nplots, naxes, nrows, ncols, sharex, sharey)
    if squeeze:
        if nplots == 1:
            axes = axarr[0]
        else:
            axes = axarr.reshape(nrows, ncols).squeeze()
    else:
        axes = axarr.reshape(nrows, ncols)
    return (fig, axes)","""""""Create a figure with a set of subplots already made.

This utility wrapper makes it convenient to create common layouts of
subplots, including the enclosing figure object, in a single call.

Parameters
----------
naxes : int
  Number of required axes. Exceeded axes are set invisible. Default is
  nrows * ncols.

sharex : bool
  If True, the X axis will be shared amongst all subplots.

sharey : bool
  If True, the Y axis will be shared amongst all subplots.

squeeze : bool

  If True, extra dimensions are squeezed out from the returned axis object:
    - if only one subplot is constructed (nrows=ncols=1), the resulting
    single Axis object is returned as a scalar.
    - for Nx1 or 1xN subplots, the returned object is a 1-d numpy object
    array of Axis objects are returned as numpy 1-d arrays.
    - for NxM subplots with N>1 and M>1 are returned as a 2d array.

  If False, no squeezing is done: the returned axis object is always
  a 2-d array containing Axis instances, even if it ends up being 1x1.

subplot_kw : dict
  Dict with keywords passed to the add_subplot() call used to create each
  subplots.

ax : Matplotlib axis object, optional

layout : tuple
  Number of rows and columns of the subplot grid.
  If not specified, calculated from naxes and layout_type

layout_type : {'box', 'horizontal', 'vertical'}, default 'box'
  Specify how to layout the subplot grid.

fig_kw : Other keyword arguments to be passed to the figure() call.
    Note that all keywords not recognized above will be
    automatically included here.

Returns
-------
fig, ax : tuple
  - fig is the Matplotlib Figure object
  - ax can be either a single axis object or an array of axis objects if
  more than one subplot was created.  The dimensions of the resulting array
  can be controlled with the squeeze keyword, see above.

Examples
--------
x = np.linspace(0, 2*np.pi, 400)
y = np.sin(x**2)

# Just a figure and one subplot
f, ax = plt.subplots()
ax.plot(x, y)
ax.set_title('Simple plot')

# Two subplots, unpack the output array immediately
f, (ax1, ax2) = plt.subplots(1, 2, sharey=True)
ax1.plot(x, y)
ax1.set_title('Sharing Y axis')
ax2.scatter(x, y)

# Four polar axes
plt.subplots(2, 2, subplot_kw=dict(polar=True))"""""""
pandas/plotting/_matplotlib/tools.py,"def _has_externally_shared_axis(ax1: Axes, compare_axis: str) -> bool:
    if compare_axis == 'x':
        axes = ax1.get_shared_x_axes()
    elif compare_axis == 'y':
        axes = ax1.get_shared_y_axes()
    else:
        raise ValueError(""_has_externally_shared_axis() needs 'x' or 'y' as a second parameter"")
    axes = axes.get_siblings(ax1)
    ax1_points = ax1.get_position().get_points()
    for ax2 in axes:
        if not np.array_equal(ax1_points, ax2.get_position().get_points()):
            return True
    return False","""""""Return whether an axis is externally shared.

Parameters
----------
ax1 : matplotlib.axes.Axes
    Axis to query.
compare_axis : str
    `""x""` or `""y""` according to whether the X-axis or Y-axis is being
    compared.

Returns
-------
bool
    `True` if the axis is externally shared. Otherwise `False`.

Notes
-----
If two axes with different positions are sharing an axis, they can be
referred to as *externally* sharing the common axis.

If two axes sharing an axis also have the same position, they can be
referred to as *internally* sharing the common axis (a.k.a twinning).

_handle_shared_axes() is only interested in axes externally sharing an
axis, regardless of whether either of the axes is also internally sharing
with a third axis."""""""
pandas/plotting/_misc.py,"def table(ax: Axes, data: DataFrame | Series, **kwargs) -> Table:
    plot_backend = _get_plot_backend('matplotlib')
    return plot_backend.table(ax=ax, data=data, rowLabels=None, colLabels=None, **kwargs)","""""""Helper function to convert DataFrame and Series to matplotlib.table.

Parameters
----------
ax : Matplotlib axes object
data : DataFrame or Series
    Data for table contents.
**kwargs
    Keyword arguments to be passed to matplotlib.table.table.
    If `rowLabels` or `colLabels` is not specified, data index or column
    name will be used.

Returns
-------
matplotlib table object

Examples
--------

.. plot::
        :context: close-figs

        >>> import matplotlib.pyplot as plt
        >>> df = pd.DataFrame({'A': [1, 2], 'B': [3, 4]})
        >>> fix, ax = plt.subplots()
        >>> ax.axis('off')
        (0.0, 1.0, 0.0, 1.0)
        >>> table = pd.plotting.table(ax, df, loc='center',
        ...                           cellLoc='center', colWidths=list([.2, .2]))"""""""
pandas/plotting/_misc.py,"def register() -> None:
    plot_backend = _get_plot_backend('matplotlib')
    plot_backend.register()","""""""Register pandas formatters and converters with matplotlib.

This function modifies the global ``matplotlib.units.registry``
dictionary. pandas adds custom converters for

* pd.Timestamp
* pd.Period
* np.datetime64
* datetime.datetime
* datetime.date
* datetime.time

See Also
--------
deregister_matplotlib_converters : Remove pandas formatters and converters.

Examples
--------
.. plot::
   :context: close-figs

    The following line is done automatically by pandas so
    the plot can be rendered:

    >>> pd.plotting.register_matplotlib_converters()

    >>> df = pd.DataFrame({'ts': pd.period_range('2020', periods=2, freq='M'),
    ...                    'y': [1, 2]
    ...                    })
    >>> plot = df.plot.line(x='ts', y='y')

Unsetting the register manually an error will be raised:

>>> pd.set_option(""plotting.matplotlib.register_converters"",
...               False)  # doctest: +SKIP
>>> df.plot.line(x='ts', y='y')  # doctest: +SKIP
Traceback (most recent call last):
TypeError: float() argument must be a string or a real number, not 'Period'"""""""
pandas/plotting/_misc.py,"def deregister() -> None:
    plot_backend = _get_plot_backend('matplotlib')
    plot_backend.deregister()","""""""Remove pandas formatters and converters.

Removes the custom converters added by :func:`register`. This
attempts to set the state of the registry back to the state before
pandas registered its own units. Converters for pandas' own types like
Timestamp and Period are removed completely. Converters for types
pandas overwrites, like ``datetime.datetime``, are restored to their
original value.

See Also
--------
register_matplotlib_converters : Register pandas formatters and converters
    with matplotlib.

Examples
--------
.. plot::
   :context: close-figs

    The following line is done automatically by pandas so
    the plot can be rendered:

    >>> pd.plotting.register_matplotlib_converters()

    >>> df = pd.DataFrame({'ts': pd.period_range('2020', periods=2, freq='M'),
    ...                    'y': [1, 2]
    ...                    })
    >>> plot = df.plot.line(x='ts', y='y')

Unsetting the register manually an error will be raised:

>>> pd.set_option(""plotting.matplotlib.register_converters"",
...               False)  # doctest: +SKIP
>>> df.plot.line(x='ts', y='y')  # doctest: +SKIP
Traceback (most recent call last):
TypeError: float() argument must be a string or a real number, not 'Period'"""""""
pandas/plotting/_misc.py,"def scatter_matrix(frame: DataFrame, alpha: float=0.5, figsize: tuple[float, float] | None=None, ax: Axes | None=None, grid: bool=False, diagonal: str='hist', marker: str='.', density_kwds: Mapping[str, Any] | None=None, hist_kwds: Mapping[str, Any] | None=None, range_padding: float=0.05, **kwargs) -> np.ndarray:
    plot_backend = _get_plot_backend('matplotlib')
    return plot_backend.scatter_matrix(frame=frame, alpha=alpha, figsize=figsize, ax=ax, grid=grid, diagonal=diagonal, marker=marker, density_kwds=density_kwds, hist_kwds=hist_kwds, range_padding=range_padding, **kwargs)","""""""Draw a matrix of scatter plots.

Parameters
----------
frame : DataFrame
alpha : float, optional
    Amount of transparency applied.
figsize : (float,float), optional
    A tuple (width, height) in inches.
ax : Matplotlib axis object, optional
grid : bool, optional
    Setting this to True will show the grid.
diagonal : {'hist', 'kde'}
    Pick between 'kde' and 'hist' for either Kernel Density Estimation or
    Histogram plot in the diagonal.
marker : str, optional
    Matplotlib marker type, default '.'.
density_kwds : keywords
    Keyword arguments to be passed to kernel density estimate plot.
hist_kwds : keywords
    Keyword arguments to be passed to hist function.
range_padding : float, default 0.05
    Relative extension of axis range in x and y with respect to
    (x_max - x_min) or (y_max - y_min).
**kwargs
    Keyword arguments to be passed to scatter function.

Returns
-------
numpy.ndarray
    A matrix of scatter plots.

Examples
--------

.. plot::
    :context: close-figs

    >>> df = pd.DataFrame(np.random.randn(1000, 4), columns=['A','B','C','D'])
    >>> pd.plotting.scatter_matrix(df, alpha=0.2)
    array([[<Axes: xlabel='A', ylabel='A'>, <Axes: xlabel='B', ylabel='A'>,
            <Axes: xlabel='C', ylabel='A'>, <Axes: xlabel='D', ylabel='A'>],
           [<Axes: xlabel='A', ylabel='B'>, <Axes: xlabel='B', ylabel='B'>,
            <Axes: xlabel='C', ylabel='B'>, <Axes: xlabel='D', ylabel='B'>],
           [<Axes: xlabel='A', ylabel='C'>, <Axes: xlabel='B', ylabel='C'>,
            <Axes: xlabel='C', ylabel='C'>, <Axes: xlabel='D', ylabel='C'>],
           [<Axes: xlabel='A', ylabel='D'>, <Axes: xlabel='B', ylabel='D'>,
            <Axes: xlabel='C', ylabel='D'>, <Axes: xlabel='D', ylabel='D'>]],
          dtype=object)"""""""
pandas/plotting/_misc.py,"def radviz(frame: DataFrame, class_column: str, ax: Axes | None=None, color: list[str] | tuple[str, ...] | None=None, colormap: Colormap | str | None=None, **kwds) -> Axes:
    plot_backend = _get_plot_backend('matplotlib')
    return plot_backend.radviz(frame=frame, class_column=class_column, ax=ax, color=color, colormap=colormap, **kwds)","""""""Plot a multidimensional dataset in 2D.

Each Series in the DataFrame is represented as a evenly distributed
slice on a circle. Each data point is rendered in the circle according to
the value on each Series. Highly correlated `Series` in the `DataFrame`
are placed closer on the unit circle.

RadViz allow to project a N-dimensional data set into a 2D space where the
influence of each dimension can be interpreted as a balance between the
influence of all dimensions.

More info available at the `original article
<https://doi.org/10.1145/331770.331775>`_
describing RadViz.

Parameters
----------
frame : `DataFrame`
    Object holding the data.
class_column : str
    Column name containing the name of the data point category.
ax : :class:`matplotlib.axes.Axes`, optional
    A plot instance to which to add the information.
color : list[str] or tuple[str], optional
    Assign a color to each category. Example: ['blue', 'green'].
colormap : str or :class:`matplotlib.colors.Colormap`, default None
    Colormap to select colors from. If string, load colormap with that
    name from matplotlib.
**kwds
    Options to pass to matplotlib scatter plotting method.

Returns
-------
:class:`matplotlib.axes.Axes`

See Also
--------
pandas.plotting.andrews_curves : Plot clustering visualization.

Examples
--------

.. plot::
    :context: close-figs

    >>> df = pd.DataFrame(
    ...     {
    ...         'SepalLength': [6.5, 7.7, 5.1, 5.8, 7.6, 5.0, 5.4, 4.6, 6.7, 4.6],
    ...         'SepalWidth': [3.0, 3.8, 3.8, 2.7, 3.0, 2.3, 3.0, 3.2, 3.3, 3.6],
    ...         'PetalLength': [5.5, 6.7, 1.9, 5.1, 6.6, 3.3, 4.5, 1.4, 5.7, 1.0],
    ...         'PetalWidth': [1.8, 2.2, 0.4, 1.9, 2.1, 1.0, 1.5, 0.2, 2.1, 0.2],
    ...         'Category': [
    ...             'virginica',
    ...             'virginica',
    ...             'setosa',
    ...             'virginica',
    ...             'virginica',
    ...             'versicolor',
    ...             'versicolor',
    ...             'setosa',
    ...             'virginica',
    ...             'setosa'
    ...         ]
    ...     }
    ... )
    >>> pd.plotting.radviz(df, 'Category')  # doctest: +SKIP"""""""
pandas/plotting/_misc.py,"def andrews_curves(frame: DataFrame, class_column: str, ax: Axes | None=None, samples: int=200, color: list[str] | tuple[str, ...] | None=None, colormap: Colormap | str | None=None, **kwargs) -> Axes:
    plot_backend = _get_plot_backend('matplotlib')
    return plot_backend.andrews_curves(frame=frame, class_column=class_column, ax=ax, samples=samples, color=color, colormap=colormap, **kwargs)","""""""Generate a matplotlib plot for visualising clusters of multivariate data.

Andrews curves have the functional form:

.. math::
    f(t) = \frac{x_1}{\sqrt{2}} + x_2 \sin(t) + x_3 \cos(t) +
    x_4 \sin(2t) + x_5 \cos(2t) + \cdots

Where :math:`x` coefficients correspond to the values of each dimension
and :math:`t` is linearly spaced between :math:`-\pi` and :math:`+\pi`.
Each row of frame then corresponds to a single curve.

Parameters
----------
frame : DataFrame
    Data to be plotted, preferably normalized to (0.0, 1.0).
class_column : label
    Name of the column containing class names.
ax : axes object, default None
    Axes to use.
samples : int
    Number of points to plot in each curve.
color : str, list[str] or tuple[str], optional
    Colors to use for the different classes. Colors can be strings
    or 3-element floating point RGB values.
colormap : str or matplotlib colormap object, default None
    Colormap to select colors from. If a string, load colormap with that
    name from matplotlib.
**kwargs
    Options to pass to matplotlib plotting method.

Returns
-------
:class:`matplotlib.axes.Axes`

Examples
--------

.. plot::
    :context: close-figs

    >>> df = pd.read_csv(
    ...     'https://raw.githubusercontent.com/pandas-dev/'
    ...     'pandas/main/pandas/tests/io/data/csv/iris.csv'
    ... )
    >>> pd.plotting.andrews_curves(df, 'Name')  # doctest: +SKIP"""""""
pandas/plotting/_misc.py,"def bootstrap_plot(series: Series, fig: Figure | None=None, size: int=50, samples: int=500, **kwds) -> Figure:
    plot_backend = _get_plot_backend('matplotlib')
    return plot_backend.bootstrap_plot(series=series, fig=fig, size=size, samples=samples, **kwds)","""""""Bootstrap plot on mean, median and mid-range statistics.

The bootstrap plot is used to estimate the uncertainty of a statistic
by relying on random sampling with replacement [1]_. This function will
generate bootstrapping plots for mean, median and mid-range statistics
for the given number of samples of the given size.

.. [1] ""Bootstrapping (statistics)"" in     https://en.wikipedia.org/wiki/Bootstrapping_%28statistics%29

Parameters
----------
series : pandas.Series
    Series from where to get the samplings for the bootstrapping.
fig : matplotlib.figure.Figure, default None
    If given, it will use the `fig` reference for plotting instead of
    creating a new one with default parameters.
size : int, default 50
    Number of data points to consider during each sampling. It must be
    less than or equal to the length of the `series`.
samples : int, default 500
    Number of times the bootstrap procedure is performed.
**kwds
    Options to pass to matplotlib plotting method.

Returns
-------
matplotlib.figure.Figure
    Matplotlib figure.

See Also
--------
pandas.DataFrame.plot : Basic plotting for DataFrame objects.
pandas.Series.plot : Basic plotting for Series objects.

Examples
--------
This example draws a basic bootstrap plot for a Series.

.. plot::
    :context: close-figs

    >>> s = pd.Series(np.random.uniform(size=100))
    >>> pd.plotting.bootstrap_plot(s)
    <Figure size 640x480 with 6 Axes>"""""""
pandas/plotting/_misc.py,"def parallel_coordinates(frame: DataFrame, class_column: str, cols: list[str] | None=None, ax: Axes | None=None, color: list[str] | tuple[str, ...] | None=None, use_columns: bool=False, xticks: list | tuple | None=None, colormap: Colormap | str | None=None, axvlines: bool=True, axvlines_kwds: Mapping[str, Any] | None=None, sort_labels: bool=False, **kwargs) -> Axes:
    plot_backend = _get_plot_backend('matplotlib')
    return plot_backend.parallel_coordinates(frame=frame, class_column=class_column, cols=cols, ax=ax, color=color, use_columns=use_columns, xticks=xticks, colormap=colormap, axvlines=axvlines, axvlines_kwds=axvlines_kwds, sort_labels=sort_labels, **kwargs)","""""""Parallel coordinates plotting.

Parameters
----------
frame : DataFrame
class_column : str
    Column name containing class names.
cols : list, optional
    A list of column names to use.
ax : matplotlib.axis, optional
    Matplotlib axis object.
color : list or tuple, optional
    Colors to use for the different classes.
use_columns : bool, optional
    If true, columns will be used as xticks.
xticks : list or tuple, optional
    A list of values to use for xticks.
colormap : str or matplotlib colormap, default None
    Colormap to use for line colors.
axvlines : bool, optional
    If true, vertical lines will be added at each xtick.
axvlines_kwds : keywords, optional
    Options to be passed to axvline method for vertical lines.
sort_labels : bool, default False
    Sort class_column labels, useful when assigning colors.
**kwargs
    Options to pass to matplotlib plotting method.

Returns
-------
matplotlib.axes.Axes

Examples
--------

.. plot::
    :context: close-figs

    >>> df = pd.read_csv(
    ...     'https://raw.githubusercontent.com/pandas-dev/'
    ...     'pandas/main/pandas/tests/io/data/csv/iris.csv'
    ... )
    >>> pd.plotting.parallel_coordinates(
    ...     df, 'Name', color=('#556270', '#4ECDC4', '#C7F464')
    ... )  # doctest: +SKIP"""""""
pandas/plotting/_misc.py,"def lag_plot(series: Series, lag: int=1, ax: Axes | None=None, **kwds) -> Axes:
    plot_backend = _get_plot_backend('matplotlib')
    return plot_backend.lag_plot(series=series, lag=lag, ax=ax, **kwds)","""""""Lag plot for time series.

Parameters
----------
series : Series
    The time series to visualize.
lag : int, default 1
    Lag length of the scatter plot.
ax : Matplotlib axis object, optional
    The matplotlib axis object to use.
**kwds
    Matplotlib scatter method keyword arguments.

Returns
-------
matplotlib.axes.Axes

Examples
--------
Lag plots are most commonly used to look for patterns in time series data.

Given the following time series

.. plot::
    :context: close-figs

    >>> np.random.seed(5)
    >>> x = np.cumsum(np.random.normal(loc=1, scale=5, size=50))
    >>> s = pd.Series(x)
    >>> s.plot()  # doctest: +SKIP

A lag plot with ``lag=1`` returns

.. plot::
    :context: close-figs

    >>> pd.plotting.lag_plot(s, lag=1)
    <Axes: xlabel='y(t)', ylabel='y(t + 1)'>"""""""
pandas/plotting/_misc.py,"def autocorrelation_plot(series: Series, ax: Axes | None=None, **kwargs) -> Axes:
    plot_backend = _get_plot_backend('matplotlib')
    return plot_backend.autocorrelation_plot(series=series, ax=ax, **kwargs)","""""""Autocorrelation plot for time series.

Parameters
----------
series : Series
    The time series to visualize.
ax : Matplotlib axis object, optional
    The matplotlib axis object to use.
**kwargs
    Options to pass to matplotlib plotting method.

Returns
-------
matplotlib.axes.Axes

Examples
--------
The horizontal lines in the plot correspond to 95% and 99% confidence bands.

The dashed line is 99% confidence band.

.. plot::
    :context: close-figs

    >>> spacing = np.linspace(-9 * np.pi, 9 * np.pi, num=1000)
    >>> s = pd.Series(0.7 * np.random.rand(1000) + 0.3 * np.sin(spacing))
    >>> pd.plotting.autocorrelation_plot(s)  # doctest: +SKIP"""""""
pandas/tests/apply/conftest.py,"@pytest.fixture
def int_frame_const_col():
    df = DataFrame(np.tile(np.arange(3, dtype='int64'), 6).reshape(6, -1) + 1, columns=['A', 'B', 'C'])
    return df","""""""Fixture for DataFrame of ints which are constant per column

Columns are ['A', 'B', 'C'], with values (per column): [1, 2, 3]"""""""
pandas/tests/apply/test_frame_transform.py,"def unpack_obj(obj, klass, axis):
    if klass is not DataFrame:
        obj = obj['A']
        if axis != 0:
            pytest.skip(f'Test is only for DataFrame with axis={axis}')
    return obj","""""""Helper to ensure we have the right type of object for a test parametrized
over frame_or_series."""""""
pandas/tests/arithmetic/common.py,"def assert_cannot_add(left, right, msg='cannot add'):
    with pytest.raises(TypeError, match=msg):
        left + right
    with pytest.raises(TypeError, match=msg):
        right + left","""""""Helper to assert that left and right cannot be added.

Parameters
----------
left : object
right : object
msg : str, default ""cannot add"""""""""
pandas/tests/arithmetic/common.py,"def assert_invalid_addsub_type(left, right, msg=None):
    with pytest.raises(TypeError, match=msg):
        left + right
    with pytest.raises(TypeError, match=msg):
        right + left
    with pytest.raises(TypeError, match=msg):
        left - right
    with pytest.raises(TypeError, match=msg):
        right - left","""""""Helper to assert that left and right can be neither added nor subtracted.

Parameters
----------
left : object
right : object
msg : str or None, default None"""""""
pandas/tests/arithmetic/common.py,"def get_upcast_box(left, right, is_cmp: bool=False):
    if isinstance(left, DataFrame) or isinstance(right, DataFrame):
        return DataFrame
    if isinstance(left, Series) or isinstance(right, Series):
        if is_cmp and isinstance(left, Index):
            return np.array
        return Series
    if isinstance(left, Index) or isinstance(right, Index):
        if is_cmp:
            return np.array
        return Index
    return tm.to_array","""""""Get the box to use for 'expected' in an arithmetic or comparison operation.

Parameters
left : Any
right : Any
is_cmp : bool, default False
    Whether the operation is a comparison method."""""""
pandas/tests/arithmetic/common.py,"def assert_invalid_comparison(left, right, box):
    xbox = box if box not in [Index, array] else np.array

    def xbox2(x):
        if isinstance(x, NumpyExtensionArray):
            return x._ndarray
        if isinstance(x, BooleanArray):
            return x.astype(bool)
        return x
    rev_box = xbox
    if isinstance(right, Index) and isinstance(left, Series):
        rev_box = np.array
    result = xbox2(left == right)
    expected = xbox(np.zeros(result.shape, dtype=np.bool_))
    tm.assert_equal(result, expected)
    result = xbox2(right == left)
    tm.assert_equal(result, rev_box(expected))
    result = xbox2(left != right)
    tm.assert_equal(result, ~expected)
    result = xbox2(right != left)
    tm.assert_equal(result, rev_box(~expected))
    msg = '|'.join(['Invalid comparison between', 'Cannot compare type', 'not supported between', 'invalid type promotion', ""The DTypes <class 'numpy.dtype\\[datetime64\\]'> and <class 'numpy.dtype\\[int64\\]'> do not have a common DType. For example they cannot be stored in a single array unless the dtype is `object`.""])
    with pytest.raises(TypeError, match=msg):
        left < right
    with pytest.raises(TypeError, match=msg):
        left <= right
    with pytest.raises(TypeError, match=msg):
        left > right
    with pytest.raises(TypeError, match=msg):
        left >= right
    with pytest.raises(TypeError, match=msg):
        right < left
    with pytest.raises(TypeError, match=msg):
        right <= left
    with pytest.raises(TypeError, match=msg):
        right > left
    with pytest.raises(TypeError, match=msg):
        right >= left","""""""Assert that comparison operations with mismatched types behave correctly.

Parameters
----------
left : np.ndarray, ExtensionArray, Index, or Series
right : object
box : {pd.DataFrame, pd.Series, pd.Index, pd.array, tm.to_array}"""""""
pandas/tests/arithmetic/conftest.py,"@pytest.fixture(params=[1, np.array(1, dtype=np.int64)])
def one(request):
    return request.param","""""""Several variants of integer value 1. The zero-dim integer array
behaves like an integer.

This fixture can be used to check that datetimelike indexes handle
addition and subtraction of integers and zero-dimensional arrays
of integers.

Examples
--------
dti = pd.date_range('2016-01-01', periods=2, freq='H')
dti
DatetimeIndex(['2016-01-01 00:00:00', '2016-01-01 01:00:00'],
dtype='datetime64[ns]', freq='H')
dti + one
DatetimeIndex(['2016-01-01 01:00:00', '2016-01-01 02:00:00'],
dtype='datetime64[ns]', freq='H')"""""""
pandas/tests/arithmetic/conftest.py,"@pytest.fixture(params=zeros)
def zero(request):
    return request.param","""""""Several types of scalar zeros and length 5 vectors of zeros.

This fixture can be used to check that numeric-dtype indexes handle
division by any zero numeric-dtype.

Uses vector of length 5 for broadcasting with `numeric_idx` fixture,
which creates numeric-dtype vectors also of length 5.

Examples
--------
arr = RangeIndex(5)
arr / zeros
Index([nan, inf, inf, inf, inf], dtype='float64')"""""""
pandas/tests/arithmetic/conftest.py,"@pytest.fixture(params=[Index(np.arange(5, dtype='float64')), Index(np.arange(5, dtype='int64')), Index(np.arange(5, dtype='uint64')), RangeIndex(5)], ids=lambda x: type(x).__name__)
def numeric_idx(request):
    return request.param","""""""Several types of numeric-dtypes Index objects"""""""
pandas/tests/arithmetic/conftest.py,"@pytest.fixture(params=[pd.Timedelta('10m7s').to_pytimedelta(), pd.Timedelta('10m7s'), pd.Timedelta('10m7s').to_timedelta64()], ids=lambda x: type(x).__name__)
def scalar_td(request):
    return request.param","""""""Several variants of Timedelta scalars representing 10 minutes and 7 seconds."""""""
pandas/tests/arithmetic/conftest.py,"@pytest.fixture(params=[pd.offsets.Day(3), pd.offsets.Hour(72), pd.Timedelta(days=3).to_pytimedelta(), pd.Timedelta('72:00:00'), np.timedelta64(3, 'D'), np.timedelta64(72, 'h')], ids=lambda x: type(x).__name__)
def three_days(request):
    return request.param","""""""Several timedelta-like and DateOffset objects that each represent
a 3-day timedelta"""""""
pandas/tests/arithmetic/conftest.py,"@pytest.fixture(params=[pd.offsets.Hour(2), pd.offsets.Minute(120), pd.Timedelta(hours=2).to_pytimedelta(), pd.Timedelta(seconds=2 * 3600), np.timedelta64(2, 'h'), np.timedelta64(120, 'm')], ids=lambda x: type(x).__name__)
def two_hours(request):
    return request.param","""""""Several timedelta-like and DateOffset objects that each represent
a 2-hour timedelta"""""""
pandas/tests/arithmetic/conftest.py,"@pytest.fixture(params=[pd.Timedelta(minutes=30).to_pytimedelta(), np.timedelta64(30, 's'), pd.Timedelta(seconds=30)] + _common_mismatch)
def not_hourly(request):
    return request.param","""""""Several timedelta-like and DateOffset instances that are _not_
compatible with Hourly frequencies."""""""
pandas/tests/arithmetic/conftest.py,"@pytest.fixture(params=[np.timedelta64(4, 'h'), pd.Timedelta(hours=23).to_pytimedelta(), pd.Timedelta('23:00:00')] + _common_mismatch)
def not_daily(request):
    return request.param","""""""Several timedelta-like and DateOffset instances that are _not_
compatible with Daily frequencies."""""""
pandas/tests/arithmetic/conftest.py,"@pytest.fixture(params=[np.timedelta64(365, 'D'), pd.Timedelta(days=365).to_pytimedelta(), pd.Timedelta(days=365)] + _common_mismatch)
def mismatched_freq(request):
    return request.param","""""""Several timedelta-like and DateOffset instances that are _not_
compatible with Monthly or Annual frequencies."""""""
pandas/tests/arithmetic/conftest.py,"@pytest.fixture(params=[Index, pd.Series, tm.to_array, np.array, list], ids=lambda x: x.__name__)
def box_1d_array(request):
    return request.param","""""""Fixture to test behavior for Index, Series, tm.to_array, numpy Array and list
classes"""""""
pandas/tests/arithmetic/test_interval.py,"@pytest.fixture(params=[(Index([0, 2, 4, 4]), Index([1, 3, 5, 8])), (Index([0.0, 1.0, 2.0, np.nan]), Index([1.0, 2.0, 3.0, np.nan])), (timedelta_range('0 days', periods=3).insert(3, pd.NaT), timedelta_range('1 day', periods=3).insert(3, pd.NaT)), (date_range('20170101', periods=3).insert(3, pd.NaT), date_range('20170102', periods=3).insert(3, pd.NaT)), (date_range('20170101', periods=3, tz='US/Eastern').insert(3, pd.NaT), date_range('20170102', periods=3, tz='US/Eastern').insert(3, pd.NaT))], ids=lambda x: str(x[0].dtype))
def left_right_dtypes(request):
    return request.param","""""""Fixture for building an IntervalArray from various dtypes"""""""
pandas/tests/arithmetic/test_interval.py,"@pytest.fixture
def interval_array(left_right_dtypes):
    (left, right) = left_right_dtypes
    return IntervalArray.from_arrays(left, right)","""""""Fixture to generate an IntervalArray of various dtypes containing NA if possible"""""""
pandas/tests/arithmetic/test_numeric.py,"@pytest.fixture(params=[Index, Series, tm.to_array])
def box_pandas_1d_array(request):
    return request.param","""""""Fixture to test behavior for Index, Series and tm.to_array classes"""""""
pandas/tests/arithmetic/test_numeric.py,"def adjust_negative_zero(zero, expected):
    if np.signbit(np.array(zero)).any():
        assert np.signbit(np.array(zero)).all()
        expected *= -1
    return expected","""""""Helper to adjust the expected result if we are dividing by -0.0
as opposed to 0.0"""""""
pandas/tests/arithmetic/test_timedelta64.py,"def assert_dtype(obj, expected_dtype):
    dtype = tm.get_dtype(obj)
    assert dtype == expected_dtype","""""""Helper to check the dtype for a Series, Index, or single-column DataFrame."""""""
pandas/tests/arrays/boolean/test_arithmetic.py,"@pytest.fixture
def data():
    return pd.array([True, False] * 4 + [np.nan] + [True, False] * 44 + [np.nan] + [True, False], dtype='boolean')","""""""Fixture returning boolean array with valid and missing values."""""""
pandas/tests/arrays/boolean/test_arithmetic.py,"@pytest.fixture
def left_array():
    return pd.array([True] * 3 + [False] * 3 + [None] * 3, dtype='boolean')","""""""Fixture returning boolean array with valid and missing values."""""""
pandas/tests/arrays/boolean/test_arithmetic.py,"@pytest.fixture
def right_array():
    return pd.array([True, False, None] * 3, dtype='boolean')","""""""Fixture returning boolean array with valid and missing values."""""""
pandas/tests/arrays/boolean/test_comparison.py,"@pytest.fixture
def data():
    return pd.array([True, False] * 4 + [np.nan] + [True, False] * 44 + [np.nan] + [True, False], dtype='boolean')","""""""Fixture returning boolean array with valid and missing data"""""""
pandas/tests/arrays/boolean/test_comparison.py,"@pytest.fixture
def dtype():
    return pd.BooleanDtype()","""""""Fixture returning BooleanDtype"""""""
pandas/tests/arrays/boolean/test_reduction.py,"@pytest.fixture
def data():
    return pd.array([True, False] * 4 + [np.nan] + [True, False] * 44 + [np.nan] + [True, False], dtype='boolean')","""""""Fixture returning boolean array, with valid and missing values."""""""
pandas/tests/arrays/categorical/conftest.py,"@pytest.fixture(params=[True, False])
def allow_fill(request):
    return request.param","""""""Boolean 'allow_fill' parameter for Categorical.take"""""""
pandas/tests/arrays/categorical/conftest.py,"@pytest.fixture
def factor():
    return Categorical(['a', 'b', 'b', 'a', 'a', 'c', 'c', 'c'], ordered=True)","""""""Fixture returning  a Categorical object"""""""
pandas/tests/arrays/categorical/test_indexing.py,"@pytest.fixture
def non_coercible_categorical(monkeypatch):

    def array(self, dtype=None):
        raise ValueError('I cannot be converted.')
    with monkeypatch.context() as m:
        m.setattr(Categorical, '__array__', array)
        yield","""""""Monkeypatch Categorical.__array__ to ensure no implicit conversion.

Raises
------
ValueError
    When Categorical.__array__ is called."""""""
pandas/tests/arrays/floating/conftest.py,"@pytest.fixture(params=[Float32Dtype, Float64Dtype])
def dtype(request):
    return request.param()","""""""Parametrized fixture returning a float 'dtype'"""""""
pandas/tests/arrays/floating/conftest.py,"@pytest.fixture
def data(dtype):
    return pd.array(list(np.arange(0.1, 0.9, 0.1)) + [pd.NA] + list(np.arange(1, 9.8, 0.1)) + [pd.NA] + [9.9, 10.0], dtype=dtype)","""""""Fixture returning 'data' array according to parametrized float 'dtype'"""""""
pandas/tests/arrays/floating/conftest.py,"@pytest.fixture
def data_missing(dtype):
    return pd.array([np.nan, 0.1], dtype=dtype)","""""""Fixture returning array with missing data according to parametrized float
'dtype'."""""""
pandas/tests/arrays/floating/conftest.py,"@pytest.fixture(params=['data', 'data_missing'])
def all_data(request, data, data_missing):
    if request.param == 'data':
        return data
    elif request.param == 'data_missing':
        return data_missing","""""""Parametrized fixture returning 'data' or 'data_missing' float arrays.

Used to test dtype conversion with and without missing values."""""""
pandas/tests/arrays/integer/conftest.py,"@pytest.fixture(params=[Int8Dtype, Int16Dtype, Int32Dtype, Int64Dtype, UInt8Dtype, UInt16Dtype, UInt32Dtype, UInt64Dtype])
def dtype(request):
    return request.param()","""""""Parametrized fixture returning integer 'dtype'"""""""
pandas/tests/arrays/integer/conftest.py,"@pytest.fixture
def data(dtype):
    return pd.array(list(range(8)) + [np.nan] + list(range(10, 98)) + [np.nan] + [99, 100], dtype=dtype)","""""""Fixture returning 'data' array with valid and missing values according to
parametrized integer 'dtype'.

Used to test dtype conversion with and without missing values."""""""
pandas/tests/arrays/integer/conftest.py,"@pytest.fixture
def data_missing(dtype):
    return pd.array([np.nan, 1], dtype=dtype)","""""""Fixture returning array with exactly one NaN and one valid integer,
according to parametrized integer 'dtype'.

Used to test dtype conversion with and without missing values."""""""
pandas/tests/arrays/integer/conftest.py,"@pytest.fixture(params=['data', 'data_missing'])
def all_data(request, data, data_missing):
    if request.param == 'data':
        return data
    elif request.param == 'data_missing':
        return data_missing","""""""Parametrized fixture returning 'data' or 'data_missing' integer arrays.

Used to test dtype conversion with and without missing values."""""""
pandas/tests/arrays/integer/test_construction.py,"@pytest.fixture(params=[pd.array, IntegerArray._from_sequence])
def constructor(request):
    return request.param","""""""Fixture returning parametrized IntegerArray from given sequence.

Used to test dtype conversions."""""""
pandas/tests/arrays/interval/test_interval.py,"@pytest.fixture(params=[(Index([0, 2, 4]), Index([1, 3, 5])), (Index([0.0, 1.0, 2.0]), Index([1.0, 2.0, 3.0])), (timedelta_range('0 days', periods=3), timedelta_range('1 day', periods=3)), (date_range('20170101', periods=3), date_range('20170102', periods=3)), (date_range('20170101', periods=3, tz='US/Eastern'), date_range('20170102', periods=3, tz='US/Eastern'))], ids=lambda x: str(x[0].dtype))
def left_right_dtypes(request):
    return request.param","""""""Fixture for building an IntervalArray from various dtypes"""""""
pandas/tests/arrays/interval/test_ops.py,"@pytest.fixture(params=[IntervalArray, IntervalIndex])
def constructor(request):
    return request.param","""""""Fixture for testing both interval container classes."""""""
pandas/tests/arrays/interval/test_ops.py,"@pytest.fixture(params=[(Timedelta('0 days'), Timedelta('1 day')), (Timestamp('2018-01-01'), Timedelta('1 day')), (0, 1)], ids=lambda x: type(x[0]).__name__)
def start_shift(request):
    return request.param","""""""Fixture for generating intervals of different types from a start value
and a shift value that can be added to start to generate an endpoint."""""""
pandas/tests/arrays/masked/test_arithmetic.py,"@pytest.fixture(params=zip(arrays, scalars), ids=[a.dtype.name for a in arrays])
def data(request):
    return request.param","""""""Fixture returning parametrized (array, scalar) tuple.

Used to test equivalence of scalars, numpy arrays with array ops, and the
equivalence of DataFrame and Series ops."""""""
pandas/tests/arrays/masked/test_arrow_compat.py,"@pytest.fixture(params=arrays, ids=[a.dtype.name for a in arrays])
def data(request):
    return request.param","""""""Fixture returning parametrized array from given dtype, including integer,
float and boolean"""""""
pandas/tests/arrays/masked/test_arrow_compat.py,"@pytest.fixture
def np_dtype_to_arrays(any_real_numpy_dtype):
    np_dtype = np.dtype(any_real_numpy_dtype)
    pa_type = pa.from_numpy_dtype(np_dtype)
    pa_array = pa.array([0, 1, 2, None], type=pa_type)
    np_expected = np.array([0, 1, 2], dtype=np_dtype)
    mask_expected = np.array([True, True, True, False])
    return (np_dtype, pa_array, np_expected, mask_expected)","""""""Fixture returning actual and expected dtype, pandas and numpy arrays and
mask from a given numpy dtype"""""""
pandas/tests/arrays/masked/test_arrow_compat.py,"def test_pyarrow_array_to_numpy_and_mask(np_dtype_to_arrays):
    (np_dtype, pa_array, np_expected, mask_expected) = np_dtype_to_arrays
    (data, mask) = pyarrow_array_to_numpy_and_mask(pa_array, np_dtype)
    tm.assert_numpy_array_equal(data[:3], np_expected)
    tm.assert_numpy_array_equal(mask, mask_expected)
    mask_buffer = pa_array.buffers()[0]
    data_buffer = pa_array.buffers()[1]
    data_buffer_bytes = pa_array.buffers()[1].to_pybytes()
    data_buffer_trail = pa.py_buffer(data_buffer_bytes + b'\x00')
    pa_array_trail = pa.Array.from_buffers(type=pa_array.type, length=len(pa_array), buffers=[mask_buffer, data_buffer_trail], offset=pa_array.offset)
    pa_array_trail.validate()
    (data, mask) = pyarrow_array_to_numpy_and_mask(pa_array_trail, np_dtype)
    tm.assert_numpy_array_equal(data[:3], np_expected)
    tm.assert_numpy_array_equal(mask, mask_expected)
    offset = b'\x00' * (pa_array.type.bit_width // 8)
    data_buffer_offset = pa.py_buffer(offset + data_buffer_bytes)
    mask_buffer_offset = pa.py_buffer(b'\x0e')
    pa_array_offset = pa.Array.from_buffers(type=pa_array.type, length=len(pa_array), buffers=[mask_buffer_offset, data_buffer_offset], offset=pa_array.offset + 1)
    pa_array_offset.validate()
    (data, mask) = pyarrow_array_to_numpy_and_mask(pa_array_offset, np_dtype)
    tm.assert_numpy_array_equal(data[:3], np_expected)
    tm.assert_numpy_array_equal(mask, mask_expected)
    np_expected_empty = np.array([], dtype=np_dtype)
    mask_expected_empty = np.array([], dtype=np.bool_)
    pa_array_offset = pa.Array.from_buffers(type=pa_array.type, length=0, buffers=[mask_buffer, data_buffer], offset=pa_array.offset)
    pa_array_offset.validate()
    (data, mask) = pyarrow_array_to_numpy_and_mask(pa_array_offset, np_dtype)
    tm.assert_numpy_array_equal(data[:3], np_expected_empty)
    tm.assert_numpy_array_equal(mask, mask_expected_empty)","""""""Test conversion from pyarrow array to numpy array.

Modifies the pyarrow buffer to contain padding and offset, which are
considered valid buffers by pyarrow.

Also tests empty pyarrow arrays with non empty buffers.
See https://github.com/pandas-dev/pandas/issues/40896"""""""
pandas/tests/arrays/masked/test_function.py,"@pytest.fixture(params=arrays, ids=[a.dtype.name for a in arrays])
def data(request):
    return request.param","""""""Fixture returning parametrized 'data' array with different integer and
floating point types"""""""
pandas/tests/arrays/masked/test_function.py,"@pytest.fixture()
def numpy_dtype(data):
    if is_integer_dtype(data):
        numpy_dtype = float
    else:
        numpy_dtype = data.dtype.type
    return numpy_dtype","""""""Fixture returning numpy dtype from 'data' input array."""""""
pandas/tests/arrays/numpy_/test_numpy.py,"@pytest.fixture(params=[np.array(['a', 'b'], dtype=object), np.array([0, 1], dtype=float), np.array([0, 1], dtype=int), np.array([0, 1 + 2j], dtype=complex), np.array([True, False], dtype=bool), np.array([0, 1], dtype='datetime64[ns]'), np.array([0, 1], dtype='timedelta64[ns]')])
def any_numpy_array(request):
    return request.param","""""""Parametrized fixture for NumPy arrays with different dtypes.

This excludes string and bytes."""""""
pandas/tests/arrays/sparse/test_arithmetics.py,"@pytest.fixture(params=['integer', 'block'])
def kind(request):
    return request.param","""""""kind kwarg to pass to SparseArray"""""""
pandas/tests/arrays/sparse/test_arithmetics.py,"@pytest.fixture(params=[True, False])
def mix(request):
    return request.param","""""""Fixture returning True or False, determining whether to operate
op(sparse, dense) instead of op(sparse, sparse)"""""""
pandas/tests/arrays/sparse/test_array.py,"@pytest.fixture
def arr_data():
    return np.array([np.nan, np.nan, 1, 2, 3, np.nan, 4, 5, np.nan, 6])","""""""Fixture returning numpy array with valid and missing entries"""""""
pandas/tests/arrays/sparse/test_array.py,"@pytest.fixture
def arr(arr_data):
    return SparseArray(arr_data)","""""""Fixture returning SparseArray from 'arr_data'"""""""
pandas/tests/arrays/sparse/test_array.py,"@pytest.fixture
def zarr():
    return SparseArray([0, 0, 1, 2, 3, 0, 4, 5, 0, 6], fill_value=0)","""""""Fixture returning SparseArray with integer entries and 'fill_value=0'"""""""
pandas/tests/arrays/string_/test_string.py,"@pytest.fixture
def dtype(string_storage):
    return pd.StringDtype(storage=string_storage)","""""""Fixture giving StringDtype from parametrized 'string_storage'"""""""
pandas/tests/arrays/string_/test_string.py,"@pytest.fixture
def cls(dtype):
    return dtype.construct_array_type()","""""""Fixture giving array type from parametrized 'dtype'"""""""
pandas/tests/arrays/test_datetimelike.py,"@pytest.fixture(params=['D', 'B', 'W', 'ME', 'Q', 'Y'])
def freqstr(request):
    return request.param","""""""Fixture returning parametrized frequency in string format."""""""
pandas/tests/arrays/test_datetimelike.py,"@pytest.fixture
def period_index(freqstr):
    with warnings.catch_warnings():
        warnings.filterwarnings('ignore', message='Period with BDay freq', category=FutureWarning)
        freqstr = freq_to_period_freqstr(1, freqstr)
        pi = pd.period_range(start=Timestamp('2000-01-01'), periods=100, freq=freqstr)
    return pi","""""""A fixture to provide PeriodIndex objects with different frequencies.

Most PeriodArray behavior is already tested in PeriodIndex tests,
so here we just test that the PeriodArray behavior matches
the PeriodIndex behavior."""""""
pandas/tests/arrays/test_datetimelike.py,"@pytest.fixture
def datetime_index(freqstr):
    dti = pd.date_range(start=Timestamp('2000-01-01'), periods=100, freq=freqstr)
    return dti","""""""A fixture to provide DatetimeIndex objects with different frequencies.

Most DatetimeArray behavior is already tested in DatetimeIndex tests,
so here we just test that the DatetimeArray behavior matches
the DatetimeIndex behavior."""""""
pandas/tests/arrays/test_datetimelike.py,"@pytest.fixture
def timedelta_index():
    return TimedeltaIndex(['1 Day', '3 Hours', 'NaT'])","""""""A fixture to provide TimedeltaIndex objects with different frequencies.
 Most TimedeltaArray behavior is already tested in TimedeltaIndex tests,
so here we just test that the TimedeltaArray behavior matches
the TimedeltaIndex behavior."""""""
pandas/tests/base/common.py,"def allow_na_ops(obj: Any) -> bool:
    is_bool_index = isinstance(obj, Index) and obj.inferred_type == 'boolean'
    return not is_bool_index and obj._can_hold_na","""""""Whether to skip test cases including NaN"""""""
pandas/tests/copy_view/util.py,"def get_array(obj, col=None):
    if isinstance(obj, Index):
        arr = obj._values
    elif isinstance(obj, Series) and (col is None or obj.name == col):
        arr = obj._values
    else:
        assert col is not None
        icol = obj.columns.get_loc(col)
        assert isinstance(icol, int)
        arr = obj._get_column_array(icol)
    if isinstance(arr, BaseMaskedArray):
        return arr._data
    elif isinstance(arr, Categorical):
        return arr
    return getattr(arr, '_ndarray', arr)","""""""Helper method to get array for a DataFrame column or a Series.

Equivalent of df[col].values, but without going through normal getitem,
which triggers tracking references / CoW (and we might be testing that
this is done by some other operation)."""""""
pandas/tests/dtypes/cast/test_promote.py,"def _check_promote(dtype, fill_value, expected_dtype, exp_val_for_scalar=None):
    assert is_scalar(fill_value)
    (result_dtype, result_fill_value) = maybe_promote(dtype, fill_value)
    expected_fill_value = exp_val_for_scalar
    assert result_dtype == expected_dtype
    _assert_match(result_fill_value, expected_fill_value)","""""""Auxiliary function to unify testing of scalar/array promotion.

Parameters
----------
dtype : dtype
    The value to pass on as the first argument to maybe_promote.
fill_value : scalar
    The value to pass on as the second argument to maybe_promote as
    a scalar.
expected_dtype : dtype
    The expected dtype returned by maybe_promote (by design this is the
    same regardless of whether fill_value was passed as a scalar or in an
    array!).
exp_val_for_scalar : scalar
    The expected value for the (potentially upcast) fill_value returned by
    maybe_promote."""""""
pandas/tests/dtypes/test_common.py,"def to_ea_dtypes(dtypes):
    return [getattr(pd, dt + 'Dtype') for dt in dtypes]","""""""convert list of string dtypes to EA dtype"""""""
pandas/tests/dtypes/test_common.py,"def to_numpy_dtypes(dtypes):
    return [getattr(np, dt) for dt in dtypes if isinstance(dt, str)]","""""""convert list of string dtypes to numpy dtype"""""""
pandas/tests/dtypes/test_common.py,"def get_is_dtype_funcs():
    fnames = [f for f in dir(com) if f.startswith('is_') and f.endswith('dtype')]
    fnames.remove('is_string_or_object_np_dtype')
    return [getattr(com, fname) for fname in fnames]","""""""Get all functions in pandas.core.dtypes.common that
begin with 'is_' and end with 'dtype'"""""""
pandas/tests/extension/conftest.py,"@pytest.fixture
def dtype():
    raise NotImplementedError","""""""A fixture providing the ExtensionDtype to validate."""""""
pandas/tests/extension/conftest.py,"@pytest.fixture
def data():
    raise NotImplementedError","""""""Length-100 array for this type.

* data[0] and data[1] should both be non missing
* data[0] and data[1] should not be equal"""""""
pandas/tests/extension/conftest.py,"@pytest.fixture
def data_for_twos(dtype):
    if not (dtype._is_numeric or dtype.kind == 'm'):
        pytest.skip('Not a numeric dtype')
    raise NotImplementedError","""""""Length-100 array in which all the elements are two.

Call pytest.skip in your fixture if the dtype does not support divmod."""""""
pandas/tests/extension/conftest.py,"@pytest.fixture
def data_missing():
    raise NotImplementedError","""""""Length-2 array with [NA, Valid]"""""""
pandas/tests/extension/conftest.py,"@pytest.fixture(params=['data', 'data_missing'])
def all_data(request, data, data_missing):
    if request.param == 'data':
        return data
    elif request.param == 'data_missing':
        return data_missing","""""""Parametrized fixture giving 'data' and 'data_missing'"""""""
pandas/tests/extension/conftest.py,"@pytest.fixture
def data_repeated(data):

    def gen(count):
        for _ in range(count):
            yield data
    return gen","""""""Generate many datasets.

Parameters
----------
data : fixture implementing `data`

Returns
-------
Callable[[int], Generator]:
    A callable that takes a `count` argument and
    returns a generator yielding `count` datasets."""""""
pandas/tests/extension/conftest.py,"@pytest.fixture
def data_for_sorting():
    raise NotImplementedError","""""""Length-3 array with a known sort order.

This should be three items [B, C, A] with
A < B < C

For boolean dtypes (for which there are only 2 values available),
set B=C=True"""""""
pandas/tests/extension/conftest.py,"@pytest.fixture
def data_missing_for_sorting():
    raise NotImplementedError","""""""Length-3 array with a known sort order.

This should be three items [B, NA, A] with
A < B and NA missing."""""""
pandas/tests/extension/conftest.py,"@pytest.fixture
def na_cmp():
    return operator.is_","""""""Binary operator for comparing NA values.

Should return a function of two arguments that returns
True if both arguments are (scalar) NA for your type.

By default, uses ``operator.is_``"""""""
pandas/tests/extension/conftest.py,"@pytest.fixture
def data_for_grouping():
    raise NotImplementedError","""""""Data for factorization, grouping, and unique tests.

Expected to be like [B, B, NA, NA, A, A, B, C]

Where A < B < C and NA is missing.

If a dtype has _is_boolean = True, i.e. only 2 unique non-NA entries,
then set C=B."""""""
pandas/tests/extension/conftest.py,"@pytest.fixture(params=[True, False])
def box_in_series(request):
    return request.param","""""""Whether to box the data in a Series"""""""
pandas/tests/extension/conftest.py,"@pytest.fixture(params=[lambda x: 1, lambda x: [1] * len(x), lambda x: Series([1] * len(x)), lambda x: x], ids=['scalar', 'list', 'series', 'object'])
def groupby_apply_op(request):
    return request.param","""""""Functions to test groupby.apply()."""""""
pandas/tests/extension/conftest.py,"@pytest.fixture(params=[True, False])
def as_frame(request):
    return request.param","""""""Boolean fixture to support Series and Series.to_frame() comparison testing."""""""
pandas/tests/extension/conftest.py,"@pytest.fixture(params=[True, False])
def as_series(request):
    return request.param","""""""Boolean fixture to support arr and Series(arr) comparison testing."""""""
pandas/tests/extension/conftest.py,"@pytest.fixture(params=[True, False])
def use_numpy(request):
    return request.param","""""""Boolean fixture to support comparison testing of ExtensionDtype array
and numpy array."""""""
pandas/tests/extension/conftest.py,"@pytest.fixture(params=['ffill', 'bfill'])
def fillna_method(request):
    return request.param","""""""Parametrized fixture giving method parameters 'ffill' and 'bfill' for
Series.fillna(method=<method>) testing."""""""
pandas/tests/extension/conftest.py,"@pytest.fixture(params=[True, False])
def as_array(request):
    return request.param","""""""Boolean fixture to support ExtensionDtype _from_sequence method testing."""""""
pandas/tests/extension/conftest.py,"@pytest.fixture
def invalid_scalar(data):
    return object.__new__(object)","""""""A scalar that *cannot* be held by this ExtensionArray.

The default should work for most subclasses, but is not guaranteed.

If the array can hold any item (i.e. object dtype), then use pytest.skip."""""""
pandas/tests/extension/conftest.py,"@pytest.fixture
def using_copy_on_write() -> bool:
    return options.mode.copy_on_write and _get_option('mode.data_manager', silent=True) == 'block'","""""""Fixture to check if Copy-on-Write is enabled."""""""
pandas/tests/extension/json/test_json.py,"@pytest.fixture
def data():
    data = make_data()
    while len(data[0]) == len(data[1]):
        data = make_data()
    return JSONArray(data)","""""""Length-100 PeriodArray for semantics test."""""""
pandas/tests/extension/json/test_json.py,"@pytest.fixture
def data_missing():
    return JSONArray([{}, {'a': 10}])","""""""Length 2 array with [NA, Valid]"""""""
pandas/tests/extension/list/test_list.py,"@pytest.fixture
def data():
    data = make_data()
    while len(data[0]) == len(data[1]):
        data = make_data()
    return ListArray(data)","""""""Length-100 ListArray for semantics test."""""""
pandas/tests/extension/test_arrow.py,"@pytest.fixture
def data_missing(data):
    return type(data)._from_sequence([None, data[0]], dtype=data.dtype)","""""""Length-2 array with [NA, Valid]"""""""
pandas/tests/extension/test_arrow.py,"@pytest.fixture(params=['data', 'data_missing'])
def all_data(request, data, data_missing):
    if request.param == 'data':
        return data
    elif request.param == 'data_missing':
        return data_missing","""""""Parametrized fixture returning 'data' or 'data_missing' integer arrays.

Used to test dtype conversion with and without missing values."""""""
pandas/tests/extension/test_arrow.py,"@pytest.fixture
def data_for_grouping(dtype):
    pa_dtype = dtype.pyarrow_dtype
    if pa.types.is_boolean(pa_dtype):
        A = False
        B = True
        C = True
    elif pa.types.is_floating(pa_dtype):
        A = -1.1
        B = 0.0
        C = 1.1
    elif pa.types.is_signed_integer(pa_dtype):
        A = -1
        B = 0
        C = 1
    elif pa.types.is_unsigned_integer(pa_dtype):
        A = 0
        B = 1
        C = 10
    elif pa.types.is_date(pa_dtype):
        A = date(1999, 12, 31)
        B = date(2010, 1, 1)
        C = date(2022, 1, 1)
    elif pa.types.is_timestamp(pa_dtype):
        A = datetime(1999, 1, 1, 1, 1, 1, 1)
        B = datetime(2020, 1, 1)
        C = datetime(2020, 1, 1, 1)
    elif pa.types.is_duration(pa_dtype):
        A = timedelta(-1)
        B = timedelta(0)
        C = timedelta(1, 4)
    elif pa.types.is_time(pa_dtype):
        A = time(0, 0)
        B = time(0, 12)
        C = time(12, 12)
    elif pa.types.is_string(pa_dtype):
        A = 'a'
        B = 'b'
        C = 'c'
    elif pa.types.is_binary(pa_dtype):
        A = b'a'
        B = b'b'
        C = b'c'
    elif pa.types.is_decimal(pa_dtype):
        A = Decimal('-1.1')
        B = Decimal('0.0')
        C = Decimal('1.1')
    else:
        raise NotImplementedError
    return pd.array([B, B, None, None, A, A, B, C], dtype=dtype)","""""""Data for factorization, grouping, and unique tests.

Expected to be like [B, B, NA, NA, A, A, B, C]

Where A < B < C and NA is missing"""""""
pandas/tests/extension/test_arrow.py,"@pytest.fixture
def data_for_sorting(data_for_grouping):
    return type(data_for_grouping)._from_sequence([data_for_grouping[0], data_for_grouping[7], data_for_grouping[4]], dtype=data_for_grouping.dtype)","""""""Length-3 array with a known sort order.

This should be three items [B, C, A] with
A < B < C"""""""
pandas/tests/extension/test_arrow.py,"@pytest.fixture
def data_missing_for_sorting(data_for_grouping):
    return type(data_for_grouping)._from_sequence([data_for_grouping[0], data_for_grouping[2], data_for_grouping[4]], dtype=data_for_grouping.dtype)","""""""Length-3 array with a known sort order.

This should be three items [B, NA, A] with
A < B and NA missing."""""""
pandas/tests/extension/test_arrow.py,"@pytest.fixture
def data_for_twos(data):
    pa_dtype = data.dtype.pyarrow_dtype
    if pa.types.is_integer(pa_dtype) or pa.types.is_floating(pa_dtype) or pa.types.is_decimal(pa_dtype) or pa.types.is_duration(pa_dtype):
        return pd.array([2] * 100, dtype=data.dtype)
    return data","""""""Length-100 array in which all the elements are two."""""""
pandas/tests/extension/test_categorical.py,"@pytest.fixture
def data():
    return Categorical(make_data())","""""""Length-100 array for this type.

* data[0] and data[1] should both be non missing
* data[0] and data[1] should not be equal"""""""
pandas/tests/extension/test_categorical.py,"@pytest.fixture
def data_missing():
    return Categorical([np.nan, 'A'])","""""""Length 2 array with [NA, Valid]"""""""
pandas/tests/extension/test_datetime.py,"@pytest.fixture
def data_for_grouping(dtype):
    a = pd.Timestamp('2000-01-01')
    b = pd.Timestamp('2000-01-02')
    c = pd.Timestamp('2000-01-03')
    na = 'NaT'
    return DatetimeArray(np.array([b, b, na, na, a, a, b, c], dtype='datetime64[ns]'), dtype=dtype)","""""""Expected to be like [B, B, NA, NA, A, A, B, C]

Where A < B < C and NA is missing"""""""
pandas/tests/extension/test_interval.py,"@pytest.fixture
def data():
    return IntervalArray(make_data())","""""""Length-100 PeriodArray for semantics test."""""""
pandas/tests/extension/test_interval.py,"@pytest.fixture
def data_missing():
    return IntervalArray.from_tuples([None, (0, 1)])","""""""Length 2 array with [NA, Valid]"""""""
pandas/tests/extension/test_numpy.py,"def _assert_attr_equal(attr: str, left, right, obj: str='Attributes'):
    if attr == 'dtype':
        lattr = getattr(left, 'dtype', None)
        rattr = getattr(right, 'dtype', None)
        if isinstance(lattr, NumpyEADtype) and (not isinstance(rattr, NumpyEADtype)):
            left = left.astype(lattr.numpy_dtype)
        elif isinstance(rattr, NumpyEADtype) and (not isinstance(lattr, NumpyEADtype)):
            right = right.astype(rattr.numpy_dtype)
    orig_assert_attr_equal(attr, left, right, obj)","""""""patch tm.assert_attr_equal so NumpyEADtype(""object"") is closed enough to
np.dtype(""object"")"""""""
pandas/tests/extension/test_numpy.py,"@pytest.fixture
def allow_in_pandas(monkeypatch):
    with monkeypatch.context() as m:
        m.setattr(NumpyExtensionArray, '_typ', 'extension')
        m.setattr(blocks, 'can_hold_element', _can_hold_element_patched)
        m.setattr(tm.asserters, 'assert_attr_equal', _assert_attr_equal)
        yield","""""""A monkeypatch to tells pandas to let us in.

By default, passing a NumpyExtensionArray to an index / series / frame
constructor will unbox that NumpyExtensionArray to an ndarray, and treat
it as a non-EA column. We don't want people using EAs without
reason.

The mechanism for this is a check against ABCNumpyExtensionArray
in each constructor.

But, for testing, we need to allow them in pandas. So we patch
the _typ of NumpyExtensionArray, so that we evade the ABCNumpyExtensionArray
check."""""""
pandas/tests/extension/test_numpy.py,"@pytest.fixture
def data_for_sorting(allow_in_pandas, dtype):
    if dtype.numpy_dtype == 'object':
        return NumpyExtensionArray(np.array([(), (2,), (3,), (1,)], dtype=object)[1:])
    return NumpyExtensionArray(np.array([1, 2, 0]))","""""""Length-3 array with a known sort order.

This should be three items [B, C, A] with
A < B < C"""""""
pandas/tests/extension/test_numpy.py,"@pytest.fixture
def data_missing_for_sorting(allow_in_pandas, dtype):
    if dtype.numpy_dtype == 'object':
        return NumpyExtensionArray(np.array([(1,), np.nan, (0,)], dtype=object))
    return NumpyExtensionArray(np.array([1, np.nan, 0]))","""""""Length-3 array with a known sort order.

This should be three items [B, NA, A] with
A < B and NA missing."""""""
pandas/tests/extension/test_numpy.py,"@pytest.fixture
def data_for_grouping(allow_in_pandas, dtype):
    if dtype.numpy_dtype == 'object':
        (a, b, c) = ((1,), (2,), (3,))
    else:
        (a, b, c) = np.arange(3)
    return NumpyExtensionArray(np.array([b, b, np.nan, np.nan, a, a, b, c], dtype=dtype.numpy_dtype))","""""""Data for factorization, grouping, and unique tests.

Expected to be like [B, B, NA, NA, A, A, B, C]

Where A < B < C and NA is missing"""""""
pandas/tests/extension/test_numpy.py,"@pytest.fixture
def skip_numpy_object(dtype, request):
    if dtype == 'object':
        mark = pytest.mark.xfail(reason='Fails for object dtype')
        request.node.add_marker(mark)","""""""Tests for NumpyExtensionArray with nested data. Users typically won't create
these objects via `pd.array`, but they can show up through `.array`
on a Series with nested data. Many of the base tests fail, as they aren't
appropriate for nested data.

This fixture allows these tests to be skipped when used as a usefixtures
marker to either an individual test or a test class."""""""
pandas/tests/extension/test_sparse.py,"@pytest.fixture(params=[0, np.nan])
def data(request):
    res = SparseArray(make_data(request.param), fill_value=request.param)
    return res","""""""Length-100 PeriodArray for semantics test."""""""
pandas/tests/extension/test_sparse.py,"@pytest.fixture(params=[0, np.nan])
def data_missing(request):
    return SparseArray([np.nan, 1], fill_value=request.param)","""""""Length 2 array with [NA, Valid]"""""""
pandas/tests/extension/test_sparse.py,"@pytest.fixture(params=[0, np.nan])
def data_repeated(request):

    def gen(count):
        for _ in range(count):
            yield SparseArray(make_data(request.param), fill_value=request.param)
    yield gen","""""""Return different versions of data for count times"""""""
pandas/tests/extension/test_string.py,"@pytest.fixture
def data_missing(dtype, chunked):
    arr = dtype.construct_array_type()._from_sequence([pd.NA, 'A'])
    return split_array(arr) if chunked else arr","""""""Length 2 array with [NA, Valid]"""""""
pandas/tests/frame/common.py,"def zip_frames(frames: list[DataFrame], axis: AxisInt=1) -> DataFrame:
    if axis == 1:
        columns = frames[0].columns
        zipped = [f.loc[:, c] for c in columns for f in frames]
        return concat(zipped, axis=1)
    else:
        index = frames[0].index
        zipped = [f.loc[i, :] for i in index for f in frames]
        return DataFrame(zipped)","""""""take a list of frames, zip them together under the
assumption that these all have the first frames' index/columns.

Returns
-------
new_frame : DataFrame"""""""
pandas/tests/frame/conftest.py,"@pytest.fixture
def float_frame_with_na():
    df = DataFrame(tm.getSeriesData())
    df.iloc[5:10] = np.nan
    df.iloc[15:20, -2:] = np.nan
    return df","""""""Fixture for DataFrame of floats with index of unique strings

Columns are ['A', 'B', 'C', 'D']; some entries are missing

                   A         B         C         D
ABwBzA0ljw -1.128865 -0.897161  0.046603  0.274997
DJiRzmbyQF  0.728869  0.233502  0.722431 -0.890872
neMgPD5UBF  0.486072 -1.027393 -0.031553  1.449522
0yWA4n8VeX -1.937191 -1.142531  0.805215 -0.462018
3slYUbbqU1  0.153260  1.164691  1.489795 -0.545826
soujjZ0A08       NaN       NaN       NaN       NaN
7W6NLGsjB9       NaN       NaN       NaN       NaN
...              ...       ...       ...       ...
uhfeaNkCR1 -0.231210 -0.340472  0.244717 -0.901590
n6p7GYuBIV -0.419052  1.922721 -0.125361 -0.727717
ZhzAeY6p1y  1.234374 -1.425359 -0.827038 -0.633189
uWdPsORyUh  0.046738 -0.980445 -1.102965  0.605503
3DJA6aN590 -0.091018 -1.684734 -1.100900  0.215947
2GBPAzdbMk -2.883405 -1.021071  1.209877  1.633083
sHadBoyVHw -2.223032 -0.326384  0.258931  0.245517

[30 rows x 4 columns]"""""""
pandas/tests/frame/conftest.py,"@pytest.fixture
def bool_frame_with_na():
    df = DataFrame(tm.getSeriesData()) > 0
    df = df.astype(object)
    df.iloc[5:10] = np.nan
    df.iloc[15:20, -2:] = np.nan
    for i in range(4):
        df.iloc[i, i] = True
    return df","""""""Fixture for DataFrame of booleans with index of unique strings

Columns are ['A', 'B', 'C', 'D']; some entries are missing

                A      B      C      D
zBZxY2IDGd  False  False  False  False
IhBWBMWllt  False   True   True   True
ctjdvZSR6R   True  False   True   True
AVTujptmxb  False   True  False   True
G9lrImrSWq  False  False  False   True
sFFwdIUfz2    NaN    NaN    NaN    NaN
s15ptEJnRb    NaN    NaN    NaN    NaN
...           ...    ...    ...    ...
UW41KkDyZ4   True   True  False  False
l9l6XkOdqV   True  False  False  False
X2MeZfzDYA  False   True  False  False
xWkIKU7vfX  False   True  False   True
QOhL6VmpGU  False  False  False   True
22PwkRJdat  False   True  False  False
kfboQ3VeIK   True  False   True  False

[30 rows x 4 columns]"""""""
pandas/tests/frame/conftest.py,"@pytest.fixture
def float_string_frame():
    df = DataFrame(tm.getSeriesData())
    df['foo'] = 'bar'
    return df","""""""Fixture for DataFrame of floats and strings with index of unique strings

Columns are ['A', 'B', 'C', 'D', 'foo'].

                   A         B         C         D  foo
w3orJvq07g -1.594062 -1.084273 -1.252457  0.356460  bar
PeukuVdmz2  0.109855 -0.955086 -0.809485  0.409747  bar
ahp2KvwiM8 -1.533729 -0.142519 -0.154666  1.302623  bar
3WSJ7BUCGd  2.484964  0.213829  0.034778 -2.327831  bar
khdAmufk0U -0.193480 -0.743518 -0.077987  0.153646  bar
LE2DZiFlrE -0.193566 -1.343194 -0.107321  0.959978  bar
HJXSJhVn7b  0.142590  1.257603 -0.659409 -0.223844  bar
...              ...       ...       ...       ...  ...
9a1Vypttgw -1.316394  1.601354  0.173596  1.213196  bar
h5d1gVFbEy  0.609475  1.106738 -0.155271  0.294630  bar
mK9LsTQG92  1.303613  0.857040 -1.019153  0.369468  bar
oOLksd9gKH  0.558219 -0.134491 -0.289869 -0.951033  bar
9jgoOjKyHg  0.058270 -0.496110 -0.413212 -0.852659  bar
jZLDHclHAO  0.096298  1.267510  0.549206 -0.005235  bar
lR0nxDp1C2 -2.119350 -0.794384  0.544118  0.145849  bar

[30 rows x 5 columns]"""""""
pandas/tests/frame/conftest.py,"@pytest.fixture
def mixed_float_frame():
    df = DataFrame(tm.getSeriesData())
    df.A = df.A.astype('float32')
    df.B = df.B.astype('float32')
    df.C = df.C.astype('float16')
    df.D = df.D.astype('float64')
    return df","""""""Fixture for DataFrame of different float types with index of unique strings

Columns are ['A', 'B', 'C', 'D'].

                   A         B         C         D
GI7bbDaEZe -0.237908 -0.246225 -0.468506  0.752993
KGp9mFepzA -1.140809 -0.644046 -1.225586  0.801588
VeVYLAb1l2 -1.154013 -1.677615  0.690430 -0.003731
kmPME4WKhO  0.979578  0.998274 -0.776367  0.897607
CPyopdXTiz  0.048119 -0.257174  0.836426  0.111266
0kJZQndAj0  0.274357 -0.281135 -0.344238  0.834541
tqdwQsaHG8 -0.979716 -0.519897  0.582031  0.144710
...              ...       ...       ...       ...
7FhZTWILQj -2.906357  1.261039 -0.780273 -0.537237
4pUDPM4eGq -2.042512 -0.464382 -0.382080  1.132612
B8dUgUzwTi -1.506637 -0.364435  1.087891  0.297653
hErlVYjVv9  1.477453 -0.495515 -0.713867  1.438427
1BKN3o7YLs  0.127535 -0.349812 -0.881836  0.489827
9S4Ekn7zga  1.445518 -2.095149  0.031982  0.373204
xN1dNn6OV6  1.425017 -0.983995 -0.363281 -0.224502

[30 rows x 4 columns]"""""""
pandas/tests/frame/conftest.py,"@pytest.fixture
def mixed_int_frame():
    df = DataFrame({k: v.astype(int) for (k, v) in tm.getSeriesData().items()})
    df.A = df.A.astype('int32')
    df.B = np.ones(len(df.B), dtype='uint64')
    df.C = df.C.astype('uint8')
    df.D = df.C.astype('int64')
    return df","""""""Fixture for DataFrame of different int types with index of unique strings

Columns are ['A', 'B', 'C', 'D'].

            A  B    C    D
mUrCZ67juP  0  1    2    2
rw99ACYaKS  0  1    0    0
7QsEcpaaVU  0  1    1    1
xkrimI2pcE  0  1    0    0
dz01SuzoS8  0  1  255  255
ccQkqOHX75 -1  1    0    0
DN0iXaoDLd  0  1    0    0
...        .. ..  ...  ...
Dfb141wAaQ  1  1  254  254
IPD8eQOVu5  0  1    0    0
CcaKulsCmv  0  1    0    0
rIBa8gu7E5  0  1    0    0
RP6peZmh5o  0  1    1    1
NMb9pipQWQ  0  1    0    0
PqgbJEzjib  0  1    3    3

[30 rows x 4 columns]"""""""
pandas/tests/frame/conftest.py,"@pytest.fixture
def timezone_frame():
    df = DataFrame({'A': date_range('20130101', periods=3), 'B': date_range('20130101', periods=3, tz='US/Eastern'), 'C': date_range('20130101', periods=3, tz='CET')})
    df.iloc[1, 1] = NaT
    df.iloc[1, 2] = NaT
    return df","""""""Fixture for DataFrame of date_range Series with different time zones

Columns are ['A', 'B', 'C']; some entries are missing

           A                         B                         C
0 2013-01-01 2013-01-01 00:00:00-05:00 2013-01-01 00:00:00+01:00
1 2013-01-02                       NaT                       NaT
2 2013-01-03 2013-01-03 00:00:00-05:00 2013-01-03 00:00:00+01:00"""""""
pandas/tests/frame/conftest.py,"@pytest.fixture
def uint64_frame():
    return DataFrame({'A': np.arange(3), 'B': [2 ** 63, 2 ** 63 + 5, 2 ** 63 + 10]}, dtype=np.uint64)","""""""Fixture for DataFrame with uint64 values

Columns are ['A', 'B']"""""""
pandas/tests/frame/conftest.py,"@pytest.fixture
def simple_frame():
    arr = np.array([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0]])
    return DataFrame(arr, columns=['one', 'two', 'three'], index=['a', 'b', 'c'])","""""""Fixture for simple 3x3 DataFrame

Columns are ['one', 'two', 'three'], index is ['a', 'b', 'c'].

   one  two  three
a  1.0  2.0    3.0
b  4.0  5.0    6.0
c  7.0  8.0    9.0"""""""
pandas/tests/frame/conftest.py,"@pytest.fixture
def frame_of_index_cols():
    df = DataFrame({'A': ['foo', 'foo', 'foo', 'bar', 'bar'], 'B': ['one', 'two', 'three', 'one', 'two'], 'C': ['a', 'b', 'c', 'd', 'e'], 'D': np.random.default_rng(2).standard_normal(5), 'E': np.random.default_rng(2).standard_normal(5), ('tuple', 'as', 'label'): np.random.default_rng(2).standard_normal(5)})
    return df","""""""Fixture for DataFrame of columns that can be used for indexing

Columns are ['A', 'B', 'C', 'D', 'E', ('tuple', 'as', 'label')];
'A' & 'B' contain duplicates (but are jointly unique), the rest are unique.

     A      B  C         D         E  (tuple, as, label)
0  foo    one  a  0.608477 -0.012500           -1.664297
1  foo    two  b -0.633460  0.249614           -0.364411
2  foo  three  c  0.615256  2.154968           -0.834666
3  bar    one  d  0.234246  1.085675            0.718445
4  bar    two  e  0.533841 -0.005702           -3.533912"""""""
pandas/tests/frame/methods/test_asof.py,"@pytest.fixture
def date_range_frame():
    N = 50
    rng = date_range('1/1/1990', periods=N, freq='53s')
    return DataFrame({'A': np.arange(N), 'B': np.arange(N)}, index=rng)","""""""Fixture for DataFrame of ints with date_range index

Columns are ['A', 'B']."""""""
pandas/tests/frame/methods/test_astype.py,"def _check_cast(df, v):
    assert all((s.dtype.name == v for (_, s) in df.items()))","""""""Check if all dtypes of df are equal to v"""""""
pandas/tests/frame/methods/test_quantile.py,"@pytest.fixture(params=[['linear', 'single'], ['nearest', 'table']], ids=lambda x: '-'.join(x))
def interp_method(request):
    return request.param","""""""(interpolation, method) arguments for quantile"""""""
pandas/tests/frame/test_arithmetic.py,"def test_arithmetic_multiindex_align():
    df1 = DataFrame([[1]], index=['a'], columns=MultiIndex.from_product([[0], [1]], names=['a', 'b']))
    df2 = DataFrame([[1]], index=['a'], columns=Index([0], name='a'))
    expected = DataFrame([[0]], index=['a'], columns=MultiIndex.from_product([[0], [1]], names=['a', 'b']))
    result = df1 - df2
    tm.assert_frame_equal(result, expected)","""""""Regression test for: https://github.com/pandas-dev/pandas/issues/33765"""""""
pandas/tests/frame/test_reductions.py,"def assert_stat_op_calc(opname, alternative, frame, has_skipna=True, check_dtype=True, check_dates=False, rtol=1e-05, atol=1e-08, skipna_alternative=None):
    f = getattr(frame, opname)
    if check_dates:
        df = DataFrame({'b': date_range('1/1/2001', periods=2)})
        with tm.assert_produces_warning(None):
            result = getattr(df, opname)()
        assert isinstance(result, Series)
        df['a'] = range(len(df))
        with tm.assert_produces_warning(None):
            result = getattr(df, opname)()
        assert isinstance(result, Series)
        assert len(result)
    if has_skipna:

        def wrapper(x):
            return alternative(x.values)
        skipna_wrapper = tm._make_skipna_wrapper(alternative, skipna_alternative)
        result0 = f(axis=0, skipna=False)
        result1 = f(axis=1, skipna=False)
        tm.assert_series_equal(result0, frame.apply(wrapper), check_dtype=check_dtype, rtol=rtol, atol=atol)
        tm.assert_series_equal(result1, frame.apply(wrapper, axis=1), rtol=rtol, atol=atol)
    else:
        skipna_wrapper = alternative
    result0 = f(axis=0)
    result1 = f(axis=1)
    tm.assert_series_equal(result0, frame.apply(skipna_wrapper), check_dtype=check_dtype, rtol=rtol, atol=atol)
    if opname in ['sum', 'prod']:
        expected = frame.apply(skipna_wrapper, axis=1)
        tm.assert_series_equal(result1, expected, check_dtype=False, rtol=rtol, atol=atol)
    if check_dtype:
        lcd_dtype = frame.values.dtype
        assert lcd_dtype == result0.dtype
        assert lcd_dtype == result1.dtype
    with pytest.raises(ValueError, match='No axis named 2'):
        f(axis=2)
    if has_skipna:
        all_na = frame * np.nan
        r0 = getattr(all_na, opname)(axis=0)
        r1 = getattr(all_na, opname)(axis=1)
        if opname in ['sum', 'prod']:
            unit = 1 if opname == 'prod' else 0
            expected = Series(unit, index=r0.index, dtype=r0.dtype)
            tm.assert_series_equal(r0, expected)
            expected = Series(unit, index=r1.index, dtype=r1.dtype)
            tm.assert_series_equal(r1, expected)","""""""Check that operator opname works as advertised on frame

Parameters
----------
opname : str
    Name of the operator to test on frame
alternative : function
    Function that opname is tested against; i.e. ""frame.opname()"" should
    equal ""alternative(frame)"".
frame : DataFrame
    The object that the tests are executed on
has_skipna : bool, default True
    Whether the method ""opname"" has the kwarg ""skip_na""
check_dtype : bool, default True
    Whether the dtypes of the result of ""frame.opname()"" and
    ""alternative(frame)"" should be checked.
check_dates : bool, default false
    Whether opname should be tested on a Datetime Series
rtol : float, default 1e-5
    Relative tolerance.
atol : float, default 1e-8
    Absolute tolerance.
skipna_alternative : function, default None
    NaN-safe version of alternative"""""""
pandas/tests/generic/test_finalize.py,"@pytest.fixture(params=_all_methods, ids=lambda x: idfn(x[-1]))
def ndframe_method(request):
    return request.param","""""""An NDFrame method returning an NDFrame."""""""
pandas/tests/generic/test_generic.py,"def construct(box, shape, value=None, dtype=None, **kwargs):
    if isinstance(shape, int):
        shape = tuple([shape] * box._AXIS_LEN)
    if value is not None:
        if is_scalar(value):
            if value == 'empty':
                arr = None
                dtype = np.float64
                kwargs.pop(box._info_axis_name, None)
            else:
                arr = np.empty(shape, dtype=dtype)
                arr.fill(value)
        else:
            fshape = np.prod(shape)
            arr = value.ravel()
            new_shape = fshape / arr.shape[0]
            if fshape % arr.shape[0] != 0:
                raise Exception('invalid value passed in construct')
            arr = np.repeat(arr, new_shape).reshape(shape)
    else:
        arr = np.random.default_rng(2).standard_normal(shape)
    return box(arr, dtype=dtype, **kwargs)","""""""construct an object for the given shape
if value is specified use that if its a scalar
if value is an array, repeat it as needed"""""""
pandas/tests/generic/test_label_or_level_utils.py,"@pytest.fixture
def df():
    return pd.DataFrame({'L1': [1, 2, 3], 'L2': [11, 12, 13], 'L3': ['A', 'B', 'C']})","""""""DataFrame with columns 'L1', 'L2', and 'L3'"""""""
pandas/tests/generic/test_label_or_level_utils.py,"@pytest.fixture(params=[[], ['L1'], ['L1', 'L2'], ['L1', 'L2', 'L3']])
def df_levels(request, df):
    levels = request.param
    if levels:
        df = df.set_index(levels)
    return df","""""""DataFrame with columns or index levels 'L1', 'L2', and 'L3'"""""""
pandas/tests/generic/test_label_or_level_utils.py,"@pytest.fixture
def df_ambig(df):
    df = df.set_index(['L1', 'L2'])
    df['L1'] = df['L3']
    return df","""""""DataFrame with levels 'L1' and 'L2' and labels 'L1' and 'L3'"""""""
pandas/tests/generic/test_label_or_level_utils.py,"@pytest.fixture
def df_duplabels(df):
    df = df.set_index(['L1'])
    df = pd.concat([df, df['L2']], axis=1)
    return df","""""""DataFrame with level 'L1' and labels 'L2', 'L3', and 'L2'"""""""
pandas/tests/groupby/__init__.py,"def get_groupby_method_args(name, obj):
    if name in ('nth', 'fillna', 'take'):
        return (0,)
    if name == 'quantile':
        return (0.5,)
    if name == 'corrwith':
        return (obj,)
    return ()","""""""Get required arguments for a groupby method.

When parametrizing a test over groupby methods (e.g. ""sum"", ""mean"", ""fillna""),
it is often the case that arguments are required for certain methods.

Parameters
----------
name: str
    Name of the method.
obj: Series or DataFrame
    pandas object that is being grouped.

Returns
-------
A tuple of required arguments for the method."""""""
pandas/tests/groupby/conftest.py,"@pytest.fixture(params=sorted(reduction_kernels))
def reduction_func(request):
    return request.param","""""""yields the string names of all groupby reduction functions, one at a time."""""""
pandas/tests/groupby/conftest.py,"@pytest.fixture(params=sorted(transformation_kernels))
def transformation_func(request):
    return request.param","""""""yields the string names of all groupby transformation functions."""""""
pandas/tests/groupby/conftest.py,"@pytest.fixture(params=sorted(reduction_kernels) + sorted(transformation_kernels))
def groupby_func(request):
    return request.param","""""""yields both aggregation and transformation functions."""""""
pandas/tests/groupby/conftest.py,"@pytest.fixture(params=[True, False])
def parallel(request):
    return request.param","""""""parallel keyword argument for numba.jit"""""""
pandas/tests/groupby/conftest.py,"@pytest.fixture(params=[False])
def nogil(request):
    return request.param","""""""nogil keyword argument for numba.jit"""""""
pandas/tests/groupby/conftest.py,"@pytest.fixture(params=[True])
def nopython(request):
    return request.param","""""""nopython keyword argument for numba.jit"""""""
pandas/tests/groupby/conftest.py,"@pytest.fixture(params=[('mean', {}), ('var', {'ddof': 1}), ('var', {'ddof': 0}), ('std', {'ddof': 1}), ('std', {'ddof': 0}), ('sum', {}), ('min', {}), ('max', {}), ('sum', {'min_count': 2}), ('min', {'min_count': 2}), ('max', {'min_count': 2})], ids=['mean', 'var_1', 'var_0', 'std_1', 'std_0', 'sum', 'min', 'max', 'sum-min_count', 'min-min_count', 'max-min_count'])
def numba_supported_reductions(request):
    return request.param","""""""reductions supported with engine='numba'"""""""
pandas/tests/groupby/test_categorical.py,"def cartesian_product_for_groupers(result, args, names, fill_value=np.nan):

    def f(a):
        if isinstance(a, (CategoricalIndex, Categorical)):
            categories = a.categories
            a = Categorical.from_codes(np.arange(len(categories)), categories=categories, ordered=a.ordered)
        return a
    index = MultiIndex.from_product(map(f, args), names=names)
    return result.reindex(index, fill_value=fill_value).sort_index()","""""""Reindex to a cartesian production for the groupers,
preserving the nature (Categorical) of each grouper"""""""
pandas/tests/groupby/test_categorical.py,"@pytest.fixture
def df_cat(df):
    df_cat = df.copy()[:4]
    df_cat['A'] = df_cat['A'].astype('category')
    df_cat['B'] = df_cat['B'].astype('category')
    df_cat['C'] = Series([1, 2, 3, 4])
    df_cat = df_cat.drop(['D'], axis=1)
    return df_cat","""""""DataFrame with multiple categorical columns and a column of integers.
Shortened so as not to contain all possible combinations of categories.
Useful for testing `observed` kwarg functionality on GroupBy objects.

Parameters
----------
df: DataFrame
    Non-categorical, longer DataFrame from another fixture, used to derive
    this one

Returns
-------
df_cat: DataFrame"""""""
pandas/tests/groupby/test_function.py,"@pytest.fixture(params=[np.int32, np.int64, np.float32, np.float64, 'Int64', 'Float64'], ids=['np.int32', 'np.int64', 'np.float32', 'np.float64', 'Int64', 'Float64'])
def dtypes_for_minmax(request):
    dtype = request.param
    np_type = dtype
    if dtype == 'Int64':
        np_type = np.int64
    elif dtype == 'Float64':
        np_type = np.float64
    min_val = np.iinfo(np_type).min if np.dtype(np_type).kind == 'i' else np.finfo(np_type).min
    max_val = np.iinfo(np_type).max if np.dtype(np_type).kind == 'i' else np.finfo(np_type).max
    return (dtype, min_val, max_val)","""""""Fixture of dtypes with min and max values used for testing
cummin and cummax"""""""
pandas/tests/groupby/test_libgroupby.py,"def _check_cython_group_transform_cumulative(pd_op, np_op, dtype):
    is_datetimelike = False
    data = np.array([[1], [2], [3], [4]], dtype=dtype)
    answer = np.zeros_like(data)
    labels = np.array([0, 0, 0, 0], dtype=np.intp)
    ngroups = 1
    pd_op(answer, data, labels, ngroups, is_datetimelike)
    tm.assert_numpy_array_equal(np_op(data), answer[:, 0], check_dtype=False)","""""""Check a group transform that executes a cumulative function.

Parameters
----------
pd_op : callable
    The pandas cumulative function.
np_op : callable
    The analogous one in NumPy.
dtype : type
    The specified dtype of the data."""""""
pandas/tests/groupby/test_timegrouper.py,"@pytest.fixture
def frame_for_truncated_bingrouper():
    df = DataFrame({'Quantity': [18, 3, 5, 1, 9, 3], 'Date': [Timestamp(2013, 9, 1, 13, 0), Timestamp(2013, 9, 1, 13, 5), Timestamp(2013, 10, 1, 20, 0), Timestamp(2013, 10, 3, 10, 0), pd.NaT, Timestamp(2013, 9, 2, 14, 0)]})
    return df","""""""DataFrame used by groupby_with_truncated_bingrouper, made into
a separate fixture for easier re-use in
test_groupby_apply_timegrouper_with_nat_apply_squeeze"""""""
pandas/tests/groupby/test_timegrouper.py,"@pytest.fixture
def groupby_with_truncated_bingrouper(frame_for_truncated_bingrouper):
    df = frame_for_truncated_bingrouper
    tdg = Grouper(key='Date', freq='5D')
    gb = df.groupby(tdg)
    assert len(gb.grouper.result_index) != len(gb.grouper.group_keys_seq)
    return gb","""""""GroupBy object such that gb.grouper is a BinGrouper and
len(gb.grouper.result_index) < len(gb.grouper.group_keys_seq)

Aggregations on this groupby should have

    dti = date_range(""2013-09-01"", ""2013-10-01"", freq=""5D"", name=""Date"")

As either the index or an index level."""""""
pandas/tests/indexes/conftest.py,"@pytest.fixture(params=[None, False])
def sort(request):
    return request.param","""""""Valid values for the 'sort' parameter used in the Index
setops methods (intersection, union, etc.)

Caution:
    Don't confuse this one with the ""sort"" fixture used
    for DataFrame.append or concat. That one has
    parameters [True, False].

    We can't combine them as sort=True is not permitted
    in the Index setops methods."""""""
pandas/tests/indexes/conftest.py,"@pytest.fixture(params=['D', '3D', '-3D', 'H', '2H', '-2H', 'min', '2min', 's', '-3s'])
def freq_sample(request):
    return request.param","""""""Valid values for 'freq' parameter used to create date_range and
timedelta_range.."""""""
pandas/tests/indexes/conftest.py,"@pytest.fixture(params=[list, tuple, np.array, array, Series])
def listlike_box(request):
    return request.param","""""""Types that may be passed as the indexer to searchsorted."""""""
pandas/tests/indexes/conftest.py,"@pytest.fixture(params=tm.ALL_REAL_NUMPY_DTYPES + ['object', 'category', 'datetime64[ns]', 'timedelta64[ns]'])
def any_dtype_for_small_pos_integer_indexes(request):
    return request.param","""""""Dtypes that can be given to an Index with small positive integers.

This means that for any dtype `x` in the params list, `Index([1, 2, 3], dtype=x)` is
valid and gives the correct Index (sub-)class."""""""
pandas/tests/indexes/datetimelike_/test_sort_values.py,"def check_freq_ascending(ordered, orig, ascending):
    if isinstance(ordered, PeriodIndex):
        assert ordered.freq == orig.freq
    elif isinstance(ordered, (DatetimeIndex, TimedeltaIndex)):
        if ascending:
            assert ordered.freq.n == orig.freq.n
        else:
            assert ordered.freq.n == -1 * orig.freq.n","""""""Check the expected freq on a PeriodIndex/DatetimeIndex/TimedeltaIndex
when the original index is generated (or generate-able) with
period_range/date_range/timedelta_range."""""""
pandas/tests/indexes/datetimelike_/test_sort_values.py,"def check_freq_nonmonotonic(ordered, orig):
    if isinstance(ordered, PeriodIndex):
        assert ordered.freq == orig.freq
    elif isinstance(ordered, (DatetimeIndex, TimedeltaIndex)):
        assert ordered.freq is None","""""""Check the expected freq on a PeriodIndex/DatetimeIndex/TimedeltaIndex
when the original index is _not_ generated (or generate-able) with
period_range/date_range//timedelta_range."""""""
pandas/tests/indexes/datetimes/test_date_range.py,"def _get_expected_range(begin_to_match, end_to_match, both_range, inclusive_endpoints):
    left_match = begin_to_match == both_range[0]
    right_match = end_to_match == both_range[-1]
    if inclusive_endpoints == 'left' and right_match:
        expected_range = both_range[:-1]
    elif inclusive_endpoints == 'right' and left_match:
        expected_range = both_range[1:]
    elif inclusive_endpoints == 'neither' and left_match and right_match:
        expected_range = both_range[1:-1]
    elif inclusive_endpoints == 'neither' and right_match:
        expected_range = both_range[:-1]
    elif inclusive_endpoints == 'neither' and left_match:
        expected_range = both_range[1:]
    elif inclusive_endpoints == 'both':
        expected_range = both_range[:]
    else:
        expected_range = both_range[:]
    return expected_range","""""""Helper to get expected range from a both inclusive range"""""""
pandas/tests/indexes/interval/test_interval_tree.py,"def skipif_32bit(param):
    marks = pytest.mark.skipif(not IS64, reason='GH 23440: int type mismatch on 32bit')
    return pytest.param(param, marks=marks)","""""""Skip parameters in a parametrize on 32bit systems. Specifically used
here to skip leaf_size parameters related to GH 23440."""""""
pandas/tests/indexes/interval/test_interval_tree.py,"@pytest.fixture(params=[skipif_32bit(1), skipif_32bit(2), 10])
def leaf_size(request):
    return request.param","""""""Fixture to specify IntervalTree leaf_size parameter; to be used with the
tree fixture."""""""
pandas/tests/indexes/multi/conftest.py,"@pytest.fixture
def narrow_multi_index():
    n = 1000
    ci = pd.CategoricalIndex(list('a' * n) + ['abc'] * n)
    dti = pd.date_range('2000-01-01', freq='s', periods=n * 2)
    return MultiIndex.from_arrays([ci, ci.codes + 9, dti], names=['a', 'b', 'dti'])","""""""Return a MultiIndex that is narrower than the display (<80 characters)."""""""
pandas/tests/indexes/multi/conftest.py,"@pytest.fixture
def wide_multi_index():
    n = 1000
    ci = pd.CategoricalIndex(list('a' * n) + ['abc'] * n)
    dti = pd.date_range('2000-01-01', freq='s', periods=n * 2)
    levels = [ci, ci.codes + 9, dti, dti, dti]
    names = ['a', 'b', 'dti_1', 'dti_2', 'dti_3']
    return MultiIndex.from_arrays(levels, names=names)","""""""Return a MultiIndex that is wider than the display (>80 characters)."""""""
pandas/tests/indexes/ranges/test_setops.py,"def assert_range_or_not_is_rangelike(index):
    if not isinstance(index, RangeIndex) and len(index) > 0:
        diff = index[:-1] - index[1:]
        assert not (diff == diff[0]).all()","""""""Check that we either have a RangeIndex or that this index *cannot*
be represented as a RangeIndex."""""""
pandas/tests/indexing/multiindex/test_getitem.py,"@pytest.fixture
def dataframe_with_duplicate_index():
    data = [['a', 'd', 'e', 'c', 'f', 'b'], [1, 4, 5, 3, 6, 2], [1, 4, 5, 3, 6, 2]]
    index = ['h1', 'h3', 'h5']
    columns = MultiIndex(levels=[['A', 'B'], ['A1', 'A2', 'B1', 'B2']], codes=[[0, 0, 0, 1, 1, 1], [0, 3, 3, 0, 1, 2]], names=['main', 'sub'])
    return DataFrame(data, index=index, columns=columns)","""""""Fixture for DataFrame used in tests for gh-4145 and gh-4146"""""""
pandas/tests/indexing/multiindex/test_iloc.py,"@pytest.fixture
def simple_multiindex_dataframe():
    data = np.random.default_rng(2).standard_normal((3, 3))
    return DataFrame(data, columns=[[2, 2, 4], [6, 8, 10]], index=[[4, 4, 8], [8, 10, 12]])","""""""Factory function to create simple 3 x 3 dataframe with
both columns and row MultiIndex using supplied data or
random data by default."""""""
pandas/tests/indexing/multiindex/test_loc.py,"@pytest.fixture
def single_level_multiindex():
    return MultiIndex(levels=[['foo', 'bar', 'baz', 'qux']], codes=[[0, 1, 2, 3]], names=['first'])","""""""single level MultiIndex"""""""
pandas/tests/indexing/test_scalar.py,"def generate_indices(f, values=False):
    axes = f.axes
    if values:
        axes = (list(range(len(ax))) for ax in axes)
    return itertools.product(*axes)","""""""generate the indices
if values is True , use the axis values
is False, use the range"""""""
pandas/tests/interchange/test_utils.py,"@pytest.mark.parametrize('pandas_dtype, c_string', [(np.dtype('bool'), 'b'), (np.dtype('int8'), 'c'), (np.dtype('uint8'), 'C'), (np.dtype('int16'), 's'), (np.dtype('uint16'), 'S'), (np.dtype('int32'), 'i'), (np.dtype('uint32'), 'I'), (np.dtype('int64'), 'l'), (np.dtype('uint64'), 'L'), (np.dtype('float16'), 'e'), (np.dtype('float32'), 'f'), (np.dtype('float64'), 'g'), (pd.Series(['a']).dtype, 'u'), (pd.Series([0]).astype('datetime64[ns]').dtype, 'tsn:'), (pd.CategoricalDtype(['a']), 'l'), (np.dtype('O'), 'u')])
def test_dtype_to_arrow_c_fmt(pandas_dtype, c_string):
    assert dtype_to_arrow_c_fmt(pandas_dtype) == c_string","""""""Test ``dtype_to_arrow_c_fmt`` utility function."""""""
pandas/tests/internals/test_internals.py,"@pytest.fixture(params=[new_block, make_block])
def block_maker(request):
    return request.param","""""""Fixture to test both the internal new_block and pseudo-public make_block."""""""
pandas/tests/internals/test_internals.py,"def create_block(typestr, placement, item_shape=None, num_offset=0, maker=new_block):
    placement = BlockPlacement(placement)
    num_items = len(placement)
    if item_shape is None:
        item_shape = (N,)
    shape = (num_items,) + item_shape
    mat = get_numeric_mat(shape)
    if typestr in ('float', 'f8', 'f4', 'f2', 'int', 'i8', 'i4', 'i2', 'i1', 'uint', 'u8', 'u4', 'u2', 'u1'):
        values = mat.astype(typestr) + num_offset
    elif typestr in ('complex', 'c16', 'c8'):
        values = 1j * (mat.astype(typestr) + num_offset)
    elif typestr in ('object', 'string', 'O'):
        values = np.reshape([f'A{i:d}' for i in mat.ravel() + num_offset], shape)
    elif typestr in ('b', 'bool'):
        values = np.ones(shape, dtype=np.bool_)
    elif typestr in ('datetime', 'dt', 'M8[ns]'):
        values = (mat * 1000000000.0).astype('M8[ns]')
    elif typestr.startswith('M8[ns'):
        m = re.search('M8\\[ns,\\s*(\\w+\\/?\\w*)\\]', typestr)
        assert m is not None, f'incompatible typestr -> {typestr}'
        tz = m.groups()[0]
        assert num_items == 1, 'must have only 1 num items for a tz-aware'
        values = DatetimeIndex(np.arange(N) * 10 ** 9, tz=tz)._data
        values = ensure_block_shape(values, ndim=len(shape))
    elif typestr in ('timedelta', 'td', 'm8[ns]'):
        values = (mat * 1).astype('m8[ns]')
    elif typestr in ('category',):
        values = Categorical([1, 1, 2, 2, 3, 3, 3, 3, 4, 4])
    elif typestr in ('category2',):
        values = Categorical(['a', 'a', 'a', 'a', 'b', 'b', 'c', 'c', 'c', 'd'])
    elif typestr in ('sparse', 'sparse_na'):
        if shape[-1] != 10:
            raise NotImplementedError
        assert all((s == 1 for s in shape[:-1]))
        if typestr.endswith('_na'):
            fill_value = np.nan
        else:
            fill_value = 0.0
        values = SparseArray([fill_value, fill_value, 1, 2, 3, fill_value, 4, 5, fill_value, 6], fill_value=fill_value)
        arr = values.sp_values.view()
        arr += num_offset - 1
    else:
        raise ValueError(f'Unsupported typestr: ""{typestr}""')
    values = maybe_coerce_values(values)
    return maker(values, placement=placement, ndim=len(shape))","""""""Supported typestr:

    * float, f8, f4, f2
    * int, i8, i4, i2, i1
    * uint, u8, u4, u2, u1
    * complex, c16, c8
    * bool
    * object, string, O
    * datetime, dt, M8[ns], M8[ns, tz]
    * timedelta, td, m8[ns]
    * sparse (SparseArray with fill_value=0.0)
    * sparse_na (SparseArray with fill_value=np.nan)
    * category, category2"""""""
pandas/tests/internals/test_internals.py,"def create_mgr(descr, item_shape=None):
    if item_shape is None:
        item_shape = (N,)
    offset = 0
    mgr_items = []
    block_placements = {}
    for d in descr.split(';'):
        d = d.strip()
        if not len(d):
            continue
        (names, blockstr) = d.partition(':')[::2]
        blockstr = blockstr.strip()
        names = names.strip().split(',')
        mgr_items.extend(names)
        placement = list(np.arange(len(names)) + offset)
        try:
            block_placements[blockstr].extend(placement)
        except KeyError:
            block_placements[blockstr] = placement
        offset += len(names)
    mgr_items = Index(mgr_items)
    blocks = []
    num_offset = 0
    for (blockstr, placement) in block_placements.items():
        typestr = blockstr.split('-')[0]
        blocks.append(create_block(typestr, placement, item_shape=item_shape, num_offset=num_offset))
        num_offset += len(placement)
    sblocks = sorted(blocks, key=lambda b: b.mgr_locs[0])
    return BlockManager(tuple(sblocks), [mgr_items] + [Index(np.arange(n)) for n in item_shape])","""""""Construct BlockManager from string description.

String description syntax looks similar to np.matrix initializer.  It looks
like this::

    a,b,c: f8; d,e,f: i8

Rules are rather simple:

* see list of supported datatypes in `create_block` method
* components are semicolon-separated
* each component is `NAME,NAME,NAME: DTYPE_ID`
* whitespace around colons & semicolons are removed
* components with same DTYPE_ID are combined into single block
* to force multiple blocks with same dtype, use '-SUFFIX'::

    'a:f8-1; b:f8-2; c:f8-foobar'"""""""
pandas/tests/io/conftest.py,"@pytest.fixture
def tips_file(datapath):
    return datapath('io', 'data', 'csv', 'tips.csv')","""""""Path to the tips dataset"""""""
pandas/tests/io/conftest.py,"@pytest.fixture
def jsonl_file(datapath):
    return datapath('io', 'parser', 'data', 'items.jsonl')","""""""Path to a JSONL dataset"""""""
pandas/tests/io/conftest.py,"@pytest.fixture
def salaries_table(datapath):
    return read_csv(datapath('io', 'parser', 'data', 'salaries.csv'), sep='\t')","""""""DataFrame with the salaries dataset"""""""
pandas/tests/io/conftest.py,"@pytest.fixture(scope='function' if is_ci_environment() else 'session')
def s3_base(worker_id, monkeysession):
    pytest.importorskip('s3fs')
    pytest.importorskip('boto3')
    monkeysession.setenv('AWS_ACCESS_KEY_ID', 'foobar_key')
    monkeysession.setenv('AWS_SECRET_ACCESS_KEY', 'foobar_secret')
    if is_ci_environment():
        if is_platform_arm() or is_platform_mac() or is_platform_windows():
            pytest.skip('S3 tests do not have a corresponding service in Windows, macOS or ARM platforms')
        else:
            yield 'http://localhost:5000'
    else:
        requests = pytest.importorskip('requests')
        pytest.importorskip('moto', minversion='1.3.14')
        pytest.importorskip('flask')
        worker_id = '5' if worker_id == 'master' else worker_id.lstrip('gw')
        endpoint_port = f'555{worker_id}'
        endpoint_uri = f'http://127.0.0.1:{endpoint_port}/'
        with subprocess.Popen(shlex.split(f'moto_server s3 -p {endpoint_port}'), stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL) as proc:
            timeout = 5
            while timeout > 0:
                try:
                    r = requests.get(endpoint_uri)
                    if r.ok:
                        break
                except Exception:
                    pass
                timeout -= 0.1
                time.sleep(0.1)
            yield endpoint_uri
            proc.terminate()","""""""Fixture for mocking S3 interaction.

Sets up moto server in separate process locally
Return url for motoserver/moto CI service"""""""
pandas/tests/io/conftest.py,"@pytest.fixture
def s3_public_bucket_with_data(s3_public_bucket, tips_file, jsonl_file, feather_file, xml_file):
    test_s3_files = [('tips#1.csv', tips_file), ('tips.csv', tips_file), ('tips.csv.gz', tips_file + '.gz'), ('tips.csv.bz2', tips_file + '.bz2'), ('items.jsonl', jsonl_file), ('simple_dataset.feather', feather_file), ('books.xml', xml_file)]
    for (s3_key, file_name) in test_s3_files:
        with open(file_name, 'rb') as f:
            s3_public_bucket.put_object(Key=s3_key, Body=f)
    return s3_public_bucket","""""""The following datasets
are loaded.

- tips.csv
- tips.csv.gz
- tips.csv.bz2
- items.jsonl"""""""
pandas/tests/io/conftest.py,"@pytest.fixture
def s3_private_bucket_with_data(s3_private_bucket, tips_file, jsonl_file, feather_file, xml_file):
    test_s3_files = [('tips#1.csv', tips_file), ('tips.csv', tips_file), ('tips.csv.gz', tips_file + '.gz'), ('tips.csv.bz2', tips_file + '.bz2'), ('items.jsonl', jsonl_file), ('simple_dataset.feather', feather_file), ('books.xml', xml_file)]
    for (s3_key, file_name) in test_s3_files:
        with open(file_name, 'rb') as f:
            s3_private_bucket.put_object(Key=s3_key, Body=f)
    return s3_private_bucket","""""""The following datasets
are loaded.

- tips.csv
- tips.csv.gz
- tips.csv.bz2
- items.jsonl"""""""
pandas/tests/io/conftest.py,"@pytest.fixture(params=['python', pytest.param('pyarrow', marks=td.skip_if_no('pyarrow'))])
def string_storage(request):
    return request.param","""""""Parametrized fixture for pd.options.mode.string_storage.

* 'python'
* 'pyarrow'"""""""
pandas/tests/io/excel/conftest.py,"@pytest.fixture
def frame(float_frame):
    return float_frame[:10]","""""""Returns the first ten items in fixture ""float_frame""."""""""
pandas/tests/io/excel/conftest.py,"@pytest.fixture
def df_ref(datapath):
    filepath = datapath('io', 'data', 'csv', 'test1.csv')
    df_ref = read_csv(filepath, index_col=0, parse_dates=True, engine='python')
    return df_ref","""""""Obtain the reference data from read_csv with the Python engine."""""""
pandas/tests/io/excel/conftest.py,"@pytest.fixture(params=['.xls', '.xlsx', '.xlsm', '.ods', '.xlsb'])
def read_ext(request):
    return request.param","""""""Valid extensions for reading Excel files."""""""
pandas/tests/io/excel/test_readers.py,"def _is_valid_engine_ext_pair(engine, read_ext: str) -> bool:
    engine = engine.values[0]
    if engine == 'openpyxl' and read_ext == '.xls':
        return False
    if engine == 'odf' and read_ext != '.ods':
        return False
    if read_ext == '.ods' and engine not in {'odf', 'calamine'}:
        return False
    if engine == 'pyxlsb' and read_ext != '.xlsb':
        return False
    if read_ext == '.xlsb' and engine not in {'pyxlsb', 'calamine'}:
        return False
    if engine == 'xlrd' and read_ext != '.xls':
        return False
    return True","""""""Filter out invalid (engine, ext) pairs instead of skipping, as that
produces 500+ pytest.skips."""""""
pandas/tests/io/excel/test_readers.py,"def _transfer_marks(engine, read_ext):
    values = engine.values + (read_ext,)
    new_param = pytest.param(values, marks=engine.marks)
    return new_param","""""""engine gives us a pytest.param object with some marks, read_ext is just
a string.  We need to generate a new pytest.param inheriting the marks."""""""
pandas/tests/io/excel/test_readers.py,"@pytest.fixture(params=[_transfer_marks(eng, ext) for eng in engine_params for ext in read_ext_params if _is_valid_engine_ext_pair(eng, ext)], ids=str)
def engine_and_read_ext(request):
    return request.param","""""""Fixture for Excel reader engine and read_ext, only including valid pairs."""""""
pandas/tests/io/excel/test_writers.py,"@pytest.fixture
def path(ext):
    with tm.ensure_clean(ext) as file_path:
        yield file_path","""""""Fixture to open file for use in each test case."""""""
pandas/tests/io/excel/test_writers.py,"@pytest.fixture
def set_engine(engine, ext):
    option_name = f""io.excel.{ext.strip('.')}.writer""
    with option_context(option_name, engine):
        yield","""""""Fixture to set engine for use in each test case.

Rather than requiring `engine=...` to be provided explicitly as an
argument in each test, this fixture sets a global option to dictate
which engine should be used to write Excel files. After executing
the test it rolls back said change to the global option."""""""
pandas/tests/io/excel/test_xlrd.py,"@pytest.fixture(params=['.xls'])
def read_ext_xlrd(request):
    return request.param","""""""Valid extensions for reading Excel files with xlrd.

Similar to read_ext, but excludes .ods, .xlsb, and for xlrd>2 .xlsx, .xlsm"""""""
pandas/tests/io/formats/style/test_bar.py,"def bar_grad(a=None, b=None, c=None, d=None):
    ret = [('width', '10em')]
    if all((x is None for x in [a, b, c, d])):
        return ret
    return ret + [('background', f""linear-gradient(90deg,{','.join([x for x in [a, b, c, d] if x])})"")]","""""""Used in multiple tests to simplify formatting of expected result"""""""
